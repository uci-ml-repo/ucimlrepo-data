{
    "ID": 305,
    "Name": "REALDISP Activity Recognition Dataset",
    "Abstract": "The REALDISP dataset is devised to evaluate techniques dealing with the effects of sensor displacement in wearable activity recognition as well as to benchmark general activity recognition algorithms ",
    "Types": "Multivariate, Time-Series",
    "Task": "Classification",
    "AttributeTypes": "Real",
    "NumInstances": 1419,
    "NumAttributes": 120,
    "DateDonated": "2014-07-25",
    "MissingValues": 0,
    "URLFolder": "../machine-learning-databases/00305/",
    "URLReadme": "#",
    "HighestAccuracy": 0,
    "RelevantInfo": "The REALDISP (REAListic sensor DISPlacement) dataset has been originally collected to investigate the effects of sensor displacement in the activity recognition process in real-world settings. It builds on the concept of ideal-placement, self-placement and induced-displacement. The ideal and mutual-displacement conditions represent extreme displacement variants and thus could represent boundary conditions for recognition algorithms. In contrast, self-placement reflects a users perception of how sensors could be attached, e.g., in a sports or lifestyle application. The dataset includes a wide range of physical activities (warm up, cool down and fitness exercises), sensor modalities (acceleration, rate of turn, magnetic field and quaternions) and participants (17 subjects). Apart from investigating sensor displacement, the dataset lend itself for benchmarking activity recognition techniques in ideal conditions.\r\n\r\n----------------------------------------------------------------------------------------------------------------------\r\nDataset summary:\r\n#Activities: 33 \r\n#Sensors: 9\r\n#Subjects: 17\r\n#Scenarios: 3\r\n----------------------------------------------------------------------------------------------------------------------\r\n\r\nACTIVITY SET:\r\nA1: Walking  \r\nA2: Jogging  \r\nA3: Running  \r\nA4: Jump up  \r\nA5: Jump front & back  \r\nA6: Jump sideways  \r\nA7: Jump leg/arms open/closed  \r\nA8: Jump rope  \r\nA9: Trunk twist (arms outstretched)  \r\nA10: Trunk twist (elbows bent)  \r\nA11: Waist bends forward  \r\nA12: Waist rotation  \r\nA13: Waist bends (reach foot with opposite hand)  \r\nA14: Reach heels backwards  \r\nA15: Lateral bend (10_ to the left + 10_ to the right)\r\nA16: Lateral bend with arm up (10_ to the left + 10_ to the right) \r\nA17: Repetitive forward stretching \r\nA18: Upper trunk and lower body opposite twist \r\nA19: Lateral elevation of arms \r\nA20: Frontal elevation of arms \r\nA21: Frontal hand claps \r\nA22: Frontal crossing of arms \r\nA23: Shoulders high-amplitude rotation \r\nA24: Shoulders low-amplitude rotation \r\nA25: Arms inner rotation \r\nA26: Knees (alternating) to the breast \r\nA27: Heels (alternating) to the backside \r\nA28: Knees bending (crouching) \r\nA29: Knees (alternating) bending forward \r\nA30: Rotation on the knees \r\nA31: Rowing \r\nA32: Elliptical bike \r\nA33: Cycling \r\n\r\nSENSOR SETUP:\r\nEach sensor provides 3D acceleration (accX,accY,accZ), 3D gyro (gyrX,gyrY,gyrZ), 3D magnetic field orientation (magX,magY,magZ) and 4D quaternions (Q1,Q2,Q3,Q4). The sensors are identified according to the body part on which is placed respectively:\r\n\r\nS1: left calf (LC)\r\nS2: left thigh (LT)\r\nS3: right calf (RC)\r\nS4: right thigh (RT)\r\nS5: back (BACK)\r\nS6: left lower arm (LLA)\r\nS7: left upper arm (LUA)\r\nS8: right lower arm (RLA)\r\nS9: right upper arm (RUA)\r\n\r\nSCENARIOS:\r\nThe dataset contains information for three different scenarios depending on whether the sensors are positioned on predefined positions or placed by the users themselves.\r\n- Ideal-placement or the default scenario. The sensors are positioned by the instructor on predefined locations within each body part. The data stemming from this scenario could be considered as the \u00e2\u20ac\u0153training set\u00e2\u20ac\u009d for supervised activity recognition systems.\r\n- Self-placement. The user is asked to position a subset of the sensors themselves on the body parts specified by the instructor, but without providing any hint on how the sensors must be exactly placed. This scenario is devised to investigate some of the variability that may occur in the day-to-day usage of an activity recognition system, involving wearable or self-attached sensors. Normally, the self-placement will lead to on-body sensor setups that differ from the ideal-placement. Nevertheless, this difference may be minimal if the subject places the sensor close to the ideal position.\r\n- Induced-displacement. An intentional mispositioning of sensors using rotations and translations with respect to the ideal placement is introduced by the instructor. One of the key interests of including this last scenario is to investigate how the performance of a certain method degrades as the system drifts far from the ideal setup.\r\n\r\nA complete and illustrated description (including table of activities, sensor setup, etc.) of the dataset is provided in the documentation facilitated along with the dataset. Also, the papers presented in the section \u00e2\u20ac\u0153Citation Requests\u00e2\u20ac\u009d provide an insightful description of the dataset and the underlying theory.  \r\n",
    "Source": "Oresti Banos, Department of Computer Architecture and Computer Technology, University of Granada, oresti@ugr.es (oresti.bl@gmail.com)\r\nMate Attila Toth, Signal Processing Systems, TU Eindhoven, A.M.Toth@tue.nl \r\nOliver Amft, Signal Processing Systems, TU Eindhoven, amft@tue.nl \r\n",
    "Acknowledgements": "Use of this dataset in publications must be acknowledged by referencing the following publications:\r\n\r\nBanos, O., Toth M. A., Damas, M., Pomares, H., Rojas, I. Dealing with the effects of sensor displacement in wearable activity recognition. Sensors vol. 14, no. 6, pp. 9995-10023 (2014).\r\nBanos, O., Toth, M. A., Damas, M., Pomares, H., Rojas, I., Amft, O. A benchmark dataset to evaluate sensor displacement in activity recognition. Proceedings of the 14th International Conference on Ubiquitous Computing (Ubicomp 2012), Pittsburgh, USA, September 5-8, (2012).\r\n\r\nWe recommend to refer to this dataset as the 'REALDISP dataset' in publications. \r\nWe would appreciate if you send us an email (oresti.bl@gmail.com) to inform us of any publication using this dataset. \r\n",
    "Area": "Computer",
    "RelevantPapers": "Banos, O., Toth M. A., Damas, M., Pomares, H., Rojas, I. Dealing with the effects of sensor displacement in wearable activity recognition. Sensors vol. 14, no. 6, pp. 9995-10023 (2014).\n\nBanos, O., Galvez, J. M., Damas, M., Pomares, H., Rojas, I. Window size impact in activity recognition. Sensors, vol. 14, no. 4, pp. 6474-6499 (2014).\n\nBanos, O., Galvez, J. M., Damas, M., Pomares, H., Rojas, I. Evaluating the effects of signal segmentation on activity recognition. Proceedings of the International Work-Conference on Bioinformatics and Biomedical Engineering (IWBBIO 2014), Granada, Spain, April 7-9, (2014).\n\nBanos, O., Damas, M., Pomares, H., Rojas, I. Handling displacement effects in on-body sensor-based activity recognition. Proceedings of the 5th International Work-conference on Ambient Assisted Living an Active Ageing (IWAAL 2013), San Jose, Costa Rica, December 2-6, (2013).\n\nBanos, O., Damas, M., Pomares, H., Rojas, I. Activity recognition based on a multi-sensor meta-classifier. Proceedings of the International Work Conference on Neural Networks (IWANN 2013), Tenerife, Spain, June 12-14, (2013). \n\nSmith, Jeremiah, et al. 'Exploring concept drift using interactive simulations' IEEE International Conference on Pervasive Computing and Communications Workshops (PERCOM 2013), San Diego, USA, March 18-22, (2013).\n\nBanos, O., Toth, M. A., Damas, M., Pomares, H., Rojas, I., Amft, O. A benchmark dataset to evaluate sensor displacement in activity recognition. Proceedings of the 14th International Conference on Ubiquitous Computing (Ubicomp 2012), Pittsburgh, USA, September 5-8, (2012).\n\nReyes-Ortiz, J.L., Luca Oneto, Albert Sam\u00e0, Xavier Parra, Davide Anguita, Transition-Aware Human Activity Recognition Using Smartphones, Neurocomputing, (Online) 2015\n\nNguyen, L. T., Zeng, M., Tague, P., Zhang, J. (2015). Recognizing New Activities with Limited Training Data. In IEEE International Symposium on Wearable Computers (ISWC).\n\nWilson, J.; Najjar, N.; Hare, J.; Gupta, S., Human activity recognition using LZW-Coded Probabilistic Finite State Automata. In IEEE International Conference on Robotics and Automation (ICRA), 2015, pp.3018-3023\n\nPunchoojit, Lumpapun, and Nuttanont Hongwarittorrn. \"A Comparative Study on Sensor Displacement Effect on Realistic Sensor Displacement Benchmark Dataset.\" Recent Advances in Information and Communication Technology 2015. 97-106.",
    "AttributeInfo": "The dataset comprises the readings of motion sensors recorded while users executed typical daily activities. The detailed format is described in the package. The attributes correspond to raw sensor readings. There is a total of 120 attributes:\r\n\r\nColumn 1: Timestamp in seconds\r\nColumn 2: Timestamp in microseconds\r\nColumn 3-15: [AccX, AccY, AccZ, GyrX, GyrY, Gyr, GyrZ, MagX, MagY, MagZ, Q1, Q2, Q3, Q4] of sensor S1\r\nColumn 16-28: [AccX, AccY, AccZ, GyrX, GyrY, Gyr, GyrZ, MagX, MagY, MagZ, Q1, Q2, Q3, Q4] of sensor S2\r\nColumn 29-41: [AccX, AccY, AccZ, GyrX, GyrY, Gyr, GyrZ, MagX, MagY, MagZ, Q1, Q2, Q3, Q4] of sensor S3\r\nColumn 42-54: [AccX, AccY, AccZ, GyrX, GyrY, Gyr, GyrZ, MagX, MagY, MagZ, Q1, Q2, Q3, Q4] of sensor S4\r\nColumn 55-67: [AccX, AccY, AccZ, GyrX, GyrY, Gyr, GyrZ, MagX, MagY, MagZ, Q1, Q2, Q3, Q4] of sensor S5\r\nColumn 68-80: [AccX, AccY, AccZ, GyrX, GyrY, Gyr, GyrZ, MagX, MagY, MagZ, Q1, Q2, Q3, Q4] of sensor S6\r\nColumn 91-93: [AccX, AccY, AccZ, GyrX, GyrY, Gyr, GyrZ, MagX, MagY, MagZ, Q1, Q2, Q3, Q4] of sensor S7\r\nColumn 94-106: [AccX, AccY, AccZ, GyrX, GyrY, Gyr, GyrZ, MagX, MagY, MagZ, Q1, Q2, Q3, Q4] of sensor S8\r\nColumn 107-119: [AccX, AccY, AccZ, GyrX, GyrY, Gyr, GyrZ, MagX, MagY, MagZ, Q1, Q2, Q3, Q4] of sensor S9\r\nColumn 120: Label (see activity set)\r\n",
    "FormatType": "Matrix",
    "NumHits": 8
}