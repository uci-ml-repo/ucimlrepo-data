[{"datasetID":1, "supportID":"005EF4795950C394FE7E4B85969C64011F9FC4D5", "rexaID":"f7fdf9dbb5f98a218956025550c1f603b3cb24f2", "author":"Alexander G. Gray and Bernd Fischer and Johann Schumann and Wray L. Buntine", "title":"Automatic Derivation of Statistical Algorithms: The EM Family and Beyond", "venue":"NIPS", "year":"2002", "window":"extension of our running example, integrating several features, yields a Gaussian Bayes classifier model # # . # # has been successfully tested on various standard benchmarks [1], e.g., the <b>Abalone</b> dataset. Currently, the number of expected classes has to be given in advance. Mixture models and EM. A wide range of # -Gaussian mixture models can be handled by AUTOBAYES, ranging from the simple 1D ( # #", "mykey":1},
 {"datasetID":40, "supportID":"007BBC1796239FF0E9C3507E864AA0E12F41F79A", "rexaID":"82cbfc464ed8d2bd0b91ffd1132022df16b7334d", "author":"Wl/odzisl/aw Duch and Karol Grudzi nski and Grzegorz Stawski", "title":"SYMBOLIC FEATURES IN NEURAL NETWORKS", "venue":"Department of Computer Methods, Nicolaus Copernicus University", "year":"", "window":"Both FSM network and k-NN algorithm achieved similar results (within statistical accuracy) on the raw data as on the converted data (the number of attributes did not change here). On the <b>Flags</b> dataset, containing 194 samples, 3 continuous and 25 symbolic attributes, 8 classes (majority rate 30.8%) and no missing values, 10-fold cross-validation tests were performed. Large improvement is observed", "mykey":2},
 {"datasetID":143, "supportID":"007BBC1796239FF0E9C3507E864AA0E12F41F79A", "rexaID":"82cbfc464ed8d2bd0b91ffd1132022df16b7334d", "author":"Wl/odzisl/aw Duch and Karol Grudzi nski and Grzegorz Stawski", "title":"SYMBOLIC FEATURES IN NEURAL NETWORKS", "venue":"Department of Computer Methods, Nicolaus Copernicus University", "year":"", "window":"significantly better and they show similar trend, comparable to the other two classifiers, therefore they are not reported here. All data was taken from the UCI repository [8]. The <b>Australian Credit</b> dataset has 690 cases, each with 6 continuous and 8 symbolic values. There are 2 classes (majority rate is 55.5%), no missing values. 10-fold cross-validation test were run therefore the variance could be", "mykey":3},
 {"datasetID":14, "supportID":"00CE503260103F3D176634B9F97342DB10E30A03", "rexaID":"3e78257004181e6dbbdfa3ec12399520412e9c5c", "author":"Pedro Domingos", "title":"Control-Sensitive Feature Selection for Lazy Learners", "venue":"Artif. Intell. Rev, 11", "year":"1997", "window":"used in the empirical study, in particular M. Zwitter and M. Soklic of the University Medical Centre, Ljubljana, for supplying the lymphography, <b>breast</b> <b>cancer</b> and primary tumor datasets, and Robert Detrano, of the V.A. Medical Center, Long Beach and Cleveland Clinic Foundation, for supplying the heart disease dataset. Please see the documentation in the UCI Repository for detailed", "mykey":4},
 {"datasetID":45, "supportID":"00CE503260103F3D176634B9F97342DB10E30A03", "rexaID":"3e78257004181e6dbbdfa3ec12399520412e9c5c", "author":"Pedro Domingos", "title":"Control-Sensitive Feature Selection for Lazy Learners", "venue":"Artif. Intell. Rev, 11", "year":"1997", "window":"and Robert Detrano, of the V.A. Medical Center, Long Beach and Cleveland Clinic Foundation, for supplying the <b>heart</b> disease dataset. Please see the documentation in the UCI Repository for detailed information on all datasets. Appendix A This appendix describes how, for each one of P prototypes, the relevant features are chosen", "mykey":5},
 {"datasetID":63, "supportID":"00CE503260103F3D176634B9F97342DB10E30A03", "rexaID":"3e78257004181e6dbbdfa3ec12399520412e9c5c", "author":"Pedro Domingos", "title":"Control-Sensitive Feature Selection for Lazy Learners", "venue":"Artif. Intell. Rev, 11", "year":"1997", "window":"used in the empirical study, in particular M. Zwitter and M. Soklic of the University Medical Centre, Ljubljana, for supplying the <b>lymphography</b>  breast cancer and primary tumor datasets, and Robert Detrano, of the V.A. Medical Center, Long Beach and Cleveland Clinic Foundation, for supplying the heart disease dataset. Please see the documentation in the UCI Repository for detailed", "mykey":6},
 {"datasetID":83, "supportID":"00CE503260103F3D176634B9F97342DB10E30A03", "rexaID":"3e78257004181e6dbbdfa3ec12399520412e9c5c", "author":"Pedro Domingos", "title":"Control-Sensitive Feature Selection for Lazy Learners", "venue":"Artif. Intell. Rev, 11", "year":"1997", "window":"used in the empirical study, in particular M. Zwitter and M. Soklic of the University Medical Centre, Ljubljana, for supplying the lymphography, breast cancer and <b>primary</b> <b>tumor</b> datasets, and Robert Detrano, of the V.A. Medical Center, Long Beach and Cleveland Clinic Foundation, for supplying the heart disease dataset. Please see the documentation in the UCI Repository for detailed", "mykey":7},
 {"datasetID":52, "supportID":"00D5BFDFF843700050588C025F7F5BDB6CE5E24A", "rexaID":"c5da3959b262c7f9f7d339d5a61d1af0ed0ee805", "author":"Federico Divina and Elena Marchiori", "title":"Knowledge-Based Evolutionary Search for Inductive Concept Learning", "venue":"Vrije Universiteit of Amsterdam", "year":"", "window":"found by ECL with the use of the EWUS operator are less simple that those found with the use of the other selection operators. In some cases, like in the <b>ionosphere</b>  the crx and the australian datasets, the difference is evident, while in other, e.g., in the glass2, the accidents and the congestions dataset, the simplicity obtained by the three methods is comparable. An explanation for this is", "mykey":8},
 {"datasetID":79, "supportID":"00D5BFDFF843700050588C025F7F5BDB6CE5E24A", "rexaID":"c5da3959b262c7f9f7d339d5a61d1af0ed0ee805", "author":"Federico Divina and Elena Marchiori", "title":"Knowledge-Based Evolutionary Search for Inductive Concept Learning", "venue":"Vrije Universiteit of Amsterdam", "year":"", "window":"training examples. It can be seen that in most of the cases, the EWUS selection operator leads to a population characterized by a higher diversity. Only in two cases (the breast and the <b>pima</b> <b>indians</b> datasets) the population evolved with the use of the standard US operators has a higher diversity. However, also in these two cases, the diversity of the two populations are comparable. The WUS selection", "mykey":9},
 {"datasetID":23, "supportID":"00FC9ADDA3A0484EF293231F4C3F8395DB067755", "rexaID":"5bc77452a8b6552aed91aa3294068d7110dc54af", "author":"Russell Greiner and Wei Zhou", "title":"Structural Extension to Logistic Regression: Discriminative Parameter Learning of Belief Net Classifiers", "venue":"AAAI/IAAI", "year":"2002", "window":"discretization [FI93]. Our accuracy values were based on 5-fold cross validation for small data, and holdout method for large data [Koh95]. See [GZ02],[FGG97] for more information about these datasets. We use the <b>CHESS</b> dataset (36 binary or ternary attributes) to illustrate the basic behaviour of the algorithms. Figure 2(a) shows the performance, on this dataset, of our NB+ELR (\"Na\u00a8iveBayes", "mykey":10},
 {"datasetID":21, "supportID":"00FC9ADDA3A0484EF293231F4C3F8395DB067755", "rexaID":"5bc77452a8b6552aed91aa3294068d7110dc54af", "author":"Russell Greiner and Wei Zhou", "title":"Structural Extension to Logistic Regression: Discriminative Parameter Learning of Belief Net Classifiers", "venue":"AAAI/IAAI", "year":"2002", "window":"discretization [FI93]. Our accuracy values were based on 5-fold cross validation for small data, and holdout method for large data [Koh95]. See [GZ02],[FGG97] for more information about these datasets. We use the <b>CHESS</b> dataset (36 binary or ternary attributes) to illustrate the basic behaviour of the algorithms. Figure 2(a) shows the performance, on this dataset, of our NB+ELR (\"Na\u00a8iveBayes", "mykey":11},
 {"datasetID":22, "supportID":"00FC9ADDA3A0484EF293231F4C3F8395DB067755", "rexaID":"5bc77452a8b6552aed91aa3294068d7110dc54af", "author":"Russell Greiner and Wei Zhou", "title":"Structural Extension to Logistic Regression: Discriminative Parameter Learning of Belief Net Classifiers", "venue":"AAAI/IAAI", "year":"2002", "window":"discretization [FI93]. Our accuracy values were based on 5-fold cross validation for small data, and holdout method for large data [Koh95]. See [GZ02],[FGG97] for more information about these datasets. We use the <b>CHESS</b> dataset (36 binary or ternary attributes) to illustrate the basic behaviour of the algorithms. Figure 2(a) shows the performance, on this dataset, of our NB+ELR (\"Na\u00a8iveBayes", "mykey":12},
 {"datasetID":60, "supportID":"00FCD4F8B110D7A81C0BC50BC6872EB10E5AA654", "rexaID":"178fbe6319b02d3cbf96285c713232c47ff6b4dc", "author":"Jochen Garcke and Michael Griebel", "title":"Data mining with sparse grids using simplicial basis functions", "venue":"KDD", "year":"2001", "window":"combination technique with linear basis functions. Left: level 4, # = 0.0035. Right: level 8, # = 0.0037 90.6 % and 91.1 % was achieved in [36] and [35], respectively, for this data set. 3.2 6-dimensional problems 3.2.1 BUPA <b>Liver</b> The BUPA Liver Disorders data set from Irvine Machine Learning Database Repository [6] consists of 345 data points with 6 features and a selector field", "mykey":13},
 {"datasetID":111, "supportID":"012477CAE382FBE25DE64B94C9BB44C79DB416BC", "rexaID":"4d3a15f17c5158aad69c6133ae41ed25b6e037a6", "author":"Eibe Frank and Stefan Kramer", "title":"Ensembles of nested dichotomies for multi-class problems", "venue":"ICML", "year":"2004", "window":"prim.-tumor 339 3.9 0 17 22 segment 2310 0.0 19 0 7 soybean 683 9.8 0 35 19 splice 3190 0.0 0 61 3 vehicle 846 0.0 18 0 4 vowel 990 0.0 10 3 11 waveform 5000 0.0 40 0 3 <b>zoo</b> 101 0.0 1 15 7 Table 1. Datasets used for the experiments differences in accuracy by using the corrected resampled t-test at the 5% significance level. This test has been shown to have Type I error at the significance level and", "mykey":14},
 {"datasetID":81, "supportID":"018D686BC93BF0C1216F1A7F8F16A2246B477829", "rexaID":"00b6f77ab5353f8974383e14fbef4cd03a846f8a", "author":"Greg Hamerly and Charles Elkan", "title":"Learning the k in k-means", "venue":"NIPS", "year":"2003", "window":"them slow for more than 8 to 12 dimensions. All our code is written in Matlab; X-means is written in C. 3.1 Discovering true clusters in labeled data We tested these algorithms on two real-world datasets for <b>handwritten</b> digit <b>recognition</b>  the NIST dataset [12] and the Pendigits dataset [2]. The goal is to cluster the data without knowledge of the labels and measure how well the clustering captures", "mykey":15},
 {"datasetID":89, "supportID":"022F79AD80E8304DD042D646DA09BA8FF6E43DE9", "rexaID":"377a3c79c5b7a108aa16ed38407c81d035a0740d", "author":"Nir Friedman and Daphne Koller", "title":"Being Bayesian about Network Structure", "venue":"UAI", "year":"2000", "window":"Edges Figure 2: Comparison of posterior probabilities using true posterior over orderings (x-axis) versus ordering-MCMC (y-axis). The figures show Markov features and Edge features in the <b>Flare</b> dataset with 100 samples. ordering obtained by flipping i j and i k . Now, consider the terms in Eq. (6); those terms corresponding to nodes i ` in the ordering # that precede i j or succeed i k do not", "mykey":16},
 {"datasetID":2, "supportID":"02D4ACD2DDAA4D862358ABC571DCD02FC2F36196", "rexaID":"0767f0171b0f74f96978a41f3e947ea91cc9dbd3", "author":"Alexander J. Smola and Vishy Vishwanathan and Eleazar Eskin", "title":"Laplace Propagation", "venue":"NIPS", "year":"2003", "window":"# \u00c4 # # # ##\u00c7 # (15) with the joint minimizer being the average of the individual solutions. 5 Experiments To test our ideas we performed a set of experiments with the widely available Web and <b>Adult</b> datasets from the UCI repository [1]. All experiments were performed on a 2.4 MHz 2 Note that we had to replace the equality with set inclusion due to the fact that \u00dc is not everywhere differentiable, hence", "mykey":17},
 {"datasetID":59, "supportID":"034639360A3FEFC40795898E14BC2BBDBCA7EE07", "rexaID":"2f0d669e8358c5a2af27cd538f8b5929dec89b48", "author":"Hirotaka Inoue and Hiroyuki Narihisa", "title":"Incremental Learning with Self-Organizing Neural Grove", "venue":"Department of Electrical Engineering and Information Science, Kure National College of Technology", "year":"", "window":"Results We investigate the relation between the number of training data and the classification accuracy, the number of nodes, and the computation time of SONG with bagging for <b>letter recognition</b> dataset in the UCI repository [4]. 2 Hirotaka Inoue and Hiroyuki Narihisa 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1 0 2000 4000 6000 8000 10000 12000 14000 16000 18000 Classification accuracy (%) # of N K=1 K=3 K=5", "mykey":18},
 {"datasetID":59, "supportID":"037B89497BBDDCD16AB2A47B6AB2EF8892C5FBEB", "rexaID":"69547b69d3133bba06b4ce74e67e2f3a6366a2f6", "author":"Adil M. Bagirov and Julien Ugon", "title":"An algorithm for computation of piecewise linear function separating two sets", "venue":"CIAO, School of Information Technology and Mathematical Sciences, The University of Ballarat", "year":"", "window":"to be known. In further research some methods to find automatically this number will 19 Table 2: Results of numerical experiments with Shuttle control, <b>Letter recognition</b> and Landsat satellite image datasets Training Test |I| |J i | a 2c a mc a 2c a mc fct eval DG eval Shuttle control dataset 1 1 97.61 97.22 97.53 97.00 925 615 2 1 99.44 97.56 99.41 97.42 2148 1676 3 1 99.61 97.57 99.59 97.50 1474 968", "mykey":19},
 {"datasetID":78, "supportID":"037B89497BBDDCD16AB2A47B6AB2EF8892C5FBEB", "rexaID":"69547b69d3133bba06b4ce74e67e2f3a6366a2f6", "author":"Adil M. Bagirov and Julien Ugon", "title":"An algorithm for computation of piecewise linear function separating two sets", "venue":"CIAO, School of Information Technology and Mathematical Sciences, The University of Ballarat", "year":"", "window":"through successive LP, Journal of Optimization Theory and Applications, 112(2), 265-293. 20 Table 3: Results of numerical experiments with Pen-based recognition of handwritten and <b>Page blocks</b> datasets Training Test |I| |J i | a 2c a mc a 2c a mc fct eval. DG. eval. Pen-based recognition of handwritten dataset 1 1 97.27 96.74 96.43 92.37 1597 1146 2 1 99.31 99.44 98.33 96.14 2607 1852 3 1 99.79", "mykey":20},
 {"datasetID":81, "supportID":"037B89497BBDDCD16AB2A47B6AB2EF8892C5FBEB", "rexaID":"69547b69d3133bba06b4ce74e67e2f3a6366a2f6", "author":"Adil M. Bagirov and Julien Ugon", "title":"An algorithm for computation of piecewise linear function separating two sets", "venue":"CIAO, School of Information Technology and Mathematical Sciences, The University of Ballarat", "year":"", "window":"Training Test |I| |J i | a 2c a mc a 2c a mc fct eval. DG. eval. <b>Pen-based</b> <b>recognition</b> of <b>handwritten</b> dataset 1 1 97.27 96.74 96.43 92.37 1597 1146 2 1 99.31 99.44 98.33 96.14 2607 1852 3 1 99.79 99.92 98.89 96.20 3040 2220 2 2 99.80 99.95 98.99 96.03 3083 2306 3 2 99.83 99.87 99.12 95.88 1806 1268 3 3", "mykey":21},
 {"datasetID":88, "supportID":"037B89497BBDDCD16AB2A47B6AB2EF8892C5FBEB", "rexaID":"69547b69d3133bba06b4ce74e67e2f3a6366a2f6", "author":"Adil M. Bagirov and Julien Ugon", "title":"An algorithm for computation of piecewise linear function separating two sets", "venue":"CIAO, School of Information Technology and Mathematical Sciences, The University of Ballarat", "year":"", "window":"accuracy (a mc in Tables 2 and 3) as described above. First accuracy is an indication of separation quality and the second one is an indication of multi-class classification quality. 5.2 Datasets The datasets used are the <b>Shuttle</b> <b>control</b> , the Letter recognition, the Landsat satellite image, the Pen-based recognition of handwritten and the Page blocks classification databases. Table 1", "mykey":22},
 {"datasetID":146, "supportID":"037B89497BBDDCD16AB2A47B6AB2EF8892C5FBEB", "rexaID":"69547b69d3133bba06b4ce74e67e2f3a6366a2f6", "author":"Adil M. Bagirov and Julien Ugon", "title":"An algorithm for computation of piecewise linear function separating two sets", "venue":"CIAO, School of Information Technology and Mathematical Sciences, The University of Ballarat", "year":"", "window":"to be known. In further research some methods to find automatically this number will 19 Table 2: Results of numerical experiments with Shuttle control, Letter recognition and <b>Landsat</b> <b>satellite</b> image datasets Training Test |I| |J i | a 2c a mc a 2c a mc fct eval DG eval Shuttle control dataset 1 1 97.61 97.22 97.53 97.00 925 615 2 1 99.44 97.56 99.41 97.42 2148 1676 3 1 99.61 97.57 99.59 97.50 1474 968", "mykey":23},
 {"datasetID":148, "supportID":"037B89497BBDDCD16AB2A47B6AB2EF8892C5FBEB", "rexaID":"69547b69d3133bba06b4ce74e67e2f3a6366a2f6", "author":"Adil M. Bagirov and Julien Ugon", "title":"An algorithm for computation of piecewise linear function separating two sets", "venue":"CIAO, School of Information Technology and Mathematical Sciences, The University of Ballarat", "year":"", "window":"accuracy (a mc in Tables 2 and 3) as described above. First accuracy is an indication of separation quality and the second one is an indication of multi-class classification quality. 5.2 Datasets The datasets used are the <b>Shuttle</b> control , the Letter recognition, the Landsat satellite image, the Pen-based recognition of handwritten and the Page blocks classification databases. Table 1", "mykey":24},
 {"datasetID":90, "supportID":"038AFE265B710AAE768EF46FE11F6165F5E614BC", "rexaID":"e3d2f48bca20059ef311938077d747d4f7f7a83a", "author":"Rich Caruana and Alexandru Niculescu-Mizil", "title":"Data Mining in Metric Space: An Empirical Analysis of Supervised Learning Performance Criteria", "venue":"ROCAI", "year":"2004", "window":"25 letters as negative, yielding a very unbalanced binary problem. LETTER.p2 uses letters A-M as positives and the rest as negatives, yielding a well balanced problem. HYPER SP is the IndianPine92 data set [4] where the difficult class <b>Soybean</b> mintill is the positive class. SLAC is a problem from collaborators at the Stanford Linear Accelerator and MEDIS is a medical data set. The characteristics of", "mykey":25},
 {"datasetID":91, "supportID":"038AFE265B710AAE768EF46FE11F6165F5E614BC", "rexaID":"e3d2f48bca20059ef311938077d747d4f7f7a83a", "author":"Rich Caruana and Alexandru Niculescu-Mizil", "title":"Data Mining in Metric Space: An Empirical Analysis of Supervised Learning Performance Criteria", "venue":"ROCAI", "year":"2004", "window":"25 letters as negative, yielding a very unbalanced binary problem. LETTER.p2 uses letters A-M as positives and the rest as negatives, yielding a well balanced problem. HYPER SP is the IndianPine92 data set [4] where the difficult class <b>Soybean</b> mintill is the positive class. SLAC is a problem from collaborators at the Stanford Linear Accelerator and MEDIS is a medical data set. The characteristics of", "mykey":26},
 {"datasetID":146, "supportID":"043791193A28E0AB9A971E9EB3C8385A52E64E52", "rexaID":"e550da2f9c24b03fec3cd94563a837e653db1c60", "author":"Xavier Giannakopoulos and Juha Karhunen and Erkki Oja", "title":"An Experimental Comparison of Neural Algorithms for Independent Component Analysis and Blind Separation", "venue":"Int. J. Neural Syst, 9", "year":"1999", "window":"methods for real-world data For comparing the performance and properties of the studied ICA or BSS algorithms in more practical circumstances, we made experiments with three dioeerent real-world data sets. These data sets, namely crab data, <b>satellite</b> data, and MEG artefact data, will be brieAEy discussed in context with the respective results in the next section. Because we are now dealing with", "mykey":27},
 {"datasetID":1, "supportID":"04403138D62B9878E0FE187F2DF9DE3B905382E9", "rexaID":"7030f724add30a4dd1b1f84109e72675c7e629f6", "author":"Bernhard Pfahringer and Hilan Bensusan and Christophe G. Giraud-Carrier", "title":"Meta-Learning by Landmarking Various Learning Algorithms", "venue":"ICML", "year":"2000", "window":"had from 5 to 12 attributes and were classified by simple parity, DNF and CNF rules as well as at random. The 18 Uci data sets were: mushrooms, <b>abalone</b>  crx, sat, acetylation, titanic, waveform, yeast, car, chess(king-rook-vs-king), led7, led24, tic-tac-toe, monk1, monk2, monk3, satimage, quisclas. The performance of every", "mykey":28},
 {"datasetID":151, "supportID":"04AEEC0F75282321D73274BB42BD6C2FEAB629D3", "rexaID":"43019420b94637b63ac4fc3f94f64abd11b64f0a", "author":"Perry Moerland", "title":"Mixtures of latent variable models for density estimation and classification", "venue":"E S E A R C H R E P R O R T I D I A P D a l l e M o l l e I n s t i t u t e f o r Pe r cep t ua l A r t i f i c i a l Intelligence ", "year":"", "window":"classifiers with Mfas are in Table 9. The set-up was as with the Bayesian Mpcas and again the Ml scores have been copied from Table 6. For the high-dimensional NIST, optical, <b>sonar</b>  and soybean data sets, a lower value of ` was chosen both for Mpcas and Mfas. This was done partly to save computation time and partly to avoid problems with the cheap and cheerful approximation already outlined in", "mykey":29},
 {"datasetID":33, "supportID":"04AEEC0F75282321D73274BB42BD6C2FEAB629D3", "rexaID":"43019420b94637b63ac4fc3f94f64abd11b64f0a", "author":"Perry Moerland", "title":"Mixtures of latent variable models for density estimation and classification", "venue":"E S E A R C H R E P R O R T I D I A P D a l l e M o l l e I n s t i t u t e f o r Pe r cep t ua l A r t i f i c i a l Intelligence ", "year":"", "window":"was used by imposing a small threshold of 0.01 upon the values of R j for Mfas and upon the variance parameters for a diagonal Gmm; this was done for the <b>dermatology</b>  NIST, optical, and soybean data sets. 7.2 Experiments: Real-World Data The results of the experiments with Bayes classifiers are listed in Table 6, where the best method and the ones that are not significantly worse (90% on the 5x2cv", "mykey":30},
 {"datasetID":52, "supportID":"04AEEC0F75282321D73274BB42BD6C2FEAB629D3", "rexaID":"43019420b94637b63ac4fc3f94f64abd11b64f0a", "author":"Perry Moerland", "title":"Mixtures of latent variable models for density estimation and classification", "venue":"E S E A R C H R E P R O R T I D I A P D a l l e M o l l e I n s t i t u t e f o r Pe r cep t ua l A r t i f i c i a l Intelligence ", "year":"", "window":"different models by specifying the total number of underlined scores for each model class. This number of wins shows that Mfas are the best density estimators. They are outperformed on only three data sets (cancer, <b>ionosphere</b>  and vowel) out of 18. With respect to the spherical Gmms, the score of only 1 win illustrates that they are too constrained to model the data. From the results, one can also", "mykey":31},
 {"datasetID":81, "supportID":"04AEEC0F75282321D73274BB42BD6C2FEAB629D3", "rexaID":"43019420b94637b63ac4fc3f94f64abd11b64f0a", "author":"Perry Moerland", "title":"Mixtures of latent variable models for density estimation and classification", "venue":"E S E A R C H R E P R O R T I D I A P D a l l e M o l l e I n s t i t u t e f o r Pe r cep t ua l A r t i f i c i a l Intelligence ", "year":"", "window":"of the ten digit classes with a 10-component 10-factor Mfa. This is comparable to the best scores we obtained with large Mlps. 7.3 Experiments: MNIST Data We also performed experiments on the MNIST data set, a large collection of <b>handwritten</b> <b>digits</b> which can be obtained from (LeCun 2000). It comes as 28#28 grey level images in a training set of 60,000 examples and a test set of 10,000 examples. Each of", "mykey":32},
 {"datasetID":90, "supportID":"04AEEC0F75282321D73274BB42BD6C2FEAB629D3", "rexaID":"43019420b94637b63ac4fc3f94f64abd11b64f0a", "author":"Perry Moerland", "title":"Mixtures of latent variable models for density estimation and classification", "venue":"E S E A R C H R E P R O R T I D I A P D a l l e M o l l e I n s t i t u t e f o r Pe r cep t ua l A r t i f i c i a l Intelligence ", "year":"", "window":"was used by imposing a small threshold of 0.01 upon the values of R j for Mfas and upon the variance parameters for a diagonal Gmm; this was done for the dermatology, NIST, optical, and <b>soybean</b> data sets. 7.2 Experiments: Real-World Data The results of the experiments with Bayes classifiers are listed in Table 6, where the best method and the ones that are not significantly worse (90% on the 5x2cv", "mykey":33},
 {"datasetID":91, "supportID":"04AEEC0F75282321D73274BB42BD6C2FEAB629D3", "rexaID":"43019420b94637b63ac4fc3f94f64abd11b64f0a", "author":"Perry Moerland", "title":"Mixtures of latent variable models for density estimation and classification", "venue":"E S E A R C H R E P R O R T I D I A P D a l l e M o l l e I n s t i t u t e f o r Pe r cep t ua l A r t i f i c i a l Intelligence ", "year":"", "window":"was used by imposing a small threshold of 0.01 upon the values of R j for Mfas and upon the variance parameters for a diagonal Gmm; this was done for the dermatology, NIST, optical, and <b>soybean</b> data sets. 7.2 Experiments: Real-World Data The results of the experiments with Bayes classifiers are listed in Table 6, where the best method and the ones that are not significantly worse (90% on the 5x2cv", "mykey":34},
 {"datasetID":2, "supportID":"04E10F745A7267453788A22F5150B5A32B2B3951", "rexaID":"4284c9cb6236847cd246f69cfb8e4209c107d18f", "author":"Bianca Zadrozny and Charles Elkan", "title":"Transforming classifier scores into accurate multiclass probability estimates", "venue":"KDD", "year":"2002", "window":"# x### s # : the number of examples with score s that belong to class c divided by the total number of examples 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 <b>Adult</b> Dataset NB Score Empirical class membership probability 8941 790 610 450 480 532 477 620 672 2710 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 The Insurance Company", "mykey":35},
 {"datasetID":125, "supportID":"04E10F745A7267453788A22F5150B5A32B2B3951", "rexaID":"4284c9cb6236847cd246f69cfb8e4209c107d18f", "author":"Bianca Zadrozny and Charles Elkan", "title":"Transforming classifier scores into accurate multiclass probability estimates", "venue":"KDD", "year":"2002", "window":"PAV to binning with bin sizes varying from 5 to 50. Although we did not have to set any parameters for the PAV method, it performed comparably to the best parameter setting for binning. The next dataset we use is The <b>Insurance</b> <b>Company</b> <b>Benchmark</b> (TIC), also known as the COIL 2000 dataset, which is available in the UCI KDD repository [3]. The decision-making task is analogous to the KDD-98 task:", "mykey":36},
 {"datasetID":137, "supportID":"04E10F745A7267453788A22F5150B5A32B2B3951", "rexaID":"4284c9cb6236847cd246f69cfb8e4209c107d18f", "author":"Bianca Zadrozny and Charles Elkan", "title":"Transforming classifier scores into accurate multiclass probability estimates", "venue":"KDD", "year":"2002", "window":"estimates using a sigmoid function. rate class membership probability estimates, while being faster. The same method can be applied to naive Bayes. This was proposed by Bennett [4] for the <b>Reuters</b> dataset. In Figure 4 we show the sigmoidal fit to the naive Bayes scores for the Adult and TIC datasets. The sigmoidal shape does not appear to fit naive Bayes scores as well as it fits SVM scores, for", "mykey":37},
 {"datasetID":151, "supportID":"0569091048E5B843360B308832B0B68069E88765", "rexaID":"a9f4b48c1d04a8a7ee0f1806f81fc670e653c058", "author":"Perry Moerland and E. Fiesler and I. Ubarretxena-Belandia", "title":"Martigny - Valais - Suisse Discrete All-Positive Multilayer Perceptrons for Optical Implementation", "venue":"E S E A R C H R E P R O R T I D I A P", "year":"1997", "window":"implementation. eXclusive OR (XOR) The training set consists of the boolean exclusive OR function. It is the classical example of a simple problem that is not linearly separable [12]. <b>Sonar</b> This data set was originally used by R. Gorman and T. Sejnowski in their study of the classification of sonar signals using a neural network. The task is to discriminate between sonar signals bounced off a metal", "mykey":38},
 {"datasetID":14, "supportID":"058E5056FB1ACE262D0F5A1F0EC0F137248D7BC0", "rexaID":"e8eb0092d0fc87ff35447cec823b43a406ac8372", "author":"G. Ratsch and B. Scholkopf and Alex Smola and K. -R Muller and T. Onoda and Sebastian Mika", "title":"Arc: Ensemble Learning in the Presence of Outliers", "venue":"GMD FIRST", "year":"", "window":"[17] explains the good generalization performance of AdaBoost in the low noise regime. However, AdaBoost performs worse on noisy tasks [10, 11], such as the iris and the <b>breast</b> <b>cancer</b> benchmark data sets [1]. On the latter tasks, a large margin on all training points cannot be achieved without adverse effects on the generalization error. This experimental observation was supported by the study of", "mykey":39},
 {"datasetID":53, "supportID":"058E5056FB1ACE262D0F5A1F0EC0F137248D7BC0", "rexaID":"e8eb0092d0fc87ff35447cec823b43a406ac8372", "author":"G. Ratsch and B. Scholkopf and Alex Smola and K. -R Muller and T. Onoda and Sebastian Mika", "title":"Arc: Ensemble Learning in the Presence of Outliers", "venue":"GMD FIRST", "year":"", "window":"[17] explains the good generalization performance of AdaBoost in the low noise regime. However, AdaBoost performs worse on noisy tasks [10, 11], such as the <b>iris</b> and the breast cancer benchmark data sets [1]. On the latter tasks, a large margin on all training points cannot be achieved without adverse effects on the generalization error. This experimental observation was supported by the study of", "mykey":40},
 {"datasetID":45, "supportID":"05B0846409BEC99AA49302D5C59278D3504620C2", "rexaID":"a11c91c5cf02794b7d1d4bdc0222b3c11dce9cac", "author":"David Page and Soumya Ray", "title":"Skewing: An Efficient Alternative to Lookahead for Decision Tree Induction", "venue":"IJCAI", "year":"2003", "window":"Skewing ID3, No Skewing Figure 8: Five-Variable Hard Targets 50 60 70 80 90 100 200 400 600 800 1000 Accuracy (%) Sample Size ID3 with Skewing ID3, No Skewing Figure 9: Six-Variable Hard Targets Data Set Standard ID3 ID3 with Skewing <b>Heart</b> 71.9 74.5 Voting 94.0 94.2 Voting-2 87.4 88.6 Contra 60.4 61.5 Monks-1 92.6 100.0 Monks-2 86.5 89.3 Monks-3 89.8 91.7 Table 4: Accuracies of ID3 and ID3 with", "mykey":41},
 {"datasetID":1, "supportID":"05B3ECF5190D3EA19BA9B7DBE8793118D0238D1B", "rexaID":"5bffb4d591b37fc0c1011681ab2cf075033ca539", "author":"Anton Schwaighofer and Volker Tresp", "title":"Transductive and Inductive Methods for Approximate Gaussian Process Regression", "venue":"NIPS", "year":"2002", "window":"costs for predictions show the cost per test point. 4 Experimental Comparison In this section we will present a comparison of the different approximation methods discussed in Sec. 3. In the <b>ABALONE</b> data set [1] with 4177 examples, the goal is to predict the age of Abalones based on 8 inputs. The KIN8NM data set 2 represents the forward dynamics of an 8 link all-revolute robot arm, based on 8192", "mykey":42},
 {"datasetID":53, "supportID":"06ACDBD2F69221EDC16B27650F29CBAE556242B6", "rexaID":"490ddcab7a72db65c0752abf46f5692f53e6bedf", "author":"Huan Li and Wenbin Chen", "title":"Supervised Local Tangent Space Alignment for Classification", "venue":" I-Fan Shen", "year":"", "window":"containing multiple classes. The results obtained with the unsupervised and supervised LTSA are expected to be different as is shown in Fig.1. The <b>iris</b> data set [Blake and Merz, 1998] includes 150 4-D data belonging to 3 different classes. Here first 100 data points are selected as training samples and mapped from the 4-D input space to a 2-D feature space", "mykey":43},
 {"datasetID":56, "supportID":"06BE354DB398057D6E61C72B9D03C0F3ED398D50", "rexaID":"0f5d89b69b182e7d068733e49bc056afc4e64e02", "author":"Rudy Setiono", "title":"Feedforward Neural Network Construction Using Cross Validation", "venue":"Neural Computation, 13", "year":"2001", "window":"and the test set. The average number of hidden units ranged from 2.46 for the <b>labor</b> data set to 19.44 for the soybean data set. Most of the networks for the latter data set contain the maximum 20 hidden units. It might be possible to improve the overall predictive accuracy of these networks", "mykey":44},
 {"datasetID":90, "supportID":"06BE354DB398057D6E61C72B9D03C0F3ED398D50", "rexaID":"0f5d89b69b182e7d068733e49bc056afc4e64e02", "author":"Rudy Setiono", "title":"Feedforward Neural Network Construction Using Cross Validation", "venue":"Neural Computation, 13", "year":"2001", "window":"and the test set. The average number of hidden units ranged from 2.46 for the labor data set to 19.44 for the <b>soybean</b> data set. Most of the networks for the latter data set contain the maximum 20 hidden units. It might be possible to improve the overall predictive accuracy of these networks", "mykey":45},
 {"datasetID":91, "supportID":"06BE354DB398057D6E61C72B9D03C0F3ED398D50", "rexaID":"0f5d89b69b182e7d068733e49bc056afc4e64e02", "author":"Rudy Setiono", "title":"Feedforward Neural Network Construction Using Cross Validation", "venue":"Neural Computation, 13", "year":"2001", "window":"and the test set. The average number of hidden units ranged from 2.46 for the labor data set to 19.44 for the <b>soybean</b> data set. Most of the networks for the latter data set contain the maximum 20 hidden units. It might be possible to improve the overall predictive accuracy of these networks", "mykey":46},
 {"datasetID":105, "supportID":"0756CD080BDDC5EA0DAF92978CDE03882601FEF1", "rexaID":"47f4421e112679b98c4ccf604a351b54fbb7a000", "author":"Huan Liu and Rudy Setiono", "title":"Incremental Feature Selection", "venue":"Appl. Intell, 9", "year":"1998", "window":"critical evaluation of intrinsic dimensionality algorithms. In E.S. Gelsema and Kanal L.N., editors, Pattern Recognition in Practice, pages 415--425. Morgan Kaufmann Publishers, Inc., 1980. 19 <b>Vote dataset</b> CPU time Percent of training samples 20.00 22.00 24.00 26.00 28.00 30.00 32.00 34.00 36.00 0.00 20.00 40.00 60.00 80.00 100.00 Mushroom dataset CPU time Percent of training samples 10.00 15.00 20.00", "mykey":47},
 {"datasetID":57, "supportID":"0756CD080BDDC5EA0DAF92978CDE03882601FEF1", "rexaID":"47f4421e112679b98c4ccf604a351b54fbb7a000", "author":"Huan Liu and Rudy Setiono", "title":"Incremental Feature Selection", "venue":"Appl. Intell, 9", "year":"1998", "window":"60 80 100 120 140 160 180 200 220 0.00 20.00 40.00 60.00 80.00 100.00 240 CPU time Percent of training samples <b>LED dataset</b> CPU time Percent of training samples 100 200 300 400 500 600 700 800 900 1000 0.00 20.00 40.00 60.00 80.00 100.00 Figure 1: Average CPU time (seconds). The result for 100% data is used as the", "mykey":48},
 {"datasetID":73, "supportID":"0756CD080BDDC5EA0DAF92978CDE03882601FEF1", "rexaID":"47f4421e112679b98c4ccf604a351b54fbb7a000", "author":"Huan Liu and Rudy Setiono", "title":"Incremental Feature Selection", "venue":"Appl. Intell, 9", "year":"1998", "window":"19 35 683 307 376 Parity5+5 2 10 1024 100 100 Vote 2 16 435 300 135 <b>Mushroom</b> 2 22 8125 7125 1000 Led17 10 24 20,000 20,000 - Krvskp 2 36 3196 3196 - Parity Mix 2 20 2 20 10,000 - ffl Mushroom The dataset has a total of 8124 patterns, of which 1000 patterns are randomly selected for testing, the rest are used for training. The data has 22 discrete features. Each feature can have 2 to 10 values. ffl", "mykey":49},
 {"datasetID":90, "supportID":"0756CD080BDDC5EA0DAF92978CDE03882601FEF1", "rexaID":"47f4421e112679b98c4ccf604a351b54fbb7a000", "author":"Huan Liu and Rudy Setiono", "title":"Incremental Feature Selection", "venue":"Appl. Intell, 9", "year":"1998", "window":"in two separate files containing 307 and 376 patterns respectively. It contains 35 features describing symptoms of 19 different diseases in <b>soybean</b> plant. ffl Vote This dataset includes votes from the U.S. House of Representatives Congress-persons on the 16 key votes identified by the Congressional Quarterly Almanac Volume XL. The dataset consists of 16 features, 300", "mykey":50},
 {"datasetID":91, "supportID":"0756CD080BDDC5EA0DAF92978CDE03882601FEF1", "rexaID":"47f4421e112679b98c4ccf604a351b54fbb7a000", "author":"Huan Liu and Rudy Setiono", "title":"Incremental Feature Selection", "venue":"Appl. Intell, 9", "year":"1998", "window":"in two separate files containing 307 and 376 patterns respectively. It contains 35 features describing symptoms of 19 different diseases in <b>soybean</b> plant. ffl Vote This dataset includes votes from the U.S. House of Representatives Congress-persons on the 16 key votes identified by the Congressional Quarterly Almanac Volume XL. The dataset consists of 16 features, 300", "mykey":51},
 {"datasetID":2, "supportID":"07753406174C42BAF1807BCBE36D602C3932B291", "rexaID":"59e7c62dd41ce2ed1741bafac723745eb7b80b65", "author":"Josep Roure Alcobe", "title":"Incremental Hill-Climbing Search Applied to Bayesian Network Structure Learning", "venue":"Escola Universitria Politcnica de Mataro", "year":"", "window":"by means of a parameter nRSS. 3.2 Experimental Results In this section we compare the performance of repeatedly using the batch algorithms against the corresponding incremental approach. We used the datasets <b>Adult</b> 48.842 instances and 13 variables), Mushroom (8.124 inst. and 23 var.) and Nursery (12.960 inst. and 9 var.) from the UCI machine learning repository [9], the Alarm dataset (20.000 inst. and", "mykey":52},
 {"datasetID":73, "supportID":"07753406174C42BAF1807BCBE36D602C3932B291", "rexaID":"59e7c62dd41ce2ed1741bafac723745eb7b80b65", "author":"Josep Roure Alcobe", "title":"Incremental Hill-Climbing Search Applied to Bayesian Network Structure Learning", "venue":"Escola Universitria Politcnica de Mataro", "year":"", "window":"approaches save a significantly amount of CPU clock ticks while the quality of the final Bayesian networks is very close to the ones obtained with the batch approaches. See also that the <b>Mushroom</b> dataset is generally the most di\u00b1cult to learn incrementally in the sense that incremental algorithms obtain the lowest time gain. This may be due to the fact that there are many arcs that bring similar", "mykey":53},
 {"datasetID":46, "supportID":"077841C4994E94913B0364A1A26CE41AC5FE7F7E", "rexaID":"d891d25459c4422e9cf2262272938d1ada438a9e", "author":"Xiaoli Z. Fern and Carla Brodley", "title":"Boosting Lazy Decision Trees", "venue":"ICML", "year":"2003", "window":"but behave less consistently. For three data sets, <b>Hepatitis</b>  Lympho and Monk2, bagging significantly degrades the performance of the base learner. This is possibly caused by the sub-sampling procedure used by bagging to generate different", "mykey":54},
 {"datasetID":7, "supportID":"082772D035779B7ECB6C4D9187573C4E4108E4DB", "rexaID":"e2b2b723df700c90e69a31a4403b740c2d2a7b2f", "author":"Alexander K. Seewald", "title":"Dissertation Towards Understanding Stacking Studies of a General Ensemble Learning Scheme ausgefuhrt zum Zwecke der Erlangung des akademischen Grades eines Doktors der technischen Naturwissenschaften", "venue":"", "year":"", "window":"with number of classes and examples, discrete and continuous attributes, baseline accuracy (%) and entropy in bits per example (Kononenko & Bratko, 1991). Dataset cl Inst disc cont bL E <b>audiology</b> 24 226 69 0 25.22 3.51 autos 7 205 10 16 32.68 2.29 balance-scale 3 625 0 4 45.76 1.32 breast-cancer 2 286 10 0 70.28 0.88 breast-w 2 699 0 9 65.52 0.93 colic 2 368", "mykey":55},
 {"datasetID":8, "supportID":"082772D035779B7ECB6C4D9187573C4E4108E4DB", "rexaID":"e2b2b723df700c90e69a31a4403b740c2d2a7b2f", "author":"Alexander K. Seewald", "title":"Dissertation Towards Understanding Stacking Studies of a General Ensemble Learning Scheme ausgefuhrt zum Zwecke der Erlangung des akademischen Grades eines Doktors der technischen Naturwissenschaften", "venue":"", "year":"", "window":"with number of classes and examples, discrete and continuous attributes, baseline accuracy (%) and entropy in bits per example (Kononenko & Bratko, 1991). Dataset cl Inst disc cont bL E <b>audiology</b> 24 226 69 0 25.22 3.51 autos 7 205 10 16 32.68 2.29 balance-scale 3 625 0 4 45.76 1.32 breast-cancer 2 286 10 0 70.28 0.88 breast-w 2 699 0 9 65.52 0.93 colic 2 368", "mykey":56},
 {"datasetID":12, "supportID":"082772D035779B7ECB6C4D9187573C4E4108E4DB", "rexaID":"e2b2b723df700c90e69a31a4403b740c2d2a7b2f", "author":"Alexander K. Seewald", "title":"Dissertation Towards Understanding Stacking Studies of a General Ensemble Learning Scheme ausgefuhrt zum Zwecke der Erlangung des akademischen Grades eines Doktors der technischen Naturwissenschaften", "venue":"", "year":"", "window":"the baseline accuracy is already 66.7%. Interestingly in this case the best model is from DecisionStump which learns a single J48 node, obtaining 88.9% accuracy, corresponding to a single error on dataset <b>balance</b> <b>scale</b>  It seems J48 is prone to overfitting on this meta dataset. The training set model is based on meanAbsSkew. All but two times, the following model appears: meanAbsSkew <= 0.31 : class", "mykey":57},
 {"datasetID":14, "supportID":"082772D035779B7ECB6C4D9187573C4E4108E4DB", "rexaID":"e2b2b723df700c90e69a31a4403b740c2d2a7b2f", "author":"Alexander K. Seewald", "title":"Dissertation Towards Understanding Stacking Studies of a General Ensemble Learning Scheme ausgefuhrt zum Zwecke der Erlangung des akademischen Grades eines Doktors der technischen Naturwissenschaften", "venue":"", "year":"", "window":"balance-scale Compressed glyph visualization for dataset <b>breast</b> <b>cancer</b> Compressed glyph visualization for dataset breast-w Compressed glyph visualization for dataset colic Compressed glyph visualization for dataset credit-a Compressed glyph visualization", "mykey":58},
 {"datasetID":151, "supportID":"082772D035779B7ECB6C4D9187573C4E4108E4DB", "rexaID":"e2b2b723df700c90e69a31a4403b740c2d2a7b2f", "author":"Alexander K. Seewald", "title":"Dissertation Towards Understanding Stacking Studies of a General Ensemble Learning Scheme ausgefuhrt zum Zwecke der Erlangung des akademischen Grades eines Doktors der technischen Naturwissenschaften", "venue":"", "year":"", "window":"segment Compressed glyph visualization for dataset <b>sonar</b> Compressed glyph visualization for dataset soybean Compressed glyph visualization for dataset vehicle Compressed glyph visualization for dataset vote Compressed glyph visualization for dataset", "mykey":59},
 {"datasetID":34, "supportID":"082772D035779B7ECB6C4D9187573C4E4108E4DB", "rexaID":"e2b2b723df700c90e69a31a4403b740c2d2a7b2f", "author":"Alexander K. Seewald", "title":"Dissertation Towards Understanding Stacking Studies of a General Ensemble Learning Scheme ausgefuhrt zum Zwecke der Erlangung des akademischen Grades eines Doktors der technischen Naturwissenschaften", "venue":"", "year":"", "window":"credit-g Compressed glyph visualization for dataset <b>diabetes</b> Compressed glyph visualization for dataset glass Compressed glyph visualization for dataset heart-c Compressed glyph visualization for dataset heart-h Compressed glyph visualization for", "mykey":60},
 {"datasetID":42, "supportID":"082772D035779B7ECB6C4D9187573C4E4108E4DB", "rexaID":"e2b2b723df700c90e69a31a4403b740c2d2a7b2f", "author":"Alexander K. Seewald", "title":"Dissertation Towards Understanding Stacking Studies of a General Ensemble Learning Scheme ausgefuhrt zum Zwecke der Erlangung des akademischen Grades eines Doktors der technischen Naturwissenschaften", "venue":"", "year":"", "window":"#10 Training set (8-9=CV, 7=75%, 6=62%,.. 1=25%) Hold-out accuracy Figure 6.2: Learning curves for dataset balance-scale to <b>glass</b>  56 0 1 2 3 4 5 6 7 8 9 0.65 0.7 0.75 0.8 0.85 0.9 Learncurve for Dataset #11 Training set (8-9=CV, 7=75%, 6=62%,.. 1=25%) Hold-out accuracy 0 1 2 3 4 5 6 7 8 9 0.65 0.7 0.75", "mykey":61},
 {"datasetID":45, "supportID":"082772D035779B7ECB6C4D9187573C4E4108E4DB", "rexaID":"e2b2b723df700c90e69a31a4403b740c2d2a7b2f", "author":"Alexander K. Seewald", "title":"Dissertation Towards Understanding Stacking Studies of a General Ensemble Learning Scheme ausgefuhrt zum Zwecke der Erlangung des akademischen Grades eines Doktors der technischen Naturwissenschaften", "venue":"", "year":"", "window":"#18 Training set (8-9=CV, 7=75%, 6=62%,.. 1=25%) Hold-out accuracy Figure 6.3: Learning curves for dataset <b>heart</b> c to lymph. 57 0 1 2 3 4 5 6 7 8 9 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0.55 0.6 Learncurve for Dataset #19 Training set (8-9=CV, 7=75%, 6=62%,.. 1=25%) Hold-out accuracy 0 1 2 3 4 5 6 7 8 9 0.86", "mykey":62},
 {"datasetID":46, "supportID":"082772D035779B7ECB6C4D9187573C4E4108E4DB", "rexaID":"e2b2b723df700c90e69a31a4403b740c2d2a7b2f", "author":"Alexander K. Seewald", "title":"Dissertation Towards Understanding Stacking Studies of a General Ensemble Learning Scheme ausgefuhrt zum Zwecke der Erlangung des akademischen Grades eines Doktors der technischen Naturwissenschaften", "venue":"", "year":"", "window":"heart-statlog Compressed glyph visualization for dataset <b>hepatitis</b> Figure 8.6: Glyph visualization for datasets audiology to hepatitis. 79 Compressed glyph visualization for dataset ionosphere Compressed glyph visualization for dataset iris Compressed", "mykey":63},
 {"datasetID":47, "supportID":"082772D035779B7ECB6C4D9187573C4E4108E4DB", "rexaID":"e2b2b723df700c90e69a31a4403b740c2d2a7b2f", "author":"Alexander K. Seewald", "title":"Dissertation Towards Understanding Stacking Studies of a General Ensemble Learning Scheme ausgefuhrt zum Zwecke der Erlangung des akademischen Grades eines Doktors der technischen Naturwissenschaften", "venue":"", "year":"", "window":"breast-w Compressed glyph visualization for dataset <b>colic</b> Compressed glyph visualization for dataset credit-a Compressed glyph visualization for dataset credit-g Compressed glyph visualization for dataset diabetes Compressed glyph visualization for", "mykey":64},
 {"datasetID":52, "supportID":"082772D035779B7ECB6C4D9187573C4E4108E4DB", "rexaID":"e2b2b723df700c90e69a31a4403b740c2d2a7b2f", "author":"Alexander K. Seewald", "title":"Dissertation Towards Understanding Stacking Studies of a General Ensemble Learning Scheme ausgefuhrt zum Zwecke der Erlangung des akademischen Grades eines Doktors der technischen Naturwissenschaften", "venue":"", "year":"", "window":"audiology to hepatitis. 79 Compressed glyph visualization for dataset <b>ionosphere</b> Compressed glyph visualization for dataset iris Compressed glyph visualization for dataset labor Compressed glyph visualization for dataset lymph Compressed glyph visualization for", "mykey":65},
 {"datasetID":53, "supportID":"082772D035779B7ECB6C4D9187573C4E4108E4DB", "rexaID":"e2b2b723df700c90e69a31a4403b740c2d2a7b2f", "author":"Alexander K. Seewald", "title":"Dissertation Towards Understanding Stacking Studies of a General Ensemble Learning Scheme ausgefuhrt zum Zwecke der Erlangung des akademischen Grades eines Doktors der technischen Naturwissenschaften", "venue":"", "year":"", "window":"ionosphere Compressed glyph visualization for dataset <b>iris</b> Compressed glyph visualization for dataset labor Compressed glyph visualization for dataset lymph Compressed glyph visualization for dataset primary-tumor Compressed glyph visualization for", "mykey":66},
 {"datasetID":56, "supportID":"082772D035779B7ECB6C4D9187573C4E4108E4DB", "rexaID":"e2b2b723df700c90e69a31a4403b740c2d2a7b2f", "author":"Alexander K. Seewald", "title":"Dissertation Towards Understanding Stacking Studies of a General Ensemble Learning Scheme ausgefuhrt zum Zwecke der Erlangung des akademischen Grades eines Doktors der technischen Naturwissenschaften", "venue":"", "year":"", "window":"vote. When removing the base classifier dependent features, IBk is still the best classifier with an additional error on <b>labor</b>  the smallest dataset. In this case 12 All base learners plus 1R and DecisionStump 36 MLR, another high-bias and global learner, is equally good. So we may tentatively conclude that for this meta dataset, there seems to", "mykey":67},
 {"datasetID":83, "supportID":"082772D035779B7ECB6C4D9187573C4E4108E4DB", "rexaID":"e2b2b723df700c90e69a31a4403b740c2d2a7b2f", "author":"Alexander K. Seewald", "title":"Dissertation Towards Understanding Stacking Studies of a General Ensemble Learning Scheme ausgefuhrt zum Zwecke der Erlangung des akademischen Grades eines Doktors der technischen Naturwissenschaften", "venue":"", "year":"", "window":"#26 Training set (8-9=CV, 7=75%, 6=62%,.. 1=25%) Hold-out accuracy Figure 6.4: Learning curves for dataset <b>primary</b> <b>tumor</b> to zoo. 58 Chapter 7 Towards a Theoretical Framework In this chapter, we show that the ensemble learning scheme Stacking is universal in the sense that most ensemble learning schemes", "mykey":68},
 {"datasetID":90, "supportID":"082772D035779B7ECB6C4D9187573C4E4108E4DB", "rexaID":"e2b2b723df700c90e69a31a4403b740c2d2a7b2f", "author":"Alexander K. Seewald", "title":"Dissertation Towards Understanding Stacking Studies of a General Ensemble Learning Scheme ausgefuhrt zum Zwecke der Erlangung des akademischen Grades eines Doktors der technischen Naturwissenschaften", "venue":"", "year":"", "window":"instances. We rejected Principal Components Analysis after initial experiments because its linear projection on orthogonal axes is less general and the representation is noticeably worse for some datasets, e.g. <b>soybean</b>  We rejected self-organizing 1 We used the Sammon Mapping implementation from Vesanto, Himberg, Alhoniemi & Parhankangas (2000), which was written in MatLab script language. It worked", "mykey":69},
 {"datasetID":91, "supportID":"082772D035779B7ECB6C4D9187573C4E4108E4DB", "rexaID":"e2b2b723df700c90e69a31a4403b740c2d2a7b2f", "author":"Alexander K. Seewald", "title":"Dissertation Towards Understanding Stacking Studies of a General Ensemble Learning Scheme ausgefuhrt zum Zwecke der Erlangung des akademischen Grades eines Doktors der technischen Naturwissenschaften", "venue":"", "year":"", "window":"instances. We rejected Principal Components Analysis after initial experiments because its linear projection on orthogonal axes is less general and the representation is noticeably worse for some datasets, e.g. <b>soybean</b>  We rejected self-organizing 1 We used the Sammon Mapping implementation from Vesanto, Himberg, Alhoniemi & Parhankangas (2000), which was written in MatLab script language. It worked", "mykey":70},
 {"datasetID":145, "supportID":"082772D035779B7ECB6C4D9187573C4E4108E4DB", "rexaID":"e2b2b723df700c90e69a31a4403b740c2d2a7b2f", "author":"Alexander K. Seewald", "title":"Dissertation Towards Understanding Stacking Studies of a General Ensemble Learning Scheme ausgefuhrt zum Zwecke der Erlangung des akademischen Grades eines Doktors der technischen Naturwissenschaften", "venue":"", "year":"", "window":"<b>heart</b> h Compressed glyph visualization for dataset heart <b>statlog</b> Compressed glyph visualization for dataset hepatitis Figure 8.6: Glyph visualization for datasets audiology to hepatitis. 79 Compressed glyph visualization for dataset ionosphere", "mykey":71},
 {"datasetID":149, "supportID":"082772D035779B7ECB6C4D9187573C4E4108E4DB", "rexaID":"e2b2b723df700c90e69a31a4403b740c2d2a7b2f", "author":"Alexander K. Seewald", "title":"Dissertation Towards Understanding Stacking Studies of a General Ensemble Learning Scheme ausgefuhrt zum Zwecke der Erlangung des akademischen Grades eines Doktors der technischen Naturwissenschaften", "venue":"", "year":"", "window":"soybean Compressed glyph visualization for dataset <b>vehicle</b> Compressed glyph visualization for dataset vote Compressed glyph visualization for dataset vowel Compressed glyph visualization for dataset zoo Figure 8.7: Glyph visualization for datasets", "mykey":72},
 {"datasetID":98, "supportID":"082772D035779B7ECB6C4D9187573C4E4108E4DB", "rexaID":"e2b2b723df700c90e69a31a4403b740c2d2a7b2f", "author":"Alexander K. Seewald", "title":"Dissertation Towards Understanding Stacking Studies of a General Ensemble Learning Scheme ausgefuhrt zum Zwecke der Erlangung des akademischen Grades eines Doktors der technischen Naturwissenschaften", "venue":"", "year":"", "window":"features which uniquely characterize the dataset. These were inspired by the <b>StatLOG</b> project (Brazdil, Gama & Henry, 1994). Space restrictions prevent us from giving exact formulas for each case, but a reference implementation is available from", "mykey":73},
 {"datasetID":111, "supportID":"082772D035779B7ECB6C4D9187573C4E4108E4DB", "rexaID":"e2b2b723df700c90e69a31a4403b740c2d2a7b2f", "author":"Alexander K. Seewald", "title":"Dissertation Towards Understanding Stacking Studies of a General Ensemble Learning Scheme ausgefuhrt zum Zwecke der Erlangung des akademischen Grades eines Doktors der technischen Naturwissenschaften", "venue":"", "year":"", "window":"#26 Training set (8-9=CV, 7=75%, 6=62%,.. 1=25%) Hold-out accuracy Figure 6.4: Learning curves for dataset primary-tumor to <b>zoo</b>  58 Chapter 7 Towards a Theoretical Framework In this chapter, we show that the ensemble learning scheme Stacking is universal in the sense that most ensemble learning schemes", "mykey":74},
 {"datasetID":32, "supportID":"084BA61B8622B1C836C11FB7AD2D898E82386C61", "rexaID":"bcf7a3acb12b41b9b37ec937c2c9aea2d1d1c855", "author":"Juan J Rodr\u00edguez Diez and Carlos Alonso Gonz\u00e1lez and Henrik Bostr\u00f6m", "title":"Learning First Order Logic Time Series Classifiers: Rules and Boosting", "venue":"PKDD", "year":"2000", "window":"for classi#cation of time series are not easy to find [16]. For this reason we have used four artificial datasets and only one \real world\" dataset. <b>Cylinder</b>  Bell and Funnel (CBF). This is an artificial problem, introduced in [26]. The learning task is to distinguish between three classes: cylinder (c), bell", "mykey":75},
 {"datasetID":107, "supportID":"084BA61B8622B1C836C11FB7AD2D898E82386C61", "rexaID":"bcf7a3acb12b41b9b37ec937c2c9aea2d1d1c855", "author":"Juan J Rodr\u00edguez Diez and Carlos Alonso Gonz\u00e1lez and Henrik Bostr\u00f6m", "title":"Learning First Order Logic Time Series Classifiers: Rules and Boosting", "venue":"PKDD", "year":"2000", "window":"x k(t). Figure 2.b shows two examples of each class. The data used were obtained from the UCI KDD Archive [4]. It contains 100 examples of each class, with 60 points in each example. <b>Waveform</b>  This dataset was introduced by [9]. The purpouse is to distinguish between three classes, defined by the evaluation for i = 1; 2 : : : 21, of the following functions: x 1 (i) = uh 1 (i) + (1 u)h 2 (i) + #(i) x 2", "mykey":76},
 {"datasetID":108, "supportID":"084BA61B8622B1C836C11FB7AD2D898E82386C61", "rexaID":"bcf7a3acb12b41b9b37ec937c2c9aea2d1d1c855", "author":"Juan J Rodr\u00edguez Diez and Carlos Alonso Gonz\u00e1lez and Henrik Bostr\u00f6m", "title":"Learning First Order Logic Time Series Classifiers: Rules and Boosting", "venue":"PKDD", "year":"2000", "window":"x k(t). Figure 2.b shows two examples of each class. The data used were obtained from the UCI KDD Archive [4]. It contains 100 examples of each class, with 60 points in each example. <b>Waveform</b>  This dataset was introduced by [9]. The purpouse is to distinguish between three classes, defined by the evaluation for i = 1; 2 : : : 21, of the following functions: x 1 (i) = uh 1 (i) + (1 u)h 2 (i) + #(i) x 2", "mykey":77},
 {"datasetID":151, "supportID":"086B189A5B7FF3F466BDB748C0CF272EC1186EDB", "rexaID":"b4b7ea88c807c5c8746bf3067f13b1d07899bcc9", "author":"Andrew Watkins and Jon Timmis and Lois C. Boggess", "title":"Artificial Immune Recognition System (AIRS): An ImmuneInspired Supervised Learning Algorithm", "venue":"(abw5,jt6@kent.ac.uk) Computing Laboratory, University of Kent", "year":"", "window":"Pima diabetes data, Ionosphere data and the <b>Sonar</b> data set, all obtained from the repository at the University of California at Irvine [4]. Table II shows the performance of AIRS on these data sets when compared with other popular classifiers [12] and [13],", "mykey":78},
 {"datasetID":34, "supportID":"086B189A5B7FF3F466BDB748C0CF272EC1186EDB", "rexaID":"b4b7ea88c807c5c8746bf3067f13b1d07899bcc9", "author":"Andrew Watkins and Jon Timmis and Lois C. Boggess", "title":"Artificial Immune Recognition System (AIRS): An ImmuneInspired Supervised Learning Algorithm", "venue":"(abw5,jt6@kent.ac.uk) Computing Laboratory, University of Kent", "year":"", "window":"where classification accuracy of 98% was achieved using a k-value of 3. This seemed to bode well, and further experiments were undertaken using the Fisher Iris data set, Pima <b>diabetes</b> data, Ionosphere data and the Sonar data set, all obtained from the repository at the University of California at Irvine [4]. Table II shows the performance of AIRS on these data sets", "mykey":79},
 {"datasetID":120, "supportID":"086B189A5B7FF3F466BDB748C0CF272EC1186EDB", "rexaID":"b4b7ea88c807c5c8746bf3067f13b1d07899bcc9", "author":"Andrew Watkins and Jon Timmis and Lois C. Boggess", "title":"Artificial Immune Recognition System (AIRS): An ImmuneInspired Supervised Learning Algorithm", "venue":"(abw5,jt6@kent.ac.uk) Computing Laboratory, University of Kent", "year":"", "window":"thorough examination of the tie-breaking mechanism in the k-nn algorithm. In the course of this latter experimentation, it was found that AIRS outperforms the best reported accuracy for the <b>E</b> <b>coli</b> data set found in the UCI repository [4]. The majority of AIS techniques use the metaphor of somatic hypermutation or a\u00b1nity proportional mutation. To date, AIRS does not employ this metaphor but instead", "mykey":80},
 {"datasetID":39, "supportID":"086B189A5B7FF3F466BDB748C0CF272EC1186EDB", "rexaID":"b4b7ea88c807c5c8746bf3067f13b1d07899bcc9", "author":"Andrew Watkins and Jon Timmis and Lois C. Boggess", "title":"Artificial Immune Recognition System (AIRS): An ImmuneInspired Supervised Learning Algorithm", "venue":"(abw5,jt6@kent.ac.uk) Computing Laboratory, University of Kent", "year":"", "window":"thorough examination of the tie-breaking mechanism in the k-nn algorithm. In the course of this latter experimentation, it was found that AIRS outperforms the best reported accuracy for the E <b>coli</b> data set found in the UCI repository [4]. The majority of AIS techniques use the metaphor of somatic hypermutation or a\u00b1nity proportional mutation. To date, AIRS does not employ this metaphor but instead", "mykey":81},
 {"datasetID":52, "supportID":"086B189A5B7FF3F466BDB748C0CF272EC1186EDB", "rexaID":"b4b7ea88c807c5c8746bf3067f13b1d07899bcc9", "author":"Andrew Watkins and Jon Timmis and Lois C. Boggess", "title":"Artificial Immune Recognition System (AIRS): An ImmuneInspired Supervised Learning Algorithm", "venue":"(abw5,jt6@kent.ac.uk) Computing Laboratory, University of Kent", "year":"", "window":"where classification accuracy of 98% was achieved using a k-value of 3. This seemed to bode well, and further experiments were undertaken using the Fisher Iris data set, Pima diabetes data, <b>Ionosphere</b> data and the Sonar data set, all obtained from the repository at the University of California at Irvine [4]. Table II shows the performance of AIRS on these data sets", "mykey":82},
 {"datasetID":53, "supportID":"086B189A5B7FF3F466BDB748C0CF272EC1186EDB", "rexaID":"b4b7ea88c807c5c8746bf3067f13b1d07899bcc9", "author":"Andrew Watkins and Jon Timmis and Lois C. Boggess", "title":"Artificial Immune Recognition System (AIRS): An ImmuneInspired Supervised Learning Algorithm", "venue":"(abw5,jt6@kent.ac.uk) Computing Laboratory, University of Kent", "year":"", "window":"where classification accuracy of 98% was achieved using a k-value of 3. This seemed to bode well, and further experiments were undertaken using the Fisher <b>Iris</b> data set, Pima diabetes data, Ionosphere data and the Sonar data set, all obtained from the repository at the University of California at Irvine [4]. Table II shows the performance of AIRS on these data sets", "mykey":83},
 {"datasetID":79, "supportID":"086B189A5B7FF3F466BDB748C0CF272EC1186EDB", "rexaID":"b4b7ea88c807c5c8746bf3067f13b1d07899bcc9", "author":"Andrew Watkins and Jon Timmis and Lois C. Boggess", "title":"Artificial Immune Recognition System (AIRS): An ImmuneInspired Supervised Learning Algorithm", "venue":"(abw5,jt6@kent.ac.uk) Computing Laboratory, University of Kent", "year":"", "window":"where classification accuracy of 98% was achieved using a k-value of 3. This seemed to bode well, and further experiments were undertaken using the Fisher Iris data set, <b>Pima</b> <b>diabetes</b> data, Ionosphere data and the Sonar data set, all obtained from the repository at the University of California at Irvine [4]. Table II shows the performance of AIRS on these data sets", "mykey":84},
 {"datasetID":14, "supportID":"08758F084F0C61926AF47778A33DC294804540EF", "rexaID":"9a75a9a8ce786d6a05ad51afa124cd4f70bfbb36", "author":"Rudy Setiono and Huan Liu", "title":"Neural-Network Feature Selector", "venue":"Department of Information Systems and Computer Science National University of Singapore", "year":"", "window":"are described below. 1. The University of Wisconsin <b>Breast</b> <b>Cancer</b> Diagnosis Dataset. The Wisconsin Breast Cancer Data (WBCD) is a large data set that consists of 699 patterns of which 458 are benign samples and 241 are malignant samples. Each of these patterns consists of nine", "mykey":85},
 {"datasetID":17, "supportID":"08758F084F0C61926AF47778A33DC294804540EF", "rexaID":"9a75a9a8ce786d6a05ad51afa124cd4f70bfbb36", "author":"Rudy Setiono and Huan Liu", "title":"Neural-Network Feature Selector", "venue":"Department of Information Systems and Computer Science National University of Singapore", "year":"", "window":"are described below. 1. The University of <b>Wisconsin</b> <b>Breast</b> <b>Cancer</b> Diagnosis Dataset. The Wisconsin Breast Cancer Data (WBCD) is a large data set that consists of 699 patterns of which 458 are benign samples and 241 are malignant samples. Each of these patterns consists of nine", "mykey":86},
 {"datasetID":15, "supportID":"08758F084F0C61926AF47778A33DC294804540EF", "rexaID":"9a75a9a8ce786d6a05ad51afa124cd4f70bfbb36", "author":"Rudy Setiono and Huan Liu", "title":"Neural-Network Feature Selector", "venue":"Department of Information Systems and Computer Science National University of Singapore", "year":"", "window":"are described below. 1. The University of <b>Wisconsin</b> <b>Breast</b> <b>Cancer</b> Diagnosis Dataset. The Wisconsin Breast Cancer Data (WBCD) is a large data set that consists of 699 patterns of which 458 are benign samples and 241 are malignant samples. Each of these patterns consists of nine", "mykey":87},
 {"datasetID":16, "supportID":"08758F084F0C61926AF47778A33DC294804540EF", "rexaID":"9a75a9a8ce786d6a05ad51afa124cd4f70bfbb36", "author":"Rudy Setiono and Huan Liu", "title":"Neural-Network Feature Selector", "venue":"Department of Information Systems and Computer Science National University of Singapore", "year":"", "window":"are described below. 1. The University of <b>Wisconsin</b> <b>Breast</b> <b>Cancer</b> Diagnosis Dataset. The Wisconsin Breast Cancer Data (WBCD) is a large data set that consists of 699 patterns of which 458 are benign samples and 241 are malignant samples. Each of these patterns consists of nine", "mykey":88},
 {"datasetID":105, "supportID":"08758F084F0C61926AF47778A33DC294804540EF", "rexaID":"9a75a9a8ce786d6a05ad51afa124cd4f70bfbb36", "author":"Rudy Setiono and Huan Liu", "title":"Neural-Network Feature Selector", "venue":"Department of Information Systems and Computer Science National University of Singapore", "year":"", "window":"1990 [17]. For our experiment, 315 samples were randomly selected for training, 35 samples were selected for cross-validation, and 349 for testing. 2. United States Congressional <b>voting records</b> Dataset. The dataset consists of the voting records of 435 congressmen on 16 major issues in the 98th Congress. The votes are classified into one of the three different types of votes: yea, nay, and", "mykey":89},
 {"datasetID":151, "supportID":"08758F084F0C61926AF47778A33DC294804540EF", "rexaID":"9a75a9a8ce786d6a05ad51afa124cd4f70bfbb36", "author":"Rudy Setiono and Huan Liu", "title":"Neural-Network Feature Selector", "venue":"Department of Information Systems and Computer Science National University of Singapore", "year":"", "window":"set consists of the remaining 384 samples. Applying the ADAP algorithm trained on 576 samples, Smith et al. [19] achieved an accuracy rate of 76 % on the remaining 192 samples. 4. <b>sonar</b> Targets Dataset. The sonar returns classification dataset [20] consists of 208 sonar returns, each of which is represented by 60 real numbers between 0.0 and 1.0. The task is to distinguish between returns from a", "mykey":90},
 {"datasetID":34, "supportID":"08758F084F0C61926AF47778A33DC294804540EF", "rexaID":"9a75a9a8ce786d6a05ad51afa124cd4f70bfbb36", "author":"Rudy Setiono and Huan Liu", "title":"Neural-Network Feature Selector", "venue":"Department of Information Systems and Computer Science National University of Singapore", "year":"", "window":"or republican. We selected 197 patterns for training randomly, 21 patterns for cross-validation, and 217 patterns for testing. Schlimmer [18] reported getting an accuracy rate of 90%-95% on this dataset. 3. Pima Indians <b>Diabetes</b> Dataset. The dataset consists of 768 samples taken from patients who may show signs of diabetes. 15 Each sample is described by 8 attributes, 1 attribute has discrete", "mykey":91},
 {"datasetID":79, "supportID":"08758F084F0C61926AF47778A33DC294804540EF", "rexaID":"9a75a9a8ce786d6a05ad51afa124cd4f70bfbb36", "author":"Rudy Setiono and Huan Liu", "title":"Neural-Network Feature Selector", "venue":"Department of Information Systems and Computer Science National University of Singapore", "year":"", "window":"or republican. We selected 197 patterns for training randomly, 21 patterns for cross-validation, and 217 patterns for testing. Schlimmer [18] reported getting an accuracy rate of 90%-95% on this dataset. 3. <b>Pima</b> <b>Indians</b> <b>Diabetes</b> Dataset. The dataset consists of 768 samples taken from patients who may show signs of diabetes. 15 Each sample is described by 8 attributes, 1 attribute has discrete", "mykey":92},
 {"datasetID":7, "supportID":"08A9B09FC745C708A203F866345105A8F76C7961", "rexaID":"530fcdd0328ad3ead3dc608a82f85bff2fd5a792", "author":"Alexander K. Seewald and Johann Petrak and Gerhard Widmer", "title":"Hybrid Decision Tree Learners with Alternative Leaf Classifiers: An Empirical Study", "venue":"FLAIRS Conference", "year":"2001", "window":"sizes of the trees generated on the complete training set. Table 2. Classification errors (%) and standard deviations for base learners: J48 with reduced error pruning, Linear, IB1 and NaiveBayes. Dataset J48-R Linear IB1 NaiveBayes <b>audiology</b> 25.44#1.87 20.93#0.98 21.90#0.56 27.79#0.65 autos 29.61#2.40 34.59#1.77 25.95#1.00 42.24#1.26 balance-scale 21.22#1.25 13.38#0.58 13.25#0.55 9.50#0.29", "mykey":93},
 {"datasetID":8, "supportID":"08A9B09FC745C708A203F866345105A8F76C7961", "rexaID":"530fcdd0328ad3ead3dc608a82f85bff2fd5a792", "author":"Alexander K. Seewald and Johann Petrak and Gerhard Widmer", "title":"Hybrid Decision Tree Learners with Alternative Leaf Classifiers: An Empirical Study", "venue":"FLAIRS Conference", "year":"2001", "window":"sizes of the trees generated on the complete training set. Table 2. Classification errors (%) and standard deviations for base learners: J48 with reduced error pruning, Linear, IB1 and NaiveBayes. Dataset J48-R Linear IB1 NaiveBayes <b>audiology</b> 25.44#1.87 20.93#0.98 21.90#0.56 27.79#0.65 autos 29.61#2.40 34.59#1.77 25.95#1.00 42.24#1.26 balance-scale 21.22#1.25 13.38#0.58 13.25#0.55 9.50#0.29", "mykey":94},
 {"datasetID":53, "supportID":"08E5909A0FF33ABB6E640D6792A0E17BAFD16EAA", "rexaID":"9ad134b043c73e8215bea14ecf8a270904b0b535", "author":"Enes Makalic and Lloyd Allison and David L. Dowe", "title":"MML INFERENCE OF SINGLE-LAYER NEURAL NETWORKS", "venue":"School of Computer Science and Software Engineering Monash University", "year":"", "window":"0.20, overfitting was observed -- MDL inferred four hidden neurons as optimal rather than three (see Fig. 4). 780 790 800 810 820 830 840 850 1 2 3 4 5 6 Message length (nits) Hidden Neurons <b>Iris</b> Dataset MML Figure 5. MML inference of the Iris dataset Finally, we have tested both MML and MDL-based criteria on a real problem: the Iris dataset from the UCI machine learning repository. This is", "mykey":95},
 {"datasetID":10, "supportID":"0900E65BB45CC80D3E4FC1A5AE3E655589EC9907", "rexaID":"e746c17201da2dd72583f7b9b0c2a6ba310412f4", "author":"Geraldine E. Rosario and Elke A. Rundensteiner and David C. Brown and Matthew O. Ward", "title":"Mapping Nominal Values to Numbers for Effective Visualization", "venue":"INFOVIS", "year":"2003", "window":"strength of association between two nominal variables)? In general, which quantification do you feel is better (easier to understand, more believable ordering and spacing)? 7.5.1 <b>Automobile</b> Data Set Case Study We chose the Automobile Data Set because it is easy to interpret. The variables we analyzed are make, fuel type, aspiration, number of doors, body type, wheels, engine location, engine", "mykey":96},
 {"datasetID":74, "supportID":"0903095816D8E6A1CBECE976A6F2BACC97BB4CFD", "rexaID":"49d2b638ade35a870fbca0a82994404cdacfa902", "author":"Zhi-Hua Zhou and Min-Ling Zhang", "title":"Ensembles of Multi-instance Learners", "venue":"ECML", "year":"2003", "window":"bags, and the number of instances contained in each bag ranges from 1 to 1,044. Detailed information on the <b>Musk</b> data is tabulated in Table 2. Ten-fold cross validation is performed on each Musk data set. In each fold, Bagging is employed to build an ensemble for each of the four base multi-instance learners, i.e. Iterated-discrim APR, Diverse Density, Citation-kNN, and EM-DD. Each ensemble", "mykey":97},
 {"datasetID":75, "supportID":"0903095816D8E6A1CBECE976A6F2BACC97BB4CFD", "rexaID":"49d2b638ade35a870fbca0a82994404cdacfa902", "author":"Zhi-Hua Zhou and Min-Ling Zhang", "title":"Ensembles of Multi-instance Learners", "venue":"ECML", "year":"2003", "window":"bags, and the number of instances contained in each bag ranges from 1 to 1,044. Detailed information on the <b>Musk</b> data is tabulated in Table 2. Ten-fold cross validation is performed on each Musk data set. In each fold, Bagging is employed to build an ensemble for each of the four base multi-instance learners, i.e. Iterated-discrim APR, Diverse Density, Citation-kNN, and EM-DD. Each ensemble", "mykey":98},
 {"datasetID":7, "supportID":"091D64F79AA96D3B413514079D26E25BC6D456DA", "rexaID":"019c2f2e77588b6e654cc5c4c27ae643d3bd0f62", "author":"Thomas G. Dietterich and Ghulum Bakiri", "title":"Solving Multiclass Learning Problems via Error-Correcting Output Codes", "venue":"CoRR, csAI/9501101", "year":"1995", "window":"as ``soybean-large'', the ``audiologyS'' data set as ` <b>audiology</b> standardized'', and the ``letter'' data set as ``letter-recognition''. 267 Dietterich & Bakiri 2.2 Learning Algorithms We employed two general classes of learning methods: algorithms", "mykey":99},
 {"datasetID":8, "supportID":"091D64F79AA96D3B413514079D26E25BC6D456DA", "rexaID":"019c2f2e77588b6e654cc5c4c27ae643d3bd0f62", "author":"Thomas G. Dietterich and Ghulum Bakiri", "title":"Solving Multiclass Learning Problems via Error-Correcting Output Codes", "venue":"CoRR, csAI/9501101", "year":"1995", "window":"as ``soybean-large'', the ``audiologyS'' data set as ` <b>audiology</b> standardized'', and the ``letter'' data set as ``letter-recognition''. 267 Dietterich & Bakiri 2.2 Learning Algorithms We employed two general classes of learning methods: algorithms", "mykey":100},
 {"datasetID":150, "supportID":"091D64F79AA96D3B413514079D26E25BC6D456DA", "rexaID":"019c2f2e77588b6e654cc5c4c27ae643d3bd0f62", "author":"Thomas G. Dietterich and Ghulum Bakiri", "title":"Solving Multiclass Learning Problems via Error-Correcting Output Codes", "venue":"CoRR, csAI/9501101", "year":"1995", "window":"employed in the study. The glass, vowel, soybean, audiologyS, ISOLET, letter, and <b>NETtalk</b> data sets are available from the Irvine Repository of machine learning databases (Murphy & Aha, 1994). 1 The POS (part of speech) data set was provided by C. Cardie (personal communication); an earlier", "mykey":101},
 {"datasetID":42, "supportID":"091D64F79AA96D3B413514079D26E25BC6D456DA", "rexaID":"019c2f2e77588b6e654cc5c4c27ae643d3bd0f62", "author":"Thomas G. Dietterich and Ghulum Bakiri", "title":"Solving Multiclass Learning Problems via Error-Correcting Output Codes", "venue":"CoRR, csAI/9501101", "year":"1995", "window":"Table 4 summarizes the data sets employed in the study. The <b>glass</b>  vowel, soybean, audiologyS, ISOLET, letter, and NETtalk data sets are available from the Irvine Repository of machine learning databases (Murphy & Aha, 1994). 1", "mykey":102},
 {"datasetID":54, "supportID":"091D64F79AA96D3B413514079D26E25BC6D456DA", "rexaID":"019c2f2e77588b6e654cc5c4c27ae643d3bd0f62", "author":"Thomas G. Dietterich and Ghulum Bakiri", "title":"Solving Multiclass Learning Problems via Error-Correcting Output Codes", "venue":"CoRR, csAI/9501101", "year":"1995", "window":"employed in the study. The glass, vowel, soybean, audiologyS, <b>ISOLET</b>  letter, and NETtalk data sets are available from the Irvine Repository of machine learning databases (Murphy & Aha, 1994). 1 The POS (part of speech) data set was provided by C. Cardie (personal communication); an earlier", "mykey":103},
 {"datasetID":59, "supportID":"091D64F79AA96D3B413514079D26E25BC6D456DA", "rexaID":"019c2f2e77588b6e654cc5c4c27ae643d3bd0f62", "author":"Thomas G. Dietterich and Ghulum Bakiri", "title":"Solving Multiclass Learning Problems via Error-Correcting Output Codes", "venue":"CoRR, csAI/9501101", "year":"1995", "window":"round, but instead performs jamming (i.e., forcing the lowest order bit to 1 when low order bits are lost due to shifting or multiplication). On the speech recognition, <b>letter recognition</b>  and vowel data sets, we employed the opt system distributed by Oregon Graduate Institute (Barnard & Cole, 1989). This implements the conjugate gradient algorithm and updates the gradient after each complete pass", "mykey":104},
 {"datasetID":90, "supportID":"091D64F79AA96D3B413514079D26E25BC6D456DA", "rexaID":"019c2f2e77588b6e654cc5c4c27ae643d3bd0f62", "author":"Thomas G. Dietterich and Ghulum Bakiri", "title":"Solving Multiclass Learning Problems via Error-Correcting Output Codes", "venue":"CoRR, csAI/9501101", "year":"1995", "window":"employed in the study. The glass, vowel, <b>soybean</b>  audiologyS, ISOLET, letter, and NETtalk data sets are available from the Irvine Repository of machine learning databases (Murphy & Aha, 1994). 1 The POS (part of speech) data set was provided by C. Cardie (personal communication); an earlier", "mykey":105},
 {"datasetID":91, "supportID":"091D64F79AA96D3B413514079D26E25BC6D456DA", "rexaID":"019c2f2e77588b6e654cc5c4c27ae643d3bd0f62", "author":"Thomas G. Dietterich and Ghulum Bakiri", "title":"Solving Multiclass Learning Problems via Error-Correcting Output Codes", "venue":"CoRR, csAI/9501101", "year":"1995", "window":"employed in the study. The glass, vowel, <b>soybean</b>  audiologyS, ISOLET, letter, and NETtalk data sets are available from the Irvine Repository of machine learning databases (Murphy & Aha, 1994). 1 The POS (part of speech) data set was provided by C. Cardie (personal communication); an earlier", "mykey":106},
 {"datasetID":151, "supportID":"093083A5664DE02C0648062E1AF92669D268BF45", "rexaID":"d8bc3a2d334e33553c2c7e052ce8c2fd9ae5718b", "author":"Marina Skurichina and Ludmila Kuncheva and Robert P W Duin", "title":"Bagging and Boosting for the Nearest Mean Classifier: Effects of Sample Size on Diversity and Accuracy", "venue":"Multiple Classifier Systems", "year":"2002", "window":"the 34-dimensional ionosphere data set and the 60-dimensional <b>sonar</b> data set. Training sets are chosen randomly and the remaining data are used for testing. All experiments are repeated 50 times on independent training sets. So all the", "mykey":107},
 {"datasetID":34, "supportID":"093083A5664DE02C0648062E1AF92669D268BF45", "rexaID":"d8bc3a2d334e33553c2c7e052ce8c2fd9ae5718b", "author":"Marina Skurichina and Ludmila Kuncheva and Robert P W Duin", "title":"Bagging and Boosting for the Nearest Mean Classifier: Effects of Sample Size on Diversity and Accuracy", "venue":"Multiple Classifier Systems", "year":"2002", "window":"are taken from the UCI Repository [22]. They are the 8dimensional pima <b>diabetes</b> data set, the 34-dimensional ionosphere data set and the 60-dimensional sonar data set. Training sets are chosen randomly and the remaining data are used for testing. All experiments are repeated 50 times on", "mykey":108},
 {"datasetID":52, "supportID":"093083A5664DE02C0648062E1AF92669D268BF45", "rexaID":"d8bc3a2d334e33553c2c7e052ce8c2fd9ae5718b", "author":"Marina Skurichina and Ludmila Kuncheva and Robert P W Duin", "title":"Bagging and Boosting for the Nearest Mean Classifier: Effects of Sample Size on Diversity and Accuracy", "venue":"Multiple Classifier Systems", "year":"2002", "window":"are taken from the UCI Repository [22]. They are the 8dimensional pima-diabetes data set, the 34-dimensional <b>ionosphere</b> data set and the 60-dimensional sonar data set. Training sets are chosen randomly and the remaining data are used for testing. All experiments are repeated 50 times on", "mykey":109},
 {"datasetID":79, "supportID":"093083A5664DE02C0648062E1AF92669D268BF45", "rexaID":"d8bc3a2d334e33553c2c7e052ce8c2fd9ae5718b", "author":"Marina Skurichina and Ludmila Kuncheva and Robert P W Duin", "title":"Bagging and Boosting for the Nearest Mean Classifier: Effects of Sample Size on Diversity and Accuracy", "venue":"Multiple Classifier Systems", "year":"2002", "window":"are taken from the UCI Repository [22]. They are the 8dimensional <b>pima</b> <b>diabetes</b> data set, the 34-dimensional ionosphere data set and the 60-dimensional sonar data set. Training sets are chosen randomly and the remaining data are used for testing. All experiments are repeated 50 times on", "mykey":110},
 {"datasetID":40, "supportID":"097855A85DDB73849B9AFA38CF76BF2DDA9D0F06", "rexaID":"d4ea16b5ea06d216f7d2be28615fa9c66b98bbea", "author":"George H. John and Ron Kohavi and Karl Pfleger", "title":"Irrelevant Features and the Subset Selection Problem", "venue":"ICML", "year":"1994", "window":"The C4.5 program is the program that comes with Quinlan's book (Quinlan 1992); the ID3 results were obtained by running C4.5 and using the unpruned trees. On the artificial datasets, we used the ``-s -m1'' C4.5 <b>flags</b>  which indicate that subset splits may be used and that splitting should continue until purity. To estimate the accuracy for feature subsets, we used 25-fold", "mykey":111},
 {"datasetID":53, "supportID":"097855A85DDB73849B9AFA38CF76BF2DDA9D0F06", "rexaID":"d4ea16b5ea06d216f7d2be28615fa9c66b98bbea", "author":"George H. John and Ron Kohavi and Karl Pfleger", "title":"Irrelevant Features and the Subset Selection Problem", "venue":"ICML", "year":"1994", "window":"performance was on parity5+5 and CorrAL using stepwise backward elimination, which reduced the error to 0% from 50% and 18.8% respectively. Experiments were also run on the <b>Iris</b>  Thyroid, and Monk1* datasets. The results on these datasets were similar to those reported in this paper. We observed high variance in the 25-fold crossvalidation estimates of the error. Since our algorithms depend on", "mykey":112},
 {"datasetID":56, "supportID":"097855A85DDB73849B9AFA38CF76BF2DDA9D0F06", "rexaID":"d4ea16b5ea06d216f7d2be28615fa9c66b98bbea", "author":"George H. John and Ron Kohavi and Karl Pfleger", "title":"Irrelevant Features and the Subset Selection Problem", "venue":"ICML", "year":"1994", "window":"was divided by Quinlan into 490 training instances and 200 test instances. <b>Labor</b> The dataset contains instances for acceptable and unacceptable contracts. It is a small dataset with 16 features, a training set of 40 instances, and a test set of 17 instances. Our results show that the main", "mykey":113},
 {"datasetID":102, "supportID":"097855A85DDB73849B9AFA38CF76BF2DDA9D0F06", "rexaID":"d4ea16b5ea06d216f7d2be28615fa9c66b98bbea", "author":"George H. John and Ron Kohavi and Karl Pfleger", "title":"Irrelevant Features and the Subset Selection Problem", "venue":"ICML", "year":"1994", "window":"performance was on parity5+5 and CorrAL using stepwise backward elimination, which reduced the error to 0% from 50% and 18.8% respectively. Experiments were also run on the Iris, <b>Thyroid</b>  and Monk1* datasets. The results on these datasets were similar to those reported in this paper. We observed high variance in the 25-fold crossvalidation estimates of the error. Since our algorithms depend on", "mykey":114},
 {"datasetID":42, "supportID":"09CF46C3DD847FF00393B67945EB9BD0773DBF22", "rexaID":"abe4b819b1c9858440502f14d418706d2446c387", "author":"Giorgio Valentini and Francesco Masulli", "title":"NEURObjects: an object-oriented library for neural network development", "venue":"Neurocomputing, 48", "year":"2002", "window":"validation [35]. The folds can be prepared using the program dofold in a simple way: dofold <b>glass</b> data -nf 10 -na 9 -name glass This command build the folds for a ten fold cross validation test. The data set is glass from the UCI Machine Learning repository [29]. Ten folds from the data file glass.data (named glass.#.train and glass.#.test varying i from 1 to 10) are extracted (the option -na specifies", "mykey":115},
 {"datasetID":2, "supportID":"0AA72445CF8CCDE0888944C44273F955FEB38357", "rexaID":"46a760d333f90e12eb326d2b17c90aa70c01d1df", "author":"John C. Platt", "title":"Using Analytic QP and Sparseness to Speed Training of Support Vector Machines", "venue":"NIPS", "year":"1998", "window":"can be found in [8, 7]. The first test set is the UCI <b>Adult</b> data set [5]. The SVM is given 14 attributes of a census form of a household and asked to predict whether that household has an income greater than $50,000. Out of the 14 attributes, eight are categorical", "mykey":116},
 {"datasetID":20, "supportID":"0AA72445CF8CCDE0888944C44273F955FEB38357", "rexaID":"46a760d333f90e12eb326d2b17c90aa70c01d1df", "author":"John C. Platt", "title":"Using Analytic QP and Sparseness to Speed Training of Support Vector Machines", "venue":"NIPS", "year":"1998", "window":"can be found in [8, 7]. The first test set is the UCI Adult data set [5]. The SVM is given 14 attributes of a <b>census</b> form of a household and asked to predict whether that household has an income greater than $50,000. Out of the 14 attributes, eight are categorical", "mykey":117},
 {"datasetID":151, "supportID":"0AC32B90D1BDE0AFF7CC7BEEF2A6B37847D956D9", "rexaID":"80c67287e3ef7fe9828d8c898a3eacb7f1f4ebac", "author":"Ronaldo C. Prati and Peter A. Flach", "title":"ROCCER: A ROC convex hull rule learning algorithm", "venue":"Institute of Mathematics and Computer Science at University of So Paulo", "year":"", "window":"# attributes # examples class distribution German 20 1000 bad (30%), good (70%) Pima 8 768 1 (34,8%), 0 (65,2%) <b>Sonar</b> 61 208 mine (46,6%), rock (53,4%) Table 1. Datasets summary. These datasets were chosen because classifiers using standard machine learning algorithms for those datasets are reported in the literature as having AUC values lower than 80%. As", "mykey":118},
 {"datasetID":45, "supportID":"0AF62FC29899C1B89A36ACA3D83C8B06709DF325", "rexaID":"7690bf25f9378bae80e6412c3eb149cd3f7ebdc6", "author":"Rudy Setiono and Wee Kheng Leow", "title":"FERNN: An Algorithm for Fast Extraction of Rules from Neural Networks", "venue":"Appl. Intell, 12", "year":"2000", "window":"decision nodes may also improve the accuracy of the tree because samples from real world problems may be better separated by oblique hyperplanes. This is the case with the <b>heart</b> disease data set (HeartD in Table 2) where significant improvement is achieved by the neural network methods over C4.5. There is no significant difference in the accuracy and size of the decision trees generated by", "mykey":119},
 {"datasetID":34, "supportID":"0AF9BBB399027FD905DBF49D122F9DB486CE2434", "rexaID":"96246754b7958de8a635165e8b145c264240cd3d", "author":"Huan Liu and Rudy Setiono", "title":"Feature Transformation and Multivariate Decision Tree Induction", "venue":"Discovery Science", "year":"1998", "window":"those of OC1's, in which two of the OC1's trees are smaller. In 9 cases, trees by BMDT are significantly different from those of CART's, in which only one of CART's trees is smaller. An example: The dataset is Pima <b>diabetes</b>  In Table 3, it is seen that C4.5 creates a UDT with average tree size of 122.4 nodes, BMDT builds an MDT with average tree size of 3 nodes. That means the MDT has one root and two", "mykey":120},
 {"datasetID":48, "supportID":"0AF9BBB399027FD905DBF49D122F9DB486CE2434", "rexaID":"96246754b7958de8a635165e8b145c264240cd3d", "author":"Huan Liu and Rudy Setiono", "title":"Feature Transformation and Multivariate Decision Tree Induction", "venue":"Discovery Science", "year":"1998", "window":"351 34 continuous 11 Iris 150 4 continuous 12 Pima-diabetes 768 8 continuous 13 Sonar 208 60 continuous 14 Australian 690 14 mixed 15 HeartDisease 297 13 mixed 16 <b>Housing</b> 506 13 mixed Table 1. Dataset Summary. #Data - data size, Type - attribute type, and #A - number of attributes. neural networks (NN) based on which BMDT builds MDT's. We want to understand whether the difference between every", "mykey":121},
 {"datasetID":79, "supportID":"0AF9BBB399027FD905DBF49D122F9DB486CE2434", "rexaID":"96246754b7958de8a635165e8b145c264240cd3d", "author":"Huan Liu and Rudy Setiono", "title":"Feature Transformation and Multivariate Decision Tree Induction", "venue":"Discovery Science", "year":"1998", "window":"those of OC1's, in which two of the OC1's trees are smaller. In 9 cases, trees by BMDT are significantly different from those of CART's, in which only one of CART's trees is smaller. An example: The dataset is <b>Pima</b> <b>diabetes</b>  In Table 3, it is seen that C4.5 creates a UDT with average tree size of 122.4 nodes, BMDT builds an MDT with average tree size of 3 nodes. That means the MDT has one root and two", "mykey":122},
 {"datasetID":152, "supportID":"0AFCE6FDD485F3847D9541C716A903945DB40B61", "rexaID":"61f13a0ee6d86b2c3576af921d81a77e5d82809e", "author":"M. Layton and M. J. F Gales", "title":"CAMBRIDGE UNIVERSITY ENGINEERING DEPARTMENT Maximum Margin Training of Generative Kernels", "venue":"CUED", "year":"2004", "window":"the number of iterations required for convergence increased significantly (34 iterations were required for C = 10, whereas 1,084 were required for C = 0.1). 15 5.2 <b>Deterding</b> data The Deterding dataset [6] consists of the steady-state portion of 11 vowels in British English, spoken in the context of h*d. The recorded speech samples were low-pass filtered at 4.7KHz before being digitised at 10KHz", "mykey":123},
 {"datasetID":151, "supportID":"0B3E886EE7E63492903EFE5385D93B5B26DCCBB2", "rexaID":"e6315f170eea3cc52ade688d0abb40f3372ec35d", "author":"Zhi-Hua Zhou and Yuan Jiang", "title":"NeC4.5: Neural Ensemble Based C4.5", "venue":"IEEE Trans. Knowl. Data Eng, 16", "year":"2004", "window":"ensemble. Moreover, Table III shows that the generalization ability of NeC4.5 with \u00b5 = 0% is still better than that of C4.5. In detail, pairwise two-tailed t-tests indicate that there are seven data sets (cleveland, diabetes, ionosphere, liver, <b>sonar</b>  waveform21, and waveform40) where NeC4.5 with \u00b5 = 0% is significantly more accurate than C4.5, while there is no significant difference on the", "mykey":124},
 {"datasetID":34, "supportID":"0B3E886EE7E63492903EFE5385D93B5B26DCCBB2", "rexaID":"e6315f170eea3cc52ade688d0abb40f3372ec35d", "author":"Zhi-Hua Zhou and Yuan Jiang", "title":"NeC4.5: Neural Ensemble Based C4.5", "venue":"IEEE Trans. Knowl. Data Eng, 16", "year":"2004", "window":"ensemble. Moreover, Table III shows that the generalization ability of NeC4.5 with \u00b5 = 0% is still better than that of C4.5. In detail, pairwise two-tailed t-tests indicate that there are seven data sets (cleveland, <b>diabetes</b>  ionosphere, liver, sonar, waveform21, and waveform40) where NeC4.5 with \u00b5 = 0% is significantly more accurate than C4.5, while there is no significant difference on the", "mykey":125},
 {"datasetID":45, "supportID":"0B3E886EE7E63492903EFE5385D93B5B26DCCBB2", "rexaID":"e6315f170eea3cc52ade688d0abb40f3372ec35d", "author":"Zhi-Hua Zhou and Yuan Jiang", "title":"NeC4.5: Neural Ensemble Based C4.5", "venue":"IEEE Trans. Knowl. Data Eng, 16", "year":"2004", "window":"are tabulated in Table III. Table III shows that the generalization ability of NeC4.5 with \u00b5 = 100% is better than that of C4.5. In detail, pairwise two-tailed t-tests indicate that there are ten data sets (balance, breast, cleveland, credit, <b>heart</b>  iris, vehicle, waveform21, waveform40, and wine) where NeC4.5 with \u00b5 = 100% is significantly more accurate than C4.5, while there is no significant", "mykey":126},
 {"datasetID":52, "supportID":"0B3E886EE7E63492903EFE5385D93B5B26DCCBB2", "rexaID":"e6315f170eea3cc52ade688d0abb40f3372ec35d", "author":"Zhi-Hua Zhou and Yuan Jiang", "title":"NeC4.5: Neural Ensemble Based C4.5", "venue":"IEEE Trans. Knowl. Data Eng, 16", "year":"2004", "window":"ensemble. Moreover, Table III shows that the generalization ability of NeC4.5 with \u00b5 = 0% is still better than that of C4.5. In detail, pairwise two-tailed t-tests indicate that there are seven data sets (cleveland, diabetes, <b>ionosphere</b>  liver, sonar, waveform21, and waveform40) where NeC4.5 with \u00b5 = 0% is significantly more accurate than C4.5, while there is no significant difference on the", "mykey":127},
 {"datasetID":60, "supportID":"0B3E886EE7E63492903EFE5385D93B5B26DCCBB2", "rexaID":"e6315f170eea3cc52ade688d0abb40f3372ec35d", "author":"Zhi-Hua Zhou and Yuan Jiang", "title":"NeC4.5: Neural Ensemble Based C4.5", "venue":"IEEE Trans. Knowl. Data Eng, 16", "year":"2004", "window":"ensemble. Moreover, Table III shows that the generalization ability of NeC4.5 with \u00b5 = 0% is still better than that of C4.5. In detail, pairwise two-tailed t-tests indicate that there are seven data sets (cleveland, diabetes, ionosphere, <b>liver</b>  sonar, waveform21, and waveform40) where NeC4.5 with \u00b5 = 0% is significantly more accurate than C4.5, while there is no significant difference on the", "mykey":128},
 {"datasetID":102, "supportID":"0B3E886EE7E63492903EFE5385D93B5B26DCCBB2", "rexaID":"e6315f170eea3cc52ade688d0abb40f3372ec35d", "author":"Zhi-Hua Zhou and Yuan Jiang", "title":"NeC4.5: Neural Ensemble Based C4.5", "venue":"IEEE Trans. Knowl. Data Eng, 16", "year":"2004", "window":"To explore this issue, further experiments are performed on data sets (australian, page, <b>thyroid</b>  voting, wdbc, wpbc) where neither NeC4.5 with \u00b5 = 100% nor NeC4.5 with \u00b5 = 0% is significantly more accurate than C4.5. The results are depicted in Fig.s 1 and 2. Note", "mykey":129},
 {"datasetID":1, "supportID":"0C11D0228D5FEA3664578A7F58126D4CAC197C3A", "rexaID":"ac35cdc4f306058543fb3fbf817fc84d85f89288", "author":"Johannes Furnkranz", "title":"Round Robin Rule Learning", "venue":"Austrian Research Institute for Artificial Intelligence", "year":"", "window":"9 Classification time is only included in the runs that had a separate test set. In general, it can be expected to be more expensive for R 3 . See Section 7 for a brief discussion of this issue. 12 dataset C5 -b vs. C5 R 3 vs. unord vs. order <b>abalone</b> 23.34 10.81 193.0 4.51 5.73 covertype --- --- --- ---- ---- letter 73.37 6.64 1250.0 0.51 1.14 sat 27.86 9.10 143.0 0.85 1.51 shuttle 35.98 5.67 277.0", "mykey":130},
 {"datasetID":31, "supportID":"0C11D0228D5FEA3664578A7F58126D4CAC197C3A", "rexaID":"ac35cdc4f306058543fb3fbf817fc84d85f89288", "author":"Johannes Furnkranz", "title":"Round Robin Rule Learning", "venue":"Austrian Research Institute for Artificial Intelligence", "year":"", "window":"6) Ripper. The first five lines are total run-times, i.e., training and test time, while the cross-validated results report training time only. We failed to measure the run-times for the <b>covertype</b> data set, where the situation was complicated because of the large test set, which had to be split into several pieces for the Ripper-based algorithms. The last line shows the average of the 17", "mykey":131},
 {"datasetID":110, "supportID":"0C11D0228D5FEA3664578A7F58126D4CAC197C3A", "rexaID":"ac35cdc4f306058543fb3fbf817fc84d85f89288", "author":"Johannes Furnkranz", "title":"Round Robin Rule Learning", "venue":"Austrian Research Institute for Artificial Intelligence", "year":"", "window":"683 --- 35 0 19 94.1 thyroid (hyper) 3772 --- 21 6 5 2.7 thyroid (hypo) 3772 --- 21 6 5 7.7 thyroid (repl.) 3772 --- 21 6 4 3.3 vehicle 846 --- 0 18 4 74.2 <b>yeast</b> 1484 --- 0 8 10 68.8 Table 1: Data sets used. The first two columns show the training and test set sizes (as specified in the description of the datasets), the next three columns show the number of symbolic and numeric attributes as well", "mykey":132},
 {"datasetID":1, "supportID":"0C543EE2403E5E986309215F48B47EF69BF46806", "rexaID":"abf0c261ab7aa462cd5cbc290cb7963cdf4f9ee8", "author":"Miguel Moreira and Alain Hertz and Eddy Mayoraz", "title":"Data binarization by discriminant elimination", "venue":"Proceedings of the ICML-99 Workshop: From Machine Learning to", "year":"", "window":"The bottom row shows the number of wins, including ties. Initial Final set size set n.con#icts entropy random choice data set size local global local global avg. min. <b>abalone</b> 5779 192 177 724 220 282.6 #21.6 240 allhyper 440 21 33 36 34 30.4 #4.3 22 allhypo 548 20 32 43 48 35.2 #6.2 17 anneal 134 31 31 30 30 30.9 #2.1 28", "mykey":133},
 {"datasetID":59, "supportID":"0C543EE2403E5E986309215F48B47EF69BF46806", "rexaID":"abf0c261ab7aa462cd5cbc290cb7963cdf4f9ee8", "author":"Miguel Moreira and Alain Hertz and Eddy Mayoraz", "title":"Data binarization by discriminant elimination", "venue":"Proceedings of the ICML-99 Workshop: From Machine Learning to", "year":"", "window":"the end, that is, to test the redundancy of all the discriminants in the initial set. Figure 3 shows the evolution of the elimination process with the conflict-based weighting method for four of the data sets. In the case of <b>Letter Recognition</b>  for example, the algorithm starts with 234 discriminants and takes almost 3500 seconds to find a final set with 59. However, after 1000 sec. only 69", "mykey":134},
 {"datasetID":90, "supportID":"0C543EE2403E5E986309215F48B47EF69BF46806", "rexaID":"abf0c261ab7aa462cd5cbc290cb7963cdf4f9ee8", "author":"Miguel Moreira and Alain Hertz and Eddy Mayoraz", "title":"Data binarization by discriminant elimination", "venue":"Proceedings of the ICML-99 Workshop: From Machine Learning to", "year":"", "window":"that Ideal is eliminative, while SG is constructive: SG is executed until consistency is attained, while in Ideal consistency is never lost, if verified initially. (This may not be the case for datasets with missing values, e.g. <b>soybean</b> ) The other, very important aspect is that Ideal is heavily grounded on the original attributes. Concerning the comparison between Ideal, SG, and decision trees,", "mykey":135},
 {"datasetID":91, "supportID":"0C543EE2403E5E986309215F48B47EF69BF46806", "rexaID":"abf0c261ab7aa462cd5cbc290cb7963cdf4f9ee8", "author":"Miguel Moreira and Alain Hertz and Eddy Mayoraz", "title":"Data binarization by discriminant elimination", "venue":"Proceedings of the ICML-99 Workshop: From Machine Learning to", "year":"", "window":"that Ideal is eliminative, while SG is constructive: SG is executed until consistency is attained, while in Ideal consistency is never lost, if verified initially. (This may not be the case for datasets with missing values, e.g. <b>soybean</b> ) The other, very important aspect is that Ideal is heavily grounded on the original attributes. Concerning the comparison between Ideal, SG, and decision trees,", "mykey":136},
 {"datasetID":2, "supportID":"0D1F16A20A2E7C102366784CCCFE8FFE8111FF7C", "rexaID":"8e003bfae7c07cdeeb0ae5f251d545873364dd1e", "author":"David R. Musicant and Alexander Feinberg", "title":"Active Set Support Vector Regression", "venue":"", "year":"", "window":"Census 30k, is a version of the US Census Bureau  <b>Adult</b>  dataset, which is publicly available from Silicon Graphics' website [39]. This \"Adult\" dataset contains nearly 300,000 data points with 11 numeric attributes, and is used for predicting income levels based", "mykey":137},
 {"datasetID":20, "supportID":"0D1F16A20A2E7C102366784CCCFE8FFE8111FF7C", "rexaID":"8e003bfae7c07cdeeb0ae5f251d545873364dd1e", "author":"David R. Musicant and Alexander Feinberg", "title":"Active Set Support Vector Regression", "venue":"", "year":"", "window":"problems. It contains 506 data points with 12 numeric attributes, and one binary categorical attribute. The goal is to determine median home values, based on various <b>census</b> attributes. This dataset is available at the UCI repository [37]. The second dataset, Comp-Activ, was obtained from the Delve website [38]. This dataset contains 8192 data points and 25 numeric attributes. We implemented", "mykey":138},
 {"datasetID":48, "supportID":"0D1F16A20A2E7C102366784CCCFE8FFE8111FF7C", "rexaID":"8e003bfae7c07cdeeb0ae5f251d545873364dd1e", "author":"David R. Musicant and Alexander Feinberg", "title":"Active Set Support Vector Regression", "venue":"", "year":"", "window":"were used for the first round of experiments. The first dataset, Boston <b>Housing</b>  is a fairly standard dataset used for testing regression problems. It contains 506 data points with 12 numeric attributes, and one binary categorical attribute. The goal is to", "mykey":139},
 {"datasetID":116, "supportID":"0D1F16A20A2E7C102366784CCCFE8FFE8111FF7C", "rexaID":"8e003bfae7c07cdeeb0ae5f251d545873364dd1e", "author":"David R. Musicant and Alexander Feinberg", "title":"Active Set Support Vector Regression", "venue":"", "year":"", "window":"attributes. We implemented the \"cpu prototask\", which involves using 21 of these attributes to predict what fraction of a CPU's processing time is devoted to a specific mode (\"user mode\"). The third dataset, Census 30k, is a version of the <b>US Census</b> Bureau \"Adult\" dataset, which is publicly available from Silicon Graphics' website [39]. This \"Adult\" dataset contains nearly 300,000 data points with 11", "mykey":140},
 {"datasetID":50, "supportID":"0D709C7F1575F29D5583948922D082D87B4B312B", "rexaID":"06d7cfa086a7ed2a236674fa2ea7271dfacf9a54", "author":"K. A. J Doherty and Rolf Adams and Neil Davey", "title":"Non-Euclidean Norms and Data Normalisation", "venue":"Department of Computer Science, University of Hertfordshire, College Lane", "year":"", "window":"We repeated the empirical test of NN search using both fractional and higher-order L r norms in [1] with the Ionosphere, Wisconsin Diagnostic Breast Cancer (WDBC), and <b>Image Segmentation</b> labelled data sets from the UCI Machine Learning Repository [5]. We report only a single representative result set. Table I shows the results of the nearest neighbour search on the raw Image Segmentation data set,", "mykey":141},
 {"datasetID":147, "supportID":"0D709C7F1575F29D5583948922D082D87B4B312B", "rexaID":"06d7cfa086a7ed2a236674fa2ea7271dfacf9a54", "author":"K. A. J Doherty and Rolf Adams and Neil Davey", "title":"Non-Euclidean Norms and Data Normalisation", "venue":"Department of Computer Science, University of Hertfordshire, College Lane", "year":"", "window":"We repeated the empirical test of NN search using both fractional and higher-order L r norms in [1] with the Ionosphere, Wisconsin Diagnostic Breast Cancer (WDBC), and <b>Image Segmentation</b> labelled data sets from the UCI Machine Learning Repository [5]. We report only a single representative result set. Table I shows the results of the nearest neighbour search on the raw Image Segmentation data set,", "mykey":142},
 {"datasetID":45, "supportID":"0DCCEDA9B9DC75A2AD584A8F81ED07BEE8E24E9B", "rexaID":"10467f2e9d4c1e175205a0dc36a18f1915286d21", "author":"Bruce H. Edmonds", "title":"Using Localised `Gossip' to Structure Distributed Learning", "venue":"Centre for Policy Modelling", "year":"", "window":"in which they are found to work. This approach is compared to the equivalent global evolutionary computation approach with respect to predicting the occurrence of <b>heart</b> disease in the Cleveland data set. It outperforms a global approach, but the space of attributes within which this evolutionary process occurs can greatly effect the efficiency of the technique. 1. Introduction The idea here is to", "mykey":143},
 {"datasetID":1, "supportID":"0E5715775ED4C7FCA23ADA1814AF77075D0C17E0", "rexaID":"e7628956b46a78e64774af25d99c0afd0cf90001", "author":"Johannes Furnkranz", "title":"Pairwise Classification as an Ensemble Technique", "venue":"Austrian Research Institute for Artificial Intelligence", "year":"", "window":"with c5.0 as a base learner. For these, we give both the absolute error rate and the performance ratio relative to the base learner c5.0. The last line shows the geometric average of these ratios. dataset c5.0 round robin boosting both <b>abalone</b> 78.48 75.08 0.957 77.88 0.992 74.67 0.951 car 7.58 5.84 0.771 3.82 0.504 1.85 0.244 glass 35.05 24.77 0.707 27.57 0.787 22.90 0.653 image 3.20 2.90 0.905 1.60", "mykey":144},
 {"datasetID":9, "supportID":"0E5715775ED4C7FCA23ADA1814AF77075D0C17E0", "rexaID":"e7628956b46a78e64774af25d99c0afd0cf90001", "author":"Johannes Furnkranz", "title":"Pairwise Classification as an Ensemble Technique", "venue":"Austrian Research Institute for Artificial Intelligence", "year":"", "window":"Four of the datasets (Pole Telecom, MV Artificial, <b>Auto</b> <b>MPG</b>  and Triazines) seem to be completely unamenable to pairwise classification, i.w., j48 performs better in all three classification settings. This, however,", "mykey":145},
 {"datasetID":2, "supportID":"0ECCC4496649F2C86A6D836C3CED3E36428AF5E1", "rexaID":"0767f0171b0f74f96978a41f3e947ea91cc9dbd3", "author":"Alexander J. Smola and Vishy Vishwanathan and Eleazar Eskin", "title":"Laplace Propagation", "venue":"NIPS", "year":"2003", "window":"X i h# i , #i, (17) with the joint minimizer being the average of the individual solutions. 5 Experiments To test our ideas we performed a set of experiments with the widely available Web and <b>Adult</b> datasets from the UCI repository [1]. All experiments were performed on a 2.4 MHz Intel Xeon machine with 1 GB RAM using MATLAB R13. We used a RBF kernel with # 2 = 10 [8], to obtain comparable results. We", "mykey":146},
 {"datasetID":144, "supportID":"0F0EB11BBB812C9BF25113DA1684F76B565C51DB", "rexaID":"7be023799304644bda7d6afa9f31043a1f31511c", "author":"Paul O' Dea and David Griffith and Colm O' Riordan", "title":"DEPARTMENT OF INFORMATION TECHNOLOGY", "venue":"P. O'Dea (NUI", "year":"", "window":"network (as described earlier), using back-propagation as a learning algorithm, is used to classify the tuples t 1 : : : t n based on the attributes s 1 : : : s n . 5 Results 5.1 The <b>German Credit</b> Data Set In order to facilitate testing of the developed approach, experiments were conducted using the german credit data set 1 . The german credit data set contains information on 1000 loan applicants.", "mykey":147},
 {"datasetID":80, "supportID":"0F5F56C26AD2F3C07049B88FF74ED4C5FA7B2C12", "rexaID":"b60b36ac9d879270bfd990b761897f1f168612b7", "author":"Claudio Gentile", "title":"A New Approximate Maximal Margin Classification Algorithm", "venue":"NIPS", "year":"2000", "window":"The 214 Approximate Maximal Margin Classification real-world datasets are well-known <b>Optical</b> Character <b>Recognition</b> (OCR) benchmarks. On these datasets we followed the experimental setting described by Cortes and Vapnik (1995), Freund and Schapire (1999), Li and Long", "mykey":148},
 {"datasetID":45, "supportID":"100D35E18A98D1714A33B97ECC2DF0C94847D9A8", "rexaID":"e876da6abdabda18a2ba345cc2b24f8053be51c9", "author":"D. Randall Wilson and Roel Martinez", "title":"Machine Learning: Proceedings of the Fourteenth International Conference, Morgan", "venue":"In Fisher", "year":"1997", "window":"seem to be especially well suited for these reduction techniques. For example, RT3 required less than 2% storage for the <b>Heart</b> Swiss dataset, yet it achieved even higher generalization accuracy than the kNN algorithm. On the other hand, some datasets were not so appropriate. On the Vowel dataset, for example, RT3 required over 45% of the", "mykey":149},
 {"datasetID":20, "supportID":"1039FE5FA3BEA62639A190437F84DA4FF4AA8235", "rexaID":"ba8db8dddfba95ce7d20f5376a39c4ff6ed38d0c", "author":"Dennis P. Groth and Edward L. Robertson", "title":"An Entropy-based Approach to Visualizing Database Structure", "venue":"VDB", "year":"2002", "window":"The data we use in our visualization is drawn from a variety of sources, including the U.S. <b>Census</b> [1], the U.C.I. Machine Learning Repository [2], and the Wisconsin Benchmark [7]. The specific dataset we used for the Census was the 1990 Indiana Public Use Microdata Sample (PUMS), which has 125 attributes. Our first application is the visualization of frequency distributions. An obvious technique", "mykey":150},
 {"datasetID":7, "supportID":"104A2E37BB70A9717D6E0AA36637C3E198E7BD71", "rexaID":"692880d7b3356df64bfa0f06a683f89e4ce6955b", "author":"Vassilis Athitsos and Stan Sclaroff", "title":"Boosting Nearest Neighbor Classifiers for Multiclass Recognition", "venue":"Boston University Computer Science Tech. Report No, 2004-006", "year":"2004", "window":"We did not use four datasets (dermatology,soybean, thyroid, <b>audiology</b>  because they have missing attributes, which our current formulation cannot handle. One dataset (ecoli) contains a nominal attribute, whichour current", "mykey":151},
 {"datasetID":8, "supportID":"104A2E37BB70A9717D6E0AA36637C3E198E7BD71", "rexaID":"692880d7b3356df64bfa0f06a683f89e4ce6955b", "author":"Vassilis Athitsos and Stan Sclaroff", "title":"Boosting Nearest Neighbor Classifiers for Multiclass Recognition", "venue":"Boston University Computer Science Tech. Report No, 2004-006", "year":"2004", "window":"We did not use four datasets (dermatology,soybean, thyroid, <b>audiology</b>  because they have missing attributes, which our current formulation cannot handle. One dataset (ecoli) contains a nominal attribute, whichour current", "mykey":152},
 {"datasetID":33, "supportID":"104A2E37BB70A9717D6E0AA36637C3E198E7BD71", "rexaID":"692880d7b3356df64bfa0f06a683f89e4ce6955b", "author":"Vassilis Athitsos and Stan Sclaroff", "title":"Boosting Nearest Neighbor Classifiers for Multiclass Recognition", "venue":"Boston University Computer Science Tech. Report No, 2004-006", "year":"2004", "window":"We ended up using only eight of those datasets. We did not use four datasets  <b>dermatology</b> soybean, thyroid, audiology) because they have missing attributes, which our current formulation cannot handle. One dataset (ecoli) contains a nominal", "mykey":153},
 {"datasetID":39, "supportID":"104A2E37BB70A9717D6E0AA36637C3E198E7BD71", "rexaID":"692880d7b3356df64bfa0f06a683f89e4ce6955b", "author":"Vassilis Athitsos and Stan Sclaroff", "title":"Boosting Nearest Neighbor Classifiers for Multiclass Recognition", "venue":"Boston University Computer Science Tech. Report No, 2004-006", "year":"2004", "window":"(dermatology,soybean, thyroid, audiology) because they have missing attributes, which our current formulation cannot handle. One dataset  <b>ecoli</b>  contains a nominal attribute, whichour current implementation cannot handle in practice; this Table 3. For each dataset, we counthowmanyvariations of AdaBoost.MO gave lower (<), equal (=),", "mykey":154},
 {"datasetID":42, "supportID":"104A2E37BB70A9717D6E0AA36637C3E198E7BD71", "rexaID":"692880d7b3356df64bfa0f06a683f89e4ce6955b", "author":"Vassilis Athitsos and Stan Sclaroff", "title":"Boosting Nearest Neighbor Classifiers for Multiclass Recognition", "venue":"Boston University Computer Science Tech. Report No, 2004-006", "year":"2004", "window":"used in the experiments, largely copied from (Allwein et al., 2000). Dataset Train Test Attributes Classes <b>glass</b> 214 - 9 6 isolet 6238 1559 617 26 letter 16000 4000 16 26 pendigits 7494 3498 16 10 satimage 4435 2000 36 6 segmentation 2310 - 19 7 vowel 528 462 10 11 yeast", "mykey":155},
 {"datasetID":54, "supportID":"104A2E37BB70A9717D6E0AA36637C3E198E7BD71", "rexaID":"692880d7b3356df64bfa0f06a683f89e4ce6955b", "author":"Vassilis Athitsos and Stan Sclaroff", "title":"Boosting Nearest Neighbor Classifiers for Multiclass Recognition", "venue":"Boston University Computer Science Tech. Report No, 2004-006", "year":"2004", "window":"and the best result attained among the 6 variations of \naive\" k-nn classification. For our method we also provide the standard deviation across multiple trials, except for the <b>isolet</b> dataset where we only ran one trial of our algorithm. Dataset Boost-NN Allwein Naive k-nn glass 24.4 # 1.7 25.2 26.8 isolet 6.5 5.3 7.6 letter 3.5 # 0.2 7.1 4.5 pendigits 3.9 # 0.6 2.9 2.2 satimage 9.6 #", "mykey":156},
 {"datasetID":90, "supportID":"104A2E37BB70A9717D6E0AA36637C3E198E7BD71", "rexaID":"692880d7b3356df64bfa0f06a683f89e4ce6955b", "author":"Vassilis Athitsos and Stan Sclaroff", "title":"Boosting Nearest Neighbor Classifiers for Multiclass Recognition", "venue":"Boston University Computer Science Tech. Report No, 2004-006", "year":"2004", "window":"We did not use four datasets (dermatology <b>soybean</b>  thyroid, audiology) because they have missing attributes, which our current formulation cannot handle. One dataset (ecoli) contains a nominal attribute, whichour current", "mykey":157},
 {"datasetID":91, "supportID":"104A2E37BB70A9717D6E0AA36637C3E198E7BD71", "rexaID":"692880d7b3356df64bfa0f06a683f89e4ce6955b", "author":"Vassilis Athitsos and Stan Sclaroff", "title":"Boosting Nearest Neighbor Classifiers for Multiclass Recognition", "venue":"Boston University Computer Science Tech. Report No, 2004-006", "year":"2004", "window":"We did not use four datasets (dermatology <b>soybean</b>  thyroid, audiology) because they have missing attributes, which our current formulation cannot handle. One dataset (ecoli) contains a nominal attribute, whichour current", "mykey":158},
 {"datasetID":102, "supportID":"104A2E37BB70A9717D6E0AA36637C3E198E7BD71", "rexaID":"692880d7b3356df64bfa0f06a683f89e4ce6955b", "author":"Vassilis Athitsos and Stan Sclaroff", "title":"Boosting Nearest Neighbor Classifiers for Multiclass Recognition", "venue":"Boston University Computer Science Tech. Report No, 2004-006", "year":"2004", "window":"We did not use four datasets (dermatology,soybean, <b>thyroid</b>  audiology) because they have missing attributes, which our current formulation cannot handle. One dataset (ecoli) contains a nominal attribute, whichour current", "mykey":159},
 {"datasetID":110, "supportID":"104A2E37BB70A9717D6E0AA36637C3E198E7BD71", "rexaID":"692880d7b3356df64bfa0f06a683f89e4ce6955b", "author":"Vassilis Athitsos and Stan Sclaroff", "title":"Boosting Nearest Neighbor Classifiers for Multiclass Recognition", "venue":"Boston University Computer Science Tech. Report No, 2004-006", "year":"2004", "window":"where that method does better than the other methods. There are also two datasets (glass and <b>yeast</b>  where the results of our algorithm and the best results from ECOC-based boosting and naive k-nn classification are quite similar. We should mention that, in the Allwein et al.", "mykey":160},
 {"datasetID":53, "supportID":"10C783A9F89DFD1FF2EE57E922D9D199970677A5", "rexaID":"03e3fbdc694fa4f16c10fbc88658f4e6e2e462e8", "author":"Sotiris B. Kotsiantis and Panayiotis E. Pintelas", "title":"Logitboost of Simple Bayesian Classifier", "venue":"Informatica", "year":"2005", "window":"were hand selected so as to come from real-world problems and to vary in characteristics. Thus, we have used data sets from the domains of: pattern recognition  <b>iris</b>  zoo), image recognition (ionosphere, sonar), medical diagnosis (breast-cancer, breast-w, colic, diabetes, heart-c, heart-h, heart-statlog, hepatitis,", "mykey":161},
 {"datasetID":14, "supportID":"1135DDB82695B44516CBCB5C1465A01420279A44", "rexaID":"f820b10d6723504d343c9741f7333fbc3393fd05", "author":"Lorne Mason and Jonathan Baxter and Peter L. Bartlett and Marcus Frean", "title":"Boosting Algorithms as Gradient Descent", "venue":"NIPS", "year":"1999", "window":"0% noise - AdaBoost 0% noise - DOOM II 15% noise - AdaBoost 15% noise - DOOM II Figure 2: Margin distributions for AdaBoost and DOOM II with 0% and 15% label noise for the <b>breast</b> <b>cancer</b> and splice data sets. Given that AdaBoost suffers from overfitting and minimizes an exponential cost function of the margins, this cost function certainly does not relate to test error. Howdoesthevalue of our proposed", "mykey":162},
 {"datasetID":151, "supportID":"1135DDB82695B44516CBCB5C1465A01420279A44", "rexaID":"f820b10d6723504d343c9741f7333fbc3393fd05", "author":"Lorne Mason and Jonathan Baxter and Peter L. Bartlett and Marcus Frean", "title":"Boosting Algorithms as Gradient Descent", "venue":"NIPS", "year":"1999", "window":"These results show that DOOM II generally outperforms AdaBoost and that the improvementis more pronounced in the presence of label noise. -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 2.5 3 3.5 Error advantage (%) Data set <b>sonar</b> cleve ionosphere vote1 credit breast-cancer pima-indians hypo1 splice 0% noise 5% noise 15% noise Figure 1: Summary of test error advantage (with standard error bars) of DOOM II over AdaBoost", "mykey":163},
 {"datasetID":52, "supportID":"1135DDB82695B44516CBCB5C1465A01420279A44", "rexaID":"f820b10d6723504d343c9741f7333fbc3393fd05", "author":"Lorne Mason and Jonathan Baxter and Peter L. Bartlett and Marcus Frean", "title":"Boosting Algorithms as Gradient Descent", "venue":"NIPS", "year":"1999", "window":"These results show that DOOM II generally outperforms AdaBoost and that the improvementis more pronounced in the presence of label noise. -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 2.5 3 3.5 Error advantage (%) Data set sonar cleve <b>ionosphere</b> vote1 credit breast-cancer pima-indians hypo1 splice 0% noise 5% noise 15% noise Figure 1: Summary of test error advantage (with standard error bars) of DOOM II over AdaBoost", "mykey":164},
 {"datasetID":56, "supportID":"1135DDB82695B44516CBCB5C1465A01420279A44", "rexaID":"f820b10d6723504d343c9741f7333fbc3393fd05", "author":"Lorne Mason and Jonathan Baxter and Peter L. Bartlett and Marcus Frean", "title":"Boosting Algorithms as Gradient Descent", "venue":"NIPS", "year":"1999", "window":"the minimum of AdaBoost's test error and the minimum of the normalized sigmoid cost very nearly coincide. In the <b>labor</b> data set AdaBoost's test error converges and overfitting does not occur. For this data set both the normalized sigmoid cost and the exponential cost converge. In the vote1 data set AdaBoost initially", "mykey":165},
 {"datasetID":69, "supportID":"1135DDB82695B44516CBCB5C1465A01420279A44", "rexaID":"f820b10d6723504d343c9741f7333fbc3393fd05", "author":"Lorne Mason and Jonathan Baxter and Peter L. Bartlett and Marcus Frean", "title":"Boosting Algorithms as Gradient Descent", "venue":"NIPS", "year":"1999", "window":"0% noise - AdaBoost 0% noise - DOOM II 15% noise - AdaBoost 15% noise - DOOM II Figure 2: Margin distributions for AdaBoost and DOOM II with 0% and 15% label noise for the breast-cancer and <b>splice</b> data sets. Given that AdaBoost suffers from overfitting and minimizes an exponential cost function of the margins, this cost function certainly does not relate to test error. Howdoesthevalue of our proposed", "mykey":166},
 {"datasetID":151, "supportID":"116EC8FF41BED0C7B593F620C732B82E6D06F1C9", "rexaID":"fcba21d24e397e9639efbad04b5be06fbe9c0df2", "author":"Jeremy Kubica and Andrew Moore", "title":"Probabilistic Noise Identification and Data Cleaning", "venue":"ICDM", "year":"2003", "window":"from the assumed models. 5.1.1 Leaf and Rock Data The leaf and rock data, summarized in Table 1, consist of attributes extracted from a series of pictures of leaves and <b>rocks</b> respectively. The leaf data set contains 71 records from pictures of living and dead leaves. As expected, the living leaves were green in color while the dead leaves were brownish or yellow. The rock data set contains 56 records", "mykey":167},
 {"datasetID":53, "supportID":"116EC8FF41BED0C7B593F620C732B82E6D06F1C9", "rexaID":"fcba21d24e397e9639efbad04b5be06fbe9c0df2", "author":"Jeremy Kubica and Andrew Moore", "title":"Probabilistic Noise Identification and Data Cleaning", "venue":"ICDM", "year":"2003", "window":"We also compared the algorithms by their ability to identify artificial corruptions. Three different test sets were used: a noise free version of the rock data described above, the UCI <b>Iris</b> data set, and the UCI Wine data set [3]. Noise was generated by choosing to corrupt each record with some probability p. For each record chosen, corruption and noise vectors were sampled from their", "mykey":168},
 {"datasetID":58, "supportID":"116EC8FF41BED0C7B593F620C732B82E6D06F1C9", "rexaID":"fcba21d24e397e9639efbad04b5be06fbe9c0df2", "author":"Jeremy Kubica and Andrew Moore", "title":"Probabilistic Noise Identification and Data Cleaning", "venue":"ICDM", "year":"2003", "window":"on the level of cells, where a single cell may be generated by a different model than cells of the corresponding attribute for the other records from this class. 5 Evaluation 5.1 Naturally Corrupted Data Sets As an initial test, we ran <b>LENS</b> on several real world data sets. These data sets contained \"natural\" corruptions that were not explicitly generated from the assumed models. 5.1.1 Leaf and Rock Data", "mykey":169},
 {"datasetID":109, "supportID":"116EC8FF41BED0C7B593F620C732B82E6D06F1C9", "rexaID":"fcba21d24e397e9639efbad04b5be06fbe9c0df2", "author":"Jeremy Kubica and Andrew Moore", "title":"Probabilistic Noise Identification and Data Cleaning", "venue":"ICDM", "year":"2003", "window":"We also compared the algorithms by their ability to identify artificial corruptions. Three different test sets were used: a noise free version of the rock data described above, the UCI Iris data set, and the UCI <b>Wine</b> data set [3]. Noise was generated by choosing to corrupt each record with some probability p. For each record chosen, corruption and noise vectors were sampled from their", "mykey":170},
 {"datasetID":2, "supportID":"11D6084E272D8538660C52FF08BB800B55A57291", "rexaID":"0e6263639bcf297001939fd27576b20c6b78f68b", "author":"Luca Zanni", "title":"An Improved Gradient Projection-based Decomposition Technique for Support Vector Machines", "venue":"Dipartimento di Matematica, Universitdi Modena e Reggio Emilia", "year":"", "window":"in [8]. In order to analyze the behaviour of the two solvers within GPDT2 we consider three large test problems of the form (1) derived by training Gaussian SVMs on the well known UCI <b>Adult</b> data set [22], WEB data set [26] and MNIST data set [18]. A detailed description of the test problems generation is reported in the Appendix. All the experiments are carried out with standard C code running", "mykey":171},
 {"datasetID":81, "supportID":"11D6084E272D8538660C52FF08BB800B55A57291", "rexaID":"0e6263639bcf297001939fd27576b20c6b78f68b", "author":"Luca Zanni", "title":"An Improved Gradient Projection-based Decomposition Technique for Support Vector Machines", "venue":"Dipartimento di Matematica, Universitdi Modena e Reggio Emilia", "year":"", "window":"sized 49749. The Gaussian SVM parameters are: C = 5 and \u00be = p 10. . MNIST data set The MNIST database of <b>handwritten</b> <b>digits</b> [18] contains 784-dimensional nonbinary sparse vectors; the size of the database is 60000 and the sparsity level of the inputs is \u00bc 81%. A Gaussian SVM for", "mykey":172},
 {"datasetID":53, "supportID":"125863C37F7A29383A2E9A817FF0B10FA30FB8E0", "rexaID":"9acaa3dfebccac9d79abb08f7eced65482be1631", "author":"Inderjit S. Dhillon and Dharmendra S. Modha and W. Scott Spangler", "title":"Class visualization of high-dimensional data with applications", "venue":"Department of Computer Sciences, University of Texas", "year":"2002", "window":"sketch the outline of the paper. Section 2 introduces class-preserving projections and class-eigenvector plots, and contains several illustrations of the <b>Iris</b> plant and ISOLET speech recognition data sets [27]. Class-similarity graphs and class tours are discussed in Sections 3 and 4. We illustrate the value of the above visualization tools in Section 5, where we present a detailed study of the", "mykey":173},
 {"datasetID":54, "supportID":"125863C37F7A29383A2E9A817FF0B10FA30FB8E0", "rexaID":"9acaa3dfebccac9d79abb08f7eced65482be1631", "author":"Inderjit S. Dhillon and Dharmendra S. Modha and W. Scott Spangler", "title":"Class visualization of high-dimensional data with applications", "venue":"Department of Computer Sciences, University of Texas", "year":"2002", "window":"(a d#n matrix where n is the number of data points), which has a computational complexity of O(d 2 n). Typically n is much larger than k as can be seen from two sample data sets we will use later in this paper. In the <b>ISOLET</b> speech recognition data set, k = 26, n = 7797 and d = 617, while in the PENDIGITS example k = 10, n = 10992 and d = 16. The non-linear SOM and MDS", "mykey":174},
 {"datasetID":7, "supportID":"1261A790F06DD7E04604E3202DDBFA94A37AC98F", "rexaID":"488b32dd59d0732ad7657f9a34f40f2aa78551cf", "author":"Bernhard Pfahringer and Ian H. Witten and Philip Chan", "title":"Improving Bagging Performance by Increasing Decision Tree Diversity", "venue":"Austrian Research Institute for AI", "year":"", "window":"procedure---either five five-fold cross-validations, or the use of a prespecified test set. D R A F T October 14, 1997, 7:30pm D R A F T IMPROVING BAGGING 11 Table 1. Characteristics of the used datasets. Dataset #Ex #C #Cat #Num Test <b>Audiology</b> 226 24 69 0 5 Theta 5 cv Breast 699 2 0 9 5 Theta 5 cv Colic 368 2 14 8 5 Theta 5 cv Credit 690 2 9 6 5 Theta 5 cv Diabetes 768 2 0 8 5 Theta 5 cv DNA", "mykey":175},
 {"datasetID":8, "supportID":"1261A790F06DD7E04604E3202DDBFA94A37AC98F", "rexaID":"488b32dd59d0732ad7657f9a34f40f2aa78551cf", "author":"Bernhard Pfahringer and Ian H. Witten and Philip Chan", "title":"Improving Bagging Performance by Increasing Decision Tree Diversity", "venue":"Austrian Research Institute for AI", "year":"", "window":"procedure---either five five-fold cross-validations, or the use of a prespecified test set. D R A F T October 14, 1997, 7:30pm D R A F T IMPROVING BAGGING 11 Table 1. Characteristics of the used datasets. Dataset #Ex #C #Cat #Num Test <b>Audiology</b> 226 24 69 0 5 Theta 5 cv Breast 699 2 0 9 5 Theta 5 cv Colic 368 2 14 8 5 Theta 5 cv Credit 690 2 9 6 5 Theta 5 cv Diabetes 768 2 0 8 5 Theta 5 cv DNA", "mykey":176},
 {"datasetID":1, "supportID":"13292517CD09ABACBA4A3AB42A79CB97C2E7526D", "rexaID":"351e173bc2176dbf14635cc5471660c911f8e79c", "author":"Edward Snelson and Carl Edward Rasmussen and Zoubin Ghahramani", "title":"Warped Gaussian Processes", "venue":"NIPS", "year":"2003", "window":"D t min t max N train N test creep 30 18 MPa 530 MPa 800 1266 <b>abalone</b> 8 1 yr 29 yrs 1000 3177 ailerons 4010Dataset Model Absolute error Squared errorcreep GP 16.4 654 4.46 GP + log 15.6 587 4.24 warped GP 15.0 554 4.19 abalone GP 1.53 4.79 2.19 GP + log 1.48 4.62 2.01 warped GP 1.47 4.63 1.96 ailerons GP 1.23 \u00d7", "mykey":177},
 {"datasetID":30, "supportID":"135C17D73BEA37CC8524AE84954DE1C2ECDF2EAA", "rexaID":"15901017775c4db876288ac0f024da2ab7bb3738", "author":"Earl Harris Jr", "title":"Information Gain Versus Gain Ratio: A Study of Split Method Biases", "venue":"The MITRE Corporation/Washington C", "year":"2001", "window":"method choiceproblem. attribute information gain 0 0.045 1 0.044 2 0.018 3 0.113 4 0.004 5 0.001 6 0.006 7 0.018 8 0.015 Table 2: Information gain scores for the <b>contraceptive</b> method choice data set (Does the woman use contraception?) Machine Learning Repository.[1] This data set, which Tjen-Sien Lim created, is a subset of the 1987 National Indonesia Contraceptive Prevalence Survey. Example", "mykey":178},
 {"datasetID":34, "supportID":"13676E4F0FA15EA1ACBC22F72C60AD21ED6B8E0E", "rexaID":"69aaccfb9601e579827a9940738bb255a8cec3b5", "author":"Mark A. Hall", "title":"Correlation-based Feature Selection for Discrete and Numeric Class Machine Learning", "venue":"ICML", "year":"2000", "window":"in accuracy. From Figure 1 it can be seen that ReliefF selects fewer attributes with a threshold of 0.01 than with a threshold of 0, but CFS selects significantly fewer attributes than both on all data sets except <b>diabetes</b>  0 5 10 15 20 25 30 35 40 0 2 4 6 8 10 12 14 16 number of features dataset Figure 1. Average number of features selected by ReliefF with threshold 0 (left), ReliefF with threshold", "mykey":179},
 {"datasetID":42, "supportID":"13676E4F0FA15EA1ACBC22F72C60AD21ED6B8E0E", "rexaID":"69aaccfb9601e579827a9940738bb255a8cec3b5", "author":"Mark A. Hall", "title":"Correlation-based Feature Selection for Discrete and Numeric Class Machine Learning", "venue":"ICML", "year":"2000", "window":"In the case of CFS, a discretized copy of each training split was made for it to operate on. The same folds were used for each feature selector{learning scheme combination. Table 1. Discrete class data sets. Data Set Instances Num. Nom. Classes 1 <b>glass</b> 2 163 9 0 2 2 anneal 898 6 32 5 3 breast-c 286 0 9 2 4 credit-g 1000 7 13 2 5 diabetes 768 8 0 2 6 horse colic 368 7 15 2 7 heart-c 303 6 7 2 8", "mykey":180},
 {"datasetID":14, "supportID":"1377C81BBCFC456A4516AF409477BF269E15B1E7", "rexaID":"e8e16dd19c85a91ca01760943d02f3fc5594eec4", "author":"John G. Cleary and Leonard E. Trigg", "title":"Experiences with OB1, An Optimal Bayes Decision Tree Learner", "venue":"Department of Computer Science University of Waikato", "year":"", "window":"all the information in vote is contained in one attribute, and for iris two attributes contain all the class information (although most of this can be obtained using only one attribute). Some datasets, such as <b>breast</b> <b>cancer</b> and credit-g appear to contain very little class information. In general, we expect to see OB1 performance increase with tree depth up to a depth that captures the most", "mykey":181},
 {"datasetID":45, "supportID":"1377C81BBCFC456A4516AF409477BF269E15B1E7", "rexaID":"e8e16dd19c85a91ca01760943d02f3fc5594eec4", "author":"John G. Cleary and Leonard E. Trigg", "title":"Experiences with OB1, An Optimal Bayes Decision Tree Learner", "venue":"Department of Computer Science University of Waikato", "year":"", "window":"however, naive Bayes performs very well, and on some datasets (such as <b>heart</b> c and labor) it performs considerably better than the OB1 results shown (presumably because its attribute independence assumption isn't violated). The next section investigates", "mykey":182},
 {"datasetID":56, "supportID":"1377C81BBCFC456A4516AF409477BF269E15B1E7", "rexaID":"e8e16dd19c85a91ca01760943d02f3fc5594eec4", "author":"John G. Cleary and Leonard E. Trigg", "title":"Experiences with OB1, An Optimal Bayes Decision Tree Learner", "venue":"Department of Computer Science University of Waikato", "year":"", "window":"however, naive Bayes performs very well, and on some datasets (such as heart-c and <b>labor</b>  it performs considerably better than the OB1 results shown (presumably because its attribute independence assumption isn't violated). The next section investigates", "mykey":183},
 {"datasetID":151, "supportID":"13A5756E40383E3FE3FA54C2F28ED1F88766C98B", "rexaID":"b2b8b059c73bebbf92cb80f2c45ca3ed56c3d971", "author":"Jakub Zavrel", "title":"An Empirical Re-Examination of Weighted Voting for k-NN", "venue":"Computational Linguistics", "year":"", "window":"40 50 60 70 80 90 100 % correct k letter \"letter.majority\" \"letter.inverse\" \"letter.dudani\" \"letter.shepard\" Figure 1: Accuracy results from experiments on the <b>sonar</b>  isolet, PP-attach, and letter datasets as a function of k. this count. Although Shepard's function performed well on the PP-attachment dataset, it is much less robust on the UCI datasets, and only slightly outperforms majority voting.", "mykey":184},
 {"datasetID":54, "supportID":"13A5756E40383E3FE3FA54C2F28ED1F88766C98B", "rexaID":"b2b8b059c73bebbf92cb80f2c45ca3ed56c3d971", "author":"Jakub Zavrel", "title":"An Empirical Re-Examination of Weighted Voting for k-NN", "venue":"Computational Linguistics", "year":"", "window":"40 50 60 70 80 90 100 % correct k letter \"letter.majority\" \"letter.inverse\" \"letter.dudani\" \"letter.shepard\" Figure 1: Accuracy results from experiments on the sonar, <b>isolet</b>  PP-attach, and letter datasets as a function of k. this count. Although Shepard's function performed well on the PP-attachment dataset, it is much less robust on the UCI datasets, and only slightly outperforms majority voting.", "mykey":185},
 {"datasetID":155, "supportID":"1457D4A2DEF6BBD29FE29251A878D171E617CABC", "rexaID":"6c590e12408ebd3b9184e8f4634612e552a823e9", "author":"Stephen D. Bay", "title":"Nearest neighbor classification from multiple feature subsets", "venue":"Intell. Data Anal, 3", "year":"1999", "window":"errors if different features were 17 selected. With this method they were able to improve performance on 7 of 10 domains tested (7 from the UCI repository and 3 proprietary <b>cloud</b> classification datasets), and they noted that ECOC accuracy gains tended to increase with increased diversity among the features selected for the two-class problems. NN-ECOC is similar to MFS as they both use NN", "mykey":186},
 {"datasetID":52, "supportID":"1457D4A2DEF6BBD29FE29251A878D171E617CABC", "rexaID":"6c590e12408ebd3b9184e8f4634612e552a823e9", "author":"Stephen D. Bay", "title":"Nearest neighbor classification from multiple feature subsets", "venue":"Intell. Data Anal, 3", "year":"1999", "window":"much as expected bychance, and 1 if the classifiers always agree. Diversity increases with smaller Kappa values. Figure 1 shows the Kappa-Error diagram for NN ensembles generated for the <b>Ionosphere</b> dataset by Bagging, randomly selecting 50 prototypes, and randomly selecting 6 6 features. Bagging results in a cloud of points centered roughly about (0.825,0.15). Using a smaller number of prototypes (50)", "mykey":187},
 {"datasetID":59, "supportID":"1457D4A2DEF6BBD29FE29251A878D171E617CABC", "rexaID":"6c590e12408ebd3b9184e8f4634612e552a823e9", "author":"Stephen D. Bay", "title":"Nearest neighbor classification from multiple feature subsets", "venue":"Intell. Data Anal, 3", "year":"1999", "window":"with n models will usually require n times the memory of a single classifier. For many problems this amount of memory may not be significant, but Dietterich [20] notes that on the <b>Letter Recognition</b> dataset (available from the UCI repository) an ensemble of 200 decision trees obtained 100% accuracy but required 59 megabytes of storage! The entire dataset was only 712 kilobytes. 4 Experiments", "mykey":188},
 {"datasetID":80, "supportID":"1457D4A2DEF6BBD29FE29251A878D171E617CABC", "rexaID":"6c590e12408ebd3b9184e8f4634612e552a823e9", "author":"Stephen D. Bay", "title":"Nearest neighbor classification from multiple feature subsets", "venue":"Intell. Data Anal, 3", "year":"1999", "window":"or disjoint) in combination with the CNN classifier to edit and reduce the prototypes. He also reported improvements (on five domains from the UCI repository and one <b>optical</b> character <b>recognition</b> dataset) over the baseline NN classifier if the training sets were sufficiently small and thus able to generate diverse classifiers. It is important to note that both of Alpaydin's and Skalak's work differ", "mykey":189},
 {"datasetID":101, "supportID":"1457D4A2DEF6BBD29FE29251A878D171E617CABC", "rexaID":"6c590e12408ebd3b9184e8f4634612e552a823e9", "author":"Stephen D. Bay", "title":"Nearest neighbor classification from multiple feature subsets", "venue":"Intell. Data Anal, 3", "year":"1999", "window":"is a board game playedona3by 3 grid. There are two players X and O, who alternate putting X's and O's on the board. In order to win a player must get three X's or O's in a row. The <b>tic-tac-toe</b> dataset contains the 958 legal tic-tac-toe endgame boards (assuming X moves first). The classification task is to determine if X won the game. 11 Table 2: Classifier error rates. Domain NN kNN FSS BSS MFS1", "mykey":190},
 {"datasetID":53, "supportID":"14BA0B46C25CB2B63AF5D52955F74DB6DA131687", "rexaID":"5736888202fbd07e1945b8a64a3e0f3ddeb2840f", "author":"Judith E. Devaney and Steven G. Satterfield and John G. Hagedorn and John T. Kelso and Adele P. Peskin and William George and Terence J. Griffin and Howard K. Hung and Ronald D. Kriz", "title":"Science at the Speed of Thought", "venue":"Ambient Intelligence for Scientific Discovery", "year":"2004", "window":"EXAMPLES Figure 1 shows part of our visualization of the <b>Iris</b> data set [2]. (The full visualization contains multiple rooms with an alternate visualization of the same data set in each room, enabling a scientist to visit each of the rooms.) On the near side of the left", "mykey":191},
 {"datasetID":48, "supportID":"1504C81BBD579B9EC905539A4F719F3303615986", "rexaID":"965bcc99a91fa65151f0985f3813992c6403c2f6", "author":"Tapani Raiko and Harri Valpola", "title":"MISSING VALUES IN NONLINEAR FACTOR ANALYSIS", "venue":"Helsinki University of Technology, Neural Networks Research Centre", "year":"", "window":"The training set contains vectors more similar to the test set now. 4. Training and testing sets are permuted and 10 percent of the values are set to miss independently of any neighbours. The second data set is Boston <b>housing</b> data, which is publicly available at [2]. It concerns housing values in suburbs of Boston. Data set contains 506 vectors of 13 dimensions excluding one binary attribute. Four of", "mykey":192},
 {"datasetID":2, "supportID":"15305207FD1D7E73FDD4926FD612E5A476BA11C2", "rexaID":"13c8e4e9474a8fbd7bd8a66695b62f218189ac6f", "author":"Andrew W. Moore and Weng-Keen Wong", "title":"Optimal Reinsertion: A New Search Operator for Accelerated and More Accurate Bayesian Network Structure Learning", "venue":"ICML", "year":"2003", "window":"if and only if an odd number of parents have value \"True\". The nodes are thus noisy exclusive-ors and so it is hard to learn a set of parents incrementally. Synth2 Synth3 Synth4 Figure 3. Synthetic datasets described in Section 3.1. R m AA <b>adult</b> 49K 15 7.7 Contributed to UCI by Ron Kohavi alarm 20K 37 2.8 Data generated from a standard Bayes Net benchmark (Beinlich et al., 1989). biosurv 150K 24 3.5", "mykey":193},
 {"datasetID":53, "supportID":"16297FE3CB0A10D7546A722F079071070DEA60F8", "rexaID":"d320ece010630c32341d927b2573372abbbca524", "author":"Stephen D. Bay", "title":"Combining Nearest Neighbor Classifiers Through Multiple Feature Subsets", "venue":"ICML", "year":"1998", "window":"comparison, we used the Wilcoxon signed rank test and found that MFS1 and MFS2 were significantly better than all others with a confidence level greater than 99%. MFS only performed poorly on two datasets: <b>Iris</b> and Tic-Tac-Toe. For Iris, both MFS1 and MFS2 gave the lowest accuracy out of all the classifiers. This can possibly be explained by the small number of features in the Iris dataset. With", "mykey":194},
 {"datasetID":74, "supportID":"16297FE3CB0A10D7546A722F079071070DEA60F8", "rexaID":"d320ece010630c32341d927b2573372abbbca524", "author":"Stephen D. Bay", "title":"Combining Nearest Neighbor Classifiers Through Multiple Feature Subsets", "venue":"ICML", "year":"1998", "window":"maintain consistency with reported results (Quinlan, 1996). For Satimage, we used the original division into a training and test set, so the results represent one run of each algorithm. For the <b>Musk</b> dataset, which has 166 features, FSS and BSS took too long to run (over 24 hours for a single trial) and no results were obtained. 3.2 ACCURACY The accuracy and parameter selection results (average k or", "mykey":195},
 {"datasetID":75, "supportID":"16297FE3CB0A10D7546A722F079071070DEA60F8", "rexaID":"d320ece010630c32341d927b2573372abbbca524", "author":"Stephen D. Bay", "title":"Combining Nearest Neighbor Classifiers Through Multiple Feature Subsets", "venue":"ICML", "year":"1998", "window":"maintain consistency with reported results (Quinlan, 1996). For Satimage, we used the original division into a training and test set, so the results represent one run of each algorithm. For the <b>Musk</b> dataset, which has 166 features, FSS and BSS took too long to run (over 24 hours for a single trial) and no results were obtained. 3.2 ACCURACY The accuracy and parameter selection results (average k or", "mykey":196},
 {"datasetID":101, "supportID":"16297FE3CB0A10D7546A722F079071070DEA60F8", "rexaID":"d320ece010630c32341d927b2573372abbbca524", "author":"Stephen D. Bay", "title":"Combining Nearest Neighbor Classifiers Through Multiple Feature Subsets", "venue":"ICML", "year":"1998", "window":"comparison, we used the Wilcoxon signed rank test and found that MFS1 and MFS2 were significantly better than all others with a confidence level greater than 99%. MFS only performed poorly on two datasets: Iris and <b>Tic-Tac-Toe</b>  For Iris, both MFS1 and MFS2 gave the lowest accuracy out of all the classifiers. This can possibly be explained by the small number of features in the Iris dataset. With", "mykey":197},
 {"datasetID":34, "supportID":"16335621FDE2CC36426D1D2E6F6F00356ABC93F9", "rexaID":"4773165a9efff672af50f07beb5d38c9afc92f5f", "author":"Lawrence O. Hall and Nitesh V. Chawla and Kevin W. Bowyer", "title":"Combining Decision Trees Learned in Parallel", "venue":"Department of Computer Science and Engineering, ENB 118 University of South Florida", "year":"", "window":"The Iris data (Fisher 1936; Merz & Murphy ) which has 4 continuous valued attributes and classifies 150 examples as one of 3 classes of Iris plant. The second is the Pima Indians <b>Diabetes</b> data set (Merz & Murphy ) which has 8 numeric attributes and classifies 768 examples into one of 2 classes. We have done an experiment simulating a parallel 2-processor implementation for both data sets and", "mykey":198},
 {"datasetID":53, "supportID":"16335621FDE2CC36426D1D2E6F6F00356ABC93F9", "rexaID":"4773165a9efff672af50f07beb5d38c9afc92f5f", "author":"Lawrence O. Hall and Nitesh V. Chawla and Kevin W. Bowyer", "title":"Combining Decision Trees Learned in Parallel", "venue":"Department of Computer Science and Engineering, ENB 118 University of South Florida", "year":"", "window":"0.6 < Petal-Width <= 1.5 and Petal-Length } 4.9 --} <b>Iris</b> Viginica R5: If 1.5 < Petal-Width <= 1.7 and Petal-Length } 4.9 --} Iris-Versicolor <= 1.7 Figure 1: The C4.5 tree produced on the full Iris dataset and the corresponding rules. adjust just one condition. For example, R1 no longer conflicts its test is adjusted to be petalwidthcm :5. A more complex problem is a condition in one rule overlaps", "mykey":199},
 {"datasetID":79, "supportID":"16335621FDE2CC36426D1D2E6F6F00356ABC93F9", "rexaID":"4773165a9efff672af50f07beb5d38c9afc92f5f", "author":"Lawrence O. Hall and Nitesh V. Chawla and Kevin W. Bowyer", "title":"Combining Decision Trees Learned in Parallel", "venue":"Department of Computer Science and Engineering, ENB 118 University of South Florida", "year":"", "window":"The Iris data (Fisher 1936; Merz & Murphy ) which has 4 continuous valued attributes and classifies 150 examples as one of 3 classes of Iris plant. The second is the <b>Pima</b> <b>Indians</b> <b>Diabetes</b> data set (Merz & Murphy ) which has 8 numeric attributes and classifies 768 examples into one of 2 classes. We have done an experiment simulating a parallel 2-processor implementation for both data sets and", "mykey":200},
 {"datasetID":98, "supportID":"16CD9FAAEAB4ABF9D1C0CAEA8AB36824BB1A3CD2", "rexaID":"3f9e71cd1f71f5f65b712d5cc5c29045d0193711", "author":"Ron Kohavi and George H. John and Richard Long and David Manley and Karl Pfleger", "title":"MLC++: A Machine Learning Library in C", "venue":"ICTAI", "year":"1994", "window":"but they are not an integrated environment, and are not very efficient. <b>StatLog</b> [14] is an ESPRIT project studying the behavior of over twenty algorithms (mostly in the MLToolbox), on over twenty datasets. StatLog is an instance of a good experimental study, but does not provide the tools to aid researchers in performing similar studies. Wray Buntine has recently suggested a unified approach to some", "mykey":201},
 {"datasetID":95, "supportID":"179D899CBA09D8237C605014832712FD8221A6D6", "rexaID":"b08639d60b3527616dc9aa4dd4770e50289e44cd", "author":"Lukasz A. Kurgan and Waldemar Swiercz and Krzysztof J. Cios", "title":"Semantic Mapping of XML Tags Using Inductive Machine Learning", "venue":"ICMLA", "year":"2002", "window":"module within the XMapper system. There are several reasons for low performance of the XMapper system without the learning module for the three domains. The <b>spect</b> domain created using the spect dataset [19] had attributes with completely different names between the two sources. Also, all attributes were binary and thus only the relationship between attributes could be used as indicator for mapping", "mykey":202},
 {"datasetID":102, "supportID":"179D899CBA09D8237C605014832712FD8221A6D6", "rexaID":"b08639d60b3527616dc9aa4dd4770e50289e44cd", "author":"Lukasz A. Kurgan and Waldemar Swiercz and Krzysztof J. Cios", "title":"Semantic Mapping of XML Tags Using Inductive Machine Learning", "venue":"ICMLA", "year":"2002", "window":"domains 73.1 course 5 10 15.6 2.3 87.5 faculty 5 10 100 0 100.0 realest 5 10 27.4 20 57.2 mean for real-life domains 81.6 total mean 77.3 Similarly, the thy domain created using the <b>thyroid</b> dataset [28, 29] includes attributes that have very different attribute names, and very similar attribute values between the two sources. 92 percent of the examples from that domain belong to the same", "mykey":203},
 {"datasetID":31, "supportID":"17E7C99313085AE9DD177C5FA7FAC92939F6A343", "rexaID":"a210d2418e04c74da13adc0356b79daa197b9d89", "author":"Zoran Obradovic and Slobodan Vucetic", "title":"Challenges in Scientific Data Mining: Heterogeneous, Biased, and Large Samples", "venue":"Center for Information Science and Technology Temple University", "year":"", "window":"levels of all attributes in an efficient manner. 1.4.2 Application 6: Reduction of spatially correlated data We performed a number of down-sampling and quantization experiments [76] on several large data sets including <b>Covertype</b> Data Set. This set is currently one of the largest databases in the UCI Database Repository [50] containing 581,012 examples with 54 attributes and 7 target classes and", "mykey":204},
 {"datasetID":123, "supportID":"17E7C99313085AE9DD177C5FA7FAC92939F6A343", "rexaID":"a210d2418e04c74da13adc0356b79daa197b9d89", "author":"Zoran Obradovic and Slobodan Vucetic", "title":"Challenges in Scientific Data Mining: Heterogeneous, Biased, and Large Samples", "venue":"Center for Information Science and Technology Temple University", "year":"", "window":"(eds: J. Abello, P. Pardalos, M. Resende). Kluwer Academic Publishers. 931. [10] Burl, M., Kamatch, C., Kumar, V. and Namburu, R. (chairs) 2001. Third Workshop on Mining Scientific Datasets. First SIAM Int'l Conf. Data Mining. <b>Chicago</b>  IL. [11] Burl, M., Kamatch, C., Kumar, V. and Namburu, R. (chairs) 2001. Fourth Workshop on Mining Scientific Datasets. Seventh ACM SIGKDD Int'l Conf.", "mykey":205},
 {"datasetID":154, "supportID":"17E7C99313085AE9DD177C5FA7FAC92939F6A343", "rexaID":"a210d2418e04c74da13adc0356b79daa197b9d89", "author":"Zoran Obradovic and Slobodan Vucetic", "title":"Challenges in Scientific Data Mining: Heterogeneous, Biased, and Large Samples", "venue":"Center for Information Science and Technology Temple University", "year":"", "window":"is compared to the accuracy of the global classifier. The algorithm proceeds by gradually partitioning disordered proteins into more subsets in an attempt to further improve the accuracy. Data sets with 145 nonredundant disordered <b>protein</b> regions and about equal number of ordered proteins of similar length were used in the experiment. For each position in a protein sequence we extracted 18", "mykey":206},
 {"datasetID":107, "supportID":"17E7C99313085AE9DD177C5FA7FAC92939F6A343", "rexaID":"a210d2418e04c74da13adc0356b79daa197b9d89", "author":"Zoran Obradovic and Slobodan Vucetic", "title":"Challenges in Scientific Data Mining: Heterogeneous, Biased, and Large Samples", "venue":"Center for Information Science and Technology Temple University", "year":"", "window":"algorithm on synthetic data of various statistical properties showed that it accurately estimates the class probabilities on unlabeled data [73]. In another experiment on a 3-class benchmark dataset called <b>Waveform</b> [7] the SL was constructed with balanced classes, while we experimented with different class distributions p j on SU . It was shown that for balanced classes p = [1=3; 1=3; 1=3] the", "mykey":207},
 {"datasetID":108, "supportID":"17E7C99313085AE9DD177C5FA7FAC92939F6A343", "rexaID":"a210d2418e04c74da13adc0356b79daa197b9d89", "author":"Zoran Obradovic and Slobodan Vucetic", "title":"Challenges in Scientific Data Mining: Heterogeneous, Biased, and Large Samples", "venue":"Center for Information Science and Technology Temple University", "year":"", "window":"algorithm on synthetic data of various statistical properties showed that it accurately estimates the class probabilities on unlabeled data [73]. In another experiment on a 3-class benchmark dataset called <b>Waveform</b> [7] the SL was constructed with balanced classes, while we experimented with different class distributions p j on SU . It was shown that for balanced classes p = [1=3; 1=3; 1=3] the", "mykey":208},
 {"datasetID":14, "supportID":"17F5F627AA64D0A726E51B382398A2E23F7997B0", "rexaID":"e4ce48114dcd770134f22df787d55e8daf02ac4a", "author":"Karthik Ramakrishnan", "title":"UNIVERSITY OF MINNESOTA", "venue":"", "year":"", "window":"the number of output classes, and the number of continuous and discrete input features. Features Data set Cases Class Continuous Discrete <b>breast</b> <b>cancer</b> w 699 2 9 - credit-a 690 2 6 9 credit-g 1000 2 7 13 glass 214 6 9 - heart-cleveland 303 2 8 5 hypo 3772 5 7 22 ionosphere 351 2 34 - iris 159 3 4 -", "mykey":209},
 {"datasetID":42, "supportID":"17F5F627AA64D0A726E51B382398A2E23F7997B0", "rexaID":"e4ce48114dcd770134f22df787d55e8daf02ac4a", "author":"Karthik Ramakrishnan", "title":"UNIVERSITY OF MINNESOTA", "venue":"", "year":"", "window":"classifier is shown as a straight line across the x-axis for comparison purposes. . . . . . . . . . . . . . . . . . 37 11 Bagging, Boosting, and Distance-Weighted test set error rates for the <b>glass</b> data set as the number of classifiers in the ensemble increases. The test set error rate for a single decision tree classifier is shown as a straight line across the x-axis for comparison purposes. . . . . .", "mykey":210},
 {"datasetID":52, "supportID":"17F5F627AA64D0A726E51B382398A2E23F7997B0", "rexaID":"e4ce48114dcd770134f22df787d55e8daf02ac4a", "author":"Karthik Ramakrishnan", "title":"UNIVERSITY OF MINNESOTA", "venue":"", "year":"", "window":"is shown as a straight line across the x-axis for comparison purposes. . . . . . . . . . . . . . . . . . 39 14 Bagging, Boosting, and Distance-Weighted test set error rates for the <b>ionosphere</b> data set as the number of classifiers in the ensemble increases. The test set error rate for a single decision tree classifier is shown as a straight line across the x-axis for comparison purposes. . . . . .", "mykey":211},
 {"datasetID":53, "supportID":"17F5F627AA64D0A726E51B382398A2E23F7997B0", "rexaID":"e4ce48114dcd770134f22df787d55e8daf02ac4a", "author":"Karthik Ramakrishnan", "title":"UNIVERSITY OF MINNESOTA", "venue":"", "year":"", "window":"classifier is shown as a straight line across the x-axis for comparison purposes. . . . . . . . . . . . . . . . 39 vi 15 Bagging, Boosting, and Distance-Weighted test set error rates for the <b>iris</b> data set as the number of classifiers in the ensemble increases. The test set error rate for a single decision tree classifier is shown as a straight line across the x-axis for comparison purposes. . . . . .", "mykey":212},
 {"datasetID":56, "supportID":"17F5F627AA64D0A726E51B382398A2E23F7997B0", "rexaID":"e4ce48114dcd770134f22df787d55e8daf02ac4a", "author":"Karthik Ramakrishnan", "title":"UNIVERSITY OF MINNESOTA", "venue":"", "year":"", "window":"classifier is shown as a straight line across the x-axis for comparison purposes. . . . . . . . . . . . . . . . . . 40 17 Bagging, Boosting, and Distance-Weighted test set error rates for the <b>labor</b> data set as the number of classifiers in the ensemble increases. The test set error rate for a single decision tree classifier is shown as a straight line across the x-axis for comparison purposes. . . . . .", "mykey":213},
 {"datasetID":34, "supportID":"18147DE2AFCFA0183DA7CEAC318DCF1157AFD813", "rexaID":"69aaccfb9601e579827a9940738bb255a8cec3b5", "author":"Mark A. Hall", "title":"Correlation-based Feature Selection for Discrete and Numeric Class Machine Learning", "venue":"ICML", "year":"2000", "window":"in accuracy. From Figure 1 it can be seen that ReliefF selects fewer attributes with a threshold of 0.01 than with a threshold of 0, but CFS selects significantly fewer attributes than both on all data sets except <b>diabetes</b>  0 5 10 15 20 25 30 35 40 0 2 4 6 8 10 12 14 16 number of features dataset Figure 1. Average number of features selected by ReliefF with threshold 0 (left), ReliefF with threshold", "mykey":214},
 {"datasetID":42, "supportID":"18147DE2AFCFA0183DA7CEAC318DCF1157AFD813", "rexaID":"69aaccfb9601e579827a9940738bb255a8cec3b5", "author":"Mark A. Hall", "title":"Correlation-based Feature Selection for Discrete and Numeric Class Machine Learning", "venue":"ICML", "year":"2000", "window":"In the case of CFS, a discretized copy of each training split was made for it to operate on. The same folds were used for each feature selector{learning scheme combination. Table 1. Discrete class data sets. Data Set Instances Num. Nom. Classes 1 <b>glass</b> 2 163 9 0 2 2 anneal 898 6 32 5 3 breast-c 286 0 9 2 4 credit-g 1000 7 13 2 5 diabetes 768 8 0 2 6 horse colic 368 7 15 2 7 heart-c 303 6 7 2 8", "mykey":215},
 {"datasetID":45, "supportID":"186BF67295B88C9428E82A3524ECD00EED064BB8", "rexaID":"c15fec7384ce8461549ba1fc09fb2d87024cf037", "author":"Ron Kohavi and George H. John", "title":"Automatic Parameter Selection by Minimizing Estimated Error", "venue":"Computer Science Dept. Stanford University", "year":"", "window":"by replacing a node's test with the test at one of its children, so perhaps m=1 gives more latitude in the pruning phase. Information-gain (turning the g parameter on) was a big winner on several datasets: vehicle, segment, hypothyroid, <b>heart</b>  and cleve. Turning on the s parameter helped in tic-tactoe and monk1. Table 5: Experimental results: Accuracies for C4.5, C4.5-AP, and C4.5* from running on", "mykey":216},
 {"datasetID":56, "supportID":"186BF67295B88C9428E82A3524ECD00EED064BB8", "rexaID":"c15fec7384ce8461549ba1fc09fb2d87024cf037", "author":"Ron Kohavi and George H. John", "title":"Automatic Parameter Selection by Minimizing Estimated Error", "venue":"Computer Science Dept. Stanford University", "year":"", "window":"69.84Sigma1.77 72.44Sigma1.73 78.18Sigma0.94 .973 X vote 435 95.64Sigma0.52 95.41Sigma0.47 97.71Sigma0.68 .172 vote1 435 88.02Sigma1.77 87.58Sigma1.52 92.40Sigma1.20 .342 <b>labor</b> negotiation dataset, note that the entire dataset is very small with only 57 instances. Because of the small size, the state evaluations in C4.5-AP had high variance, and the search did not find good parameter values.", "mykey":217},
 {"datasetID":67, "supportID":"186BF67295B88C9428E82A3524ECD00EED064BB8", "rexaID":"c15fec7384ce8461549ba1fc09fb2d87024cf037", "author":"Ron Kohavi and George H. John", "title":"Automatic Parameter Selection by Minimizing Estimated Error", "venue":"Computer Science Dept. Stanford University", "year":"", "window":"so if we evaluate n nodes in the search, the total time is O(ntkA). In our experiments, t was limited to three, k was 10, and the number of nodes expanded was about 60. For example, on the <b>dna</b> dataset (which took the most time) C4.5 took just under 2 minutes, while C4.5-AP took 6.8 hours. 6 Related Work In statistics, model selection refers to the general problem of selecting a learning algorithm", "mykey":218},
 {"datasetID":148, "supportID":"186BF67295B88C9428E82A3524ECD00EED064BB8", "rexaID":"c15fec7384ce8461549ba1fc09fb2d87024cf037", "author":"Ron Kohavi and George H. John", "title":"Automatic Parameter Selection by Minimizing Estimated Error", "venue":"Computer Science Dept. Stanford University", "year":"", "window":"represent all of the available StatLog datasets except the <b>Shuttle</b> database (which was too large), all of the UCI datasets used by Holte (1993), all of the Monks datasets (Thrun et al. 1991), and Corral which is an artificial dataset presented", "mykey":219},
 {"datasetID":149, "supportID":"186BF67295B88C9428E82A3524ECD00EED064BB8", "rexaID":"c15fec7384ce8461549ba1fc09fb2d87024cf037", "author":"Ron Kohavi and George H. John", "title":"Automatic Parameter Selection by Minimizing Estimated Error", "venue":"Computer Science Dept. Stanford University", "year":"", "window":"by replacing a node's test with the test at one of its children, so perhaps m=1 gives more latitude in the pruning phase. Information-gain (turning the g parameter on) was a big winner on several datasets: <b>vehicle</b>  segment, hypothyroid, heart, and cleve. Turning on the s parameter helped in tic-tactoe and monk1. Table 5: Experimental results: Accuracies for C4.5, C4.5-AP, and C4.5* from running on", "mykey":220},
 {"datasetID":98, "supportID":"186BF67295B88C9428E82A3524ECD00EED064BB8", "rexaID":"c15fec7384ce8461549ba1fc09fb2d87024cf037", "author":"Ron Kohavi and George H. John", "title":"Automatic Parameter Selection by Minimizing Estimated Error", "venue":"Computer Science Dept. Stanford University", "year":"", "window":"being studied. We report experiments with this method on 33 datasets selected from the UCI and <b>StatLog</b> collections using C4.5 as the basic induction algorithm. At a 90% confidence level, our method improves the performance of C4.5 on nine domains, degrades", "mykey":221},
 {"datasetID":101, "supportID":"186BF67295B88C9428E82A3524ECD00EED064BB8", "rexaID":"c15fec7384ce8461549ba1fc09fb2d87024cf037", "author":"Ron Kohavi and George H. John", "title":"Automatic Parameter Selection by Minimizing Estimated Error", "venue":"Computer Science Dept. Stanford University", "year":"", "window":"Significantly, the value 25% (the default value in C4.5) was chosen for all datasets except <b>tic-tac-toe</b> (where C4.5-AP halved the error rate of C4.5) and glass2 (where C4.5-AP improved absolute accuracy by over 4%). For the m parameter, 1 was often the best choice, or a tie for", "mykey":222},
 {"datasetID":70, "supportID":"19030C6CE6520E243F630D6BDC186F18B7007D5F", "rexaID":"0b84bc02dcedbeae5f76f03a2226874e828db0ea", "author":"Alexey Tsymbal and Seppo Puuronen and Vagan Y. Terziyan", "title":"Arbiter Meta-Learning with Dynamic Selection of Classifiers and Its Experimental Investigation", "venue":"ADBIS", "year":"1999", "window":"from the UCI machine learning repository: three <b>MONK</b> s problem datasets donated by Sebastian Thrun and the Tic-Tac-Toe Endgame dataset donated by David W. Aha [8]. The MONK's problems are a collection of three artificial binary classification problems over the same", "mykey":223},
 {"datasetID":101, "supportID":"19030C6CE6520E243F630D6BDC186F18B7007D5F", "rexaID":"0b84bc02dcedbeae5f76f03a2226874e828db0ea", "author":"Alexey Tsymbal and Seppo Puuronen and Vagan Y. Terziyan", "title":"Arbiter Meta-Learning with Dynamic Selection of Classifiers and Its Experimental Investigation", "venue":"ADBIS", "year":"1999", "window":"from the UCI machine learning repository: three MONK's problem datasets donated by Sebastian Thrun and the <b>Tic-Tac-Toe</b> Endgame dataset donated by David W. Aha [8]. The MONK's problems are a collection of three artificial binary classification problems over the same", "mykey":224},
 {"datasetID":52, "supportID":"197E3A315C57C9278876D95B7E522700AA112886", "rexaID":"a3fca86cb1f854c7fd7975e66aae2a5ba3710f8a", "author":"Mikhail Bilenko and Sugato Basu and Raymond J. Mooney", "title":"Integrating constraints and metric learning in semi-supervised clustering", "venue":"ICML", "year":"2004", "window":"from the UCI repository: Iris, Wine, and <b>Ionosphere</b> (Blake & Merz, 1998); the Protein dataset used by Xing et al. (2003) and Bar-Hillel et al. (2003), and randomly sampled subsets from the Digits and Letters handwritten character recognition datasets, also from the UCI repository. For Digits", "mykey":225},
 {"datasetID":53, "supportID":"197E3A315C57C9278876D95B7E522700AA112886", "rexaID":"a3fca86cb1f854c7fd7975e66aae2a5ba3710f8a", "author":"Mikhail Bilenko and Sugato Basu and Raymond J. Mooney", "title":"Integrating constraints and metric learning in semi-supervised clustering", "venue":"ICML", "year":"2004", "window":"Experiments were conducted on three datasets from the UCI repository: <b>Iris</b>  Wine, and Ionosphere (Blake & Merz, 1998); the Protein dataset used by Xing et al. (2003) and Bar-Hillel et al. (2003), and randomly sampled subsets from the Digits", "mykey":226},
 {"datasetID":81, "supportID":"197E3A315C57C9278876D95B7E522700AA112886", "rexaID":"a3fca86cb1f854c7fd7975e66aae2a5ba3710f8a", "author":"Mikhail Bilenko and Sugato Basu and Raymond J. Mooney", "title":"Integrating constraints and metric learning in semi-supervised clustering", "venue":"ICML", "year":"2004", "window":"used by Xing et al. (2003) and Bar-Hillel et al. (2003), and randomly sampled subsets from the <b>Digits</b> and Letters <b>handwritten</b> character <b>recognition</b> datasets, also from the UCI repository. For Digits and Letters, we chose two sets of three classes: {I, J, L} from Letters and {3, 8, 9} from Digits, sampling 10% of the data points from the original", "mykey":227},
 {"datasetID":154, "supportID":"197E3A315C57C9278876D95B7E522700AA112886", "rexaID":"a3fca86cb1f854c7fd7975e66aae2a5ba3710f8a", "author":"Mikhail Bilenko and Sugato Basu and Raymond J. Mooney", "title":"Integrating constraints and metric learning in semi-supervised clustering", "venue":"ICML", "year":"2004", "window":"from the UCI repository: Iris, Wine, and Ionosphere (Blake & Merz, 1998); the <b>Protein</b> dataset used by Xing et al. (2003) and Bar-Hillel et al. (2003), and randomly sampled subsets from the Digits and Letters handwritten character recognition datasets, also from the UCI repository. For Digits", "mykey":228},
 {"datasetID":109, "supportID":"197E3A315C57C9278876D95B7E522700AA112886", "rexaID":"a3fca86cb1f854c7fd7975e66aae2a5ba3710f8a", "author":"Mikhail Bilenko and Sugato Basu and Raymond J. Mooney", "title":"Integrating constraints and metric learning in semi-supervised clustering", "venue":"ICML", "year":"2004", "window":"Experiments were conducted on three datasets from the UCI repository: Iris, <b>Wine</b>  and Ionosphere (Blake & Merz, 1998); the Protein dataset used by Xing et al. (2003) and Bar-Hillel et al. (2003), and randomly sampled subsets from the Digits", "mykey":229},
 {"datasetID":53, "supportID":"19B51725937B2E53552E50AE2E8D86CB46EF3FDE", "rexaID":"c6b935ac4dda0316f104278bd74029b224d8ed74", "author":"A. da Valls and Vicen Torra", "title":"Explaining the consensus of opinions with the vocabulary of the experts", "venue":"Dept. d'Enginyeria Informtica i Matemtiques Universitat Rovira i Virgili", "year":"", "window":"as L i+1 end if return d(P i ,P c ) calculated with the definition (1). end. 4.1 Experimental results We have made different tests on different domains. Particularly, we have considered a well-known data set: <b>Iris</b> [10], which has 150 flowers described by means of 4 numerical attributes: petal and sepal length, and petal and sepal width; and a second set of data built by 5 colleagues who have described", "mykey":230},
 {"datasetID":14, "supportID":"19B945138E4E6F70ECFAEF7D6B1292AB687B3A00", "rexaID":"eef68e57188bfabe4dff279cf32efe545bd7fe3c", "author":"Fei Sha and Lawrence K. Saul and Daniel D. Lee", "title":"Multiplicative Updates for Nonnegative Quadratic Programming in Support Vector Machines", "venue":"NIPS", "year":"2002", "window":"Kernel Polynomial Radial Data k=4 k=6 #=0.3 #=1.0 #=3.0 Sonar 9.6% 9.6% 7.6% 6.7% 10.6% <b>breast</b> <b>cancer</b> 5.1% 3.6% 4.4% 4.4% 4.4% Table 1: Misclassification error rates on the sonar and breast cancer data sets after 512 iterations of the multiplicative updates. 3.1 Multiplicative updates The loss function in eq. (6) is a special case of eq. (1) with A ij = y i y j K(x i , x j ) and b i =- 1. Thus, the", "mykey":231},
 {"datasetID":151, "supportID":"19B945138E4E6F70ECFAEF7D6B1292AB687B3A00", "rexaID":"eef68e57188bfabe4dff279cf32efe545bd7fe3c", "author":"Fei Sha and Lawrence K. Saul and Daniel D. Lee", "title":"Multiplicative Updates for Nonnegative Quadratic Programming in Support Vector Machines", "venue":"NIPS", "year":"2002", "window":"Kernel Polynomial Radial Data k=4 k=6 #=0.3 #=1.0 #=3.0 <b>sonar</b> 9.6% 9.6% 7.6% 6.7% 10.6% Breast cancer 5.1% 3.6% 4.4% 4.4% 4.4% Table 1: Misclassification error rates on the sonar and breast cancer data sets after 512 iterations of the multiplicative updates. 3.1 Multiplicative updates The loss function in eq. (6) is a special case of eq. (1) with A ij = y i y j K(x i , x j ) and b i =- 1. Thus, the", "mykey":232},
 {"datasetID":23, "supportID":"19EF7B758D745E9B4AD44318CD1DDEF4BEF19F9B", "rexaID":"a0c05cdac8e9929a4113d37aa2588e7ae70a6b76", "author":"Tanzeem Choudhury and James M. Rehg and Vladimir Pavlovic and Alex Pentland", "title":"Boosting and Structure Learning in Dynamic Bayesian Networks for Audio-Visual Speaker Detection", "venue":"ICPR (3)", "year":"2002", "window":"accuracy. We compare its performance to both standard structure learning and boosted parameter learning on a \u00a3xed structure. We present results for speaker detection and for the UCI  <b>chess</b>  dataset. 1. Introduction Human-centered user-interfaces based on vision and speech present challenging sensing problems in which multiple sources of information must be combined to infer the user's actions", "mykey":233},
 {"datasetID":21, "supportID":"19EF7B758D745E9B4AD44318CD1DDEF4BEF19F9B", "rexaID":"a0c05cdac8e9929a4113d37aa2588e7ae70a6b76", "author":"Tanzeem Choudhury and James M. Rehg and Vladimir Pavlovic and Alex Pentland", "title":"Boosting and Structure Learning in Dynamic Bayesian Networks for Audio-Visual Speaker Detection", "venue":"ICPR (3)", "year":"2002", "window":"accuracy. We compare its performance to both standard structure learning and boosted parameter learning on a \u00a3xed structure. We present results for speaker detection and for the UCI  <b>chess</b>  dataset. 1. Introduction Human-centered user-interfaces based on vision and speech present challenging sensing problems in which multiple sources of information must be combined to infer the user's actions", "mykey":234},
 {"datasetID":22, "supportID":"19EF7B758D745E9B4AD44318CD1DDEF4BEF19F9B", "rexaID":"a0c05cdac8e9929a4113d37aa2588e7ae70a6b76", "author":"Tanzeem Choudhury and James M. Rehg and Vladimir Pavlovic and Alex Pentland", "title":"Boosting and Structure Learning in Dynamic Bayesian Networks for Audio-Visual Speaker Detection", "venue":"ICPR (3)", "year":"2002", "window":"accuracy. We compare its performance to both standard structure learning and boosted parameter learning on a \u00a3xed structure. We present results for speaker detection and for the UCI  <b>chess</b>  dataset. 1. Introduction Human-centered user-interfaces based on vision and speech present challenging sensing problems in which multiple sources of information must be combined to infer the user's actions", "mykey":235},
 {"datasetID":60, "supportID":"1AAC6453FBB8333EE638B6D8B2BB2AFF06C3654B", "rexaID":"16dd8d9f932c39ea0ea893358ed2642560fd6244", "author":"Greg Ridgeway", "title":"The State of Boosting", "venue":"Department of Statistics University of Washington", "year":"", "window":"to examine the performance of boosting Cox's proportional hazards model, I turn to a clinical trial for testing the drug DPCA for the treatment of primary biliary cirrhosis of the <b>liver</b> (PBC). This dataset has been the subject of several modern data analyses (Fleming and Harrington 1991). I tested this method by comparing the out-of-sample predictive performance of the linear Cox model to the Cox", "mykey":236},
 {"datasetID":50, "supportID":"1B1C671C9F423A7622E15D2E11BCCF149E2884EE", "rexaID":"865daf468f10c99d290f47cc77c2a1c463e001ce", "author":"Manoranjan Dash and Huan Liu and Peter Scheuermann and Kian-Lee Tan", "title":"Fast hierarchical clustering and its validation", "venue":"Data Knowl. Eng, 44", "year":"2003", "window":"the estimated K 0 is close to K; e.g. for C5 type with M = 20, N = 10k, and K = 100, the estimated K 0 is 110. Results for high-dimensional benchmark datasets (e.g. Pendigit, <b>image segmentation</b>  and character) also show similar close estimates. The reason is with increasing dimensionality the difference between intra-cluster and inter-cluster distances", "mykey":237},
 {"datasetID":53, "supportID":"1B1C671C9F423A7622E15D2E11BCCF149E2884EE", "rexaID":"865daf468f10c99d290f47cc77c2a1c463e001ce", "author":"Manoranjan Dash and Huan Liu and Peter Scheuermann and Kian-Lee Tan", "title":"Fast hierarchical clustering and its validation", "venue":"Data Knowl. Eng, 44", "year":"2003", "window":"consists of 10,992 objects in 16 dimensions. There are 10 classes corresponding to digits 0...9. The 16 dimensions are drawn by re-sampling from handwritten digits. <b>Iris</b> dataset has 150 points in 4 dimensions in 3 clusters. Dimensions are sepal length, sepal width, petal length, and petal width. Clusters are Iris Setosa, Iris Versicolour, and Iris Virginia. Each of the 3", "mykey":238},
 {"datasetID":81, "supportID":"1B1C671C9F423A7622E15D2E11BCCF149E2884EE", "rexaID":"865daf468f10c99d290f47cc77c2a1c463e001ce", "author":"Manoranjan Dash and Huan Liu and Peter Scheuermann and Kian-Lee Tan", "title":"Fast hierarchical clustering and its validation", "venue":"Data Knowl. Eng, 44", "year":"2003", "window":"consists of 10,992 objects in 16 dimensions. There are 10 classes corresponding to <b>digits</b> 0...9. The 16 dimensions are drawn by re-sampling from <b>handwritten</b> digits. Iris dataset has 150 points in 4 dimensions in 3 clusters. Dimensions are sepal length, sepal width, petal length, and petal width. Clusters are Iris Setosa, Iris Versicolour, and Iris Virginia. Each of the 3", "mykey":239},
 {"datasetID":147, "supportID":"1B1C671C9F423A7622E15D2E11BCCF149E2884EE", "rexaID":"865daf468f10c99d290f47cc77c2a1c463e001ce", "author":"Manoranjan Dash and Huan Liu and Peter Scheuermann and Kian-Lee Tan", "title":"Fast hierarchical clustering and its validation", "venue":"Data Knowl. Eng, 44", "year":"2003", "window":"the estimated K 0 is close to K; e.g. for C5 type with M = 20, N = 10k, and K = 100, the estimated K 0 is 110. Results for high-dimensional benchmark datasets (e.g. Pendigit, <b>image segmentation</b>  and character) also show similar close estimates. The reason is with increasing dimensionality the difference between intra-cluster and inter-cluster distances", "mykey":240},
 {"datasetID":14, "supportID":"1C80B1A27A5F5F75422EBEB846B3E74BF5D4D21E", "rexaID":"eef68e57188bfabe4dff279cf32efe545bd7fe3c", "author":"Fei Sha and Lawrence K. Saul and Daniel D. Lee", "title":"Multiplicative Updates for Nonnegative Quadratic Programming in Support Vector Machines", "venue":"NIPS", "year":"2002", "window":"Kernel Polynomial Radial Data k=4 k=6 #=0.3 #=1.0 #=3.0 Sonar 9.6% 9.6% 7.6% 6.7% 10.6% <b>breast</b> <b>cancer</b> 5.1% 3.6% 4.4% 4.4% 4.4% Table 1: Misclassification error rates on the sonar and breast cancer data sets after 512 iterations of the multiplicative updates. 3.1 Multiplicative updates The loss function in eq. (6) is a special case of eq. (1) with A ij = y i y j K(x i , x j ) and b i =- 1. Thus, the", "mykey":241},
 {"datasetID":151, "supportID":"1C80B1A27A5F5F75422EBEB846B3E74BF5D4D21E", "rexaID":"eef68e57188bfabe4dff279cf32efe545bd7fe3c", "author":"Fei Sha and Lawrence K. Saul and Daniel D. Lee", "title":"Multiplicative Updates for Nonnegative Quadratic Programming in Support Vector Machines", "venue":"NIPS", "year":"2002", "window":"Kernel Polynomial Radial Data k=4 k=6 #=0.3 #=1.0 #=3.0 <b>sonar</b> 9.6% 9.6% 7.6% 6.7% 10.6% Breast cancer 5.1% 3.6% 4.4% 4.4% 4.4% Table 1: Misclassification error rates on the sonar and breast cancer data sets after 512 iterations of the multiplicative updates. 3.1 Multiplicative updates The loss function in eq. (6) is a special case of eq. (1) with A ij = y i y j K(x i , x j ) and b i =- 1. Thus, the", "mykey":242},
 {"datasetID":151, "supportID":"1C91B509103D75DED62D2CA74299D9822DF091DB", "rexaID":"fcba21d24e397e9639efbad04b5be06fbe9c0df2", "author":"Jeremy Kubica and Andrew Moore", "title":"Probabilistic Noise Identification and Data Cleaning", "venue":"ICDM", "year":"2003", "window":"from the assumed models. 5.1.1 Leaf and Rock Data The leaf and rock data, summarized in Table 1, consist of attributes extracted from a series of pictures of leaves and <b>rocks</b> respectively. The leaf data set contains 71 records from pictures of living and dead leaves. As expected, the living leaves were green in color while the dead leaves were brownish or yellow. The rock data set contains 56 records", "mykey":243},
 {"datasetID":53, "supportID":"1C91B509103D75DED62D2CA74299D9822DF091DB", "rexaID":"fcba21d24e397e9639efbad04b5be06fbe9c0df2", "author":"Jeremy Kubica and Andrew Moore", "title":"Probabilistic Noise Identification and Data Cleaning", "venue":"ICDM", "year":"2003", "window":"We also compared the algorithms by their ability to identify artificial corruptions. Three different test sets were used: a noise free version of the rock data described above, the UCI <b>Iris</b> data set, and the UCI Wine data set [3]. Noise was generated by choosing to corrupt each record with some probability p. For each record chosen, corruption and noise vectors were sampled from their", "mykey":244},
 {"datasetID":58, "supportID":"1C91B509103D75DED62D2CA74299D9822DF091DB", "rexaID":"fcba21d24e397e9639efbad04b5be06fbe9c0df2", "author":"Jeremy Kubica and Andrew Moore", "title":"Probabilistic Noise Identification and Data Cleaning", "venue":"ICDM", "year":"2003", "window":"on the level of cells, where a single cell may be generated by a different model than cells of the corresponding attribute for the other records from this class. 5 Evaluation 5.1 Naturally Corrupted Data Sets As an initial test, we ran <b>LENS</b> on several real world data sets. These data sets contained \"natural\" corruptions that were not explicitly generated from the assumed models. 5.1.1 Leaf and Rock Data", "mykey":245},
 {"datasetID":109, "supportID":"1C91B509103D75DED62D2CA74299D9822DF091DB", "rexaID":"fcba21d24e397e9639efbad04b5be06fbe9c0df2", "author":"Jeremy Kubica and Andrew Moore", "title":"Probabilistic Noise Identification and Data Cleaning", "venue":"ICDM", "year":"2003", "window":"We also compared the algorithms by their ability to identify artificial corruptions. Three different test sets were used: a noise free version of the rock data described above, the UCI Iris data set, and the UCI <b>Wine</b> data set [3]. Noise was generated by choosing to corrupt each record with some probability p. For each record chosen, corruption and noise vectors were sampled from their", "mykey":246},
 {"datasetID":14, "supportID":"1D30EA4DBDE5E58C849A69627B625E68825ABD21", "rexaID":"eef68e57188bfabe4dff279cf32efe545bd7fe3c", "author":"Fei Sha and Lawrence K. Saul and Daniel D. Lee", "title":"Multiplicative Updates for Nonnegative Quadratic Programming in Support Vector Machines", "venue":"NIPS", "year":"2002", "window":"Kernel Polynomial Radial Data k=4 k=6 #=0.3 #=1.0 #=3.0 Sonar 9.6% 9.6% 7.6% 6.7% 10.6% <b>breast</b> <b>cancer</b> 5.1% 3.6% 4.4% 4.4% 4.4% Table 1: Misclassification error rates on the sonar and breast cancer data sets after 512 iterations of the multiplicative updates. 3.1 Multiplicative updates The loss function in eq. (6) is a special case of eq. (1) with A ij = y i y j K(x i , x j ) and b i =- 1. Thus, the", "mykey":247},
 {"datasetID":151, "supportID":"1D30EA4DBDE5E58C849A69627B625E68825ABD21", "rexaID":"eef68e57188bfabe4dff279cf32efe545bd7fe3c", "author":"Fei Sha and Lawrence K. Saul and Daniel D. Lee", "title":"Multiplicative Updates for Nonnegative Quadratic Programming in Support Vector Machines", "venue":"NIPS", "year":"2002", "window":"Kernel Polynomial Radial Data k=4 k=6 #=0.3 #=1.0 #=3.0 <b>sonar</b> 9.6% 9.6% 7.6% 6.7% 10.6% Breast cancer 5.1% 3.6% 4.4% 4.4% 4.4% Table 1: Misclassification error rates on the sonar and breast cancer data sets after 512 iterations of the multiplicative updates. 3.1 Multiplicative updates The loss function in eq. (6) is a special case of eq. (1) with A ij = y i y j K(x i , x j ) and b i =- 1. Thus, the", "mykey":248},
 {"datasetID":109, "supportID":"1E0AB2F30DE8ABD4D779BAAABCAFCD4877A14233", "rexaID":"e726c8c19c80616c6de5614e578f75aca791130a", "author":"Wl/odzisl/aw Duch", "title":"Coloring black boxes: visualization of neural network decisions", "venue":"School of Computer Engineering, Nanyang Technological University", "year":"", "window":"a linear projection method is introduced, projecting the network outputs into K vertices of a polygon. Section three presents a detailed case study using an MLP and RBF networks for the 3class <b>Wine</b> dataset, and some examples for 5-class Satimage dataset. In the last section discussion and some remarks on the usefulness and further development of such visualization methods are given. Since the use of", "mykey":249},
 {"datasetID":14, "supportID":"1E1E5668889FFB40110704260547C35AF052839E", "rexaID":"ca3e1e0bf335a97cedb76be7b64610181e0f6684", "author":"Kristin P. Bennett and Ayhan Demiriz and Richard Maclin", "title":"Exploiting unlabeled data in ensemble methods", "venue":"KDD", "year":"2002", "window":"experiments we used simple multilayer perceptrons with a single layer of hidden units. The networks were trained using backpropagation with a learning rate of 0.15 and a momentum value of 0.90. The datasets for the experiments are <b>breast</b> <b>cancer</b> wisconsin, pima-indians diabetes, and letter-recognition drawn from the UCI Machine Learning repository [3]. The number of units in the hidden layer for the", "mykey":250},
 {"datasetID":17, "supportID":"1E1E5668889FFB40110704260547C35AF052839E", "rexaID":"ca3e1e0bf335a97cedb76be7b64610181e0f6684", "author":"Kristin P. Bennett and Ayhan Demiriz and Richard Maclin", "title":"Exploiting unlabeled data in ensemble methods", "venue":"KDD", "year":"2002", "window":"experiments we used simple multilayer perceptrons with a single layer of hidden units. The networks were trained using backpropagation with a learning rate of 0.15 and a momentum value of 0.90. The datasets for the experiments are <b>breast</b> <b>cancer</b> <b>wisconsin</b>  pima-indians diabetes, and letter-recognition drawn from the UCI Machine Learning repository [3]. The number of units in the hidden layer for the", "mykey":251},
 {"datasetID":15, "supportID":"1E1E5668889FFB40110704260547C35AF052839E", "rexaID":"ca3e1e0bf335a97cedb76be7b64610181e0f6684", "author":"Kristin P. Bennett and Ayhan Demiriz and Richard Maclin", "title":"Exploiting unlabeled data in ensemble methods", "venue":"KDD", "year":"2002", "window":"experiments we used simple multilayer perceptrons with a single layer of hidden units. The networks were trained using backpropagation with a learning rate of 0.15 and a momentum value of 0.90. The datasets for the experiments are <b>breast</b> <b>cancer</b> <b>wisconsin</b>  pima-indians diabetes, and letter-recognition drawn from the UCI Machine Learning repository [3]. The number of units in the hidden layer for the", "mykey":252},
 {"datasetID":16, "supportID":"1E1E5668889FFB40110704260547C35AF052839E", "rexaID":"ca3e1e0bf335a97cedb76be7b64610181e0f6684", "author":"Kristin P. Bennett and Ayhan Demiriz and Richard Maclin", "title":"Exploiting unlabeled data in ensemble methods", "venue":"KDD", "year":"2002", "window":"experiments we used simple multilayer perceptrons with a single layer of hidden units. The networks were trained using backpropagation with a learning rate of 0.15 and a momentum value of 0.90. The datasets for the experiments are <b>breast</b> <b>cancer</b> <b>wisconsin</b>  pima-indians diabetes, and letter-recognition drawn from the UCI Machine Learning repository [3]. The number of units in the hidden layer for the", "mykey":253},
 {"datasetID":34, "supportID":"1E1E5668889FFB40110704260547C35AF052839E", "rexaID":"ca3e1e0bf335a97cedb76be7b64610181e0f6684", "author":"Kristin P. Bennett and Ayhan Demiriz and Richard Maclin", "title":"Exploiting unlabeled data in ensemble methods", "venue":"KDD", "year":"2002", "window":"for the experiments are breast-cancer-wisconsin, pima-indians <b>diabetes</b>  and letter-recognition drawn from the UCI Machine Learning repository [3]. The number of units in the hidden layer for the datasets was 5 for the breast-cancer and diabetes datasets and 40 in the letter-recognition dataset. The number of training epochs was set to 20 for breastcancer, 30 for diabetes, and 30 for letter", "mykey":254},
 {"datasetID":59, "supportID":"1E1E5668889FFB40110704260547C35AF052839E", "rexaID":"ca3e1e0bf335a97cedb76be7b64610181e0f6684", "author":"Kristin P. Bennett and Ayhan Demiriz and Richard Maclin", "title":"Exploiting unlabeled data in ensemble methods", "venue":"KDD", "year":"2002", "window":"ufraction 0.10 superv ufraction 0.10 semi_sup ufraction 0.25 superv ufraction 0.25 semi_sup ufraction 0.5 superv ufraction 0.5 semi_sup Figure 1: Neural network results for the <b>Letter Recognition</b> dataset using networks with 5, 10 and 20 hidden units. Results shown are for 10, 25, and 50 percent of the data marked as unlabeled (ufractions of 0.10, 0.25, and 0.5) for AdaBoost (superv) and ASSEMBLE", "mykey":255},
 {"datasetID":2, "supportID":"2074D12FB5C513921EE3900922CD1424B2372743", "rexaID":"47354ca48da5014e0a8f5e4da7f3a7e9aaa6e9e5", "author":"Jie Cheng and Russell Greiner", "title":"Comparing Bayesian Network Classifiers", "venue":"UAI", "year":"1999", "window":"used in the experiments. Instances Dataset Attributes. Classes Train Test <b>Adult</b> 13 2 32561 16281 Nursery 8 5 8640 4320 Mushroom 22 2 5416 2708 Chess 36 2 2130 1066 Car 6 4 1728 CV5 Flare 10 3 1066 CV5 Vote 16 2 435 CV5 Brief descriptions of", "mykey":256},
 {"datasetID":19, "supportID":"2074D12FB5C513921EE3900922CD1424B2372743", "rexaID":"47354ca48da5014e0a8f5e4da7f3a7e9aaa6e9e5", "author":"Jie Cheng and Russell Greiner", "title":"Comparing Bayesian Network Classifiers", "venue":"UAI", "year":"1999", "window":"person makes over 50K a year. The discretization process ignores \"fnlwgt\" (which is one of the 14 attributes). We therefore omit \"flnwgt\" and use the remainder 13 attributes in our experiments. <b>car</b> dataset: <b>car evaluation</b> based on the six features of a car. Chess: chess end-game result classification based on board-descriptions. Flare: classifying the number of times of occurrence of certain type of", "mykey":257},
 {"datasetID":20, "supportID":"2074D12FB5C513921EE3900922CD1424B2372743", "rexaID":"47354ca48da5014e0a8f5e4da7f3a7e9aaa6e9e5", "author":"Jie Cheng and Russell Greiner", "title":"Comparing Bayesian Network Classifiers", "venue":"UAI", "year":"1999", "window":"are given below. Adult dataset: The data was extracted from the <b>census</b> bureau database. Prediction task is to determine whether a person makes over 50K a year. The discretization process ignores \"fnlwgt\" (which is one of the 14", "mykey":258},
 {"datasetID":76, "supportID":"2074D12FB5C513921EE3900922CD1424B2372743", "rexaID":"47354ca48da5014e0a8f5e4da7f3a7e9aaa6e9e5", "author":"Jie Cheng and Russell Greiner", "title":"Comparing Bayesian Network Classifiers", "venue":"UAI", "year":"1999", "window":"and GBN and TAN each did best on two of the datasets. On the data sets  <b>Nursery</b>  and \"Car\", the GBN classifier was inferior to the Na\u00efve-Bayes. The reason is, in both cases the GBN actually reduced to the Na\u00efve-Bayes with missing links (the reduced", "mykey":259},
 {"datasetID":53, "supportID":"2109C986763450F2DC87F24A1CF45261358A55B9", "rexaID":"b7408989feafd783e1ebdcdac949100e6d133b9e", "author":"David Horn and A. Gottlieb", "title":"The Method of Quantum Clustering", "venue":"NIPS", "year":"2001", "window":"minima appear, as seen in Fig. 3. Nonetheless, they lie high and contain only a few data points. The major minima are the same as in Fig. 2. 3.2 <b>iris</b> Data Our second example consists of the iris data set [10], which is a standard benchmark obtainable from the UCI repository [11]. Here we use the first two principal components to define the two dimensions in which we apply our method. Fig. 4, which", "mykey":260},
 {"datasetID":14, "supportID":"21103B8C4EAA40BC1DC85B3F063F9FE1E7ADD1B3", "rexaID":"93e4d326f6a322d66e034c1b88773f3a7f621526", "author":"Justin Bradley and Kristin P. Bennett and Bennett A. Demiriz", "title":"Constrained K-Means Clustering", "venue":"Microsoft Research Dept. of Mathematical Sciences One Microsoft Way Dept. of Decision Sciences and Eng. Sys", "year":"2000", "window":"the Johns Hopkins Ionosphere dataset and the Wisconsin Diagnostic <b>Breast</b> <b>Cancer</b> dataset (WDBC) [7]. The Ionosphere dataset contains 351 data points in R 33 and values along each dimension Contrained K-Means Clustering 6 0 5 10 15 20 25", "mykey":261},
 {"datasetID":17, "supportID":"21103B8C4EAA40BC1DC85B3F063F9FE1E7ADD1B3", "rexaID":"93e4d326f6a322d66e034c1b88773f3a7f621526", "author":"Justin Bradley and Kristin P. Bennett and Bennett A. Demiriz", "title":"Constrained K-Means Clustering", "venue":"Microsoft Research Dept. of Mathematical Sciences One Microsoft Way Dept. of Decision Sciences and Eng. Sys", "year":"2000", "window":"the Johns Hopkins Ionosphere dataset and the <b>Wisconsin</b> Diagnostic <b>Breast</b> <b>Cancer</b> dataset (WDBC) [7]. The Ionosphere dataset contains 351 data points in R 33 and values along each dimension Contrained K-Means Clustering 6 0 5 10 15 20 25", "mykey":262},
 {"datasetID":15, "supportID":"21103B8C4EAA40BC1DC85B3F063F9FE1E7ADD1B3", "rexaID":"93e4d326f6a322d66e034c1b88773f3a7f621526", "author":"Justin Bradley and Kristin P. Bennett and Bennett A. Demiriz", "title":"Constrained K-Means Clustering", "venue":"Microsoft Research Dept. of Mathematical Sciences One Microsoft Way Dept. of Decision Sciences and Eng. Sys", "year":"2000", "window":"the Johns Hopkins Ionosphere dataset and the <b>Wisconsin</b> Diagnostic <b>Breast</b> <b>Cancer</b> dataset (WDBC) [7]. The Ionosphere dataset contains 351 data points in R 33 and values along each dimension Contrained K-Means Clustering 6 0 5 10 15 20 25", "mykey":263},
 {"datasetID":16, "supportID":"21103B8C4EAA40BC1DC85B3F063F9FE1E7ADD1B3", "rexaID":"93e4d326f6a322d66e034c1b88773f3a7f621526", "author":"Justin Bradley and Kristin P. Bennett and Bennett A. Demiriz", "title":"Constrained K-Means Clustering", "venue":"Microsoft Research Dept. of Mathematical Sciences One Microsoft Way Dept. of Decision Sciences and Eng. Sys", "year":"2000", "window":"the Johns Hopkins Ionosphere dataset and the <b>Wisconsin</b> Diagnostic <b>Breast</b> <b>Cancer</b> dataset (WDBC) [7]. The Ionosphere dataset contains 351 data points in R 33 and values along each dimension Contrained K-Means Clustering 6 0 5 10 15 20 25", "mykey":264},
 {"datasetID":52, "supportID":"21103B8C4EAA40BC1DC85B3F063F9FE1E7ADD1B3", "rexaID":"93e4d326f6a322d66e034c1b88773f3a7f621526", "author":"Justin Bradley and Kristin P. Bennett and Bennett A. Demiriz", "title":"Constrained K-Means Clustering", "venue":"Microsoft Research Dept. of Mathematical Sciences One Microsoft Way Dept. of Decision Sciences and Eng. Sys", "year":"2000", "window":"tailored to network optimization [2]. These codes usually run 1 or 2 orders of magnitude faster than general linear programming (LP) codes. 4 Numerical Evaluation We report results using two real datasets: the Johns Hopkins <b>Ionosphere</b> dataset and the Wisconsin Diagnostic Breast Cancer dataset (WDBC) [7]. The Ionosphere dataset contains 351 data points in R 33 and values along each dimension", "mykey":265},
 {"datasetID":1, "supportID":"21293DC5F01635E96573C3E86ACA447B82345E12", "rexaID":"f7fdf9dbb5f98a218956025550c1f603b3cb24f2", "author":"Alexander G. Gray and Bernd Fischer and Johann Schumann and Wray L. Buntine", "title":"Automatic Derivation of Statistical Algorithms: The EM Family and Beyond", "venue":"NIPS", "year":"2002", "window":"\u00e3 \u00a3 . A slight extension of the model (toward several features) yields a Gaussian Bayes classifier model \u00e3 q . \u00e3 q has been successfully tested on various standard benchmarks [1], e.g., the <b>Abalone</b> dataset. Currently, the number of expected classes has to be given in advance. Mixture models and EM. A wide range of \u00e4 -Gaussian mixture models can be handled by AUTOBAYES, ranging from the simple 1D ( \u00e5 \u00a3", "mykey":266},
 {"datasetID":45, "supportID":"21D3B6831256AD31183159806931A1ECE15FBB46", "rexaID":"0fe919006360334c17c021088209b2a45eadb84d", "author":"Wl odzisl/aw Duch and Karol Grudzinski", "title":"Search and global minimization in similarity-based methods", "venue":"Department of Computer Methods, Nicholas Copernicus University", "year":"", "window":"features, the same as we have selected before using our logical rule extraction methods. Using all features the accuracy of 87.8\u00b11.1% was achieved, TABLE I RESULTS FOR THE CLEVELAND <b>HEART</b> DISEASE DATASET. Method Accuracy % Reference IncNet 90.0 [17] k-NN, k=28, 7 features 85.1\u00b10.5 this paper Linear Discriminant Anal. 84.5 [16] Fisher LDA 84.2 [16] k-NN, k=16 84.0\u00b10.6 this paper FSM, Feature Space", "mykey":267},
 {"datasetID":98, "supportID":"21D3B6831256AD31183159806931A1ECE15FBB46", "rexaID":"0fe919006360334c17c021088209b2a45eadb84d", "author":"Wl odzisl/aw Duch and Karol Grudzinski", "title":"Search and global minimization in similarity-based methods", "venue":"Department of Computer Methods, Nicholas Copernicus University", "year":"", "window":"significant improvements of results obtained with reduced feature set or with weighted features are obtained. Other tests that we have performed indicate that for more than half of the <b>Statlog</b> datasets [1] feature selection and weighting makes k-NN results better than that of any other classifiers used in this project. The same feature selection and weighting methods may be used to improve", "mykey":268},
 {"datasetID":60, "supportID":"223245303EF4309073B59FE1D954C48BE5BD09C9", "rexaID":"8978cfdbc009102478ae8b1d327fafd076c14538", "author":"Petri Kontkanen and Jussi Lahtinen and Petri Myllym\u00e4ki and Henry Tirri", "title":"Unsupervised Bayesian visualization of high-dimensional data", "venue":"KDD", "year":"2000", "window":"experimental results confirm this hypothesis: in cases where the leave-one-out crossvalidated classification accuracy of the NB classifier is poor in the absolute sense (as with the <b>Liver</b> Disorders data set), or in the relative sense with respect to the default classification accuracy (as with the Postoperative Patient data set), the class labeled colored images are somewhat blurred. Nevertheless,", "mykey":269},
 {"datasetID":82, "supportID":"223245303EF4309073B59FE1D954C48BE5BD09C9", "rexaID":"8978cfdbc009102478ae8b1d327fafd076c14538", "author":"Petri Kontkanen and Jussi Lahtinen and Petri Myllym\u00e4ki and Henry Tirri", "title":"Unsupervised Bayesian visualization of high-dimensional data", "venue":"KDD", "year":"2000", "window":"or in the relative sense with respect to the default classification accuracy (as with the <b>Postoperative</b> <b>Patient data</b> set), the class labeled colored images are somewhat blurred. Nevertheless, wewould like to emphasize that this does not mean that the unsupervised visualization technique has failed'' in these cases,", "mykey":270},
 {"datasetID":102, "supportID":"223245303EF4309073B59FE1D954C48BE5BD09C9", "rexaID":"8978cfdbc009102478ae8b1d327fafd076c14538", "author":"Petri Kontkanen and Jussi Lahtinen and Petri Myllym\u00e4ki and Henry Tirri", "title":"Unsupervised Bayesian visualization of high-dimensional data", "venue":"KDD", "year":"2000", "window":"purposes a reasonable approximation is usually quite sufficient. How to find effectively good approximations of the optimal visualization is however a wide 327 Figure 1: The <b>Thyroid</b> Disease data set: an example of the unsupervised visualizations obtained with the suggested method. research problem on its own, and is not discussed in detail here. In the experiments reported here we used a simple", "mykey":271},
 {"datasetID":50, "supportID":"2232BCB796B214EAF95BB80BF5193EC483C3B230", "rexaID":"3c1b37ca3a2f0825890509a5ff17081cf012fffd", "author":"Anthony K H Tung and Xin Xu and Beng Chin Ooi", "title":"CURLER: Finding and Visualizing Nonlinear Correlated Clusters", "venue":"SIGMOD Conference", "year":"2005", "window":"of three helix clusters with different cluster existence spaces, the iris plant dataset and the <b>image segmentation</b> dataset from the UCI Repository of Machine Learning Databases and Domain Theories [6], and the Iyer time series gene expression data with 10 well-known linear clusters", "mykey":272},
 {"datasetID":53, "supportID":"2232BCB796B214EAF95BB80BF5193EC483C3B230", "rexaID":"3c1b37ca3a2f0825890509a5ff17081cf012fffd", "author":"Anthony K H Tung and Xin Xu and Beng Chin Ooi", "title":"CURLER: Finding and Visualizing Nonlinear Correlated Clusters", "venue":"SIGMOD Conference", "year":"2005", "window":"of three helix clusters with different cluster existence spaces, the <b>iris</b> plant dataset and the image segmentation dataset from the UCI Repository of Machine Learning Databases and Domain Theories [6], and the Iyer time series gene expression data with 10 well-known linear clusters", "mykey":273},
 {"datasetID":147, "supportID":"2232BCB796B214EAF95BB80BF5193EC483C3B230", "rexaID":"3c1b37ca3a2f0825890509a5ff17081cf012fffd", "author":"Anthony K H Tung and Xin Xu and Beng Chin Ooi", "title":"CURLER: Finding and Visualizing Nonlinear Correlated Clusters", "venue":"SIGMOD Conference", "year":"2005", "window":"of three helix clusters with different cluster existence spaces, the iris plant dataset and the <b>image segmentation</b> dataset from the UCI Repository of Machine Learning Databases and Domain Theories [6], and the Iyer time series gene expression data with 10 well-known linear clusters", "mykey":274},
 {"datasetID":10, "supportID":"23173A7B208FE12A4A7F2875943B55CD152BAEF4", "rexaID":"e746c17201da2dd72583f7b9b0c2a6ba310412f4", "author":"Geraldine E. Rosario and Elke A. Rundensteiner and David C. Brown and Matthew O. Ward", "title":"Mapping Nominal Values to Numbers for Effective Visualization", "venue":"INFOVIS", "year":"2003", "window":"strength of association between two nominal variables)? In general, which quantification do you feel is better (easier to understand, more believable ordering and spacing)? 7.2.1 <b>Automobile</b> Data Set Case Study We chose the Automobile Data Set as an initial test because it is easy to interpret. Figures 12, 13 and 14 display the quantified versions of selected variables in a Parallel Coordinates", "mykey":275},
 {"datasetID":50, "supportID":"2362D2C6BB2575C56F708612FA4C97BC4248CB1A", "rexaID":"51d0dcaced0a60e995d1d228b618f89c00aab45b", "author":"Amund Tveit", "title":"Empirical Comparison of Accuracy and Performance for the MIPSVM classifier with Existing Classifiers", "venue":"Division of Intelligent Systems Department of Computer and Information Science, Norwegian University of Science and Technology", "year":"", "window":"As we can see from the results in figure 1, MIPSVM performs comparably well when it comes to classification accuracy for the Waveform and <b>Image Segment</b> datasets. For the Letter Recognition dataset it performs considerably worse than the other classifiers. This is likely to be caused by that MIPSVM doesn't have any balancing mechanisms one-against-the-rest", "mykey":276},
 {"datasetID":59, "supportID":"2362D2C6BB2575C56F708612FA4C97BC4248CB1A", "rexaID":"51d0dcaced0a60e995d1d228b618f89c00aab45b", "author":"Amund Tveit", "title":"Empirical Comparison of Accuracy and Performance for the MIPSVM classifier with Existing Classifiers", "venue":"Division of Intelligent Systems Department of Computer and Information Science, Norwegian University of Science and Technology", "year":"", "window":"As we can see from the results in figure 1, MIPSVM performs comparably well when it comes to classification accuracy for the Waveform and Image Segment datasets. For the <b>Letter Recognition</b> dataset it performs considerably worse than the other classifiers. This is likely to be caused by that MIPSVM doesn't have any balancing mechanisms one-against-the-rest", "mykey":277},
 {"datasetID":147, "supportID":"2362D2C6BB2575C56F708612FA4C97BC4248CB1A", "rexaID":"51d0dcaced0a60e995d1d228b618f89c00aab45b", "author":"Amund Tveit", "title":"Empirical Comparison of Accuracy and Performance for the MIPSVM classifier with Existing Classifiers", "venue":"Division of Intelligent Systems Department of Computer and Information Science, Norwegian University of Science and Technology", "year":"", "window":"As we can see from the results in figure 1, MIPSVM performs comparably well when it comes to classification accuracy for the Waveform and <b>Image Segment</b> datasets. For the Letter Recognition dataset it performs considerably worse than the other classifiers. This is likely to be caused by that MIPSVM doesn't have any balancing mechanisms one-against-the-rest", "mykey":278},
 {"datasetID":107, "supportID":"2362D2C6BB2575C56F708612FA4C97BC4248CB1A", "rexaID":"51d0dcaced0a60e995d1d228b618f89c00aab45b", "author":"Amund Tveit", "title":"Empirical Comparison of Accuracy and Performance for the MIPSVM classifier with Existing Classifiers", "venue":"Division of Intelligent Systems Department of Computer and Information Science, Norwegian University of Science and Technology", "year":"", "window":"As we can see from the results in figure 1, MIPSVM performs comparably well when it comes to classification accuracy for the <b>Waveform</b> and Image Segment datasets. For the Letter Recognition dataset it performs considerably worse than the other classifiers. This is likely to be caused by that MIPSVM doesn't have any balancing mechanisms one-against-the-rest", "mykey":279},
 {"datasetID":108, "supportID":"2362D2C6BB2575C56F708612FA4C97BC4248CB1A", "rexaID":"51d0dcaced0a60e995d1d228b618f89c00aab45b", "author":"Amund Tveit", "title":"Empirical Comparison of Accuracy and Performance for the MIPSVM classifier with Existing Classifiers", "venue":"Division of Intelligent Systems Department of Computer and Information Science, Norwegian University of Science and Technology", "year":"", "window":"As we can see from the results in figure 1, MIPSVM performs comparably well when it comes to classification accuracy for the <b>Waveform</b> and Image Segment datasets. For the Letter Recognition dataset it performs considerably worse than the other classifiers. This is likely to be caused by that MIPSVM doesn't have any balancing mechanisms one-against-the-rest", "mykey":280},
 {"datasetID":48, "supportID":"2390FD17B6347F54DA62041C4A2997AD03DB6657", "rexaID":"949c026b9a5a6fba34e6c93763433b9eb516a7bf", "author":"Martin H C Law and James T. Kwok", "title":"Applying the Bayesian Evidence Framework to \nu -Support Vector Regression", "venue":"ECML", "year":"2001", "window":"task is to predict the ages of the abalones based on 8 input attributes. We used 256 patterns for training and the remaining 3921 patterns for testing. The experiment is repeated 25 times. The third data set is the Boston <b>housing</b> data, and the task is to predict housing values in the Boston suburbs using 13 input attributes. We used 128 patterns for training and the remaining 378 patterns for testing.", "mykey":281},
 {"datasetID":2, "supportID":"23F6B75B952346223698E1C617BD806A25F58F4E", "rexaID":"97c489fe4c5d81f7fde00a62d3a5d043ec9ddcb6", "author":"Rich Caruana and Alexandru Niculescu-Mizil", "title":"An Empirical Evaluation of Supervised Learning for ROC Area", "venue":"ROCAI", "year":"2004", "window":"selection is done using the 1k validation sets, SVMs move slightly ahead of the neural nets.) Boosted stumps and plain decision trees are not competitive, though boosted stumps are best on the <b>Adult</b> data set. It is interesting to note that boosting weaker stump models is clearly inferior to boosting full decision trees on most of the test problems: boosting full decision trees yields better performance", "mykey":282},
 {"datasetID":90, "supportID":"23F6B75B952346223698E1C617BD806A25F58F4E", "rexaID":"97c489fe4c5d81f7fde00a62d3a5d043ec9ddcb6", "author":"Rich Caruana and Alexandru Niculescu-Mizil", "title":"An Empirical Evaluation of Supervised Learning for ROC Area", "venue":"ROCAI", "year":"2004", "window":"letters as negative, yielding a very unbalanced binary problem. LETTER.p2 uses letters A-M as positives and the rest as negatives, yielding a well balanced problem. HYPER SPECT is the IndianPine92 data set [9] where the difficult class <b>Soybean</b> mintill is the positive class. SLAC is a problem from the Stanford Linear 1 Department of Computer Science, Cornell University, Ithaca, NY 14853 USA email", "mykey":283},
 {"datasetID":91, "supportID":"23F6B75B952346223698E1C617BD806A25F58F4E", "rexaID":"97c489fe4c5d81f7fde00a62d3a5d043ec9ddcb6", "author":"Rich Caruana and Alexandru Niculescu-Mizil", "title":"An Empirical Evaluation of Supervised Learning for ROC Area", "venue":"ROCAI", "year":"2004", "window":"letters as negative, yielding a very unbalanced binary problem. LETTER.p2 uses letters A-M as positives and the rest as negatives, yielding a well balanced problem. HYPER SPECT is the IndianPine92 data set [9] where the difficult class <b>Soybean</b> mintill is the positive class. SLAC is a problem from the Stanford Linear 1 Department of Computer Science, Cornell University, Ithaca, NY 14853 USA email", "mykey":284},
 {"datasetID":95, "supportID":"23F6B75B952346223698E1C617BD806A25F58F4E", "rexaID":"97c489fe4c5d81f7fde00a62d3a5d043ec9ddcb6", "author":"Rich Caruana and Alexandru Niculescu-Mizil", "title":"An Empirical Evaluation of Supervised Learning for ROC Area", "venue":"ROCAI", "year":"2004", "window":"letters as negative, yielding a very unbalanced binary problem. LETTER.p2 uses letters A-M as positives and the rest as negatives, yielding a well balanced problem. HYPER <b>SPECT</b> is the IndianPine92 data set [9] where the difficult class Soybean-mintill is the positive class. SLAC is a problem from the Stanford Linear 1 Department of Computer Science, Cornell University, Ithaca, NY 14853 USA email", "mykey":285},
 {"datasetID":2, "supportID":"2434E4B7E650DFAA074247083DCC8AFD7E8D0FBE", "rexaID":"d6a42d10713c10dbc3efa4b7de53fb7f807caa0b", "author":"Dmitry Pavlov and Jianchang Mao and Byron Dom", "title":"Scaling-Up Support Vector Machines Using Boosting Algorithm", "venue":"ICPR", "year":"2000", "window":"one week of February 1998. The classification task, as we pose it, is to predict whether a user will visit the most popular site S based on his/her visiting pattern of all other sites. The <b>Adult</b> data set is available at UCI machine learning repository [1]. The task is to predict if the income of a person is greater than 50K based on several census parameters, such as age, education, marital status", "mykey":286},
 {"datasetID":4, "supportID":"2434E4B7E650DFAA074247083DCC8AFD7E8D0FBE", "rexaID":"d6a42d10713c10dbc3efa4b7de53fb7f807caa0b", "author":"Dmitry Pavlov and Jianchang Mao and Byron Dom", "title":"Scaling-Up Support Vector Machines Using Boosting Algorithm", "venue":"ICPR", "year":"2000", "window":"by the standard SVM training algorithms. 3. Experiments We compared performance of linear classifiers trained with the Boost-SMO and the Full-SMO (conventional SMO algorithm) on the following three data sets: the Reuters Data, the <b>Microsoft</b> <b>Web</b> Data and the UCI Adult Data. For the Reuters Data we looked at the classes \"acq\" and \"earn\" that have the greatest number of positive examples. The Microsoft", "mykey":287},
 {"datasetID":137, "supportID":"2434E4B7E650DFAA074247083DCC8AFD7E8D0FBE", "rexaID":"d6a42d10713c10dbc3efa4b7de53fb7f807caa0b", "author":"Dmitry Pavlov and Jianchang Mao and Byron Dom", "title":"Scaling-Up Support Vector Machines Using Boosting Algorithm", "venue":"ICPR", "year":"2000", "window":"by the standard SVM training algorithms. 3. Experiments We compared performance of linear classifiers trained with the Boost-SMO and the Full-SMO (conventional SMO algorithm) on the following three data sets: the <b>Reuters</b> Data, the Microsoft Web Data and the UCI Adult Data. For the Reuters Data we looked at the classes \"acq\" and \"earn\" that have the greatest number of positive examples. The Microsoft", "mykey":288},
 {"datasetID":151, "supportID":"24FE229EF9E1C59693D37500628A6CE1DFE997BE", "rexaID":"ff68b57a8755189599b4cbefec533e7fc64a5195", "author":"Dennis DeCoste", "title":"Anytime Interval-Valued Outputs for Kernel Machines: Fast Support Vector Machine Classification via Distance Geometry", "venue":"ICML", "year":"2002", "window":"the S i in our embedding point sequence being defined in terms of multiple z i vectors (and/or non-uniform # i 's) seems worthy of future research. 7. Experiments We checked our approach on two UCI datasets (Blake & Merz, 1998): <b>Sonar</b> (to test relatively high d (high kernel costs)) and Haberman (to contrast with related experiments (Downs et al., 2001)). We also report MISR cloud classification", "mykey":289},
 {"datasetID":43, "supportID":"24FE229EF9E1C59693D37500628A6CE1DFE997BE", "rexaID":"ff68b57a8755189599b4cbefec533e7fc64a5195", "author":"Dennis DeCoste", "title":"Anytime Interval-Valued Outputs for Kernel Machines: Fast Support Vector Machine Classification via Distance Geometry", "venue":"ICML", "year":"2002", "window":"for FL = FH using all training data as queries. Rows 23-24 report the same for when w + ,w (recall Equation 34) are not used as S 1 ,S 2 . Rows 31-34 similarly report both cases for a second (test) dataset. For the small Sonar and <b>Haberman</b>  this second set is the non-SV training examples, demonstrating that examples farther from the discriminant hyperplane often require much smaller k. Our Haberman", "mykey":290},
 {"datasetID":1, "supportID":"254311AE858749685C1ED4FEFEE93394629C0403", "rexaID":"e93397aa87eb765144f433d5488e172fa37333e5", "author":"", "title":"Efficiently Updating and Tracking the Dominant Kernel Eigenspace", "venue":"(a) Katholieke Universiteit Leuven Department of Electrical Engineering, ESAT-SCD-SISTA", "year":"", "window":"true eigenvectors are approximated whilst tracking (ie up/downdating and downsizing) a kernel matrix. We consider a representative example of a kernel matrix, based upon the known <b>Abalone</b> benchmark dataset [14] with n = 3000 training instances having dimension p = 7. We consider the common choice of the radial basis kernel function (with h the kernel width parameter, fixed at 18.57): k(x i , x j ) =", "mykey":291},
 {"datasetID":155, "supportID":"2559B9D91960755BD75BD8DAE7A9D59122DAF30D", "rexaID":"6c590e12408ebd3b9184e8f4634612e552a823e9", "author":"Stephen D. Bay", "title":"Nearest neighbor classification from multiple feature subsets", "venue":"Intell. Data Anal, 3", "year":"1999", "window":"errors if different features were 17 selected. With this method they were able to improve performance on 7 of 10 domains tested (7 from the UCI repository and 3 proprietary <b>cloud</b> classification datasets), and they noted that ECOC accuracy gains tended to increase with increased diversity among the features selected for the two-class problems. NN-ECOC is similar to MFS as they both use NN", "mykey":292},
 {"datasetID":52, "supportID":"2559B9D91960755BD75BD8DAE7A9D59122DAF30D", "rexaID":"6c590e12408ebd3b9184e8f4634612e552a823e9", "author":"Stephen D. Bay", "title":"Nearest neighbor classification from multiple feature subsets", "venue":"Intell. Data Anal, 3", "year":"1999", "window":"much as expected by chance, and 1 if the classifiers always agree. Diversity increases with smaller Kappa values. Figure 1 shows the Kappa-Error diagram for NN ensembles generated for the <b>Ionosphere</b> dataset by Bagging, randomly selecting 50 prototypes, and randomly selecting 6 6 features. Bagging results in a cloud of points centered roughly about (0.825,0.15). Using a smaller number of prototypes (50)", "mykey":293},
 {"datasetID":59, "supportID":"2559B9D91960755BD75BD8DAE7A9D59122DAF30D", "rexaID":"6c590e12408ebd3b9184e8f4634612e552a823e9", "author":"Stephen D. Bay", "title":"Nearest neighbor classification from multiple feature subsets", "venue":"Intell. Data Anal, 3", "year":"1999", "window":"with n models will usually require n times the memory of a single classifier. For many problems this amount of memory may not be significant, but Dietterich [20] notes that on the <b>Letter Recognition</b> dataset (available from the UCI repository) an ensemble of 200 decision trees obtained 100% accuracy but required 59 megabytes of storage! The entire dataset was only 712 kilobytes. 4 Experiments", "mykey":294},
 {"datasetID":80, "supportID":"2559B9D91960755BD75BD8DAE7A9D59122DAF30D", "rexaID":"6c590e12408ebd3b9184e8f4634612e552a823e9", "author":"Stephen D. Bay", "title":"Nearest neighbor classification from multiple feature subsets", "venue":"Intell. Data Anal, 3", "year":"1999", "window":"or disjoint) in combination with the CNN classifier to edit and reduce the prototypes. He also reported improvements (on five domains from the UCI repository and one <b>optical</b> character <b>recognition</b> dataset) over the baseline NN classifier if the training sets were sufficiently small and thus able to generate diverse classifiers. It is important to note that both of Alpaydin's and Skalak's work differ", "mykey":295},
 {"datasetID":101, "supportID":"2559B9D91960755BD75BD8DAE7A9D59122DAF30D", "rexaID":"6c590e12408ebd3b9184e8f4634612e552a823e9", "author":"Stephen D. Bay", "title":"Nearest neighbor classification from multiple feature subsets", "venue":"Intell. Data Anal, 3", "year":"1999", "window":"is a board game played on a 3 by 3 grid. There are two players X and O, who alternate putting X's and O's on the board. In order to win a player must get three X's or O's in a row. The <b>tic-tac-toe</b> dataset contains the 958 legal tic-tac-toe endgame boards (assuming X moves first). The classification task is to determine if X won the game. 11 Table 2: Classifier error rates. Domain NN kNN FSS BSS MFS1", "mykey":296},
 {"datasetID":62, "supportID":"25A993AF0FF666BB2BE338D3882C12ECEFD969F8", "rexaID":"8fe250a2bb9e7ca099dfd176a16a08f24bb95939", "author":"Manoranjan Dash and Huan Liu", "title":"Hybrid Search of Feature Subsets", "venue":"PRICAI", "year":"1998", "window":"having a large N and a small M values such as <b>Lung Cancer</b>  Promoters, Soybean, Splice datasets ABB takes very long time (a number of hours) to terminate. For datasets having large N value and substantially big M value such as Splice dataset FocusM takes many hours to terminate. The", "mykey":297},
 {"datasetID":69, "supportID":"25A993AF0FF666BB2BE338D3882C12ECEFD969F8", "rexaID":"8fe250a2bb9e7ca099dfd176a16a08f24bb95939", "author":"Manoranjan Dash and Huan Liu", "title":"Hybrid Search of Feature Subsets", "venue":"PRICAI", "year":"1998", "window":"having a large N and a small M values such as Lung Cancer, Promoters, Soybean, <b>Splice</b> datasets ABB takes very long time (a number of hours) to terminate. For datasets having large N value and substantially big M value such as Splice dataset FocusM takes many hours to terminate. The", "mykey":298},
 {"datasetID":90, "supportID":"25A993AF0FF666BB2BE338D3882C12ECEFD969F8", "rexaID":"8fe250a2bb9e7ca099dfd176a16a08f24bb95939", "author":"Manoranjan Dash and Huan Liu", "title":"Hybrid Search of Feature Subsets", "venue":"PRICAI", "year":"1998", "window":"having a large N and a small M values such as Lung Cancer, Promoters, <b>Soybean</b>  Splice datasets ABB takes very long time (a number of hours) to terminate. For datasets having large N value and substantially big M value such as Splice dataset FocusM takes many hours to terminate. The", "mykey":299},
 {"datasetID":91, "supportID":"25A993AF0FF666BB2BE338D3882C12ECEFD969F8", "rexaID":"8fe250a2bb9e7ca099dfd176a16a08f24bb95939", "author":"Manoranjan Dash and Huan Liu", "title":"Hybrid Search of Feature Subsets", "venue":"PRICAI", "year":"1998", "window":"having a large N and a small M values such as Lung Cancer, Promoters, <b>Soybean</b>  Splice datasets ABB takes very long time (a number of hours) to terminate. For datasets having large N value and substantially big M value such as Splice dataset FocusM takes many hours to terminate. The", "mykey":300},
 {"datasetID":111, "supportID":"25A993AF0FF666BB2BE338D3882C12ECEFD969F8", "rexaID":"8fe250a2bb9e7ca099dfd176a16a08f24bb95939", "author":"Manoranjan Dash and Huan Liu", "title":"Hybrid Search of Feature Subsets", "venue":"PRICAI", "year":"1998", "window":"1500 2000 2500 Ratio of #selected features to M Crossing Points of LVF and ABB 'LungCancer' 'Promoters' 'Soybean' 'Lymphography' 'Mushroom' 'Splice'  <b>Zoo</b>  'Vote' 'LED17' 'Par3+3' (b) Added over all datasets 10 11 12 13 14 15 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 Sum of ratios of #selected features to M for all datafiles Crossing Points of LVF and ABB (* TotalRuns) 'TotalRuns=1000' 'TotalRuns=2000'", "mykey":301},
 {"datasetID":45, "supportID":"26649007A9B47FF3B290FFF2BCF6C2A93ED238E4", "rexaID":"4eff6dda32898a8bdc8268d2a790a96b93f2e262", "author":"Thomas G. Dietterich", "title":"An Experimental Comparison of Three Methods for Constructing Ensembles of Decision Trees: Bagging, Boosting, and Randomization", "venue":"Machine Learning, 40", "year":"2000", "window":"attribute values rather than as missing values. In auto, the class variable was the make of the automobile. In the breast cancer domains, all features were treated as continuous. The <b>heart</b> disease data sets were recoded to use discrete values where appropriate. All attributes were treated as continuous in the kingrook-vs-king (krk) data set. In lymphography, the lymph-nodes-dimin, lymph-nodes-enlar,", "mykey":302},
 {"datasetID":63, "supportID":"26649007A9B47FF3B290FFF2BCF6C2A93ED238E4", "rexaID":"4eff6dda32898a8bdc8268d2a790a96b93f2e262", "author":"Thomas G. Dietterich", "title":"An Experimental Comparison of Three Methods for Constructing Ensembles of Decision Trees: Bagging, Boosting, and Randomization", "venue":"Machine Learning, 40", "year":"2000", "window":"were recoded to use discrete values where appropriate. All attributes were treated as continuous in the kingrook-vs-king (krk) data set. In <b>lymphography</b>  the lymph-nodes-dimin, lymph-nodes-enlar, and no-of-nodes-in attributes were treated as continuous. In segment, all features were rounded to four significant digits to avoid", "mykey":303},
 {"datasetID":69, "supportID":"26649007A9B47FF3B290FFF2BCF6C2A93ED238E4", "rexaID":"4eff6dda32898a8bdc8268d2a790a96b93f2e262", "author":"Thomas G. Dietterich", "title":"An Experimental Comparison of Three Methods for Constructing Ensembles of Decision Trees: Bagging, Boosting, and Randomization", "venue":"Machine Learning, 40", "year":"2000", "window":"the effect of classification noise, we added random class noise to nine domains (audiology, hypo, king-rook-vs-king-pawn (krkp), satimage, sick, <b>splice</b>  segment, vehicle, and waveform). These data sets were chosen because at least one pair of the ensemble methods gave statistically significantly different performance on these domains. We did not perform noise experiments with letter-recognition", "mykey":304},
 {"datasetID":149, "supportID":"26649007A9B47FF3B290FFF2BCF6C2A93ED238E4", "rexaID":"4eff6dda32898a8bdc8268d2a790a96b93f2e262", "author":"Thomas G. Dietterich", "title":"An Experimental Comparison of Three Methods for Constructing Ensembles of Decision Trees: Bagging, Boosting, and Randomization", "venue":"Machine Learning, 40", "year":"2000", "window":"the effect of classification noise, we added random class noise to nine domains (audiology, hypo, king-rook-vs-king-pawn (krkp), satimage, sick, splice, segment, <b>vehicle</b>  and waveform). These data sets were chosen because at least one pair of the ensemble methods gave statistically significantly different performance on these domains. We did not perform noise experiments with letter-recognition", "mykey":305},
 {"datasetID":107, "supportID":"26649007A9B47FF3B290FFF2BCF6C2A93ED238E4", "rexaID":"4eff6dda32898a8bdc8268d2a790a96b93f2e262", "author":"Thomas G. Dietterich", "title":"An Experimental Comparison of Three Methods for Constructing Ensembles of Decision Trees: Bagging, Boosting, and Randomization", "venue":"Machine Learning, 40", "year":"2000", "window":"the effect of classification noise, we added random class noise to nine domains (audiology, hypo, king-rook-vs-king-pawn (krkp), satimage, sick, splice, segment, vehicle, and <b>waveform</b> . These data sets were chosen because at least one pair of the ensemble methods gave statistically significantly different performance on these domains. We did not perform noise experiments with letter-recognition", "mykey":306},
 {"datasetID":108, "supportID":"26649007A9B47FF3B290FFF2BCF6C2A93ED238E4", "rexaID":"4eff6dda32898a8bdc8268d2a790a96b93f2e262", "author":"Thomas G. Dietterich", "title":"An Experimental Comparison of Three Methods for Constructing Ensembles of Decision Trees: Bagging, Boosting, and Randomization", "venue":"Machine Learning, 40", "year":"2000", "window":"the effect of classification noise, we added random class noise to nine domains (audiology, hypo, king-rook-vs-king-pawn (krkp), satimage, sick, splice, segment, vehicle, and <b>waveform</b> . These data sets were chosen because at least one pair of the ensemble methods gave statistically significantly different performance on these domains. We did not perform noise experiments with letter-recognition", "mykey":307},
 {"datasetID":124, "supportID":"268BA89188289CA7DAF65351FB60E4D2671AB24A", "rexaID":"9182a5f903de880381f975a70365a46fe603bdf3", "author":"Xiaofeng He and Partha Niyogi", "title":"Locality Preserving Projections", "venue":"NIPS", "year":"2003", "window":"Images Much research [5][6][7] has suggested that the human <b>face</b> images reside on a manifold embedded in the image space. In this subsection, we applied LPP to images of faces. The same <b>face image</b> dataset used in [5] is used for this experiment. Fig. 2 shows the mapping results. The images of faces are mapped into the 2-dimensional plane described by the first two coordinates of the Locality", "mykey":308},
 {"datasetID":14, "supportID":"26A38131EF881142FB5EB6443AE96E7BAC9D9068", "rexaID":"705b438dbe9ed18fe23005c774d2993019da030f", "author":"Jarkko Salojarvi and Samuel Kaski and Janne Sinkkonen", "title":"Discriminative clustering in Fisher metrics", "venue":"Neural Networks Research Centre Helsinki University of Technology", "year":"", "window":"and secondly through the density function estimate that generates the metric used to define the Fisherian Voronoi regions. IV. EXPERIMENTS Experiments were run with the Wisconsin <b>breast</b> <b>cancer</b> data set from the UCI machine learning repository [9]. The 569 samples consisted of 30 attributes, measured from malignant and benign tumors. We chose the ordinary k-means as the baseline reference method.", "mykey":309},
 {"datasetID":17, "supportID":"26A38131EF881142FB5EB6443AE96E7BAC9D9068", "rexaID":"705b438dbe9ed18fe23005c774d2993019da030f", "author":"Jarkko Salojarvi and Samuel Kaski and Janne Sinkkonen", "title":"Discriminative clustering in Fisher metrics", "venue":"Neural Networks Research Centre Helsinki University of Technology", "year":"", "window":"and secondly through the density function estimate that generates the metric used to define the Fisherian Voronoi regions. IV. EXPERIMENTS Experiments were run with the <b>Wisconsin</b> <b>breast</b> <b>cancer</b> data set from the UCI machine learning repository [9]. The 569 samples consisted of 30 attributes, measured from malignant and benign tumors. We chose the ordinary k-means as the baseline reference method.", "mykey":310},
 {"datasetID":15, "supportID":"26A38131EF881142FB5EB6443AE96E7BAC9D9068", "rexaID":"705b438dbe9ed18fe23005c774d2993019da030f", "author":"Jarkko Salojarvi and Samuel Kaski and Janne Sinkkonen", "title":"Discriminative clustering in Fisher metrics", "venue":"Neural Networks Research Centre Helsinki University of Technology", "year":"", "window":"and secondly through the density function estimate that generates the metric used to define the Fisherian Voronoi regions. IV. EXPERIMENTS Experiments were run with the <b>Wisconsin</b> <b>breast</b> <b>cancer</b> data set from the UCI machine learning repository [9]. The 569 samples consisted of 30 attributes, measured from malignant and benign tumors. We chose the ordinary k-means as the baseline reference method.", "mykey":311},
 {"datasetID":16, "supportID":"26A38131EF881142FB5EB6443AE96E7BAC9D9068", "rexaID":"705b438dbe9ed18fe23005c774d2993019da030f", "author":"Jarkko Salojarvi and Samuel Kaski and Janne Sinkkonen", "title":"Discriminative clustering in Fisher metrics", "venue":"Neural Networks Research Centre Helsinki University of Technology", "year":"", "window":"and secondly through the density function estimate that generates the metric used to define the Fisherian Voronoi regions. IV. EXPERIMENTS Experiments were run with the <b>Wisconsin</b> <b>breast</b> <b>cancer</b> data set from the UCI machine learning repository [9]. The 569 samples consisted of 30 attributes, measured from malignant and benign tumors. We chose the ordinary k-means as the baseline reference method.", "mykey":312},
 {"datasetID":150, "supportID":"26AEA27D034C1DA2F37B2AD0DF34C786F73E1E32", "rexaID":"1c251864a7292b2f635e211e0027653df4b382a2", "author":"Dietrich Wettschereck and David W. Aha", "title":"Weighting Features", "venue":"ICCBR", "year":"1995", "window":"contains some redundant features, while the <b>NETtalk</b> dataset has no irrelevant or redundant features (Wettschereck, 1994). Each dataset was randomly partitioned 25 times into disjoint training and test sets. Table 4 lists the algorithms' average test set", "mykey":313},
 {"datasetID":107, "supportID":"26AEA27D034C1DA2F37B2AD0DF34C786F73E1E32", "rexaID":"1c251864a7292b2f635e211e0027653df4b382a2", "author":"Dietrich Wettschereck and David W. Aha", "title":"Weighting Features", "venue":"ICCBR", "year":"1995", "window":"Feature Weights Computed by MI in the <b>Waveform</b> 19 Task Table 5. Average Accuracies on the Waveform Tasks Relative to k-NN Feature Weight Learning Algorithm Training Feedback Method Ignorant Method Dataset Size k-NN Relief-F k-NNV SM MI Waveform 100 77.0Sigma1.0 79.1 77.2 78.0 300 82.1Sigma0.9 82.4 81.6 82.6 Waveform+19 100 73.4Sigma1.0 78.4 76.7 78.6 300 81.3Sigma0.9 83.0 82.5 82.3 features to", "mykey":314},
 {"datasetID":108, "supportID":"26AEA27D034C1DA2F37B2AD0DF34C786F73E1E32", "rexaID":"1c251864a7292b2f635e211e0027653df4b382a2", "author":"Dietrich Wettschereck and David W. Aha", "title":"Weighting Features", "venue":"ICCBR", "year":"1995", "window":"Feature Weights Computed by MI in the <b>Waveform</b> 19 Task Table 5. Average Accuracies on the Waveform Tasks Relative to k-NN Feature Weight Learning Algorithm Training Feedback Method Ignorant Method Dataset Size k-NN Relief-F k-NNV SM MI Waveform 100 77.0Sigma1.0 79.1 77.2 78.0 300 82.1Sigma0.9 82.4 81.6 82.6 Waveform+19 100 73.4Sigma1.0 78.4 76.7 78.6 300 81.3Sigma0.9 83.0 82.5 82.3 features to", "mykey":315},
 {"datasetID":2, "supportID":"26BC7F3FF5880D0E159C4823BAF333F8C5B6DD21", "rexaID":"47354ca48da5014e0a8f5e4da7f3a7e9aaa6e9e5", "author":"Jie Cheng and Russell Greiner", "title":"Comparing Bayesian Network Classifiers", "venue":"UAI", "year":"1999", "window":"used in the experiments. Instances Dataset Attributes. Classes Train Test <b>Adult</b> 13 2 32561 16281 Nursery 8 5 8640 4320 Mushroom 22 2 5416 2708 Chess 36 2 2130 1066 DNA 60 3 2000 1186 Car 6 4 1728 CV5 Flare 10 3 1066 CV5 Vote 16 2 435 CV5", "mykey":316},
 {"datasetID":19, "supportID":"26BC7F3FF5880D0E159C4823BAF333F8C5B6DD21", "rexaID":"47354ca48da5014e0a8f5e4da7f3a7e9aaa6e9e5", "author":"Jie Cheng and Russell Greiner", "title":"Comparing Bayesian Network Classifiers", "venue":"UAI", "year":"1999", "window":"the value \"?\" in our experiments. Chess: Chess end-game result classification based on board-descriptions. DNA: Recognizing the boundaries between exons and introns given a sequence of DNA. <b>Car</b> dataset: <b>Car evaluation</b> based on the six features of a car. Flare: Classifying the number of times of occurrence of certain type of solar flare. Vote: Using voting records to classify Congressmen as", "mykey":317},
 {"datasetID":20, "supportID":"26BC7F3FF5880D0E159C4823BAF333F8C5B6DD21", "rexaID":"47354ca48da5014e0a8f5e4da7f3a7e9aaa6e9e5", "author":"Jie Cheng and Russell Greiner", "title":"Comparing Bayesian Network Classifiers", "venue":"UAI", "year":"1999", "window":"are given below. Adult dataset: The data was extracted from the <b>census</b> bureau database. Prediction task is to determine whether a person makes over 50K a year. As the discretization process ignores one of the 14 attributes", "mykey":318},
 {"datasetID":67, "supportID":"26BC7F3FF5880D0E159C4823BAF333F8C5B6DD21", "rexaID":"47354ca48da5014e0a8f5e4da7f3a7e9aaa6e9e5", "author":"Jie Cheng and Russell Greiner", "title":"Comparing Bayesian Network Classifiers", "venue":"UAI", "year":"1999", "window":"the value \"?\" in our experiments. Chess: Chess end-game result classification based on board-descriptions. <b>DNA</b>  Recognizing the boundaries between exons and introns given a sequence of DNA. Car dataset: Car evaluation based on the six features of a car. Flare: Classifying the number of times of occurrence of certain type of solar flare. Vote: Using voting records to classify Congressmen as", "mykey":319},
 {"datasetID":76, "supportID":"26BC7F3FF5880D0E159C4823BAF333F8C5B6DD21", "rexaID":"47354ca48da5014e0a8f5e4da7f3a7e9aaa6e9e5", "author":"Jie Cheng and Russell Greiner", "title":"Comparing Bayesian Network Classifiers", "venue":"UAI", "year":"1999", "window":"and GBN and TAN each did best on two of the datasets. On the data sets  <b>Nursery</b>  and \"Car\", the GBN classifier was inferior to the Na\u00efve-Bayes. The reason is, in both cases the GBN actually reduced to the Na\u00efve-Bayes with missing links (the reduced", "mykey":320},
 {"datasetID":3, "supportID":"2703BDDB7960D5BBF5FD4F42E55A3E664472D72E", "rexaID":"0a882383e36d72c5890e2d191326433e23e53c9b", "author":"James J. Liu and James Tin and Yau Kwok", "title":"An Extended Genetic Rule Induction Algorithm", "venue":"Department of Computer Science Wuhan University", "year":"", "window":"368 15 7 2 iris 150 0 4 3 vehicle 846 0 18 4 Table 2: Average accuracies and standard deviations over the ten folds (Numbers in bold indicate the highest accuracy obtained over the four methods). Dataset majority RISE SIA ESIA <b>annealing</b> 76.17#0.06 90.65#0.02 86.53#0.03 93.32 #0.01 australian 55.51#0.04 85.36#0.02 72.46#0.19 80.58#0.10 breast 65.52#0.02 91.85#0.07 84.84#0.02 94.71 #0.04 cleveland", "mykey":321},
 {"datasetID":42, "supportID":"2703BDDB7960D5BBF5FD4F42E55A3E664472D72E", "rexaID":"0a882383e36d72c5890e2d191326433e23e53c9b", "author":"James J. Liu and James Tin and Yau Kwok", "title":"An Extended Genetic Rule Induction Algorithm", "venue":"Department of Computer Science Wuhan University", "year":"", "window":"With ESIA and other GA-based rule induction algorithms, this can easily be done by incorporating various \"interestingness\" measures [8] into 5 Following [6, 14], the glass2 variant of the <b>glass</b> dataset has classes 1 and 3 combined and classes 4 to 7 deleted, and the horse-colic dataset has attributes 3, 25, 26, 27, 28 deleted and with attribute 24 being used as the class label. We also deleted all", "mykey":322},
 {"datasetID":47, "supportID":"2703BDDB7960D5BBF5FD4F42E55A3E664472D72E", "rexaID":"0a882383e36d72c5890e2d191326433e23e53c9b", "author":"James J. Liu and James Tin and Yau Kwok", "title":"An Extended Genetic Rule Induction Algorithm", "venue":"Department of Computer Science Wuhan University", "year":"", "window":"has classes 1 and 3 combined and classes 4 to 7 deleted, and the <b>horse</b> <b>colic</b> dataset has attributes 3, 25, 26, 27, 28 deleted and with attribute 24 being used as the class label. We also deleted all identifier attributes from the datasets. Table 1: Datasets used in the experiments.", "mykey":323},
 {"datasetID":90, "supportID":"27852E813C4F41417CD4B7D4A75222F4AAADA322", "rexaID":"75334c6670ccfe65b0bcd8e7a6f01c5711511e70", "author":"Nir Friedman and Dan Geiger and Mois\u00e9s Goldszmidt", "title":"Bayesian Network Classifiers", "venue":"Machine Learning, 29", "year":"1997", "window":"where the unrestricted networks performed substantially worse reveals that in these networks the number of relevant attributes influencing the classification is rather small. While these data sets (` <b>soybean</b> large'' and ``satimage'') contain 35 and 36 attributes, respectively, the classifiers induced relied only on five attributes for the class prediction. We base our definition of relevant", "mykey":324},
 {"datasetID":91, "supportID":"27852E813C4F41417CD4B7D4A75222F4AAADA322", "rexaID":"75334c6670ccfe65b0bcd8e7a6f01c5711511e70", "author":"Nir Friedman and Dan Geiger and Mois\u00e9s Goldszmidt", "title":"Bayesian Network Classifiers", "venue":"Machine Learning, 29", "year":"1997", "window":"where the unrestricted networks performed substantially worse reveals that in these networks the number of relevant attributes influencing the classification is rather small. While these data sets (` <b>soybean</b> large'' and ``satimage'') contain 35 and 36 attributes, respectively, the classifiers induced relied only on five attributes for the class prediction. We base our definition of relevant", "mykey":325},
 {"datasetID":53, "supportID":"28136B126205188A74DC58D7A8C246AABE8F86B2", "rexaID":"0e637175e298e5a11f5e7a69a442a65d7245f297", "author":"Sugato Basu", "title":"Also Appears as Technical Report, UT-AI", "venue":"PhD Proposal", "year":"2003", "window":"like stop-word removal, tf-idf weighting, and removal of very high-frequency and very low-frequency words (Dhillon & Modha, 2001). From the UCI collection we selected <b>Iris</b>  which is a well-known dataset having 150 points in 4 dimensions. We used the active pairwise constrained version of KMeans on Iris, and SPKMeans on Classic3-subset. Learning curves with cross validation For all algorithms on", "mykey":326},
 {"datasetID":109, "supportID":"28136B126205188A74DC58D7A8C246AABE8F86B2", "rexaID":"0e637175e298e5a11f5e7a69a442a65d7245f297", "author":"Sugato Basu", "title":"Also Appears as Technical Report, UT-AI", "venue":"PhD Proposal", "year":"2003", "window":"Experiments were conducted on several datasets from the UCI repository: Iris, <b>Wine</b>  and representative randomly sampled subsets from the Pen-Digits and Letter datasets. For Pen-Digits and Letter, we chose two sets of three classes: # I, J, L #", "mykey":327},
 {"datasetID":90, "supportID":"2837E081D5B39E72FB24F1894BEBFFB0FEABC9E1", "rexaID":"72c214298c31cc6c0701010f03fc82f44f22969f", "author":"Geoffrey I. Webb", "title":"OPUS: A systematic search algorithm and its application to categorical attribute-value datadriven machine learning", "venue":"School of Computing and Mathematics, Deakin University", "year":"1993", "window":"contained the least conjuncts. The OPUS o algorithm was modified to provide systematic search in this context. Cover was applied using the same experimental design as in the first study to the same data sets as well as the <b>soybean</b> large data set also from the UCI machine learning repository [19]. This data set concerns the diagnosis of soybean plant disease. It has 35 attributes, 135 attribute values,", "mykey":328},
 {"datasetID":91, "supportID":"2837E081D5B39E72FB24F1894BEBFFB0FEABC9E1", "rexaID":"72c214298c31cc6c0701010f03fc82f44f22969f", "author":"Geoffrey I. Webb", "title":"OPUS: A systematic search algorithm and its application to categorical attribute-value datadriven machine learning", "venue":"School of Computing and Mathematics, Deakin University", "year":"1993", "window":"contained the least conjuncts. The OPUS o algorithm was modified to provide systematic search in this context. Cover was applied using the same experimental design as in the first study to the same data sets as well as the <b>soybean</b> large data set also from the UCI machine learning repository [19]. This data set concerns the diagnosis of soybean plant disease. It has 35 attributes, 135 attribute values,", "mykey":329},
 {"datasetID":151, "supportID":"2874A0E977200B03C9F1A3DADEB68D41C11C8089", "rexaID":"df61ac3ab404fccb23262b177e5475d1f4c236de", "author":"Hiroshi Shimodaira and Jun Okui and Mitsuru Nakai", "title":"Modified Minimum Classification Error Learning and Its Application to Neural Networks", "venue":"SSPR/SPR", "year":"1998", "window":"generalization performance of the original MCE learning, optimization of the network architecture and learning parameters is not very important. Table 1. Performance comparison in two-class problems Data set Cancer House <b>Sonar</b> # classes 2 2 2 # training data 420 265 141 # test data 279 170 67 # attributes 9 15 60 Method # hidden units 12 12 12 Bayes/ML 95.0 98.8 100.0 NN/EBP training 91.9 96.3 95.0", "mykey":330},
 {"datasetID":54, "supportID":"2874A0E977200B03C9F1A3DADEB68D41C11C8089", "rexaID":"df61ac3ab404fccb23262b177e5475d1f4c236de", "author":"Hiroshi Shimodaira and Jun Okui and Mitsuru Nakai", "title":"Modified Minimum Classification Error Learning and Its Application to Neural Networks", "venue":"SSPR/SPR", "year":"1998", "window":"#. This shows the proposed approach is more effective than the McDermott's approach [5] discussed in Section 3. B. Results of Multi-Class Problems In order to evaluate the performance on different datasets, speech database  <b>isolet</b>  (isolated alphabet letters) of the UCI repository, and \"vowels\" (Japanese five vowels) made from the ATR continuous speech database \"Set-B\" were collected. In the \"isolet\"", "mykey":331},
 {"datasetID":32, "supportID":"28BBE8AB2BB3C6F5C8073C41BE6E10ACA0D05814", "rexaID":"43960812b02c29db1368816cd996112a9af54a99", "author":"Juan J. Rodr##guez and Carlos J. Alonso", "title":"Applying Boosting to Similarity Literals for Time Series Classification", "venue":"Department of Informatics University of Valladolid, Spain", "year":"2000", "window":"in the same way than the previous one, but 19 points are added at the end of each example, with mean 0 and variance 6 1. Again, we used the first 300 examples of each class of the corresponding dataset from the UCI ML Repository. <b>Cylinder</b>  Bell and Funnel (CBF). This is an artificial problem, introduced by Saito [Saito, 1994]. The learning task is to distinguish between these three classes:", "mykey":332},
 {"datasetID":107, "supportID":"28BBE8AB2BB3C6F5C8073C41BE6E10ACA0D05814", "rexaID":"43960812b02c29db1368816cd996112a9af54a99", "author":"Juan J. Rodr##guez and Carlos J. Alonso", "title":"Applying Boosting to Similarity Literals for Time Series Classification", "venue":"Department of Informatics University of Valladolid, Spain", "year":"2000", "window":"are sumarised in table 2. The main criterion for selecting them has been that the number of examples available were big enough, to ensure that the results were reliable. <b>Waveform</b>  This dataset was introduced by [Breiman et al., 1993]. The purpouse is to distinguish between three classes, defined by the evaluation in 1; 2 : : : 21, of the following functions: x 1 (i) = uh 1 (i) + (1 u)h 2", "mykey":333},
 {"datasetID":108, "supportID":"28BBE8AB2BB3C6F5C8073C41BE6E10ACA0D05814", "rexaID":"43960812b02c29db1368816cd996112a9af54a99", "author":"Juan J. Rodr##guez and Carlos J. Alonso", "title":"Applying Boosting to Similarity Literals for Time Series Classification", "venue":"Department of Informatics University of Valladolid, Spain", "year":"2000", "window":"are sumarised in table 2. The main criterion for selecting them has been that the number of examples available were big enough, to ensure that the results were reliable. <b>Waveform</b>  This dataset was introduced by [Breiman et al., 1993]. The purpouse is to distinguish between three classes, defined by the evaluation in 1; 2 : : : 21, of the following functions: x 1 (i) = uh 1 (i) + (1 u)h 2", "mykey":334},
 {"datasetID":52, "supportID":"2903227037619BF1B20DD0F14300D666B5D690E7", "rexaID":"b1a2a0423ea7baac76c75e758f784505eb8fc18f", "author":"Jennifer G. Dy and Carla Brodley", "title":"Feature Subset Selection and Order Identification for Unsupervised Learning", "venue":"ICML", "year":"2000", "window":"To illustrate FSSEM on real data, we present results for two data sets: <b>ionosphere</b> (Blake & Merz, 1998) and a high resolution computed tomography images of the lungs (HRCT-lung) data set (Dy et al., 1999). See Dy (1999) for experiments on additional data sets.", "mykey":335},
 {"datasetID":69, "supportID":"298B53B022520D83B1092623E4AF2B5E6A14EB82", "rexaID":"50aaf7ca6a34bd2aadbd1cb87235bda27650bb18", "author":"Blaz Zupan and Marko Bohanec and Janez Dem#sar and Ivan Bratko", "title":"Learning by Discovering Concept Hierarchies", "venue":"Artif. Intell, 109", "year":"1999", "window":"do not necessarily have such characteristics, which may be the reason why for these domains HINT's performance is worse. For example, the domain theory given with the <b>SPLICE</b> dataset [31] mentions several potentially useful intermediate concepts that share attributes. Thus these concepts form a concept lattice rather than a concept tree, and therefore can not be discovered by", "mykey":336},
 {"datasetID":155, "supportID":"29B7AF4B495899591C5EE165FB74CD8EAC151CED", "rexaID":"379f34d22ceaf54a97c9a95f0cb0e6eb2a44c1dc", "author":"Cesar Guerra-Salcedo and Stephen Chen and Darrell Whitley and Sarah Smith", "title":"Fast and Accurate Feature Selection Using Hybrid Genetic Strategies", "venue":"Department of Computer Science Colorado State University", "year":"", "window":"(LandSat), a DNA classification dataset and a <b>Cloud</b> classification dataset. On the other hand, the artificially generated classification problem rely on a LED identification problem. LED cases are artificially generated using a test case", "mykey":337},
 {"datasetID":67, "supportID":"29B7AF4B495899591C5EE165FB74CD8EAC151CED", "rexaID":"379f34d22ceaf54a97c9a95f0cb0e6eb2a44c1dc", "author":"Cesar Guerra-Salcedo and Stephen Chen and Darrell Whitley and Sarah Smith", "title":"Fast and Accurate Feature Selection Using Hybrid Genetic Strategies", "venue":"Department of Computer Science Colorado State University", "year":"", "window":"were employed and one artificially generated classification problem. The real-world classification problems are: satellite classification dataset (LandSat), a <b>DNA</b> classification dataset and a Cloud classification dataset. On the other hand, the artificially generated classification problem rely on a LED identification problem. LED cases are", "mykey":338},
 {"datasetID":69, "supportID":"29B7AF4B495899591C5EE165FB74CD8EAC151CED", "rexaID":"379f34d22ceaf54a97c9a95f0cb0e6eb2a44c1dc", "author":"Cesar Guerra-Salcedo and Stephen Chen and Darrell Whitley and Sarah Smith", "title":"Fast and Accurate Feature Selection Using Hybrid Genetic Strategies", "venue":"Department of Computer Science Colorado State University", "year":"", "window":"is a DNA dataset. The dataset represents Primate <b <b>splice</b> junction</b> gene sequences (DNA). There are 2000 training cases, 1186 test cases, and 180 binary features for each case. Three different classes exist in this", "mykey":339},
 {"datasetID":146, "supportID":"29B7AF4B495899591C5EE165FB74CD8EAC151CED", "rexaID":"379f34d22ceaf54a97c9a95f0cb0e6eb2a44c1dc", "author":"Cesar Guerra-Salcedo and Stephen Chen and Darrell Whitley and Sarah Smith", "title":"Fast and Accurate Feature Selection Using Hybrid Genetic Strategies", "venue":"Department of Computer Science Colorado State University", "year":"", "window":"were employed and one artificially generated classification problem. The real-world classification problems are: <b>satellite</b> classification dataset  <b>LandSat</b> , a DNA classification dataset and a Cloud classification dataset. On the other hand, the artificially generated classification problem rely on a LED identification problem. LED cases are", "mykey":340},
 {"datasetID":45, "supportID":"29B7EEBC893ACD2C2596DE227333480E7A118AF8", "rexaID":"58642563848aaece947320829da7d338224cb8b3", "author":"Yoav Freund and Lorne Mason", "title":"The Alternating Decision Tree Learning Algorithm", "venue":"ICML", "year":"1999", "window":"representation. To demonstrate our interpretation, we consider the alternating tree presented in Figure 4. This tree is the result of running our learning algorithm for six iterations on the cleve data set from Irvine. This is a data set of <b>heart</b> disease diagnostics for which the goal is to discriminate between sick and healthy people 3 In our mapping positive classification correspond to healthy and", "mykey":341},
 {"datasetID":69, "supportID":"29B7EEBC893ACD2C2596DE227333480E7A118AF8", "rexaID":"58642563848aaece947320829da7d338224cb8b3", "author":"Yoav Freund and Lorne Mason", "title":"The Alternating Decision Tree Learning Algorithm", "venue":"ICML", "year":"1999", "window":"1 Proportion +ve Prediction <b>splice</b> train test 0 0.2 0.4 0.6 0.8 1 -1 -0.5 0 0.5 1 Proportion +ve Prediction sick-euthyroid train test Figure 7: Calibration graphs for the splice and sick-euthyroid data sets for train and test after 100 rounds of ADTree. In addition to justifying our interpretation, the calibration graphs can potentially be used to improveour performance situation where our", "mykey":342},
 {"datasetID":73, "supportID":"29E52B3F5C3A27BD4C4A1381A4818A677678CE7A", "rexaID":"761a4165c301101283f2d272e60d2064bfdfd5f8", "author":"Farhad Hussain and Huan Liu and Einoshin Suzuki and Hongjun Lu", "title":"Exception Rule Mining with a Relative Interestingness Measure", "venue":"PAKDD", "year":"2000", "window":"Noise Strong exception Strong exception Fig. 1. Rules in the data 5 Experiments In this section we explain our interesting rules obtained from Japanese credit data and <b>mushroom</b> data [7]. The credit data set has 10 attributes and 125 instances with a binary class. The two types of classes define when a credit is given to a particular person depending on other attribute values. Based on our approach we", "mykey":343},
 {"datasetID":56, "supportID":"29EF8B34FF529284BA3CFEA8EF5231829D91646F", "rexaID":"82d533c91dcbffe00f026dfeb08fdee463030c72", "author":"Oya Ekin and Peter L. Hammer and Alexander Kogan and Pawel Winter", "title":"Distance-Based Classification Methods", "venue":"e p o r t RUTCOR ffl Rutgers Center for Operations Research ffl Rutgers University", "year":"1996", "window":"that was produced by Strathclyde University. In this version each case is described by 24 continuous attributes. There are no missing values. RRR 3-96 Page 7 4.7 <b>Labor</b> Negotiations This data set includes all collective agreements reached in the business and personal services sector for locals with at least 500 members (teachers, nurses, university staff, police, etc) in Canada in 1987 and", "mykey":344},
 {"datasetID":144, "supportID":"29EF8B34FF529284BA3CFEA8EF5231829D91646F", "rexaID":"82d533c91dcbffe00f026dfeb08fdee463030c72", "author":"Oya Ekin and Peter L. Hammer and Alexander Kogan and Pawel Winter", "title":"Distance-Based Classification Methods", "venue":"e p o r t RUTCOR ffl Rutgers Center for Operations Research ffl Rutgers University", "year":"1996", "window":"653 instances with 15 attributes each. Carter and Catlett [3] reported an 85.5% correct prediction rate, when using 71% of all 690 instances as the training set. 4.6 <b>German Credit</b> (Statlog) This data set contains data used to evaluate credit applications in Germany. It has 1000 instances. We used a version of this data set that was produced by Strathclyde University. In this version each case is", "mykey":345},
 {"datasetID":98, "supportID":"29EF8B34FF529284BA3CFEA8EF5231829D91646F", "rexaID":"82d533c91dcbffe00f026dfeb08fdee463030c72", "author":"Oya Ekin and Peter L. Hammer and Alexander Kogan and Pawel Winter", "title":"Distance-Based Classification Methods", "venue":"e p o r t RUTCOR ffl Rutgers Center for Operations Research ffl Rutgers University", "year":"1996", "window":"653 instances with 15 attributes each. Carter and Catlett [3] reported an 85.5% correct prediction rate, when using 71% of all 690 instances as the training set. 4.6 German Credit  <b>Statlog</b>  This data set contains data used to evaluate credit applications in Germany. It has 1000 instances. We used a version of this data set that was produced by Strathclyde University. In this version each case is", "mykey":346},
 {"datasetID":20, "supportID":"2A4145EF34FAADE570182E2B4269FD27D0279194", "rexaID":"57df1fe4db314b15e12a8036d1c677dd1b9633af", "author":"Aristides Gionis and Heikki Mannila and Panayiotis Tsaparas", "title":"Clustering Aggregation", "venue":"ICDE", "year":"2005", "window":"by 22 categorical attributes, such as shape, color, odor, etc. There is a class label describing if a mushroom is poisonous or edible, and there are 2,480 missing values in total. Finally, the third dataset, <b>census</b>  has been extracted from the census bureau database, and it contains demographic information on 32,561 people in the US. There are 8 categorical attributes (such as education, occupation,", "mykey":347},
 {"datasetID":105, "supportID":"2A4145EF34FAADE570182E2B4269FD27D0279194", "rexaID":"57df1fe4db314b15e12a8036d1c677dd1b9633af", "author":"Aristides Gionis and Heikki Mannila and Panayiotis Tsaparas", "title":"Clustering Aggregation", "venue":"ICDE", "year":"2005", "window":"completely parameter-free! Neither a threshold nor the number of clusters need to be specified. The number of clusters discovered by our algorithms seem to be very reasonable choices: for the <b>Votes dataset</b>  most people vote according to the official position of their political parties, so having two clusters is natural; for the Mushrooms dataset, notice that both ROCK and LIMBO achieve much better", "mykey":348},
 {"datasetID":12, "supportID":"2A6116D38E933FFADD1F86E0AE5D2722B87F17CA", "rexaID":"c171fe96d278ffa703aa1310c738cad8586b4cb3", "author":"Hirotaka Inoue and Hiroyuki Narihisa", "title":"Experiments with an Ensemble Self-Generating Neural Network", "venue":"Okayama University of Science", "year":"", "window":"The average classification accuracy of ten trials for single SGNN, ESGNN, nearest neighbor (1-NN), and 3-nearest neighbor (3-NN) with shuffling. The standard deviation is given inside the bracket. Dataset SGNN ESGNN 1-NN 3-NN <b>balance</b> <b>scale</b> 0.781(0.053) 0.843(0.059) 0.771(0.057) 0.816(0.049) breast-cancer-w 0.954(0.020) 0.967(0.023) 0.954(0.025) 0.963(0.024) glass 0.632(0.102) 0.692(0.075)", "mykey":349},
 {"datasetID":148, "supportID":"2AA5E90A66BF5BC9C0B0D598FFF551EEBB9EA039", "rexaID":"4d0ce57cae961e5c90bd1b3e7fdcad67731c6b5f", "author":"Nir Friedman and Mois\u00e9s Goldszmidt", "title":"Discretizing Continuous Attributes While Learning Bayesian Networks", "venue":"ICML", "year":"1996", "window":"from the Irvine repository [15]. We estimated the accuracy of the learned classifiers using 5-fold cross-validation, except for the  <b>shuttle</b> small\" and \"waveform-21\" datasets where we used the hold-out method. We report the mean of the prediction accuracies over all cross-validation folds. We also report the standard deviation of the accuracies found in each fold. These", "mykey":350},
 {"datasetID":107, "supportID":"2AA5E90A66BF5BC9C0B0D598FFF551EEBB9EA039", "rexaID":"4d0ce57cae961e5c90bd1b3e7fdcad67731c6b5f", "author":"Nir Friedman and Mois\u00e9s Goldszmidt", "title":"Discretizing Continuous Attributes While Learning Bayesian Networks", "venue":"ICML", "year":"1996", "window":"from the Irvine repository [15]. We estimated the accuracy of the learned classifiers using 5-fold cross-validation, except for the \"shuttle-small\" and  <b>waveform</b> 21\" datasets where we used the hold-out method. We report the mean of the prediction accuracies over all cross-validation folds. We also report the standard deviation of the accuracies found in each fold. These", "mykey":351},
 {"datasetID":108, "supportID":"2AA5E90A66BF5BC9C0B0D598FFF551EEBB9EA039", "rexaID":"4d0ce57cae961e5c90bd1b3e7fdcad67731c6b5f", "author":"Nir Friedman and Mois\u00e9s Goldszmidt", "title":"Discretizing Continuous Attributes While Learning Bayesian Networks", "venue":"ICML", "year":"1996", "window":"from the Irvine repository [15]. We estimated the accuracy of the learned classifiers using 5-fold cross-validation, except for the \"shuttle-small\" and  <b>waveform</b> 21\" datasets where we used the hold-out method. We report the mean of the prediction accuracies over all cross-validation folds. We also report the standard deviation of the accuracies found in each fold. These", "mykey":352},
 {"datasetID":14, "supportID":"2AB8DE7C9F66672FCD4973B34BA970D127A21C50", "rexaID":"eef68e57188bfabe4dff279cf32efe545bd7fe3c", "author":"Fei Sha and Lawrence K. Saul and Daniel D. Lee", "title":"Multiplicative Updates for Nonnegative Quadratic Programming in Support Vector Machines", "venue":"NIPS", "year":"2002", "window":"Kernel Polynomial Radial Data k=4 k=6 #=0.3 #=1.0 #=3.0 Sonar 9.6% 9.6% 7.6% 6.7% 10.6% <b>breast</b> <b>cancer</b> 5.1% 3.6% 4.4% 4.4% 4.4% Table 1: Misclassification error rates on the sonar and breast cancer data sets after 512 iterations of the multiplicative updates. 3.1 Multiplicative updates The loss function in eq. (6) is a special case of eq. (1) with A ij = y i y j K(x i , x j ) and b i =- 1. Thus, the", "mykey":353},
 {"datasetID":151, "supportID":"2AB8DE7C9F66672FCD4973B34BA970D127A21C50", "rexaID":"eef68e57188bfabe4dff279cf32efe545bd7fe3c", "author":"Fei Sha and Lawrence K. Saul and Daniel D. Lee", "title":"Multiplicative Updates for Nonnegative Quadratic Programming in Support Vector Machines", "venue":"NIPS", "year":"2002", "window":"Kernel Polynomial Radial Data k=4 k=6 #=0.3 #=1.0 #=3.0 <b>sonar</b> 9.6% 9.6% 7.6% 6.7% 10.6% Breast cancer 5.1% 3.6% 4.4% 4.4% 4.4% Table 1: Misclassification error rates on the sonar and breast cancer data sets after 512 iterations of the multiplicative updates. 3.1 Multiplicative updates The loss function in eq. (6) is a special case of eq. (1) with A ij = y i y j K(x i , x j ) and b i =- 1. Thus, the", "mykey":354},
 {"datasetID":2, "supportID":"2B1C028380934DB2B3BE5F2C86D6A8A105DF4211", "rexaID":"13c8e4e9474a8fbd7bd8a66695b62f218189ac6f", "author":"Andrew W. Moore and Weng-Keen Wong", "title":"Optimal Reinsertion: A New Search Operator for Accelerated and More Accurate Bayesian Network Structure Learning", "venue":"ICML", "year":"2003", "window":"if and only if an odd number of parents have value \"True\". The nodes are thus noisy exclusive-ors and so it is hard to learn a set of parents incrementally. Synth2 Synth3 Synth4 Figure 3. Synthetic datasets described in Section 3.1. RmAA <b>adult</b> 49K 15 7.7 Contributed to UCI by Ron Kohavi alarm 20K 37 2.8 Data generated from a standard Bayes Net benchmark (Beinlich et al., 1989). biosurv 150K 24 3.5", "mykey":355},
 {"datasetID":45, "supportID":"2B45C5C156180FC58E353A696C61DCE476B2B350", "rexaID":"56eb4e64499baa9ef620d2aeb5383e1739a2e6e3", "author":"Remco R. Bouckaert and Eibe Frank", "title":"Evaluating the Replicability of Significance Tests for Comparing Learning Algorithms", "venue":"PAKDD", "year":"2004", "window":"perform differently in 19 out of 27 cases. For some rows, the test consistently indicates no difference between any two of the three schemes, in particular for the iris and Hungarian <b>heart</b> disease datasets. However, most rows contain at least one cell where the outcomes of the test are not consistent. The row labeled \"consistent\" at the bottom of the table lists the number of datasets for which all", "mykey":356},
 {"datasetID":53, "supportID":"2B45C5C156180FC58E353A696C61DCE476B2B350", "rexaID":"56eb4e64499baa9ef620d2aeb5383e1739a2e6e3", "author":"Remco R. Bouckaert and Eibe Frank", "title":"Evaluating the Replicability of Significance Tests for Comparing Learning Algorithms", "venue":"PAKDD", "year":"2004", "window":"perform differently in 19 out of 27 cases. For some rows, the test consistently indicates no difference between any two of the three schemes, in particular for the <b>iris</b> and Hungarian heart disease datasets. However, most rows contain at least one cell where the outcomes of the test are not consistent. The row labeled \"consistent\" at the bottom of the table lists the number of datasets for which all", "mykey":357},
 {"datasetID":149, "supportID":"2B45C5C156180FC58E353A696C61DCE476B2B350", "rexaID":"56eb4e64499baa9ef620d2aeb5383e1739a2e6e3", "author":"Remco R. Bouckaert and Eibe Frank", "title":"Evaluating the Replicability of Significance Tests for Comparing Learning Algorithms", "venue":"PAKDD", "year":"2004", "window":"very sensitive to the particular partitioning of the anneal data. Looking at the column for naive Bayes vs. C4.5, this test could be used to justify the claim that the two perform the same for all datasets except the <b>vehicle</b> dataset just by choosing appropriate random number seeds. However, it could just as well be used to support the claim that the two algorithms perform differently in 19 out of 27", "mykey":358},
 {"datasetID":42, "supportID":"2B523CE5C92F095B443C424A714B2C8DB80EB769", "rexaID":"78b94c51025c63bb39ea776347fe151e203eaa24", "author":"Francesco Masulli", "title":"An experimental analysis of the dependence among codeword bit errors in ECOC learning machines", "venue":"and Giorgio Valentini b,c", "year":"2003", "window":"machine, varying the number of hidden units between 5 to 50, yielding to 11\u00d720 = 220 evaluations of I E , I SE , \u00a9R and \u00a9 S both for ECOC monolithic and ECOC PND learning machines. For the UCI data sets <b>glass</b>  letter and optdigits we used only 2 di\u00aeerent structures, using, respectively, 5 and 9, 120 and 140, 60 and 70 hidden units, yielding to 2 \u00d7 20 = 40 evaluations of the mutual information", "mykey":359},
 {"datasetID":34, "supportID":"2B92384C002B697B9645D8E3FCA47F906B407675", "rexaID":"b569ce235d64605aedc2e0bddb7006bd30fcad9f", "author":"Fran ois Poulet", "title":"Cooperation between automatic algorithms, interactive algorithms and visualization tools for Visual Data Mining", "venue":"ESIEA Recherche", "year":"", "window":"hyperplane and the accuracy of the algorithm. We visualize the intersection of this hyperplane with the 2D scatter plot matrices, i.e. a line in each matrix (as shown in Figure 7 with the <b>diabetes</b> data set, from the UCI repository). As we can see on figure 7, the resulting lines do not necessarily separate the two classes, the hyperplane does separate the data (the accuracy of the incremental SVM is", "mykey":360},
 {"datasetID":53, "supportID":"2B92384C002B697B9645D8E3FCA47F906B407675", "rexaID":"b569ce235d64605aedc2e0bddb7006bd30fcad9f", "author":"Fran ois Poulet", "title":"Cooperation between automatic algorithms, interactive algorithms and visualization tools for Visual Data Mining", "venue":"ESIEA Recherche", "year":"", "window":"by the user on the screen and the right part shows the transformed line (the best separating plane computed with the convex hulls). Fig. 6. An example of the automatic best separating plane on <b>iris</b> data set 2.3 Clustering The interactive algorithm described in the previous section can also be used for unsupervised classification. The computation of the convex hulls and the nearest points can be", "mykey":361},
 {"datasetID":9, "supportID":"2BBB90362B759FBA9908E7A3148EEEDD31EBE63A", "rexaID":"3caf773de7b1ad6c9236cbd03763058bc8846e9d", "author":"Dan Pelleg", "title":"Scalable and Practical Probability Density Estimators for Scientific Anomaly Detection", "venue":"School of Computer Science Carnegie Mellon University", "year":"2004", "window":"would be to first try and estimate # (say, using a model with spherical Gaussians) and use the estimate to set the rectangle tails. Experiments on real-life data were done on the  <b>mpg</b>  and \"census\" datasets from the UCI repository (Blake & Merz, 1998). The \"mpg\" data has about 400 records with 7 continuous 2 attributes. Running on this data with the number of components set to three, we get the", "mykey":362},
 {"datasetID":20, "supportID":"2BBB90362B759FBA9908E7A3148EEEDD31EBE63A", "rexaID":"3caf773de7b1ad6c9236cbd03763058bc8846e9d", "author":"Dan Pelleg", "title":"Scalable and Practical Probability Density Estimators for Scientific Anomaly Detection", "venue":"School of Computer Science Carnegie Mellon University", "year":"2004", "window":"would be to first try and estimate # (say, using a model with spherical Gaussians) and use the estimate to set the rectangle tails. Experiments on real-life data were done on the \"mpg\" and  <b>census</b>  datasets from the UCI repository (Blake & Merz, 1998). The \"mpg\" data has about 400 records with 7 continuous 2 attributes. Running on this data with the number of components set to three, we get the", "mykey":363},
 {"datasetID":29, "supportID":"2BBB90362B759FBA9908E7A3148EEEDD31EBE63A", "rexaID":"3caf773de7b1ad6c9236cbd03763058bc8846e9d", "author":"Dan Pelleg", "title":"Scalable and Practical Probability Density Estimators for Scientific Anomaly Detection", "venue":"School of Computer Science Carnegie Mellon University", "year":"2004", "window":"of points and linearly with the number of clusters. This allows for clustering with tens of thousands of centroids and millions of points using commodity <b>hardware</b>  7 1.1 Introduction Consider a dataset with R records, each having M attributes. Given a constant k, the clustering problem is to partition the data into k subsets such that each subset behaves \"well\" under some measure. For example, we", "mykey":364},
 {"datasetID":1, "supportID":"2BF600B43C073DC5CAB44DE66CBE5592909F6DD6", "rexaID":"e3cb1b3c129bc2be18036ad91073724543894185", "author":"Luc Hoegaerts and J. A. K Suykens and J. Vandewalle and Bart De Moor", "title":"Subset Based Least Squares Subspace Regression in RKHS", "venue":"Katholieke Universiteit Leuven Department of Electrical Engineering, ESAT-SCD-SISTA", "year":"", "window":"the methods relative to the standard deviations. Apart from this nondi#erence, we must remark that the mean of the KPCA based model is approximately equal to the mean of the KPLS1 model. The <b>Abalone</b> data set is another benchmark from the same UCI repository [33], consisting of 4177 cases, having p = 7 input variables. The aim is to predict the age of abalone fish from physical measurements. We picked at", "mykey":365},
 {"datasetID":2, "supportID":"2BF600B43C073DC5CAB44DE66CBE5592909F6DD6", "rexaID":"e3cb1b3c129bc2be18036ad91073724543894185", "author":"Luc Hoegaerts and J. A. K Suykens and J. Vandewalle and Bart De Moor", "title":"Subset Based Least Squares Subspace Regression in RKHS", "venue":"Katholieke Universiteit Leuven Department of Electrical Engineering, ESAT-SCD-SISTA", "year":"", "window":"side our approach achieves overall a much smaller O(nm) memory cost, compared to the typical O(n 2 ) and a computational complexity of O(nm 3 ) compared to the typical O(n 2 ). The <b>ADULT</b> UCI data set [33] consists of 45222 cases having 14 input variables. The aim is to classify if the income of a person is greater than 50K based on several census parameters, such as age, education, marital", "mykey":366},
 {"datasetID":48, "supportID":"2BF600B43C073DC5CAB44DE66CBE5592909F6DD6", "rexaID":"e3cb1b3c129bc2be18036ad91073724543894185", "author":"Luc Hoegaerts and J. A. K Suykens and J. Vandewalle and Bart De Moor", "title":"Subset Based Least Squares Subspace Regression in RKHS", "venue":"Katholieke Universiteit Leuven Department of Electrical Engineering, ESAT-SCD-SISTA", "year":"", "window":"took unity values. The use of other kernels, like the polynomial or the sigmoidal kernel, did not produce such good results as the Gaussian kernel. 6.2 Real world data examples The Boston <b>Housing</b> data set [31] consists of 506 cases having p = 13 input variables. The aim is to predict the housing prices. We standardized the data to zero mean and unit variance. We picked at random a training set of", "mykey":367},
 {"datasetID":42, "supportID":"2C06F816C0778144F70F4350830A259AA22E285D", "rexaID":"b9cc48070522b3c3f7b5d7bc3b49a06213978f8e", "author":"Ron Kohavi and Mehran Sahami", "title":"Error-Based and Entropy-Based Discretization of Continuous Features", "venue":"KDD", "year":"1996", "window":"to discretize using Ent-MDL were Sick-euthyroid and Hypothyroid, which each took about 31 seconds per fold on an SGI Challenge. The longest running time for ErrorMin was encountered with the <b>Glass</b> dataset which took 153 seconds per fold to discretize, although this was much longer than any other of the datasets examined. The ErrorMin method could not be run on the Letter domain with 300MB of main", "mykey":368},
 {"datasetID":149, "supportID":"2C06F816C0778144F70F4350830A259AA22E285D", "rexaID":"b9cc48070522b3c3f7b5d7bc3b49a06213978f8e", "author":"Ron Kohavi and Mehran Sahami", "title":"Error-Based and Entropy-Based Discretization of Continuous Features", "venue":"KDD", "year":"1996", "window":"6 13 155 20.83 12 horse-colic 7 15 368 36.91 13 hypothyroid 7 18 3163 4.77 14 ionosphere 34 0 351 35.87 15 iris 4 0 150 76.67 16 sick-euthyroid 7 18 3163 9.26 17 <b>vehicle</b> 18 0 846 77.41 Table 1: Datasets, the number of continuous features, nominal features, dataset size, and baseline error (majority inducer on the 10 folds). MDL ErrorMin-T2 ErrorMin-MDL C4.5-Disc 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15", "mykey":369},
 {"datasetID":7, "supportID":"2C9C29E8266044832AFB0F0833620E35B60BB2E9", "rexaID":"a914c3920de02d035db122f72172f31446e9f5e1", "author":"Richard Nock and Marc Sebban and David Bernard", "title":"A SIMPLE LOCALLY ADAPTIVE NEAREST NEIGHBOR RULE WITH APPLICATION TO POLLUTION FORECASTING", "venue":"International Journal of Pattern Recognition and Artificial Intelligence Vol", "year":"2003", "window":"The average accuracy of sNN is bold-faced because a paired t-test reveals a threshold risk of order < 1/10% when comparing it w.r.t. NN and tNN (see text). Accuracy# Dataset NN tNN sNN <b>Audiology</b> 65.45 4.63 65.96 4.48 68.79 2.48 Australian 77.89 0.88 77.88 0.83 77.68 1.27 Balance 85.35 1.11 86.48 1.53 86.59 1.48 Bigpole 64.73 1.23 64.75 1.34 65.43 1.19 Breast-W 95.89", "mykey":370},
 {"datasetID":8, "supportID":"2C9C29E8266044832AFB0F0833620E35B60BB2E9", "rexaID":"a914c3920de02d035db122f72172f31446e9f5e1", "author":"Richard Nock and Marc Sebban and David Bernard", "title":"A SIMPLE LOCALLY ADAPTIVE NEAREST NEIGHBOR RULE WITH APPLICATION TO POLLUTION FORECASTING", "venue":"International Journal of Pattern Recognition and Artificial Intelligence Vol", "year":"2003", "window":"The average accuracy of sNN is bold-faced because a paired t-test reveals a threshold risk of order < 1/10% when comparing it w.r.t. NN and tNN (see text). Accuracy# Dataset NN tNN sNN <b>Audiology</b> 65.45 4.63 65.96 4.48 68.79 2.48 Australian 77.89 0.88 77.88 0.83 77.68 1.27 Balance 85.35 1.11 86.48 1.53 86.59 1.48 Bigpole 64.73 1.23 64.75 1.34 65.43 1.19 Breast-W 95.89", "mykey":371},
 {"datasetID":47, "supportID":"2C9C29E8266044832AFB0F0833620E35B60BB2E9", "rexaID":"a914c3920de02d035db122f72172f31446e9f5e1", "author":"Richard Nock and Marc Sebban and David Bernard", "title":"A SIMPLE LOCALLY ADAPTIVE NEAREST NEIGHBOR RULE WITH APPLICATION TO POLLUTION FORECASTING", "venue":"International Journal of Pattern Recognition and Artificial Intelligence Vol", "year":"2003", "window":"section. The justification for the better choice of the additional neighbors in the k-sNN algorithm is now visually evident from Fig. 2, when looking at the \u00b5(k) curves. For the <b>Horse</b> <b>Colic</b> dataset, the curve of the sNN rule is clearly located over the two other curves. For particular points of 45 # \u00b5(k) # 60, the accuracies of the NN and tNN rules are similar, but they are beaten by the sNN", "mykey":372},
 {"datasetID":89, "supportID":"2CAB3FE00DD3909AA45E149A1DB413A366D644E6", "rexaID":"b7aae67d343da47c2531e6094277dc3cab591888", "author":"Christophe G. Giraud-Carrier and Tony R. Martinez", "title":"An Integrated Framework for Learning and Reasoning", "venue":"J. Artif. Intell. Res. (JAIR, 3", "year":"1995", "window":"by Lifschitz (1988) and the UCI repository (Murphy & Aha, 1992) contains many useful training sets for inductive learning. This section reports results obtained with <b>FLARE</b> on several of these datasets. Results on a number of other uses of the framework, including two expert systems, are also presented. Finally, some of the limitations of the system are described. One artifact of the", "mykey":373},
 {"datasetID":2, "supportID":"2CD7BBBD2D5540C75329333FAE0DF53C9F8932A0", "rexaID":"c26b03be3bf94e423d30b545d09efd83310e97bd", "author":"Luc Hoegaerts and J. A. K Suykens and J. Vandewalle and Bart De Moor", "title":"Primal Space Sparse Kernel Partial Least Squares Regression for Large Scale Problems Special Session paper ", "venue":"Katholieke Universiteit Leuven Department of Electrical Engineering, ESAT-SCD-SISTA", "year":"", "window":"sample with added noise (dots). The subset consists of 5 points, marked with a full dot on the figure. The <b>ADULT</b> UCI data set [24] consists of 45222 cases having 14 input variables. The aim is to classify if the income of a person is greater than 50K based on several census parameters, such as age, education, marital", "mykey":374},
 {"datasetID":48, "supportID":"2D2B0745D475E1A2D27129FBA3F201EF685877EB", "rexaID":"10e99a623872cac01bf1cd18cc027b2559db0a86", "author":"Christopher J. Merz and Michael J. Pazzani", "title":"A Principal Components Approach to Combining Regression Estimates", "venue":"Machine Learning, 36", "year":"1999", "window":"No method in the first block does particularly well for the bodyfat or <b>housing</b> data sets indicating that a moderate amount of regularization is required there. Examining the more advanced methods for handling multicollinearity in the second block of rows reveals that PCR*, EG, and CR", "mykey":375},
 {"datasetID":87, "supportID":"2D2B0745D475E1A2D27129FBA3F201EF685877EB", "rexaID":"10e99a623872cac01bf1cd18cc027b2559db0a86", "author":"Christopher J. Merz and Michael J. Pazzani", "title":"A Principal Components Approach to Combining Regression Estimates", "venue":"Machine Learning, 36", "year":"1999", "window":"Attributes Numeric Source baseball 263 16 16 CMU bodyfat 252 14 14 CMU cpu 209 6 6 UCI dementia 118 26 26 UCI-MC hansch 111 13 0 QSAR housing 506 12 12 UCI imports 160 15 15 UCI <b>servo</b> 167 4 0 UCI data set from the UCI Medical Center. The imports data set had 41 examples with missing values which were not used due to limitations in one of the learning algorithms used. 6.2. Constituent Learners The set", "mykey":376},
 {"datasetID":14, "supportID":"2DC5CBA49F8EA07F3434BC5955B1A7E4B2E2AA56", "rexaID":"51a8a23b90cb1c249ff8e01ef7d2a8e1c04e08e0", "author":"Kaizhu Huang and Haiqin Yang and Irwin King and Michael R. Lyu and Laiwan Chan", "title":"Biased Minimax Probability Machine for Medical Diagnosis", "venue":"AMAI", "year":"2004", "window":"Then we apply it to two real-world medical diagnosis datasets, the <b>breast</b> <b>cancer</b> dataset and the heart disease dataset. 4.1. A Synthetic Dataset A two-variable synthetic dataset is generated by the two-dimensional gamma distribution. Two classes of data are", "mykey":377},
 {"datasetID":45, "supportID":"2DC5CBA49F8EA07F3434BC5955B1A7E4B2E2AA56", "rexaID":"51a8a23b90cb1c249ff8e01ef7d2a8e1c04e08e0", "author":"Kaizhu Huang and Haiqin Yang and Irwin King and Michael R. Lyu and Laiwan Chan", "title":"Biased Minimax Probability Machine for Medical Diagnosis", "venue":"AMAI", "year":"2004", "window":"Then we apply it to two real-world medical diagnosis datasets, the breast-cancer dataset and the <b>heart</b> disease dataset. 4.1. A Synthetic Dataset A two-variable synthetic dataset is generated by the two-dimensional gamma distribution. Two classes of data are", "mykey":378},
 {"datasetID":45, "supportID":"2E14DB36D3B8A34E0CFFE55464560ACDA342AD24", "rexaID":"7690bf25f9378bae80e6412c3eb149cd3f7ebdc6", "author":"Rudy Setiono and Wee Kheng Leow", "title":"FERNN: An Algorithm for Fast Extraction of Rules from Neural Networks", "venue":"Appl. Intell, 12", "year":"2000", "window":"decision nodes may also improve the accuracy of the tree because samples from real world problems may be better separated by oblique hyperplanes. This is the case with the <b>heart</b> disease data set (HeartD in Table 2) where significant improvement is achieved by the neural network methods over C4.5. There is no significant difference in the accuracy and size of the decision trees generated by", "mykey":379},
 {"datasetID":48, "supportID":"2E536803D722A87FA2164BCA17510481136EE1BA", "rexaID":"49b5aef7a515640b1dde8f9f777d5a1f0c014022", "author":"Glenn Fung and M. Murat Dundar and Jinbo Bi and Bharat Rao", "title":"A fast iterative algorithm for fisher discriminant using heterogeneous kernels", "venue":"ICML", "year":"2004", "window":"used in the literature for benchmarking from the UCI Machine Learning Repository (Murphy & Aha, 1992): Ionosphere, Cleveland Heart, Pima Indians, BUPA Liver and Boston <b>Housing</b>  Additionally, a sixth dataset, the colon CAD dataset, relates to colorectal cancer diagnosis using virtual colonoscopy derived from computer tomographic images. We will refer to this dataset as the colon CAD dataset. The", "mykey":380},
 {"datasetID":52, "supportID":"2E536803D722A87FA2164BCA17510481136EE1BA", "rexaID":"49b5aef7a515640b1dde8f9f777d5a1f0c014022", "author":"Glenn Fung and M. Murat Dundar and Jinbo Bi and Bharat Rao", "title":"A fast iterative algorithm for fisher discriminant using heterogeneous kernels", "venue":"ICML", "year":"2004", "window":"p-values obtained show that there is no significant difference between A-KFD and the the standard KFD where the kernel model is chosen using a cross-validation tuning procedure. Only on two of the datasets, <b>ionosphere</b> and housing there is a small statistically significant difference for the two methods, with the performance of A-KFD being the better of the two for the ionosphere dataset and the", "mykey":381},
 {"datasetID":60, "supportID":"2E536803D722A87FA2164BCA17510481136EE1BA", "rexaID":"49b5aef7a515640b1dde8f9f777d5a1f0c014022", "author":"Glenn Fung and M. Murat Dundar and Jinbo Bi and Bharat Rao", "title":"A fast iterative algorithm for fisher discriminant using heterogeneous kernels", "venue":"ICML", "year":"2004", "window":"used in the literature for benchmarking from the UCI Machine Learning Repository (Murphy & Aha, 1992): Ionosphere, Cleveland Heart, Pima Indians, BUPA <b>Liver</b> and Boston Housing. Additionally, a sixth dataset, the colon CAD dataset, relates to colorectal cancer diagnosis using virtual colonoscopy derived from computer tomographic images. We will refer to this dataset as the colon CAD dataset. The", "mykey":382},
 {"datasetID":151, "supportID":"2E667370F535FB0CBB2AE778EE98D758DD97CA6E", "rexaID":"7a13c34f809076592c23c1e2c9c6a9a72464d911", "author":"Ayhan Demiriz and Kristin P. Bennett and Mark J. Embrechts", "title":"A Genetic Algorithm Approach for Semi-Supervised Clustering", "venue":"E-Business Department, Verizon Inc.", "year":"2002", "window":"the transduction result (using both labeled and unlabeled data) cases. Note that on the center cluster transduction does work appropriately. The inductive 1 The (k, \u00af, \u00ae) values applied for each dataset were bright (15, 0.01,0.99), <b>sonar</b> (7,0.1,1), heart (7,0.25,0.75), ionosphere (7, 0.01,0.99), house (7,0.1,0.9), housing (11,0.01, 0.99), diagnostic (11,0.4,0.6), pima (11,0.01,0.99) and", "mykey":383},
 {"datasetID":53, "supportID":"2E667370F535FB0CBB2AE778EE98D758DD97CA6E", "rexaID":"7a13c34f809076592c23c1e2c9c6a9a72464d911", "author":"Ayhan Demiriz and Kristin P. Bennett and Mark J. Embrechts", "title":"A Genetic Algorithm Approach for Semi-Supervised Clustering", "venue":"E-Business Department, Verizon Inc.", "year":"2002", "window":"506 points), House Votes (16 variables, 435 points), Breast Cancer Diagnostic (30 variables, 569 points), Pima Diabetes ( 8 variables, 769 points), and <b>Iris</b> ( 4 variables, 150 points). The datasets have categorical dependent variables except Housing. The continuous dependent variable for this dataset was categorized at the level of 21.5. Iris is a three class problem. The other datasets are", "mykey":384},
 {"datasetID":1, "supportID":"2E95A52FB5E9C8FF85AB2956A24AF6FCCB84CF3D", "rexaID":"f7fdf9dbb5f98a218956025550c1f603b3cb24f2", "author":"Alexander G. Gray and Bernd Fischer and Johann Schumann and Wray L. Buntine", "title":"Automatic Derivation of Statistical Algorithms: The EM Family and Beyond", "venue":"NIPS", "year":"2002", "window":"G 1 . A slight extension of the model (toward several features) yields a Gaussian Bayes classifier model G 2 . G 2 has been successfully tested on various standard benchmarks [1], e.g., the <b>Abalone</b> dataset. Currently, the number of expected classes has to be given in advance. Mixture models and EM. A wide range of k-Gaussian mixture models can be handled by AUTOBAYES, ranging from the simple 1D (M 1 )", "mykey":385},
 {"datasetID":1, "supportID":"2E9A9E140E5BDF2C466542A7884D758DA4A8522A", "rexaID":"5bffb4d591b37fc0c1011681ab2cf075033ca539", "author":"Anton Schwaighofer and Volker Tresp", "title":"Transductive and Inductive Methods for Approximate Gaussian Process Regression", "venue":"NIPS", "year":"2002", "window":"article, we always use the BCM with clustered data. 4 Experimental Comparison In this section we will present an evaluation of the different approximation methods discussed in Sec. 2 and 3 on four data sets. In the <b>ABALONE</b> data set [1] with 4177 examples, the goal is to predict the age of Abalones based on 8 inputs. The KIN8NM data set 4 represents the forward dynamics of an 8 link all-revolute robot", "mykey":386},
 {"datasetID":34, "supportID":"2ED9C34090CC5ADFEC26C7D1CCA02D4B695ADC0F", "rexaID":"4018f616747e1e9c96771daa20bca34c484b966a", "author":"Jochen Garcke and Michael Griebel and Michael Thess", "title":"Data Mining with Sparse Grids", "venue":"Computing, 67", "year":"2001", "window":"more than 96 % of the computation time is spent for the matrix assembly. Again, the execution times scale linearly with the number of data points. 3.3 8-dimensional problem The Pima Indians <b>Diabetes</b> data set from Irvine Machine Learning Database Repository consists of 768 instances with 8 features plus a class label which splits the data into 2 sets with 500 instances and 268 instances respectively, see", "mykey":387},
 {"datasetID":60, "supportID":"2ED9C34090CC5ADFEC26C7D1CCA02D4B695ADC0F", "rexaID":"4018f616747e1e9c96771daa20bca34c484b966a", "author":"Jochen Garcke and Michael Griebel and Michael Thess", "title":"Data Mining with Sparse Grids", "venue":"Computing, 67", "year":"2001", "window":"# = 0:01 20 3.2 6-dimensional problems 3.2.1 BUPA <b>Liver</b> The BUPA Liver Disorders data set from Irvine Machine Learning Database Repository [8] consists of 345 data points with 6 features plus a selector field used to split the data into 2 sets with 145 instances and 200 instances", "mykey":388},
 {"datasetID":79, "supportID":"2ED9C34090CC5ADFEC26C7D1CCA02D4B695ADC0F", "rexaID":"4018f616747e1e9c96771daa20bca34c484b966a", "author":"Jochen Garcke and Michael Griebel and Michael Thess", "title":"Data Mining with Sparse Grids", "venue":"Computing, 67", "year":"2001", "window":"more than 96 % of the computation time is spent for the matrix assembly. Again, the execution times scale linearly with the number of data points. 3.3 8-dimensional problem The <b>Pima</b> <b>Indians</b> <b>Diabetes</b> data set from Irvine Machine Learning Database Repository consists of 768 instances with 8 features plus a class label which splits the data into 2 sets with 500 instances and 268 instances respectively, see", "mykey":389},
 {"datasetID":148, "supportID":"2ED9C34090CC5ADFEC26C7D1CCA02D4B695ADC0F", "rexaID":"4018f616747e1e9c96771daa20bca34c484b966a", "author":"Jochen Garcke and Michael Griebel and Michael Thess", "title":"Data Mining with Sparse Grids", "venue":"Computing, 67", "year":"2001", "window":"for B l is su\u00c6ciently limited. The operations of the matrices C l and G l on the vectors are then computed on the fly when needed in the conjugate gradient iteration. 3.4.1 <b>shuttle</b> Data The shuttle data set comes from the StatLog Project [52]. It consists of 43 500 observations in the training set and 14 500 data in the testing set and has 9 attributes and 7 classes in 22 the original version. To", "mykey":390},
 {"datasetID":98, "supportID":"2ED9C34090CC5ADFEC26C7D1CCA02D4B695ADC0F", "rexaID":"4018f616747e1e9c96771daa20bca34c484b966a", "author":"Jochen Garcke and Michael Griebel and Michael Thess", "title":"Data Mining with Sparse Grids", "venue":"Computing, 67", "year":"2001", "window":"for B l is su\u00c6ciently limited. The operations of the matrices C l and G l on the vectors are then computed on the fly when needed in the conjugate gradient iteration. 3.4.1 Shuttle Data The shuttle data set comes from the <b>StatLog</b> Project [52]. It consists of 43 500 observations in the training set and 14 500 data in the testing set and has 9 attributes and 7 classes in 22 the original version. To", "mykey":391},
 {"datasetID":101, "supportID":"2ED9C34090CC5ADFEC26C7D1CCA02D4B695ADC0F", "rexaID":"4018f616747e1e9c96771daa20bca34c484b966a", "author":"Jochen Garcke and Michael Griebel and Michael Thess", "title":"Data Mining with Sparse Grids", "venue":"Computing, 67", "year":"2001", "window":"17572 0.1 94.4 % 94.8 % 4019 324 0.01 98.0 % 98.0 % 7491 692 43500 0.001 99.5 % 99.5 % 32587 2137 0.0001 99.7 % 99.6 % 74642 6776 0.00001 99.8 % 99.8 % 203227 19930 Table 8: Results for the Shuttle data set, only Level 1 3.4.2 <b>Tic-Tac-Toe</b> This data set comes from the UCI Machine Learning Repository [8]. The 958 instances encode the complete set of possible board configurations at the end of tic-tac-toe", "mykey":392},
 {"datasetID":1, "supportID":"2F28E254695E10AA2AA689127AC8C6BB1F16625B", "rexaID":"48d6beec2a36a87d9d88b6de85dd85a75e5ed24d", "author":"C. Titus Brown and Harry W. Bullen and Sean P. Kelly and Robert K. Xiao and Steven G. Satterfield and John G. Hagedorn and Judith E. Devaney", "title":"Visualization and Data Mining in an 3D Immersive Environment: Summer Project 2003", "venue":"", "year":"", "window":"an overview of the entire museum layout. 45 4.13 <b>abalone</b> The abalone data set was analysed by Sean Kelly. This dataset contains roughly 2000 instances of measurements of abalone shellfish. The biggest issue in visualizing it was the lack of distinct target attribute for", "mykey":393},
 {"datasetID":9, "supportID":"2F28E254695E10AA2AA689127AC8C6BB1F16625B", "rexaID":"48d6beec2a36a87d9d88b6de85dd85a75e5ed24d", "author":"C. Titus Brown and Harry W. Bullen and Sean P. Kelly and Robert K. Xiao and Steven G. Satterfield and John G. Hagedorn and Judith E. Devaney", "title":"Visualization and Data Mining in an 3D Immersive Environment: Summer Project 2003", "venue":"", "year":"", "window":"was analysed by Christian Brown. Overview The Miles Per Gallon  <b>MPG</b>  data set consisted of data regarding the engines of numerous cars. Each car had 8 attributes, intended to be used to predict miles per gallon for each car. The attributes were a mix of both discrete and", "mykey":394},
 {"datasetID":42, "supportID":"2F28E254695E10AA2AA689127AC8C6BB1F16625B", "rexaID":"48d6beec2a36a87d9d88b6de85dd85a75e5ed24d", "author":"C. Titus Brown and Harry W. Bullen and Sean P. Kelly and Robert K. Xiao and Steven G. Satterfield and John G. Hagedorn and Judith E. Devaney", "title":"Visualization and Data Mining in an 3D Immersive Environment: Summer Project 2003", "venue":"", "year":"", "window":"whole museum from abovelack of support for numeric targets by InfoGain. 50 4.14 <b>glass</b> The glass data set was analysed by Sean Kelly. This is a purely numeric dataset containing roughly 200 glass samples with information on amounts of various chemical elements in the samples and what purpose the glass", "mykey":395},
 {"datasetID":48, "supportID":"2F28E254695E10AA2AA689127AC8C6BB1F16625B", "rexaID":"48d6beec2a36a87d9d88b6de85dd85a75e5ed24d", "author":"C. Titus Brown and Harry W. Bullen and Sean P. Kelly and Robert K. Xiao and Steven G. Satterfield and John G. Hagedorn and Judith E. Devaney", "title":"Visualization and Data Mining in an 3D Immersive Environment: Summer Project 2003", "venue":"", "year":"", "window":"We decided to explore labeling systems and layouts more in the future. 19 Figure 4.2: A closeup of two of the MPG graphs. 20 4.3 <b>Housing</b> This data set was analysed by Robert Xiao. Overview The housing data set consisted of data regarding 506 houses in Boston, Massachusetts. Thirteen continuous attributes, including a target variable of median", "mykey":396},
 {"datasetID":50, "supportID":"2F28E254695E10AA2AA689127AC8C6BB1F16625B", "rexaID":"48d6beec2a36a87d9d88b6de85dd85a75e5ed24d", "author":"C. Titus Brown and Harry W. Bullen and Sean P. Kelly and Robert K. Xiao and Steven G. Satterfield and John G. Hagedorn and Judith E. Devaney", "title":"Visualization and Data Mining in an 3D Immersive Environment: Summer Project 2003", "venue":"", "year":"", "window":"be reduced to about three or four. 21 Figure 4.3: The graph of the ChiSquared analysis of the housing data set. 22 4.4 <b>Image Segmentation</b> This data set was analysed by Christian Brown. Overview The Image Segmentation (Seg) data set consisted of data relating numerous analyses of the colors in subdivided", "mykey":397},
 {"datasetID":60, "supportID":"2F28E254695E10AA2AA689127AC8C6BB1F16625B", "rexaID":"48d6beec2a36a87d9d88b6de85dd85a75e5ed24d", "author":"C. Titus Brown and Harry W. Bullen and Sean P. Kelly and Robert K. Xiao and Steven G. Satterfield and John G. Hagedorn and Judith E. Devaney", "title":"Visualization and Data Mining in an 3D Immersive Environment: Summer Project 2003", "venue":"", "year":"", "window":"either upgrading the system to handle a larger number of polygons or creating a series of graphs using different randomly selected data points. 27 Figure 4.5: A long shot of the full tree cover data set display. 28 4.6 BUPA <b>Liver</b> Disorder This data set was analysed by Robert Xiao. Overview This data set consisted of 6 attributes: the results of 5 different blood tests which were thought to be", "mykey":398},
 {"datasetID":73, "supportID":"2F28E254695E10AA2AA689127AC8C6BB1F16625B", "rexaID":"48d6beec2a36a87d9d88b6de85dd85a75e5ed24d", "author":"C. Titus Brown and Harry W. Bullen and Sean P. Kelly and Robert K. Xiao and Steven G. Satterfield and John G. Hagedorn and Judith E. Devaney", "title":"Visualization and Data Mining in an 3D Immersive Environment: Summer Project 2003", "venue":"", "year":"", "window":"whole museum at distance. 54 Figure 4.26: Glass dataset: whole museum from above. 55 4.15 <b>Mushroom</b> The mushroom data set was analysed by Sean kelly. The mushroom dataset reflects all the problems found in the tic-tac-toe data, but on a larger scale. Once", "mykey":399},
 {"datasetID":78, "supportID":"2F28E254695E10AA2AA689127AC8C6BB1F16625B", "rexaID":"48d6beec2a36a87d9d88b6de85dd85a75e5ed24d", "author":"C. Titus Brown and Harry W. Bullen and Sean P. Kelly and Robert K. Xiao and Steven G. Satterfield and John G. Hagedorn and Judith E. Devaney", "title":"Visualization and Data Mining in an 3D Immersive Environment: Summer Project 2003", "venue":"", "year":"", "window":"from Italy. Wines from three different types of grapes are included. There are levels of 13 chemicals provided for each of 178 instances. The target attribute is the type of grape. Figure 4.8: Wine data set in museum environment. 33 4.9 <b>Page Block</b> The Block data set was processed by Harry Bullen. This data set concerns to document analysis. Each instance describes a block of a page in a document. Each", "mykey":400},
 {"datasetID":89, "supportID":"2F28E254695E10AA2AA689127AC8C6BB1F16625B", "rexaID":"48d6beec2a36a87d9d88b6de85dd85a75e5ed24d", "author":"C. Titus Brown and Harry W. Bullen and Sean P. Kelly and Robert K. Xiao and Steven G. Satterfield and John G. Hagedorn and Judith E. Devaney", "title":"Visualization and Data Mining in an 3D Immersive Environment: Summer Project 2003", "venue":"", "year":"", "window":"whole floor from above; extended semicircular layout. 42 4.12 <b>Solar <b>Flare</b> /b> The Solar Flare data set was processed by Harry Bullen. This data set has been made up of only descrete variables. Unfortunately the 3D visualization did not produce interesting results. This is because most of the data", "mykey":401},
 {"datasetID":94, "supportID":"2F28E254695E10AA2AA689127AC8C6BB1F16625B", "rexaID":"48d6beec2a36a87d9d88b6de85dd85a75e5ed24d", "author":"C. Titus Brown and Harry W. Bullen and Sean P. Kelly and Robert K. Xiao and Steven G. Satterfield and John G. Hagedorn and Judith E. Devaney", "title":"Visualization and Data Mining in an 3D Immersive Environment: Summer Project 2003", "venue":"", "year":"", "window":"was to determine what attributes were most meaningful in determining what the block contained. The most important attributes turned out to be the size and shape of the block. Figure 4.9: Page block data set in museum environment. 34 4.10 <b>Spambase</b> The spambase data set was analysed by Sean Kelly. This dataset contains roughly 4000 instances of 58 attributes each, representing e-mail messages. One", "mykey":402},
 {"datasetID":147, "supportID":"2F28E254695E10AA2AA689127AC8C6BB1F16625B", "rexaID":"48d6beec2a36a87d9d88b6de85dd85a75e5ed24d", "author":"C. Titus Brown and Harry W. Bullen and Sean P. Kelly and Robert K. Xiao and Steven G. Satterfield and John G. Hagedorn and Judith E. Devaney", "title":"Visualization and Data Mining in an 3D Immersive Environment: Summer Project 2003", "venue":"", "year":"", "window":"be reduced to about three or four. 21 Figure 4.3: The graph of the ChiSquared analysis of the housing data set. 22 4.4 <b>Image Segmentation</b> This data set was analysed by Christian Brown. Overview The Image Segmentation (Seg) data set consisted of data relating numerous analyses of the colors in subdivided", "mykey":403},
 {"datasetID":101, "supportID":"2F28E254695E10AA2AA689127AC8C6BB1F16625B", "rexaID":"48d6beec2a36a87d9d88b6de85dd85a75e5ed24d", "author":"C. Titus Brown and Harry W. Bullen and Sean P. Kelly and Robert K. Xiao and Steven G. Satterfield and John G. Hagedorn and Judith E. Devaney", "title":"Visualization and Data Mining in an 3D Immersive Environment: Summer Project 2003", "venue":"", "year":"", "window":"the Museum Environment . . . . . . . . . . 7 3.4 Visual Component Tools . . . . . . . . . . . . . . . . . . . . . . 8 3.5 Dataflow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 4 Data Sets Analysed 17 4.1 <b>Tic-Tac-Toe</b> . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 4.2 Miles Per Gallon . . . . . . . . . . . . . . . . . . . . . . . . . . 19 4.3 Housing . . . . . . . . . .", "mykey":404},
 {"datasetID":109, "supportID":"2F28E254695E10AA2AA689127AC8C6BB1F16625B", "rexaID":"48d6beec2a36a87d9d88b6de85dd85a75e5ed24d", "author":"C. Titus Brown and Harry W. Bullen and Sean P. Kelly and Robert K. Xiao and Steven G. Satterfield and John G. Hagedorn and Judith E. Devaney", "title":"Visualization and Data Mining in an 3D Immersive Environment: Summer Project 2003", "venue":"", "year":"", "window":"any patterns in the RGB values of the roads and neighboring terrain due to significant rounding of the RGB values. 31 Figure 4.7: The two-color graph for all clear road images. 32 4.8 <b>Wine</b> The Wine data set was analysed by Harry Bullen. This data set contains the chemical analysis of wines from Italy. Wines from three different types of grapes are included. There are levels of 13 chemicals provided for", "mykey":405},
 {"datasetID":39, "supportID":"2F472C3373C3F7E50370FBDBA9E98F99FECB1A95", "rexaID":"1c1424dac83eee4c354e1de77ef9daf5f3fa5742", "author":"Charles X. Ling and Qiang Yang and Jianning Wang and Shichao Zhang", "title":"Decision trees with minimal costs", "venue":"ICML", "year":"2004", "window":"total cost.   Aimed at minimizing the total cost of test and  misclassification, our new decision-tree algorithm has  several desirable features. We will discuss these features  below, using the dataset  <b>Ecoli</b>  as an example (Blake &  Merz 1998). This dataset, after pre-processing, has 332  labelled examples, which are described by six attributes.  The numerical attributes are first discretized", "mykey":406},
 {"datasetID":9, "supportID":"2F7FEB6955567DF1ECCF0BDE3AC76BCFC7C02701", "rexaID":"d688186b7174c0a53391d92f70d64a41209d3358", "author":"D. Greig and Hava T. Siegelmann and Michael Zibulevsky", "title":"A New Class of Sigmoid Activation Functions That Don't Saturate", "venue":"", "year":"1997", "window":"a sequence of 100 networks was trained using different values of \u00f8 for each hidden node. For the <b>auto</b> <b>mpg</b>  servo and Tecator data sets (3 hidden nodes) the \u00f8 values (0:5; 1:5; 2:5) were used, for the glass data set (6 hidden nodes), the values (0:5; 1:0; 1:5; 2:0; 2:5; 3:0) were used, and for the bodyfat data set (7 hidden nodes)", "mykey":407},
 {"datasetID":42, "supportID":"2F7FEB6955567DF1ECCF0BDE3AC76BCFC7C02701", "rexaID":"d688186b7174c0a53391d92f70d64a41209d3358", "author":"D. Greig and Hava T. Siegelmann and Michael Zibulevsky", "title":"A New Class of Sigmoid Activation Functions That Don't Saturate", "venue":"", "year":"1997", "window":"(3 hidden nodes) the \u00f8 values (0:5; 1:5; 2:5) were used, for the <b>glass</b> data set (6 hidden nodes), the values (0:5; 1:0; 1:5; 2:0; 2:5; 3:0) were used, and for the bodyfat data set (7 hidden nodes) the values (0:5; 1:0; 1:5; 2:0; 2:5; 3:0; 3:5) were used. The results for these", "mykey":408},
 {"datasetID":87, "supportID":"2F7FEB6955567DF1ECCF0BDE3AC76BCFC7C02701", "rexaID":"d688186b7174c0a53391d92f70d64a41209d3358", "author":"D. Greig and Hava T. Siegelmann and Michael Zibulevsky", "title":"A New Class of Sigmoid Activation Functions That Don't Saturate", "venue":"", "year":"1997", "window":"a sequence of 100 networks was trained using different values of \u00f8 for each hidden node. For the auto-mpg, <b>servo</b> and Tecator data sets (3 hidden nodes) the \u00f8 values (0:5; 1:5; 2:5) were used, for the glass data set (6 hidden nodes), the values (0:5; 1:0; 1:5; 2:0; 2:5; 3:0) were used, and for the bodyfat data set (7 hidden nodes)", "mykey":409},
 {"datasetID":148, "supportID":"2F8FCADC514CEB994E202A03C0EA8CE3F427C2B0", "rexaID":"af8d7cc40c6d9dd6d1b261e68d612c371055b313", "author":"Jeffrey P. Bradford and Clayton Kunz and Ron Kohavi and Clifford Brunk and Carla Brodley", "title":"Appears in ECML-98 as a research note Pruning Decision Trees with Misclassification Costs", "venue":"School of Electrical Engineering", "year":"", "window":"classification based on census bureau data), breast cancer diagnosis, chess, crx (credit), german (credit), pima diabetes, road (dirt), satellite images, <b>shuttle</b>  and vehicle. In choosing the datasets, we decided on the following desiderata: 1. Datasets should be two-class to make the evaluation easier. This desideratum was hard to satisfy and we resorted to converting several multi-class", "mykey":410},
 {"datasetID":149, "supportID":"2F8FCADC514CEB994E202A03C0EA8CE3F427C2B0", "rexaID":"af8d7cc40c6d9dd6d1b261e68d612c371055b313", "author":"Jeffrey P. Bradford and Clayton Kunz and Ron Kohavi and Clifford Brunk and Carla Brodley", "title":"Appears in ECML-98 as a research note Pruning Decision Trees with Misclassification Costs", "venue":"School of Electrical Engineering", "year":"", "window":"classification based on census bureau data), breast cancer diagnosis, chess, crx (credit), german (credit), pima diabetes, road (dirt), satellite images, shuttle, and <b>vehicle</b>  In choosing the datasets, we decided on the following desiderata: 1. Datasets should be two-class to make the evaluation easier. This desideratum was hard to satisfy and we resorted to converting several multi-class", "mykey":411},
 {"datasetID":14, "supportID":"309591BC1585381F6F1E857DBF3DF20A741DFADF", "rexaID":"ec368cf4dedc904072be4ba3a84a01ae2707ae6e", "author":"David Kwartowitz and Sean Brophy and Horace Mann", "title":"Session S2D Work In Progress: Establishing multiple contexts for student's progressive refinement of data mining", "venue":"", "year":"", "window":"version of WEKA became available. Students used this version to complete an end of semester project that asked them to compare and contrast three data mining techniques to analyze the <b>Breast</b> <b>Cancer</b> Data set. Students reported having little difficulty understanding how to use the software and spent most of their time making decisions about how to prepare the data for analysis and analyzing the results.", "mykey":412},
 {"datasetID":151, "supportID":"3117D5527496F0B6A29CF12677AB275300AB7D9A", "rexaID":"8432b52f06e00a68cb8a7a49e3938190338231de", "author":"Yin Zhang and W. Nick Street", "title":"Bagging with Adaptive Costs", "venue":"Management Sciences Department University of Iowa Iowa City", "year":"", "window":"[2]: Autompg, Bupa, Glass, Haberman, Housing, Cleveland-heart-disease, Hepatitis, Ion, Pima, <b>Sonar</b>  Vehicle, WDBC, Wine and WPBC. Some of the data sets do not originally depict two-class problems so we did some transformation on the dependent variables to get binary class labels. Specifically in our experiments, Autompg data is labeled by whether", "mykey":413},
 {"datasetID":42, "supportID":"3117D5527496F0B6A29CF12677AB275300AB7D9A", "rexaID":"8432b52f06e00a68cb8a7a49e3938190338231de", "author":"Yin Zhang and W. Nick Street", "title":"Bagging with Adaptive Costs", "venue":"Management Sciences Department University of Iowa Iowa City", "year":"", "window":"and the out-of-bag margin estimation will result in better generalization as it does in stacking. 3. Computational Experiments Bacing was implemented using MATLAB and tested on 14 UCI repository data sets [2]: Autompg, Bupa, <b>Glass</b>  Haberman, Housing, Cleveland-heart-disease, Hepatitis, Ion, Pima, Sonar, Vehicle, WDBC, Wine and WPBC. Some of the data sets do not originally depict two-class problems", "mykey":414},
 {"datasetID":43, "supportID":"3117D5527496F0B6A29CF12677AB275300AB7D9A", "rexaID":"8432b52f06e00a68cb8a7a49e3938190338231de", "author":"Yin Zhang and W. Nick Street", "title":"Bagging with Adaptive Costs", "venue":"Management Sciences Department University of Iowa Iowa City", "year":"", "window":"and the out-of-bag margin estimation will result in better generalization as it does in stacking. 3. Computational Experiments Bacing was implemented using MATLAB and tested on 14 UCI repository data sets [2]: Autompg, Bupa, Glass, <b>Haberman</b>  Housing, Cleveland-heart-disease, Hepatitis, Ion, Pima, Sonar, Vehicle, WDBC, Wine and WPBC. Some of the data sets do not originally depict two-class problems", "mykey":415},
 {"datasetID":48, "supportID":"3117D5527496F0B6A29CF12677AB275300AB7D9A", "rexaID":"8432b52f06e00a68cb8a7a49e3938190338231de", "author":"Yin Zhang and W. Nick Street", "title":"Bagging with Adaptive Costs", "venue":"Management Sciences Department University of Iowa Iowa City", "year":"", "window":"and the out-of-bag margin estimation will result in better generalization as it does in stacking. 3. Computational Experiments Bacing was implemented using MATLAB and tested on 14 UCI repository data sets [2]: Autompg, Bupa, Glass, Haberman, <b>Housing</b>  Cleveland-heart-disease, Hepatitis, Ion, Pima, Sonar, Vehicle, WDBC, Wine and WPBC. Some of the data sets do not originally depict two-class problems", "mykey":416},
 {"datasetID":149, "supportID":"3117D5527496F0B6A29CF12677AB275300AB7D9A", "rexaID":"8432b52f06e00a68cb8a7a49e3938190338231de", "author":"Yin Zhang and W. Nick Street", "title":"Bagging with Adaptive Costs", "venue":"Management Sciences Department University of Iowa Iowa City", "year":"", "window":"[2]: Autompg, Bupa, Glass, Haberman, Housing, Cleveland-heart-disease, Hepatitis, Ion, Pima, Sonar, <b>Vehicle</b>  WDBC, Wine and WPBC. Some of the data sets do not originally depict two-class problems so we did some transformation on the dependent variables to get binary class labels. Specifically in our experiments, Autompg data is labeled by whether", "mykey":417},
 {"datasetID":109, "supportID":"3117D5527496F0B6A29CF12677AB275300AB7D9A", "rexaID":"8432b52f06e00a68cb8a7a49e3938190338231de", "author":"Yin Zhang and W. Nick Street", "title":"Bagging with Adaptive Costs", "venue":"Management Sciences Department University of Iowa Iowa City", "year":"", "window":"[2]: Autompg, Bupa, Glass, Haberman, Housing, Cleveland-heart-disease, Hepatitis, Ion, Pima, Sonar, Vehicle, WDBC, <b>Wine</b> and WPBC. Some of the data sets do not originally depict two-class problems so we did some transformation on the dependent variables to get binary class labels. Specifically in our experiments, Autompg data is labeled by whether", "mykey":418},
 {"datasetID":107, "supportID":"313FEA130984C76B95CDB6735CA6530ACA66E046", "rexaID":"8f88ed1098d62276d3e69961c8c154b13865c989", "author":"Juan J. Rodr and guez Diez and Carlos J. Alonso", "title":"Learning Classification RBF Networks by Boosting", "venue":"Lenguajes y Sistemas Inform#aticos", "year":"", "window":"are summarized in table 1. The data sets <b>waveform</b>  waveform with noise [5, 6], CBF (cylinder, bell and funnel) [19] and control charts [1, 3] were already used in our work on boosting distance literals [18]. Auslan is the Australian sign", "mykey":419},
 {"datasetID":108, "supportID":"313FEA130984C76B95CDB6735CA6530ACA66E046", "rexaID":"8f88ed1098d62276d3e69961c8c154b13865c989", "author":"Juan J. Rodr and guez Diez and Carlos J. Alonso", "title":"Learning Classification RBF Networks by Boosting", "venue":"Lenguajes y Sistemas Inform#aticos", "year":"", "window":"are summarized in table 1. The data sets <b>waveform</b>  waveform with noise [5, 6], CBF (cylinder, bell and funnel) [19] and control charts [1, 3] were already used in our work on boosting distance literals [18]. Auslan is the Australian sign", "mykey":420},
 {"datasetID":53, "supportID":"31669C0811DCDF5AA0F23E5FD7BB7D2A34967108", "rexaID":"030d9becf3287c5da5f82e2c2da60937121b3ce5", "author":"Wl odzisl/aw Duch and Rafal Adamczak and Norbert Jankowski", "title":"Initialization of adaptive parameters in density networks", "venue":"Department of Computer Methods, Nicholas Copernicus University", "year":"", "window":"network parameters, but it is interesting to note that these results are frequently already of rather high quality. Except for galaxies all other data was obtained from the UCI repository [13]. <b>Iris</b> dataset contains 150 cases in 3 classes. After initialization with Gaussian functions including rotations only 4 classification errors are made (97.3% accuracy), which is a better results than many", "mykey":421},
 {"datasetID":14, "supportID":"31B54D3A573A6383A9C3FA9DDFD11C4B51855F70", "rexaID":"25c83dd7b43db4459e6eec12a71a229e10afac90", "author":"M. V. Fidelis and Heitor S. Lopes and Alex Alves Freitas", "title":"Discovering Comprehensible Classification Rules with a Genetic Algorithm", "venue":"UEPG, CPD CEFET-PR, CPGEI PUC-PR, PPGIA Praa Santos Andrade, s/n Av. Sete de Setembro", "year":"", "window":"in the medical domains of dermatology and <b>breast</b> <b>cancer</b>  These data sets were obtained from the UCI (University of California at Irvine) - Machine Learning Repository [17]. These data sets have been used extensively for classification tasks using different paradigms,", "mykey":422},
 {"datasetID":33, "supportID":"31B54D3A573A6383A9C3FA9DDFD11C4B51855F70", "rexaID":"25c83dd7b43db4459e6eec12a71a229e10afac90", "author":"M. V. Fidelis and Heitor S. Lopes and Alex Alves Freitas", "title":"Discovering Comprehensible Classification Rules with a Genetic Algorithm", "venue":"UEPG, CPD CEFET-PR, CPGEI PUC-PR, PPGIA Praa Santos Andrade, s/n Av. Sete de Setembro", "year":"", "window":"is fixed, the number of rule conditions (phenotype) is variable. The GA also has specific mutation operators for this chromosome encoding. The algorithm was evaluated on two public domain, realworld data sets (on the medical domains of <b>dermatology</b> and breast cancer). 1 Introduction This work presents a system based on genetic algorithms (GAs) to perform the task of classification. The system is", "mykey":423},
 {"datasetID":14, "supportID":"3296733E38810A4F714914BA7D59AAFBEBFEF473", "rexaID":"716db199e5f006b8b564ef192177f08d5edefe9b", "author":"K. A. J Doherty and Rolf Adams and Neil Davey", "title":"Unsupervised Learning with Normalised Data and Non-Euclidean Norms", "venue":"University of Hertfordshire", "year":"", "window":"considered were the Ionosphere, Image Segmentation (training data), Wisconsin Diagnostic <b>Breast</b> <b>Cancer</b> (WDBC) and Wine data sets. These data sets were selected to show our approach on data with a range of classes, dimensionality and data distributions. The basic characteristics of each data set are shown in table 2. Tab l e", "mykey":424},
 {"datasetID":50, "supportID":"3296733E38810A4F714914BA7D59AAFBEBFEF473", "rexaID":"716db199e5f006b8b564ef192177f08d5edefe9b", "author":"K. A. J Doherty and Rolf Adams and Neil Davey", "title":"Unsupervised Learning with Normalised Data and Non-Euclidean Norms", "venue":"University of Hertfordshire", "year":"", "window":"Segmentation Training Data K L 0.1 L 0.5 L 1 L 2 L 4 L# 3 518 539 494 450 446 437 5 818 874 772 692 720 678 9 1345 1424 1249 1184 1167 1066 Using the UCI Ionosphere, WDBC and <b>Image Segmentation</b> data sets we performed a K -NN search. For each member of the data set of class c,where c # C (q.v. Table 2), the K-NNs are identified and a count maintained of those neighbours whose class was also c. Table", "mykey":425},
 {"datasetID":52, "supportID":"3296733E38810A4F714914BA7D59AAFBEBFEF473", "rexaID":"716db199e5f006b8b564ef192177f08d5edefe9b", "author":"K. A. J Doherty and Rolf Adams and Neil Davey", "title":"Unsupervised Learning with Normalised Data and Non-Euclidean Norms", "venue":"University of Hertfordshire", "year":"", "window":"from the UCI Machine Learning Repository [5]. The data sets considered were the <b>Ionosphere</b>  Image Segmentation (training data), Wisconsin Diagnostic Breast Cancer (WDBC) and Wine data sets. These data sets were selected to show our approach on data with a", "mykey":426},
 {"datasetID":147, "supportID":"3296733E38810A4F714914BA7D59AAFBEBFEF473", "rexaID":"716db199e5f006b8b564ef192177f08d5edefe9b", "author":"K. A. J Doherty and Rolf Adams and Neil Davey", "title":"Unsupervised Learning with Normalised Data and Non-Euclidean Norms", "venue":"University of Hertfordshire", "year":"", "window":"Segmentation Training Data K L 0.1 L 0.5 L 1 L 2 L 4 L# 3 518 539 494 450 446 437 5 818 874 772 692 720 678 9 1345 1424 1249 1184 1167 1066 Using the UCI Ionosphere, WDBC and <b>Image Segmentation</b> data sets we performed a K -NN search. For each member of the data set of class c,where c # C (q.v. Table 2), the K-NNs are identified and a count maintained of those neighbours whose class was also c. Table", "mykey":427},
 {"datasetID":109, "supportID":"3296733E38810A4F714914BA7D59AAFBEBFEF473", "rexaID":"716db199e5f006b8b564ef192177f08d5edefe9b", "author":"K. A. J Doherty and Rolf Adams and Neil Davey", "title":"Unsupervised Learning with Normalised Data and Non-Euclidean Norms", "venue":"University of Hertfordshire", "year":"", "window":"considered were the Ionosphere, Image Segmentation (training data), Wisconsin Diagnostic Breast Cancer (WDBC) and <b>Wine</b> data sets. These data sets were selected to show our approach on data with a range of classes, dimensionality and data distributions. The basic characteristics of each data set are shown in table 2. Tab l e", "mykey":428},
 {"datasetID":38, "supportID":"333DDF13DA5B127679CD0E2A1CC5556E9B6EA3F5", "rexaID":"954de642cab661d060a2dbc68d3023ba3a9763e1", "author":"Federico Divina and Elena Marchiori", "title":"Handling Continuous Attributes in an Evolutionary Inductive Learner", "venue":"Department of Computer Science Vrije Universiteit", "year":"", "window":"deviation between brackets. Tables 4 and 5 contain the results of the experiments on the training and test sets, respectively. On the training sets ECL-LSDf obtains the best performance on all datasets, with optimal performance on the <b>Echocardiogram</b>  However, on the test sets, ECL-LSDc achieves the best accuracy in most of the cases, with simplicity (that is, the number of clauses of the output", "mykey":429},
 {"datasetID":42, "supportID":"333DDF13DA5B127679CD0E2A1CC5556E9B6EA3F5", "rexaID":"954de642cab661d060a2dbc68d3023ba3a9763e1", "author":"Federico Divina and Elena Marchiori", "title":"Handling Continuous Attributes in an Evolutionary Inductive Learner", "venue":"Department of Computer Science Vrije Universiteit", "year":"", "window":"and ECL-LSDc (together with ECL-GSD) becomes significantly better than ECL-LSDf and ECL-LUD on the German dataset. The other datasets (Echocardiogram, <b>Glass</b> 2, Heart, and Hepatitis) are small, and the results of the experiments are not normally distributed, so the t-test cannot be applied. Dataset ECL-LSDc", "mykey":430},
 {"datasetID":45, "supportID":"333DDF13DA5B127679CD0E2A1CC5556E9B6EA3F5", "rexaID":"954de642cab661d060a2dbc68d3023ba3a9763e1", "author":"Federico Divina and Elena Marchiori", "title":"Handling Continuous Attributes in an Evolutionary Inductive Learner", "venue":"Department of Computer Science Vrije Universiteit", "year":"", "window":"The other datasets (Echocardiogram, Glass 2, <b>Heart</b>  and Hepatitis) are small, and the results of the experiments are not normally distributed, so the t-test cannot be applied. Dataset ECL-LSDc ECL-LSDf ECL-LUD", "mykey":431},
 {"datasetID":46, "supportID":"333DDF13DA5B127679CD0E2A1CC5556E9B6EA3F5", "rexaID":"954de642cab661d060a2dbc68d3023ba3a9763e1", "author":"Federico Divina and Elena Marchiori", "title":"Handling Continuous Attributes in an Evolutionary Inductive Learner", "venue":"Department of Computer Science Vrije Universiteit", "year":"", "window":"in most of the cases, with simplicity (that is, the number of clauses of the output program) that is second best after ECL-GSD. ECL-LSDf produces best results on the Echocardiogram and <b>Hepatitis</b> dataset, ECL-GSD on Glass2, but the results are only slightly better than those of ECL-LSDc. The unsupervised variant ECL-LUD produces satisfactory approximate solutions, yet of quality inferior to that of", "mykey":432},
 {"datasetID":52, "supportID":"333DDF13DA5B127679CD0E2A1CC5556E9B6EA3F5", "rexaID":"954de642cab661d060a2dbc68d3023ba3a9763e1", "author":"Federico Divina and Elena Marchiori", "title":"Handling Continuous Attributes in an Evolutionary Inductive Learner", "venue":"Department of Computer Science Vrije Universiteit", "year":"", "window":"better than ECL-GSD on the Breast dataset, and better than ECL-LUD on the <b>Ionosphere</b> dataset, together with ECL-LSDf and ECL-GSD. If we increase the confidence level to 5% then we get that ECL-LUD and ECL-LSDc are significantly better than", "mykey":433},
 {"datasetID":79, "supportID":"333DDF13DA5B127679CD0E2A1CC5556E9B6EA3F5", "rexaID":"954de642cab661d060a2dbc68d3023ba3a9763e1", "author":"Federico Divina and Elena Marchiori", "title":"Handling Continuous Attributes in an Evolutionary Inductive Learner", "venue":"Department of Computer Science Vrije Universiteit", "year":"", "window":"by the results of the t-test, summarized in Table 7. Using 1% confidence level we get that ECL-LSDc is never outperformed, while it is significantly better than the other methods on the <b>Pima</b> <b>Indians</b> dataset, better than ECL-GSD on the Breast dataset, and better than ECL-LUD on the Ionosphere dataset, together with ECL-LSDf and ECL-GSD. If we increase the confidence level to 5% then we get that ECL-LUD", "mykey":434},
 {"datasetID":52, "supportID":"334FFF63B6414E1ACB2A253C01A91B7B3CE2F1AD", "rexaID":"5b3417aa2824988405f9ac934b692af30729b447", "author":"Jennifer G. Dy and Carla Brodley", "title":"Feature Selection for Unsupervised Learning", "venue":"Journal of Machine Learning Research, 5", "year":"2004", "window":"EM-k-STD (e) Figure 9: Feature selection versus without feature selection on the four-class data. 6.5 Experiments on Real Data We examine the FSSEM variants on the iris, wine, and <b>ionosphere</b> data set from the UCI learning repository (Blake and Merz, 1998), and on a high resolution computed tomography (HRCT) lung 867 DY AND BRODLEY image data which we collected from IUPUI medical center (Dy et", "mykey":435},
 {"datasetID":53, "supportID":"334FFF63B6414E1ACB2A253C01A91B7B3CE2F1AD", "rexaID":"5b3417aa2824988405f9ac934b692af30729b447", "author":"Jennifer G. Dy and Carla Brodley", "title":"Feature Selection for Unsupervised Learning", "venue":"Journal of Machine Learning Research, 5", "year":"2004", "window":"EM-k-STD (e) Figure 9: Feature selection versus without feature selection on the four-class data. 6.5 Experiments on Real Data We examine the FSSEM variants on the <b>iris</b>  wine, and ionosphere data set from the UCI learning repository (Blake and Merz, 1998), and on a high resolution computed tomography (HRCT) lung 867 DY AND BRODLEY image data which we collected from IUPUI medical center (Dy et", "mykey":436},
 {"datasetID":109, "supportID":"334FFF63B6414E1ACB2A253C01A91B7B3CE2F1AD", "rexaID":"5b3417aa2824988405f9ac934b692af30729b447", "author":"Jennifer G. Dy and Carla Brodley", "title":"Feature Selection for Unsupervised Learning", "venue":"Journal of Machine Learning Research, 5", "year":"2004", "window":"EM-k-STD (e) Figure 9: Feature selection versus without feature selection on the four-class data. 6.5 Experiments on Real Data We examine the FSSEM variants on the iris, <b>wine</b>  and ionosphere data set from the UCI learning repository (Blake and Merz, 1998), and on a high resolution computed tomography (HRCT) lung 867 DY AND BRODLEY image data which we collected from IUPUI medical center (Dy et", "mykey":437},
 {"datasetID":14, "supportID":"341105F12992761F2B22C880671604A5D406850C", "rexaID":"ce75ba6b3cf7315e2578b0181306beb521c91fbd", "author":"Paul D. Wilson and Tony R. Martinez", "title":"Combining Cross-Validation and Confidence to Measure Fitness", "venue":"fonix corporation Brigham Young University", "year":"", "window":"at the bottom of Table 1, CVC had a significantly higher average generalization accuracy on this set of classification tasks than both the static and LCV methods at a 99% confidence level or higher. Dataset Anneal Australian <b>Breast</b> <b>Cancer</b> WI) Bridges Crx Echocardiogram Flag Glass Heart Heart(Cleveland) Heart(Hungarian) Heart(Long Beach) Heart(More) Heart(Swiss) Hepatitis Horse Colic Image Segmentation", "mykey":438},
 {"datasetID":18, "supportID":"341105F12992761F2B22C880671604A5D406850C", "rexaID":"ce75ba6b3cf7315e2578b0181306beb521c91fbd", "author":"Paul D. Wilson and Tony R. Martinez", "title":"Combining Cross-Validation and Confidence to Measure Fitness", "venue":"fonix corporation Brigham Young University", "year":"", "window":"at the bottom of Table 1, CVC had a significantly higher average generalization accuracy on this set of classification tasks than both the static and LCV methods at a 99% confidence level or higher. Dataset Anneal Australian Breast Cancer(WI) <b>Bridges</b> Crx Echocardiogram Flag Glass Heart Heart(Cleveland) Heart(Hungarian) Heart(Long Beach) Heart(More) Heart(Swiss) Hepatitis Horse Colic Image Segmentation", "mykey":439},
 {"datasetID":47, "supportID":"34594C57ED32BFD864F207DE50C312621815DB40", "rexaID":"e69241ee87f9d58fd07b8ccbee48fbaa881fd695", "author":"Huan Liu and Hiroshi Motoda and Lei Yu", "title":"Feature Selection with Selective Sampling", "venue":"ICML", "year":"2002", "window":"2 and 3 in Table 2) by simply treating them as continuous. The results are reported in Table 5. ReliefS works as well as or better than ReliefF except for 3 cases (some particular bucket sizes for data sets PrimaryTumor, Zoo, <b>Colic</b> . The detailed re0.95 0.955 0.96 0.965 0.97 0.975 0.98 0.985 0.99 0.995 1 102030405060708090100 Precision Percentage by bucket size from 7 to 1 ReliefS ReliefF 0.95 0.955", "mykey":440},
 {"datasetID":111, "supportID":"34594C57ED32BFD864F207DE50C312621815DB40", "rexaID":"e69241ee87f9d58fd07b8ccbee48fbaa881fd695", "author":"Huan Liu and Hiroshi Motoda and Lei Yu", "title":"Feature Selection with Selective Sampling", "venue":"ICML", "year":"2002", "window":"2 and 3 in Table 2) by simply treating them as continuous. The results are reported in Table 5. ReliefS works as well as or better than ReliefF except for 3 cases (some particular bucket sizes for data sets PrimaryTumor, <b>Zoo</b>  Colic). The detailed re0.95 0.955 0.96 0.965 0.97 0.975 0.98 0.985 0.99 0.995 1 102030405060708090100 Precision Percentage by bucket size from 7 to 1 ReliefS ReliefF 0.95 0.955", "mykey":441},
 {"datasetID":34, "supportID":"349220366FEDA2EB2EC3B0F53ED88F5A526C6F52", "rexaID":"87afa910f706df9e28bfa697d6d2eea7c0cb53ef", "author":"Ilya Blayvas and Ron Kimmel", "title":"Efficient Classification via Multiresolution Training Set Approximation", "venue":"CS Dept. Technion", "year":"", "window":"\u00b7 D). 3 Experimental Results The proposed method was implemented in VC++ 6.0 and run on `IBM PC 300 PL' with 600MHZ Pentium III processor and 256MB RAM. It was tested on the Pima Indians <b>Diabetes</b> dataset [10], and a large artificial dataset generated with the DatGen program [11]. The results were compared to the Smooth SVM [12] and Sparse Grids [3]. Figure 7: Partition of 2D feature space for a", "mykey":442},
 {"datasetID":79, "supportID":"349220366FEDA2EB2EC3B0F53ED88F5A526C6F52", "rexaID":"87afa910f706df9e28bfa697d6d2eea7c0cb53ef", "author":"Ilya Blayvas and Ron Kimmel", "title":"Efficient Classification via Multiresolution Training Set Approximation", "venue":"CS Dept. Technion", "year":"", "window":"\u00b7 D). 3 Experimental Results The proposed method was implemented in VC++ 6.0 and run on `IBM PC 300 PL' with 600MHZ Pentium III processor and 256MB RAM. It was tested on the <b>Pima</b> <b>Indians</b> <b>Diabetes</b> dataset [10], and a large artificial dataset generated with the DatGen program [11]. The results were compared to the Smooth SVM [12] and Sparse Grids [3]. Figure 7: Partition of 2D feature space for a", "mykey":443},
 {"datasetID":14, "supportID":"34A3FC6906DD972E03A41348B99C0232AD1EAA78", "rexaID":"d25c86892bfa80849ecd629de4c45936a52a1590", "author":"Michael G. Madden", "title":"Evaluation of the Performance of the Markov Blanket Bayesian Classifier Algorithm", "venue":"CoRR, csLG/0211003", "year":"2002", "window":"and all four are equally good on the <b>Breast</b> <b>Cancer</b> dataset. Na\u00efve TAN K2 MBBC Chess 87.63\u00b1 1.61 91.68\u00b1 1.09 94.03\u00b1 0.87 97.03\u00b1 0.54 WBCD 97.81\u00b1 0.51 97.47\u00b1 0.68 97.17\u00b1 1.05 97.30\u00b1 1.01 LED-24 73.28\u00b1 0.70 73.18\u00b1 0.63 73.14\u00b1 0.73 73.14\u00b1 0.73 DNA 94.80\u00b1 0.44", "mykey":444},
 {"datasetID":23, "supportID":"34A3FC6906DD972E03A41348B99C0232AD1EAA78", "rexaID":"d25c86892bfa80849ecd629de4c45936a52a1590", "author":"Michael G. Madden", "title":"Evaluation of the Performance of the Markov Blanket Bayesian Classifier Algorithm", "venue":"CoRR, csLG/0211003", "year":"2002", "window":"were selected that had these characteristics and that were not very small. The datasets are listed in Table 1. Dataset #I #A <b>Chess</b> (King & Rook vs King & Pawn) 3196 32 Wisconsin Breast Cancer Diagnosis 699 9 LED-24 (17 irrelevant attributes) 3200 24 DNA: Splice Junction Gene Sequences", "mykey":445},
 {"datasetID":21, "supportID":"34A3FC6906DD972E03A41348B99C0232AD1EAA78", "rexaID":"d25c86892bfa80849ecd629de4c45936a52a1590", "author":"Michael G. Madden", "title":"Evaluation of the Performance of the Markov Blanket Bayesian Classifier Algorithm", "venue":"CoRR, csLG/0211003", "year":"2002", "window":"were selected that had these characteristics and that were not very small. The datasets are listed in Table 1. Dataset #I #A <b>Chess</b> (King & Rook vs King & Pawn) 3196 32 Wisconsin Breast Cancer Diagnosis 699 9 LED-24 (17 irrelevant attributes) 3200 24 DNA: Splice Junction Gene Sequences", "mykey":446},
 {"datasetID":22, "supportID":"34A3FC6906DD972E03A41348B99C0232AD1EAA78", "rexaID":"d25c86892bfa80849ecd629de4c45936a52a1590", "author":"Michael G. Madden", "title":"Evaluation of the Performance of the Markov Blanket Bayesian Classifier Algorithm", "venue":"CoRR, csLG/0211003", "year":"2002", "window":"were selected that had these characteristics and that were not very small. The datasets are listed in Table 1. Dataset #I #A <b>Chess</b> (King & Rook vs King & Pawn) 3196 32 Wisconsin Breast Cancer Diagnosis 699 9 LED-24 (17 irrelevant attributes) 3200 24 DNA: Splice Junction Gene Sequences", "mykey":447},
 {"datasetID":63, "supportID":"34A3FC6906DD972E03A41348B99C0232AD1EAA78", "rexaID":"d25c86892bfa80849ecd629de4c45936a52a1590", "author":"Michael G. Madden", "title":"Evaluation of the Performance of the Markov Blanket Bayesian Classifier Algorithm", "venue":"CoRR, csLG/0211003", "year":"2002", "window":"with the fewest instances, this procedure was repeated 10 times. For the SPECT and <b>Lymphography</b> datasets, the procedure was repeated 50 times to reduce variability. Prediction accuracy results and standard deviations are reported in Table 2. Following usual conventions, for each dataset the algorithm", "mykey":448},
 {"datasetID":67, "supportID":"34A3FC6906DD972E03A41348B99C0232AD1EAA78", "rexaID":"d25c86892bfa80849ecd629de4c45936a52a1590", "author":"Michael G. Madden", "title":"Evaluation of the Performance of the Markov Blanket Bayesian Classifier Algorithm", "venue":"CoRR, csLG/0211003", "year":"2002", "window":"(based on a paired T-test at the 99% confidence level) and they outperform the other algorithms, they are both highlighted in bold. For example, K2 and MBBC are both best on the <b>DNA</b> Splice dataset and all four are equally good on the Breast Cancer dataset. Na\u00efve TAN K2 MBBC Chess 87.63\u00b1 1.61 91.68\u00b1 1.09 94.03\u00b1 0.87 97.03\u00b1 0.54 WBCD 97.81\u00b1 0.51 97.47\u00b1 0.68 97.17\u00b1 1.05 97.30\u00b1 1.01 LED-24 73.28\u00b1", "mykey":449},
 {"datasetID":69, "supportID":"34A3FC6906DD972E03A41348B99C0232AD1EAA78", "rexaID":"d25c86892bfa80849ecd629de4c45936a52a1590", "author":"Michael G. Madden", "title":"Evaluation of the Performance of the Markov Blanket Bayesian Classifier Algorithm", "venue":"CoRR, csLG/0211003", "year":"2002", "window":"(based on a paired T-test at the 99% confidence level) and they outperform the other algorithms, they are both highlighted in bold. For example, K2 and MBBC are both best on the DNA <b>Splice</b> dataset and all four are equally good on the Breast Cancer dataset. Na\u00efve TAN K2 MBBC Chess 87.63\u00b1 1.61 91.68\u00b1 1.09 94.03\u00b1 0.87 97.03\u00b1 0.54 WBCD 97.81\u00b1 0.51 97.47\u00b1 0.68 97.17\u00b1 1.05 97.30\u00b1 1.01 LED-24 73.28\u00b1", "mykey":450},
 {"datasetID":76, "supportID":"34A3FC6906DD972E03A41348B99C0232AD1EAA78", "rexaID":"d25c86892bfa80849ecd629de4c45936a52a1590", "author":"Michael G. Madden", "title":"Evaluation of the Performance of the Markov Blanket Bayesian Classifier Algorithm", "venue":"CoRR, csLG/0211003", "year":"2002", "window":"one for each of the analyses described above in Section 4.2. ROC graphs are best suited to two-class problems, which all but one of the datasets are. For the <b>Nursery</b> dataset, the ROC curve is for the prediction of the `Priority' class. On a ROC graph, the point (0 0) represents the strategy of never returning a positive classification, no", "mykey":451},
 {"datasetID":95, "supportID":"34A3FC6906DD972E03A41348B99C0232AD1EAA78", "rexaID":"d25c86892bfa80849ecd629de4c45936a52a1590", "author":"Michael G. Madden", "title":"Evaluation of the Performance of the Markov Blanket Bayesian Classifier Algorithm", "venue":"CoRR, csLG/0211003", "year":"2002", "window":"with the fewest instances, this procedure was repeated 10 times. For the <b>SPECT</b> and Lymphography datasets, the procedure was repeated 50 times to reduce variability. Prediction accuracy results and standard deviations are reported in Table 2. Following usual conventions, for each dataset the algorithm", "mykey":452},
 {"datasetID":151, "supportID":"35300281792001C42FD90DBDA218AF4CF423FCD6", "rexaID":"fe3900fd025ca81857d2c7cd69e8f44e9836fe1b", "author":"Perry Moerland and E. Fiesler and I. Ubarretxena-Belandia", "title":"Incorporating LCLV Non-Linearities in Optical Multilayer Neural Networks", "venue":"Preprint of an article published in Applied Optics", "year":"", "window":"the exclusive or (XOR) problem, the 3-bit parity problem (Par), and the 4-bit addition problem (Add), where the modulo 2 sum of 2 numbers of 2 bits has to be calculated. Furthermore, two real-world data sets have been used, namely the <b>sonar</b> benchmark [13] and the wine data set [14]: Sonar This data set was originally used by R. Gorman and T. Sejnowski in their study of the classification of sonar", "mykey":453},
 {"datasetID":109, "supportID":"35300281792001C42FD90DBDA218AF4CF423FCD6", "rexaID":"fe3900fd025ca81857d2c7cd69e8f44e9836fe1b", "author":"Perry Moerland and E. Fiesler and I. Ubarretxena-Belandia", "title":"Incorporating LCLV Non-Linearities in Optical Multilayer Neural Networks", "venue":"Preprint of an article published in Applied Optics", "year":"", "window":"have been used, namely the sonar benchmark [13] and the <b>wine</b> data set [14]: Sonar This data set was originally used by R. Gorman and T. Sejnowski in their study of the classification of sonar signals using a neural network. The task is to discriminate between sonar", "mykey":454},
 {"datasetID":42, "supportID":"3575C8302580A62693F32D5438C2ABB2F762C138", "rexaID":"0244677960291674fa0816f5d724ca20eef22bf7", "author":"Georg Thimm and E. Fiesler", "title":"Optimal Setting of Weights, Learning Rate, and Gain", "venue":"E S E A R C H R E P R O R T I D I A P", "year":"1997", "window":"Multilayer perceptrons behave similarly, as shown in figure 4, as confirmed by experiments performed with the Solar, Wine, <b>Glass</b> and Servo data sets. The most important difference with high order perceptrons is that the networks do not or only very slowly converge for weight variances close to zero. Such variances should therefore not be used", "mykey":455},
 {"datasetID":87, "supportID":"3575C8302580A62693F32D5438C2ABB2F762C138", "rexaID":"0244677960291674fa0816f5d724ca20eef22bf7", "author":"Georg Thimm and E. Fiesler", "title":"Optimal Setting of Weights, Learning Rate, and Gain", "venue":"E S E A R C H R E P R O R T I D I A P", "year":"1997", "window":"Multilayer perceptrons behave similarly, as shown in figure 4, as confirmed by experiments performed with the Solar, Wine, Glass and <b>Servo</b> data sets. The most important difference with high order perceptrons is that the networks do not or only very slowly converge for weight variances close to zero. Such variances should therefore not be used", "mykey":456},
 {"datasetID":109, "supportID":"3575C8302580A62693F32D5438C2ABB2F762C138", "rexaID":"0244677960291674fa0816f5d724ca20eef22bf7", "author":"Georg Thimm and E. Fiesler", "title":"Optimal Setting of Weights, Learning Rate, and Gain", "venue":"E S E A R C H R E P R O R T I D I A P", "year":"1997", "window":"Multilayer perceptrons behave similarly, as shown in figure 4, as confirmed by experiments performed with the Solar, <b>Wine</b>  Glass and Servo data sets. The most important difference with high order perceptrons is that the networks do not or only very slowly converge for weight variances close to zero. Such variances should therefore not be used", "mykey":457},
 {"datasetID":45, "supportID":"35CC6C6ADCE760B30A9044481C187EB38155C56E", "rexaID":"4f29b49d0d83edf9e0a21d102467df5ffdf64fcb", "author":"Jinyan Li and Limsoon Wong", "title":"Using Rules to Analyse Bio-medical Data: A Comparison between C4.5 and PCL", "venue":"WAIM", "year":"2003", "window":"For a simple comparison, we give the following statistics numbers: -- Comparing PCL, C4.5, Bagging and Boosting, PCL won the best accuracy on 5 data sets (i.e., breast-w, cleve, <b>heart</b>  HIV, and promoter); Bagging won on 1 data set (hypothyroid); and Boosting won the best accuracy on 4 data sets (i.e., hepatitis, lymph, sick and splice). -- Comparing", "mykey":458},
 {"datasetID":46, "supportID":"35CC6C6ADCE760B30A9044481C187EB38155C56E", "rexaID":"4f29b49d0d83edf9e0a21d102467df5ffdf64fcb", "author":"Jinyan Li and Limsoon Wong", "title":"Using Rules to Analyse Bio-medical Data: A Comparison between C4.5 and PCL", "venue":"WAIM", "year":"2003", "window":"(hypothyroid); and Boosting won the best accuracy on 4 data sets (i.e., <b>hepatitis</b>  lymph, sick and splice). -- Comparing between PCL and C4.5, PCL won on 8 data sets, while C4.5 won on the rest 2 data sets. -- Comparing between PCL and Bagging, PCL won on 6 data", "mykey":459},
 {"datasetID":62, "supportID":"35CC6C6ADCE760B30A9044481C187EB38155C56E", "rexaID":"4f29b49d0d83edf9e0a21d102467df5ffdf64fcb", "author":"Jinyan Li and Limsoon Wong", "title":"Using Rules to Analyse Bio-medical Data: A Comparison between C4.5 and PCL", "venue":"WAIM", "year":"2003", "window":"[15], it is aimed to classify tumor and normal cells for diagnostic purpose; while in the <b>lung cancer</b> data set [9], it is aimed to differentiate two types of disease. We use Table 4 to summarize the background information of 6 data sets for the subtype classification of the childhood leukemia disease. All", "mykey":460},
 {"datasetID":67, "supportID":"35CC6C6ADCE760B30A9044481C187EB38155C56E", "rexaID":"4f29b49d0d83edf9e0a21d102467df5ffdf64fcb", "author":"Jinyan Li and Limsoon Wong", "title":"Using Rules to Analyse Bio-medical Data: A Comparison between C4.5 and PCL", "venue":"WAIM", "year":"2003", "window":"(i.e., breast-w, cleve, heart, HIV, and <b>promoter</b> ; Bagging won on 1 data set (hypothyroid); and Boosting won the best accuracy on 4 data sets (i.e., hepatitis, lymph, sick and splice). -- Comparing between PCL and C4.5, PCL won on 8 data sets, while C4.5 won on the rest 2", "mykey":461},
 {"datasetID":69, "supportID":"35CC6C6ADCE760B30A9044481C187EB38155C56E", "rexaID":"4f29b49d0d83edf9e0a21d102467df5ffdf64fcb", "author":"Jinyan Li and Limsoon Wong", "title":"Using Rules to Analyse Bio-medical Data: A Comparison between C4.5 and PCL", "venue":"WAIM", "year":"2003", "window":"(hypothyroid); and Boosting won the best accuracy on 4 data sets (i.e., hepatitis, lymph, sick and <b>splice</b> . -- Comparing between PCL and C4.5, PCL won on 8 data sets, while C4.5 won on the rest 2 data sets. -- Comparing between PCL and Bagging, PCL won on 6 data", "mykey":462},
 {"datasetID":25, "supportID":"35EAF3C1D0284590525F860D36B9466161712EA2", "rexaID":"cac8f7b952b6dc917dd68834c825e7f48373cafa", "author":"Zoubin Ghahramani and Michael I. Jordan", "title":"Factorial Hidden Markov Models", "venue":"Machine Learning, 29", "year":"1997", "window":"HMMs of varying sizes (K ranging from 2 to 6; M ranging from 2 to 9) were also trained on the same data. To approximate the FACTORIAL HIDDEN MARKOV MODELS 17 Table 2. Attributes in the <b>Bach</b> chorale data set. The key signature and time signature attributes were constant over the duration of the chorale. All attributes were treated as real numbers and modeled using the linear-Gaussian observation model", "mykey":463},
 {"datasetID":14, "supportID":"360AF352974FAD0D05CC43A8976EA71C4178A141", "rexaID":"8ec82e2fc13e61b5b22da67e7ba2ac9681ec5016", "author":"I\u00f1aki Inza and Pedro Larra\u00f1aga and Basilio Sierra and Ramon Etxeberria and Jose Antonio Lozano and Jos Manuel Pe\u00f1a", "title":"Representing the behaviour of supervised classification learning algorithms by Bayesian networks", "venue":"Pattern Recognition Letters, 20", "year":"1999", "window":"1,055 cases, a sufficient amount to obtain a 'not-overfitted' Bayesian network. Figure 1 summarizes the explained process. As an example, the induced simplified Bayesian network for <b>Breast</b> <b>cancer</b> dataset can be seen in Figure 2. 3.4 Concepts for interpreting the joint behaviour Once the Bayesian networks are induced, our aim is to extract assertions on the joint behaviour of Machine Learning", "mykey":464},
 {"datasetID":34, "supportID":"360AF352974FAD0D05CC43A8976EA71C4178A141", "rexaID":"8ec82e2fc13e61b5b22da67e7ba2ac9681ec5016", "author":"I\u00f1aki Inza and Pedro Larra\u00f1aga and Basilio Sierra and Ramon Etxeberria and Jose Antonio Lozano and Jos Manuel Pe\u00f1a", "title":"Representing the behaviour of supervised classification learning algorithms by Bayesian networks", "venue":"Pattern Recognition Letters, 20", "year":"1999", "window":"treatment is done for unknown values, exploiting each algorithm its own characteristics. PEBLS and HOODG algorithms are not able to handle unknown values: thus, they are only used in the four datasets without unknown values  <b>diabetes</b>  heart, liver and lymphography). For each database and algorithm, a classification model is induced using the specified training set: when run with fixed default", "mykey":465},
 {"datasetID":45, "supportID":"360AF352974FAD0D05CC43A8976EA71C4178A141", "rexaID":"8ec82e2fc13e61b5b22da67e7ba2ac9681ec5016", "author":"I\u00f1aki Inza and Pedro Larra\u00f1aga and Basilio Sierra and Ramon Etxeberria and Jose Antonio Lozano and Jos Manuel Pe\u00f1a", "title":"Representing the behaviour of supervised classification learning algorithms by Bayesian networks", "venue":"Pattern Recognition Letters, 20", "year":"1999", "window":"treatment is done for unknown values, exploiting each algorithm its own characteristics. PEBLS and HOODG algorithms are not able to handle unknown values: thus, they are only used in the four datasets without unknown values (diabetes, <b>heart</b>  liver and lymphography). For each database and algorithm, a classification model is induced using the specified training set: when run with fixed default", "mykey":466},
 {"datasetID":60, "supportID":"360AF352974FAD0D05CC43A8976EA71C4178A141", "rexaID":"8ec82e2fc13e61b5b22da67e7ba2ac9681ec5016", "author":"I\u00f1aki Inza and Pedro Larra\u00f1aga and Basilio Sierra and Ramon Etxeberria and Jose Antonio Lozano and Jos Manuel Pe\u00f1a", "title":"Representing the behaviour of supervised classification learning algorithms by Bayesian networks", "venue":"Pattern Recognition Letters, 20", "year":"1999", "window":"treatment is done for unknown values, exploiting each algorithm its own characteristics. PEBLS and HOODG algorithms are not able to handle unknown values: thus, they are only used in the four datasets without unknown values (diabetes, heart, <b>liver</b> and lymphography). For each database and algorithm, a classification model is induced using the specified training set: when run with fixed default", "mykey":467},
 {"datasetID":33, "supportID":"36129A67AD5415B48EC9DE0F46F3F70A7CF53F2E", "rexaID":"30b640cb0a4ba73302609a2f1ec74291c70efd85", "author":"Gisele L. Pappa and Alex Alves Freitas and Celso A A Kaestner", "title":"Attribute Selection with a Multi-objective Genetic Algorithm", "venue":"SBIA", "year":"2002", "window":"used in the experiments Data Set # examples # attributes # classes <b>Dermatology</b> 366 36 6 Vehicle 846 18 4 Promoters 106 57 2 Ionosphere 351 34 2 Crx 690 15 2 Arritymia 452 269 16 All the experiments were performed by using a", "mykey":468},
 {"datasetID":149, "supportID":"36129A67AD5415B48EC9DE0F46F3F70A7CF53F2E", "rexaID":"30b640cb0a4ba73302609a2f1ec74291c70efd85", "author":"Gisele L. Pappa and Alex Alves Freitas and Celso A A Kaestner", "title":"Attribute Selection with a Multi-objective Genetic Algorithm", "venue":"SBIA", "year":"2002", "window":"One disadvantage of the use of the GA is that it is computationally expensive. In the two largest data sets used in our experiments, <b>Vehicle</b> (with the largest number of examples) and Arritymia (with the largest number of attributes, viz. 269), a single run of the GA took about 25 minutes and 5 hours and", "mykey":469},
 {"datasetID":53, "supportID":"375D7356B6B014034E7DB28D8EFD6D18D379115D", "rexaID":"f9cf2dcf39f23bae2973a65af6de81e6438989d3", "author":"Anthony Robins and Marcus Frean", "title":"Learning and generalisation in a stable network", "venue":"Computer Science, The University of Otago", "year":"", "window":"network. The effectiveness of pseudorehearsal at reducing catastrophic forgetting has been proven using a range of populations, including: randomly constructed autoassociative and hetroassociative data sets [Robins, 1995]; the <b>Iris</b> data set [Robins, 1996]; a classification task using the Mushroom data set [French, 1997]; and an alphanumeric character set using a Hopfield type network [Robins and", "mykey":470},
 {"datasetID":73, "supportID":"375D7356B6B014034E7DB28D8EFD6D18D379115D", "rexaID":"f9cf2dcf39f23bae2973a65af6de81e6438989d3", "author":"Anthony Robins and Marcus Frean", "title":"Learning and generalisation in a stable network", "venue":"Computer Science, The University of Otago", "year":"", "window":"[Robins, 1996]; a classification task using the <b>Mushroom</b> data set [French, 1997]; and an alphanumeric character set using a Hopfield type network [Robins and McCallum, 1997] 2 . 2 The Iris and Mushroom data sets are taken from [Murphy and Aha, 1994]. 1 0 1 Input 1", "mykey":471},
 {"datasetID":10, "supportID":"378F1087B1A3D4890BF805C24551D64F291A2B89", "rexaID":"1b37a079312a4d90da6e76dc5d18023c57d0d935", "author":"Yongge Wang", "title":"A New Approach to Fitting Linear Models in High Dimensional Spaces", "venue":"Alastair Scott (Department of Statistics, University of Auckland)", "year":"", "window":"together with the number of observations # and the number of variables D in each dataset. The datasets Autos  <b>Automobile</b> , Cpu (Computer Hardware), and Cleveland (Heart Disease---Processed Cleveland) 141 Autos Bankbill Bodyfat Cholesterol Cleveland Cpu n / k 159 / 16 71 / 16 252 / 15", "mykey":472},
 {"datasetID":29, "supportID":"378F1087B1A3D4890BF805C24551D64F291A2B89", "rexaID":"1b37a079312a4d90da6e76dc5d18023c57d0d935", "author":"Yongge Wang", "title":"A New Approach to Fitting Linear Models in High Dimensional Spaces", "venue":"Alastair Scott (Department of Statistics, University of Auckland)", "year":"", "window":"The datasets Autos (Automobile), Cpu  <b>Computer <b>Hardware</b> /b> , and Cleveland (Heart Disease---Processed Cleveland) 141 Autos Bankbill Bodyfat Cholesterol Cleveland Cpu n / k 159 / 16 71 / 16 252 / 15 297 / 14 297 /", "mykey":473},
 {"datasetID":46, "supportID":"37AFAF3ED198D94CD20884E8C79C3E3004FEC840", "rexaID":"1c7b9b0abcac674ef129893e65f67ae6458161fb", "author":"Takashi Matsuda and Hiroshi Motoda and Tetsuya Yoshida and Takashi Washio", "title":"Mining Patterns from Structured Data by Beam-Wise Graph-Based Induction", "venue":"Discovery Science", "year":"2002", "window":"attributes and predictive accuracy was evaluated. The best result obtained by this approach was better than the previously best known result. B-GBI was then applied to a real-world data, <b>Hepatitis</b> dataset provided by Chiba University. Our very preliminary results indicate that B-GBI can actually handle graphs with a few thousands nodes and extract discriminatory patterns. 1 Introduction Over the last", "mykey":474},
 {"datasetID":67, "supportID":"37AFAF3ED198D94CD20884E8C79C3E3004FEC840", "rexaID":"1c7b9b0abcac674ef129893e65f67ae6458161fb", "author":"Takashi Matsuda and Hiroshi Motoda and Tetsuya Yoshida and Takashi Washio", "title":"Mining Patterns from Structured Data by Beam-Wise Graph-Based Induction", "venue":"Discovery Science", "year":"2002", "window":"canonical labeling to enumerate identical patterns accurately. This new algorithm is implemented and now called Beam-wise GBI, B-GBI for short. Second, we report on an experiment using the <b>promoter</b> dataset (a small <b>DNA</b> dataset) from UCI repository and show the improvements work as intended. Effect of beam width on the number of discovered patterns and predictive accuracy were evaluated. The best", "mykey":475},
 {"datasetID":14, "supportID":"38BDCC93D897E1FC0B6F7686AB09609909513D0E", "rexaID":"63e63c88edc486c3b1b2aeebb790f88a119536c9", "author":"Lorne Mason and Peter L. Bartlett and Jonathan Baxter", "title":"Improved Generalization Through Explicit Optimization of Margins", "venue":"Machine Learning, 38", "year":"2000", "window":"chosen as the final solution. In some cases the training sets were reduced in size to makeoverfitting more likely (so that complexity regularization with DOOM could have an effect). In three of the datasets (Credit Application, Wisconsin <b>Breast</b> <b>Cancer</b> and Pima Indians Diabetes), AdaBoost gained no advantage from using more than a single classifier. In these datasets, the number of classifiers was", "mykey":476},
 {"datasetID":17, "supportID":"38BDCC93D897E1FC0B6F7686AB09609909513D0E", "rexaID":"63e63c88edc486c3b1b2aeebb790f88a119536c9", "author":"Lorne Mason and Peter L. Bartlett and Jonathan Baxter", "title":"Improved Generalization Through Explicit Optimization of Margins", "venue":"Machine Learning, 38", "year":"2000", "window":"chosen as the final solution. In some cases the training sets were reduced in size to makeoverfitting more likely (so that complexity regularization with DOOM could have an effect). In three of the datasets (Credit Application, <b>Wisconsin</b> <b>Breast</b> <b>Cancer</b> and Pima Indians Diabetes), AdaBoost gained no advantage from using more than a single classifier. In these datasets, the number of classifiers was", "mykey":477},
 {"datasetID":15, "supportID":"38BDCC93D897E1FC0B6F7686AB09609909513D0E", "rexaID":"63e63c88edc486c3b1b2aeebb790f88a119536c9", "author":"Lorne Mason and Peter L. Bartlett and Jonathan Baxter", "title":"Improved Generalization Through Explicit Optimization of Margins", "venue":"Machine Learning, 38", "year":"2000", "window":"chosen as the final solution. In some cases the training sets were reduced in size to makeoverfitting more likely (so that complexity regularization with DOOM could have an effect). In three of the datasets (Credit Application, <b>Wisconsin</b> <b>Breast</b> <b>Cancer</b> and Pima Indians Diabetes), AdaBoost gained no advantage from using more than a single classifier. In these datasets, the number of classifiers was", "mykey":478},
 {"datasetID":16, "supportID":"38BDCC93D897E1FC0B6F7686AB09609909513D0E", "rexaID":"63e63c88edc486c3b1b2aeebb790f88a119536c9", "author":"Lorne Mason and Peter L. Bartlett and Jonathan Baxter", "title":"Improved Generalization Through Explicit Optimization of Margins", "venue":"Machine Learning, 38", "year":"2000", "window":"chosen as the final solution. In some cases the training sets were reduced in size to makeoverfitting more likely (so that complexity regularization with DOOM could have an effect). In three of the datasets (Credit Application, <b>Wisconsin</b> <b>Breast</b> <b>Cancer</b> and Pima Indians Diabetes), AdaBoost gained no advantage from using more than a single classifier. In these datasets, the number of classifiers was", "mykey":479},
 {"datasetID":151, "supportID":"38BDCC93D897E1FC0B6F7686AB09609909513D0E", "rexaID":"63e63c88edc486c3b1b2aeebb790f88a119536c9", "author":"Lorne Mason and Peter L. Bartlett and Jonathan Baxter", "title":"Improved Generalization Through Explicit Optimization of Margins", "venue":"Machine Learning, 38", "year":"2000", "window":"cases a maximization of minimum margin at the expense of all other margins generally gave worse generalization performance than AdaBoost. As can be seen in Figure 3 (Credit Application and <b>Sonar</b> data sets), the generalization performance of the combined classifier produced by DOOM can be as good or better than that of the classifier produced by AdaBoost, despite having dramatically worse minimum", "mykey":480},
 {"datasetID":27, "supportID":"38BDCC93D897E1FC0B6F7686AB09609909513D0E", "rexaID":"63e63c88edc486c3b1b2aeebb790f88a119536c9", "author":"Lorne Mason and Peter L. Bartlett and Jonathan Baxter", "title":"Improved Generalization Through Explicit Optimization of Margins", "venue":"Machine Learning, 38", "year":"2000", "window":"chosen as the final solution. In some cases the training sets were reduced in size to makeoverfitting more likely (so that complexity regularization with DOOM could have an effect). In three of the datasets  <b>Credit Application</b>  Wisconsin Breast Cancer and Pima Indians Diabetes), AdaBoost gained no advantage from using more than a single classifier. In these datasets, the number of classifiers was", "mykey":481},
 {"datasetID":45, "supportID":"38BDCC93D897E1FC0B6F7686AB09609909513D0E", "rexaID":"63e63c88edc486c3b1b2aeebb790f88a119536c9", "author":"Lorne Mason and Peter L. Bartlett and Jonathan Baxter", "title":"Improved Generalization Through Explicit Optimization of Margins", "venue":"Machine Learning, 38", "year":"2000", "window":"used in the experiments. Data Set Training Test Attributes Cleveland <b>Heart</b> Disease 103 100 14 Credit Application 100 295 15 German 300 350 24 Glass 70 72 10 Ionosphere 101 125 34 King Rook vs King Pawn 100 1000 36 Pima Indians", "mykey":482},
 {"datasetID":52, "supportID":"38BDCC93D897E1FC0B6F7686AB09609909513D0E", "rexaID":"63e63c88edc486c3b1b2aeebb790f88a119536c9", "author":"Lorne Mason and Peter L. Bartlett and Jonathan Baxter", "title":"Improved Generalization Through Explicit Optimization of Margins", "venue":"Machine Learning, 38", "year":"2000", "window":"classifier produced by DOOM can be as good or better than that of the classifier produced by AdaBoost, despite having dramatically worse minimum training margin. Conversely, Figure 3  <b>Ionosphere</b> data set) shows that an improved minimum margin can result in improved generalization performance. These results clearly demonstrate that the minimum margin is not the importantquantity. Second, the margin", "mykey":483},
 {"datasetID":73, "supportID":"3909C2E60D19614E99F3FE05D45FDDAC5F4970E4", "rexaID":"e347b029b8cda44f5008c70b3d68e38ae9f23e95", "author":"Huan Liu and Hongjun Lu and Ling Feng and Farhad Hussain", "title":"Efficient Search of Reliable Exceptions", "venue":"PAKDD", "year":"1999", "window":"[8], and indeed find out some interesting exception patterns. From the <b>mushroom</b> data set, we can achieve more reliable exceptions, compared to the results from [16]. The remainder of the paper is organized as follows: section 2 gives a detailed description of the proposed approach.", "mykey":484},
 {"datasetID":9, "supportID":"392FCB3E25E092E87DCDA3DCCE6F93EC1FCB3B07", "rexaID":"39037b34a030869c4d11077a2cb171ac3ba3ff0d", "author":"Thomas Melluish and Craig Saunders and Ilia Nouretdinov and Volodya Vovk and Carol S. Saunders and I. Nouretdinov V.", "title":"The typicalness framework: a comparison with the Bayesian approach", "venue":"Department of Computer Science", "year":"2001", "window":"regions for data with w ~ N(0,1) % confidence Mean tolerance region width a=1 a=1000 a=10000 Figure 1 Bayesian RR and RRCM on data generated with w \u00b8 N(0; 1) We also experimented on two benchmark dataset, the <b>auto</b> <b>mpg</b> dataset and the Boston housing dataset. For each experiment, we show the percentage confidence against the percentage of labels outside the tolerance region predicted for that", "mykey":485},
 {"datasetID":45, "supportID":"392FCB3E25E092E87DCDA3DCCE6F93EC1FCB3B07", "rexaID":"39037b34a030869c4d11077a2cb171ac3ba3ff0d", "author":"Thomas Melluish and Craig Saunders and Ilia Nouretdinov and Volodya Vovk and Carol S. Saunders and I. Nouretdinov V.", "title":"The typicalness framework: a comparison with the Bayesian approach", "venue":"Department of Computer Science", "year":"2001", "window":"Recognition Experiments In this section we compare the Bayesian-Transduction (BT) algorithm and the kernel perceptron when used within the typicalness framework. We ran experiments on two toy datasets, and the well-known <b>heart</b> benchmark data set. For the artificial data, one dataset was created using a uniform prior over w such that jjwjj = 1 (this is the correct prior for Bayesian", "mykey":486},
 {"datasetID":48, "supportID":"392FCB3E25E092E87DCDA3DCCE6F93EC1FCB3B07", "rexaID":"39037b34a030869c4d11077a2cb171ac3ba3ff0d", "author":"Thomas Melluish and Craig Saunders and Ilia Nouretdinov and Volodya Vovk and Carol S. Saunders and I. Nouretdinov V.", "title":"The typicalness framework: a comparison with the Bayesian approach", "venue":"Department of Computer Science", "year":"2001", "window":"regions for data with w ~ N(0,1) % confidence Mean tolerance region width a=1 a=1000 a=10000 Figure 1 Bayesian RR and RRCM on data generated with w \u00b8 N(0; 1) We also experimented on two benchmark dataset, the auto-mpg dataset and the Boston <b>housing</b> dataset. For each experiment, we show the percentage confidence against the percentage of labels outside the tolerance region predicted for that", "mykey":487},
 {"datasetID":7, "supportID":"39D3C4C0A61F0C51D5B8496DC707322624EA1BAA", "rexaID":"329f1bacb6745bda5eb7f1b6c79608376af72b8f", "author":"Alexander K. Seewald", "title":"How to Make Stacking Better and Faster While Also Taking Care of an Unknown Weakness", "venue":"ICML", "year":"2002", "window":"with number of classes and examples, discrete and continuous attributes, baseline accuracy (%) and entropy in bits per example (Kononenko & Bratko, 1991). Dataset cl Inst disc cont bL E <b>audiology</b> 24 226 69 0 25.22 3.51 autos 7 205 10 16 32.68 2.29 balance-scale 3 625 0 4 45.76 1.32 breast-cancer 2 286 10 0 70.28 0.88 breast-w 2 699 0 9 65.52 0.93 colic 2 368", "mykey":488},
 {"datasetID":8, "supportID":"39D3C4C0A61F0C51D5B8496DC707322624EA1BAA", "rexaID":"329f1bacb6745bda5eb7f1b6c79608376af72b8f", "author":"Alexander K. Seewald", "title":"How to Make Stacking Better and Faster While Also Taking Care of an Unknown Weakness", "venue":"ICML", "year":"2002", "window":"with number of classes and examples, discrete and continuous attributes, baseline accuracy (%) and entropy in bits per example (Kononenko & Bratko, 1991). Dataset cl Inst disc cont bL E <b>audiology</b> 24 226 69 0 25.22 3.51 autos 7 205 10 16 32.68 2.29 balance-scale 3 625 0 4 45.76 1.32 breast-cancer 2 286 10 0 70.28 0.88 breast-w 2 699 0 9 65.52 0.93 colic 2 368", "mykey":489},
 {"datasetID":34, "supportID":"3A7609A6C4AD4A023A67B6FFA51B96A9227C367B", "rexaID":"26619ba87b875d7e1b11e285a01de5485a24ff88", "author":"Zhihua Zhang and James T. Kwok and Dit-Yan Yeung", "title":"Parametric Distance Metric Learning with Label Information", "venue":"IJCAI", "year":"2003", "window":"(Numbers in bold indicate the better results). data set Euclidean metric learned metric <b>diabetes</b> 459/638 480/638 soybean 37/37 37/37 wine 85/118 117/118 WBC 412/469 446/469 ionosphere 168/251 221/251 iris 107/120 110/120 without changing the clustering", "mykey":490},
 {"datasetID":32, "supportID":"3B55EB5E0C241BD29BF85360A0568ACAE0A123F4", "rexaID":"9755b662d1579511b54a4b2c69eea04c30c2142c", "author":"Juan J. Rodr##guez and Carlos J. Alonso and Henrik Bostrom", "title":"Learning First Order Logic Time Series Classifiers: Rules and Boosting", "venue":"Grupo de Sistemas Inteligentes, Departamento de Inform#atica Universidad de Valladolid, Spain", "year":"", "window":"for classification of time series are not easy to find [12]. For this reason we have used four artificial datasets and only one \real world\" dataset: { <b>Cylinder</b>  Bell and Funnel (CBF). This is an artificial problem [12], in which there are there are 3 classes: cylinder, bell and funnel. Figure 2.a shows two", "mykey":491},
 {"datasetID":107, "supportID":"3B55EB5E0C241BD29BF85360A0568ACAE0A123F4", "rexaID":"9755b662d1579511b54a4b2c69eea04c30c2142c", "author":"Juan J. Rodr##guez and Carlos J. Alonso and Henrik Bostrom", "title":"Learning First Order Logic Time Series Classifiers: Rules and Boosting", "venue":"Grupo de Sistemas Inteligentes, Departamento de Inform#atica Universidad de Valladolid, Spain", "year":"", "window":"[1]. Figure 2.b shows some examples of three of the classes. The data used were obtained from the UCI KDD Archive [4]. The number of examples is 600, with 60 points in each series. { <b>Waveform</b>  This dataset was introduced by [9]. We used the version from the UCI ML Repository [7]. The number of examples is 900 and the number of points in each series is 21. { Wave + Noise. This dataset was generated in", "mykey":492},
 {"datasetID":108, "supportID":"3B55EB5E0C241BD29BF85360A0568ACAE0A123F4", "rexaID":"9755b662d1579511b54a4b2c69eea04c30c2142c", "author":"Juan J. Rodr##guez and Carlos J. Alonso and Henrik Bostrom", "title":"Learning First Order Logic Time Series Classifiers: Rules and Boosting", "venue":"Grupo de Sistemas Inteligentes, Departamento de Inform#atica Universidad de Valladolid, Spain", "year":"", "window":"[1]. Figure 2.b shows some examples of three of the classes. The data used were obtained from the UCI KDD Archive [4]. The number of examples is 600, with 60 points in each series. { <b>Waveform</b>  This dataset was introduced by [9]. We used the version from the UCI ML Repository [7]. The number of examples is 900 and the number of points in each series is 21. { Wave + Noise. This dataset was generated in", "mykey":493},
 {"datasetID":2, "supportID":"3BA0B8B5518DA698FBDBB104F9E3C33FDB52EEE6", "rexaID":"510872c81d0a05f14c583cf3e8245b6c88d43db6", "author":"Bernhard Pfahringer and Geoffrey Holmes and Richard Kirkby", "title":"Optimizing the Induction of Alternating Decision Trees", "venue":"PAKDD", "year":"2001", "window":"kr-vs-kp 3196 0.0 0 36 labor 57 33.6 8 8 mushroom 8124 1.3 0 22 promoters 106 0.0 0 57 sick-euthyroid 3163 6.5 7 18 sonar 208 0.0 60 0 splice 3190 0.0 0 61 vote 435 5.3 0 16 vote1 3 435 5.5 0 15 KDD Datasets coil 5822/4000 0.0 85 0 <b>adult</b> 32561/16281 0.2 6 8 art1 50000/50000 0.0 0 50 art2 50000/50000 0.0 25 25 art3 50000/50000 0.0 50 0 This section compares the performance of the original optimized", "mykey":494},
 {"datasetID":14, "supportID":"3BA0B8B5518DA698FBDBB104F9E3C33FDB52EEE6", "rexaID":"510872c81d0a05f14c583cf3e8245b6c88d43db6", "author":"Bernhard Pfahringer and Geoffrey Holmes and Richard Kirkby", "title":"Optimizing the Induction of Alternating Decision Trees", "venue":"PAKDD", "year":"2001", "window":"Instances Missing Numeric Nominal values (%) attributes UCI Datasets <b>breast</b> <b>cancer</b> 699 0.2 9 0 cleveland 303 0.2 6 7 credit 690 0.6 6 9 diabetes 768 0.0 8 0 hepatitis 155 5.4 6 13 hypothyroid 3772 5.4 7 22 ionosphere 351 0.0 34 0 kr-vs-kp 3196 0.0 0 36 labor 57 33.6", "mykey":495},
 {"datasetID":1, "supportID":"3BBB4EEC31DC99D50332F05C1F75C1A468315D14", "rexaID":"14f025e969e3a0418fd852ee46e54039ab3f216a", "author":"Jianbin Tan and David L. Dowe", "title":"MML Inference of Decision Graphs with Multi-way Joins and Dynamic Attributes", "venue":"Australian Conference on Artificial Intelligence", "year":"2003", "window":"#6.7 5.2 Discussions of above test results Tables 2 and 3 clearly show the decision graph with dynamic attributes to always be either outright first or (sometimes) equal first. When testing on the data sets with disjunctions (like <b>abalone</b>  scale, tic-tac-toe and XD6), decision graph with dynamic attributes has a much lower error rate. On other data sets, it returns results not worse than those from", "mykey":496},
 {"datasetID":151, "supportID":"3BFC2FF6C91B26CB2B08FCFDAB7068C921BCE513", "rexaID":"b4cdcd80235e99bc622bb39208ba35f943eeb7ba", "author":"Thomas G. Dietterich", "title":"Machine-Learning Research", "venue":"AI Magazine, 18", "year":"1997", "window":"fast fourier transform). The resulting ensemble classifier was able to match the performance of human experts in identifying volcanoes. Tumer and Ghosh (1996) applied a similar technique to a <b>sonar</b> dataset with 25 input features. However, they found that deleting even a few of the input features hurt the performance of the individual classifiers so much that the voted ensemble did not perform very", "mykey":497},
 {"datasetID":9, "supportID":"3D00E90CFD9FA60E27463F8386806BBFA51DFCE1", "rexaID":"6d06e4392537787017a43aa34b5412cfa28e31e8", "author":"Dan Pelleg and Andrew W. Moore", "title":"Mixtures of Rectangles: Interpretable Soft Clustering", "venue":"ICML", "year":"2001", "window":"form of a rectangle (in this case a line-segment) with tails. An M-dimensional tailed rectangle is simply a product of these. Experiments on real-life data were done on the ` <b>mpg</b> ' and ``census'' datasets from the UCI repository (Blake & Merz, 1998). The ``mpg'' data has about 400 records with 7 continuous 3 attributes. Running on this data with the number of components set to three, we get the", "mykey":498},
 {"datasetID":20, "supportID":"3D00E90CFD9FA60E27463F8386806BBFA51DFCE1", "rexaID":"6d06e4392537787017a43aa34b5412cfa28e31e8", "author":"Dan Pelleg and Andrew W. Moore", "title":"Mixtures of Rectangles: Interpretable Soft Clustering", "venue":"ICML", "year":"2001", "window":"form of a rectangle (in this case a line-segment) with tails. An M-dimensional tailed rectangle is simply a product of these. Experiments on real-life data were done on the ``mpg'' and ` <b>census</b> ' datasets from the UCI repository (Blake & Merz, 1998). The ``mpg'' data has about 400 records with 7 continuous 3 attributes. Running on this data with the number of components set to three, we get the", "mykey":499},
 {"datasetID":67, "supportID":"3D3E01D238B1E4A2573A265A6A3824710500D2A2", "rexaID":"7354900bd4fb9f6d03256986c4181c92ab534f80", "author":"Ivor W. Tsang and James T. Kwok", "title":"Distance Metric Learning with Kernels", "venue":"Department of Computer Science Hong Kong University of Science and Technology Clear Water Bay Hong Kong", "year":"", "window":"the data distributions using the two distance metrics. As can be seen, similar patterns become more 2 ionosphere, sonar and wine are from the UCI repository [2], and microarray (a <b>DNA</b> microarray dataset for colon cancer) is from http://www.kyb.tuebingen.mpg.de/bs/people/weston/l0. clustered while dissimilar patterns are more separated. Table II reports the classification accuracies, averaged over", "mykey":500},
 {"datasetID":3, "supportID":"3DCE4F86E1DFD9E600AD74DCA551DBFCE947CB8C", "rexaID":"f5291409ace8c375dbd9f26d026f58363815f214", "author":"Jihoon Yang and Rajesh Parekh and Vasant Honavar", "title":"DistAl: An inter-pattern distance-based constructive learning algorithm", "venue":"Intell. Data Anal, 3", "year":"1999", "window":"TABLE II Comparison of generalization accuracy between various algorithms. DistAl is the results of our approach and NN is the best results in [40]. Dataset DistAl NN <b>Annealing</b> 96.6 96.1 Audiology 66.0 77.5 Bridge 63.0 60.6 Cancer 97.8 95.6 Credit 87.7 81.5 Flag 65.8 58.8 Glass 70.5 72.4 Heart 86.7 83.1 Heart (Cleveland) 85.3 80.2 Heart (Hungary) 85.9", "mykey":501},
 {"datasetID":7, "supportID":"3DCE4F86E1DFD9E600AD74DCA551DBFCE947CB8C", "rexaID":"f5291409ace8c375dbd9f26d026f58363815f214", "author":"Jihoon Yang and Rajesh Parekh and Vasant Honavar", "title":"DistAl: An inter-pattern distance-based constructive learning algorithm", "venue":"Intell. Data Anal, 3", "year":"1999", "window":"TABLE II Comparison of generalization accuracy between various algorithms. DistAl is the results of our approach and NN is the best results in [40]. Dataset DistAl NN Annealing 96.6 96.1 <b>Audiology</b> 66.0 77.5 Bridge 63.0 60.6 Cancer 97.8 95.6 Credit 87.7 81.5 Flag 65.8 58.8 Glass 70.5 72.4 Heart 86.7 83.1 Heart (Cleveland) 85.3 80.2 Heart (Hungary) 85.9", "mykey":502},
 {"datasetID":8, "supportID":"3DCE4F86E1DFD9E600AD74DCA551DBFCE947CB8C", "rexaID":"f5291409ace8c375dbd9f26d026f58363815f214", "author":"Jihoon Yang and Rajesh Parekh and Vasant Honavar", "title":"DistAl: An inter-pattern distance-based constructive learning algorithm", "venue":"Intell. Data Anal, 3", "year":"1999", "window":"TABLE II Comparison of generalization accuracy between various algorithms. DistAl is the results of our approach and NN is the best results in [40]. Dataset DistAl NN Annealing 96.6 96.1 <b>Audiology</b> 66.0 77.5 Bridge 63.0 60.6 Cancer 97.8 95.6 Credit 87.7 81.5 Flag 65.8 58.8 Glass 70.5 72.4 Heart 86.7 83.1 Heart (Cleveland) 85.3 80.2 Heart (Hungary) 85.9", "mykey":503},
 {"datasetID":2, "supportID":"3DEA50949D8BD6D943BC878D9E1AA5BF11FC3FC4", "rexaID":"e2e72927eb590e2b7daf9095e42d6ed43ce21e68", "author":"Shi Zhong and Weiyu Tang and Taghi M. Khoshgoftaar", "title":"Boosted Noise Filters for Identifying Mislabeled Data", "venue":"Department of Computer Science and Engineering Florida Atlantic University", "year":"", "window":"in Table 1. Overall, BBF-I significantly outperforms BBF-II, except for low ( 20%) noise levels for the <b>adult</b>  car, and nursery datasets. The reason BBF-II performs poorly may be that too many clean instances are weighted low. The noise filter constructed in the next round loses strong support from clean data instances, which are", "mykey":504},
 {"datasetID":19, "supportID":"3DEA50949D8BD6D943BC878D9E1AA5BF11FC3FC4", "rexaID":"e2e72927eb590e2b7daf9095e42d6ed43ce21e68", "author":"Shi Zhong and Weiyu Tang and Taghi M. Khoshgoftaar", "title":"Boosted Noise Filters for Identifying Mislabeled Data", "venue":"Department of Computer Science and Engineering Florida Atlantic University", "year":"", "window":"in Table 1. Overall, BBF-I significantly outperforms BBF-II, except for low ( 20%) noise levels for the adult, <b>car</b>  and nursery datasets. The reason BBF-II performs poorly may be that too many clean instances are weighted low. The noise filter constructed in the next round loses strong support from clean data instances, which are", "mykey":505},
 {"datasetID":26, "supportID":"3DEA50949D8BD6D943BC878D9E1AA5BF11FC3FC4", "rexaID":"e2e72927eb590e2b7daf9095e42d6ed43ce21e68", "author":"Shi Zhong and Weiyu Tang and Taghi M. Khoshgoftaar", "title":"Boosted Noise Filters for Identifying Mislabeled Data", "venue":"Department of Computer Science and Engineering Florida Atlantic University", "year":"", "window":"Recall Precision BF BBF-1 BBF-12 0 0.2 0.4 0.6 0.8 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 40% noise Recall Precision BFBBF-3 BBF-36 (e) (f) Figure 5. Noise detection results on the <b>connect-4</b> dataset with six different noise levels: (a) 5%; (b) 10%; (c) 15%; (d) 25%; (e) 35%; and (f) 40%. 0 0.2 0.4 0.6 0.8 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 5% noise Recall Precision BF BBF-2 BBF-21 0 0.2", "mykey":506},
 {"datasetID":73, "supportID":"3DEA50949D8BD6D943BC878D9E1AA5BF11FC3FC4", "rexaID":"e2e72927eb590e2b7daf9095e42d6ed43ce21e68", "author":"Shi Zhong and Weiyu Tang and Taghi M. Khoshgoftaar", "title":"Boosted Noise Filters for Identifying Mislabeled Data", "venue":"Department of Computer Science and Engineering Florida Atlantic University", "year":"", "window":"noise Recall Precision BF BBF-1 BBF-12 0 0.2 0.4 0.6 0.8 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 40% noise Recall Precision BFBBF-3 BBF-36 (e) (f) Figure 8. Noise detection results on the <b>mushroom</b> dataset with six different noise levels: (a) 5%; (b) 10%; (c) 15%; (d) 25%; (e) 35%; and (f) 40%. 0 0.2 0.4 0.6 0.8 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 5% noise Recall Precision BF BBF-2 BBF-21 0 0.2", "mykey":507},
 {"datasetID":76, "supportID":"3DEA50949D8BD6D943BC878D9E1AA5BF11FC3FC4", "rexaID":"e2e72927eb590e2b7daf9095e42d6ed43ce21e68", "author":"Shi Zhong and Weiyu Tang and Taghi M. Khoshgoftaar", "title":"Boosted Noise Filters for Identifying Mislabeled Data", "venue":"Department of Computer Science and Engineering Florida Atlantic University", "year":"", "window":"in Table 1. Overall, BBF-I significantly outperforms BBF-II, except for low ( 20%) noise levels for the adult, car, and <b>nursery</b> datasets. The reason BBF-II performs poorly may be that too many clean instances are weighted low. The noise filter constructed in the next round loses strong support from clean data instances, which are", "mykey":508},
 {"datasetID":101, "supportID":"3DEA50949D8BD6D943BC878D9E1AA5BF11FC3FC4", "rexaID":"e2e72927eb590e2b7daf9095e42d6ed43ce21e68", "author":"Shi Zhong and Weiyu Tang and Taghi M. Khoshgoftaar", "title":"Boosted Noise Filters for Identifying Mislabeled Data", "venue":"Department of Computer Science and Engineering Florida Atlantic University", "year":"", "window":"Recall Precision BF BBF-1 BBF-12 0 0.2 0.4 0.6 0.8 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 40% noise Recall Precision BFBBF-3 BBF-36 (e) (f) Figure 13. Noise detection results on the <b>tic-tac-toe</b> dataset with six different noise levels: (a) 5%; (b) 10%; (c) 15%; (d) 25%; (e) 35%; and (f) 40%. 0 0.2 0.4 0.6 0.8 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 5% noise Recall Precision BF BBF-2 BBF-21 0 0.2", "mykey":509},
 {"datasetID":14, "supportID":"3E2A354E0823F33D8DCB3667E28624F9793079BB", "rexaID":"dd4500e327a5f555d2f594711dc50b0f9faccd30", "author":"Gavin Brown", "title":"Diversity in Neural Network Ensembles", "venue":"The University of Birmingham", "year":"2004", "window":"critical to consider values for the strength parameter outside the originally specified range. Table 5.3 shows the classification error rates of two empirical tests, on the Wisconsin <b>breast</b> <b>cancer</b> dataset from the UCI repository (699 patterns), and the Heart disease dataset from Statlog (270 patterns). An ensemble consisting of two networks, each with five hidden nodes, was trained using NC. We use", "mykey":510},
 {"datasetID":17, "supportID":"3E2A354E0823F33D8DCB3667E28624F9793079BB", "rexaID":"dd4500e327a5f555d2f594711dc50b0f9faccd30", "author":"Gavin Brown", "title":"Diversity in Neural Network Ensembles", "venue":"The University of Birmingham", "year":"2004", "window":"critical to consider values for the strength parameter outside the originally specified range. Table 5.3 shows the classification error rates of two empirical tests, on the <b>Wisconsin</b> <b>breast</b> <b>cancer</b> dataset from the UCI repository (699 patterns), and the Heart disease dataset from Statlog (270 patterns). An ensemble consisting of two networks, each with five hidden nodes, was trained using NC. We use", "mykey":511},
 {"datasetID":15, "supportID":"3E2A354E0823F33D8DCB3667E28624F9793079BB", "rexaID":"dd4500e327a5f555d2f594711dc50b0f9faccd30", "author":"Gavin Brown", "title":"Diversity in Neural Network Ensembles", "venue":"The University of Birmingham", "year":"2004", "window":"critical to consider values for the strength parameter outside the originally specified range. Table 5.3 shows the classification error rates of two empirical tests, on the <b>Wisconsin</b> <b>breast</b> <b>cancer</b> dataset from the UCI repository (699 patterns), and the Heart disease dataset from Statlog (270 patterns). An ensemble consisting of two networks, each with five hidden nodes, was trained using NC. We use", "mykey":512},
 {"datasetID":16, "supportID":"3E2A354E0823F33D8DCB3667E28624F9793079BB", "rexaID":"dd4500e327a5f555d2f594711dc50b0f9faccd30", "author":"Gavin Brown", "title":"Diversity in Neural Network Ensembles", "venue":"The University of Birmingham", "year":"2004", "window":"critical to consider values for the strength parameter outside the originally specified range. Table 5.3 shows the classification error rates of two empirical tests, on the <b>Wisconsin</b> <b>breast</b> <b>cancer</b> dataset from the UCI repository (699 patterns), and the Heart disease dataset from Statlog (270 patterns). An ensemble consisting of two networks, each with five hidden nodes, was trained using NC. We use", "mykey":513},
 {"datasetID":45, "supportID":"3E2A354E0823F33D8DCB3667E28624F9793079BB", "rexaID":"dd4500e327a5f555d2f594711dc50b0f9faccd30", "author":"Gavin Brown", "title":"Diversity in Neural Network Ensembles", "venue":"The University of Birmingham", "year":"2004", "window":"from the UCI repository (699 patterns), and the <b>Heart</b> disease dataset from Statlog (270 patterns). An ensemble consisting of two networks, each with five hidden nodes, was trained using NC. We use 5-fold cross-validation, and 40 trials from uniform random weights in", "mykey":514},
 {"datasetID":48, "supportID":"3E2A354E0823F33D8DCB3667E28624F9793079BB", "rexaID":"dd4500e327a5f555d2f594711dc50b0f9faccd30", "author":"Gavin Brown", "title":"Diversity in Neural Network Ensembles", "venue":"The University of Birmingham", "year":"2004", "window":"2 we use a sigmoid output activation function. The ensemble is combined by a uniformly weighted linear combination. Dataset 1: Boston <b>Housing</b> This regression dataset concerns housing values in suburbs of Boston, the problem is to predict the median house price given a number of demographic features. There are 506", "mykey":515},
 {"datasetID":145, "supportID":"3E2A354E0823F33D8DCB3667E28624F9793079BB", "rexaID":"dd4500e327a5f555d2f594711dc50b0f9faccd30", "author":"Gavin Brown", "title":"Diversity in Neural Network Ensembles", "venue":"The University of Birmingham", "year":"2004", "window":"from the UCI repository (699 patterns), and the <b>Heart</b> disease dataset from <b>Statlog</b> (270 patterns). An ensemble consisting of two networks, each with five hidden nodes, was trained using NC. We use 5-fold cross-validation, and 40 trials from uniform random weights in", "mykey":516},
 {"datasetID":98, "supportID":"3E2A354E0823F33D8DCB3667E28624F9793079BB", "rexaID":"dd4500e327a5f555d2f594711dc50b0f9faccd30", "author":"Gavin Brown", "title":"Diversity in Neural Network Ensembles", "venue":"The University of Birmingham", "year":"2004", "window":"from the UCI repository (699 patterns), and the Heart disease dataset from <b>Statlog</b> (270 patterns). An ensemble consisting of two networks, each with five hidden nodes, was trained using NC. We use 5-fold cross-validation, and 40 trials from uniform random weights in", "mykey":517},
 {"datasetID":2, "supportID":"3E40EC8BCAC796786665E3F039284AA28425E3D8", "rexaID":"df1dc2bd013ef17735eb85d2de85701f833db3ff", "author":"I. Yoncaci", "title":"Maximum a Posteriori Tree Augmented Naive Bayes Classifiers", "venue":"O EN INTEL.LIG ` ENCIA ARTIFICIAL CSIC", "year":"2003", "window":"In the rest of the section we discuss and justify these assertions into more detail. 14 Dataset MAPTAN MAPTAN+BMA sTAN sTAN+BMA <b>adult</b> 17.18 \u00b1 0.68 17.19 \u00b1 0.71 17.60 \u00b1 0.82 17.60 \u00b1 0.80 australian 19.91 \u00b1 1.14 19.62 \u00b1 1.13 25.39 \u00b1 1.18 24.96 \u00b1 1.13 breast 17.23 \u00b1 1.21 16.89 \u00b1 1.28 8.73 \u00b1 0.87", "mykey":518},
 {"datasetID":50, "supportID":"3E9249B13A8BB18FC4231D0C5FDE1A7CEE7005AB", "rexaID":"d293cd6bea79dd218c3a9e76177d026bf1d581f5", "author":"Aristidis Likas and Nikos A. Vlassis and Jakob J. Verbeek", "title":"The global k-means clustering algorithm", "venue":"Pattern Recognition, 36", "year":"2003", "window":"[8], the synthetic data set [9] and the <b>image segmentation</b> data set [8]. In all data sets we conducted experiments for the clustering problems obtained by considering only feature vectors and ignoring class labels. The iris", "mykey":519},
 {"datasetID":53, "supportID":"3E9249B13A8BB18FC4231D0C5FDE1A7CEE7005AB", "rexaID":"d293cd6bea79dd218c3a9e76177d026bf1d581f5", "author":"Aristidis Likas and Nikos A. Vlassis and Jakob J. Verbeek", "title":"The global k-means clustering algorithm", "venue":"Pattern Recognition, 36", "year":"2003", "window":"it is also possible to employ the above presented k-d tree approach with the global k-means algorithm. 4 Experimental results We have tested the proposed clustering algorithms on several well-known data sets, namely the <b>iris</b> data set [8], the synthetic data set [9] and the image segmentation data set [8]. In all data sets we conducted experiments for the clustering problems obtained by considering only", "mykey":520},
 {"datasetID":147, "supportID":"3E9249B13A8BB18FC4231D0C5FDE1A7CEE7005AB", "rexaID":"d293cd6bea79dd218c3a9e76177d026bf1d581f5", "author":"Aristidis Likas and Nikos A. Vlassis and Jakob J. Verbeek", "title":"The global k-means clustering algorithm", "venue":"Pattern Recognition, 36", "year":"2003", "window":"[8], the synthetic data set [9] and the <b>image segmentation</b> data set [8]. In all data sets we conducted experiments for the clustering problems obtained by considering only feature vectors and ignoring class labels. The iris", "mykey":521},
 {"datasetID":53, "supportID":"3EC13CA70DBF3A01899B8D79376A99E81A970C7C", "rexaID":"cfcdd7693a196acd2e60944a030abaf8d477e337", "author":"Manoranjan Dash and Huan Liu", "title":"Feature Selection for Clustering", "venue":"PAKDD", "year":"2000", "window":"in Figure 3. The X-axis of the plots is for number of most important features and Y -axis is for tr(P Gamma 1 W PB ) value for the corresponding subset of most important features. For <b>Iris</b> data set trace value was the maximum for the two most important features. For D3C, D4C and D6C data trace value increases with addition of important features in a fast rate but slows down to almost a halt", "mykey":522},
 {"datasetID":1, "supportID":"3ED26DE59E6B04767815B636D509561388063922", "rexaID":"55c53fc4220e8b51b81b1ec2ed81f4b54f3747ff", "author":"Tapio Elomaa and Juho Rousu", "title":"General and Efficient Multisplitting of Numerical Attributes", "venue":"Machine Learning, 36", "year":"1999", "window":"values that they produce. In order to assess how different multisplitting strategies 226 T. ELOMAA AND J. ROUSU Table 1. Characteristic figures of the thirty test domains. Attributes Data set Nomin. Int. Real V B Examples Classes <b>Abalone</b> 1 7 863:7 826:4 4;177 29 Adult 8 6 3; 673:7 1; 668:2 32;561 2 Annealing 10 4 6 27:5 17:7 798 5 Australian 9 6 188:2 129:7 690 2 Auto insuran. 10 8 7", "mykey":523},
 {"datasetID":90, "supportID":"3F78339A922160388929703B2451FB2070C35050", "rexaID":"7958d545e6316bdee900fd450ad26eb57cbe6326", "author":"Jarinee Chattratichart and John Darlington and Moustafa Ghanem and Yang Guo and Harold Huning and Martin Kohler and Janjao Sutiwaraphun and Hing Wing and Dan Yang", "title":"Large Scale Data Mining: The Challenges and The Solutions", "venue":"Department of Computing", "year":"", "window":"the data parallel scheme performs far better. In contrast for the <b>Soybean</b> data set, the task parallel versions are significantly superior. The reasons for this can be traced back to the shape of the decision tree which is constructed and the number of predefined classes. The", "mykey":524},
 {"datasetID":91, "supportID":"3F78339A922160388929703B2451FB2070C35050", "rexaID":"7958d545e6316bdee900fd450ad26eb57cbe6326", "author":"Jarinee Chattratichart and John Darlington and Moustafa Ghanem and Yang Guo and Harold Huning and Martin Kohler and Janjao Sutiwaraphun and Hing Wing and Dan Yang", "title":"Large Scale Data Mining: The Challenges and The Solutions", "venue":"Department of Computing", "year":"", "window":"the data parallel scheme performs far better. In contrast for the <b>Soybean</b> data set, the task parallel versions are significantly superior. The reasons for this can be traced back to the shape of the decision tree which is constructed and the number of predefined classes. The", "mykey":525},
 {"datasetID":58, "supportID":"4031CBA624BADC7214BD68E5509DAA5AC990FBC2", "rexaID":"9a423aa0a4e7cdbd40006aeae5a5fecb4d28c279", "author":"Jim Prentzas and Ioannis Hatzilygeroudis and Athanasios K. Tsakalidis", "title":"Updating a Hybrid Rule Base with New Empirical Source Knowledge", "venue":"ICTAI", "year":"2002", "window":"taken from the UCI Repository of Machine Learning and Domain Theories [2]. More specifically, we used the <b>lenses</b> dataset containing 24 examples/patterns of 9 component values and the tictac-toe dataset containing 958 examples/patterns of 27 component values. Additionally, we used three datasets of ours produced from a", "mykey":526},
 {"datasetID":57, "supportID":"40863506086353FC045BBDDF0B6A6FA344C0518B", "rexaID":"7286601416a1cec780621a415323a2bc6c958e11", "author":"Vikas Sindhwani and P. Bhattacharya and Subrata Rakshit", "title":"Information Theoretic Feature Crediting in Multiclass Support Vector Machines", "venue":"", "year":"", "window":"1. The relevant features are very sharply identified. Feature 1 gets maximum credit on account of being relevant for the most informative SVM, in this case decided purely by the input bias. <b>LED Dataset</b> This 10-class dataset, drawn from the UCI repository, consists of 200 training and 500 test examples of 24 binary-valued features each. The first 7 features are relevant, and correspond to LEDs on a", "mykey":527},
 {"datasetID":67, "supportID":"40863506086353FC045BBDDF0B6A6FA344C0518B", "rexaID":"7286601416a1cec780621a415323a2bc6c958e11", "author":"Vikas Sindhwani and P. Bhattacharya and Subrata Rakshit", "title":"Information Theoretic Feature Crediting in Multiclass Support Vector Machines", "venue":"", "year":"", "window":"include : a synthetic dataset that we constructed, LED-24, Waveform-40, <b>DNA</b>  Vehicle, and Satellite Images (SAT) drawn from the UCI repository [19]; and three datasets which are a subset of the Reuters document collection [20].", "mykey":528},
 {"datasetID":137, "supportID":"40863506086353FC045BBDDF0B6A6FA344C0518B", "rexaID":"7286601416a1cec780621a415323a2bc6c958e11", "author":"Vikas Sindhwani and P. Bhattacharya and Subrata Rakshit", "title":"Information Theoretic Feature Crediting in Multiclass Support Vector Machines", "venue":"", "year":"", "window":"that we constructed, LED-24, Waveform-40, DNA, Vehicle, and Satellite Images (SAT) drawn from the UCI repository [19]; and three datasets which are a subset of the <b>Reuters</b> document collection [20]. Table 5 lists the details of these datasets. We first examine the informativeness of features using SVM-infoprop in each of these", "mykey":529},
 {"datasetID":146, "supportID":"40863506086353FC045BBDDF0B6A6FA344C0518B", "rexaID":"7286601416a1cec780621a415323a2bc6c958e11", "author":"Vikas Sindhwani and P. Bhattacharya and Subrata Rakshit", "title":"Information Theoretic Feature Crediting in Multiclass Support Vector Machines", "venue":"", "year":"", "window":"is a 6-class and 36-feature dataset containing <b>Landsat</b> <b>satellite</b> data. We tested SVM-Infoprop for non-linear SVMs with these datasets. Support Vector Machines with polynomial kernels of degree 2 were trained with 564 examples and", "mykey":530},
 {"datasetID":149, "supportID":"40863506086353FC045BBDDF0B6A6FA344C0518B", "rexaID":"7286601416a1cec780621a415323a2bc6c958e11", "author":"Vikas Sindhwani and P. Bhattacharya and Subrata Rakshit", "title":"Information Theoretic Feature Crediting in Multiclass Support Vector Machines", "venue":"", "year":"", "window":"0.02 0.04 0 20 40 60 80 100 120 140 160 180 Credits Features <b>Vehicle</b> and Satellite Datasets The Vehicle dataset is a multiclass pattern recognition problem of classifying a given silhouette as one of four types of vehicle. There are 18 features. The Satellite dataset is a 6-class and", "mykey":531},
 {"datasetID":107, "supportID":"40863506086353FC045BBDDF0B6A6FA344C0518B", "rexaID":"7286601416a1cec780621a415323a2bc6c958e11", "author":"Vikas Sindhwani and P. Bhattacharya and Subrata Rakshit", "title":"Information Theoretic Feature Crediting in Multiclass Support Vector Machines", "venue":"", "year":"", "window":"include : a synthetic dataset that we constructed, LED-24, <b>Waveform</b> 40, DNA, Vehicle, and Satellite Images (SAT) drawn from the UCI repository [19]; and three datasets which are a subset of the Reuters document collection [20].", "mykey":532},
 {"datasetID":108, "supportID":"40863506086353FC045BBDDF0B6A6FA344C0518B", "rexaID":"7286601416a1cec780621a415323a2bc6c958e11", "author":"Vikas Sindhwani and P. Bhattacharya and Subrata Rakshit", "title":"Information Theoretic Feature Crediting in Multiclass Support Vector Machines", "venue":"", "year":"", "window":"include : a synthetic dataset that we constructed, LED-24, <b>Waveform</b> 40, DNA, Vehicle, and Satellite Images (SAT) drawn from the UCI repository [19]; and three datasets which are a subset of the Reuters document collection [20].", "mykey":533},
 {"datasetID":14, "supportID":"408763331A7D1D0FBAA9EAE40B7EB5C967470261", "rexaID":"b0bf518f2c1c4ab72ebac0d17757aa8f52a6badf", "author":"Andrew I. Schein and Lyle H. Ungar", "title":"A-Optimality for Active Learning of Logistic Regression Classifiers", "venue":"Department of Computer and Information Science Levine Hall", "year":"", "window":"54. The lodgepole pine variety of tree happens to represent about 50% of the observations and so we merge all other tree types into a single category. The Wisconsin Diagnostic <b>Breast</b> <b>Cancer</b> (WDBC) data set consists of evaluation measurements (predictors) and final diagnosis for 569 patients. The goal is to predict the diagnosis using the measurements. The number of predictors is 30. The Thyroid Domain", "mykey":534},
 {"datasetID":17, "supportID":"408763331A7D1D0FBAA9EAE40B7EB5C967470261", "rexaID":"b0bf518f2c1c4ab72ebac0d17757aa8f52a6badf", "author":"Andrew I. Schein and Lyle H. Ungar", "title":"A-Optimality for Active Learning of Logistic Regression Classifiers", "venue":"Department of Computer and Information Science Levine Hall", "year":"", "window":"54. The lodgepole pine variety of tree happens to represent about 50% of the observations and so we merge all other tree types into a single category. The <b>Wisconsin</b> Diagnostic <b>Breast</b> <b>Cancer</b> (WDBC) data set consists of evaluation measurements (predictors) and final diagnosis for 569 patients. The goal is to predict the diagnosis using the measurements. The number of predictors is 30. The Thyroid Domain", "mykey":535},
 {"datasetID":15, "supportID":"408763331A7D1D0FBAA9EAE40B7EB5C967470261", "rexaID":"b0bf518f2c1c4ab72ebac0d17757aa8f52a6badf", "author":"Andrew I. Schein and Lyle H. Ungar", "title":"A-Optimality for Active Learning of Logistic Regression Classifiers", "venue":"Department of Computer and Information Science Levine Hall", "year":"", "window":"54. The lodgepole pine variety of tree happens to represent about 50% of the observations and so we merge all other tree types into a single category. The <b>Wisconsin</b> Diagnostic <b>Breast</b> <b>Cancer</b> (WDBC) data set consists of evaluation measurements (predictors) and final diagnosis for 569 patients. The goal is to predict the diagnosis using the measurements. The number of predictors is 30. The Thyroid Domain", "mykey":536},
 {"datasetID":16, "supportID":"408763331A7D1D0FBAA9EAE40B7EB5C967470261", "rexaID":"b0bf518f2c1c4ab72ebac0d17757aa8f52a6badf", "author":"Andrew I. Schein and Lyle H. Ungar", "title":"A-Optimality for Active Learning of Logistic Regression Classifiers", "venue":"Department of Computer and Information Science Levine Hall", "year":"", "window":"54. The lodgepole pine variety of tree happens to represent about 50% of the observations and so we merge all other tree types into a single category. The <b>Wisconsin</b> Diagnostic <b>Breast</b> <b>Cancer</b> (WDBC) data set consists of evaluation measurements (predictors) and final diagnosis for 569 patients. The goal is to predict the diagnosis using the measurements. The number of predictors is 30. The Thyroid Domain", "mykey":537},
 {"datasetID":102, "supportID":"408763331A7D1D0FBAA9EAE40B7EB5C967470261", "rexaID":"b0bf518f2c1c4ab72ebac0d17757aa8f52a6badf", "author":"Andrew I. Schein and Lyle H. Ungar", "title":"A-Optimality for Active Learning of Logistic Regression Classifiers", "venue":"Department of Computer and Information Science Levine Hall", "year":"", "window":"chosen from the UC Irvine data repository (Blake & Merz, 1998): Forest Cover Type (FCT), Wisconsin Diagnostic Breast Cancer (WDBC), Splice Junction Gene Sequence (SJGS), and <b>Thyroid</b> Domain (TD). The data sets were converted to a binary classification task by merging all but the most representative class label into a single class. Table 1 describes the data set characteristics after formatting while the", "mykey":538},
 {"datasetID":34, "supportID":"40B90647358CA7818C58648AFE394632A26BE48B", "rexaID":"ba4305796a9e2896e810fd6cf96fbe11c6d5858b", "author":"Stefan R uping", "title":"A Simple Method For Estimating Conditional Probabilities For SVMs", "venue":"CS Department, AI Unit Dortmund University", "year":"", "window":"including 7 data sets from the UCI Repository [9] (covtype, <b>diabetes</b>  digits, digits, ionosphere, liver, mushroom, promoters) and 4 other real-world data sets: a business cycle analysis problem (business), an analysis", "mykey":539},
 {"datasetID":125, "supportID":"40B90647358CA7818C58648AFE394632A26BE48B", "rexaID":"ba4305796a9e2896e810fd6cf96fbe11c6d5858b", "author":"Stefan R uping", "title":"A Simple Method For Estimating Conditional Probabilities For SVMs", "venue":"CS Department, AI Unit Dortmund University", "year":"", "window":"a business cycle analysis problem (business), an analysis of a direct mailing application (directmailing), a data set from a life <b>insurance</b> <b>company</b> (insurance) and intensive care patient monitoring data (medicine). Prior to learning, nominal attributes were binarised and the attributes were scaled to expectancy 0", "mykey":540},
 {"datasetID":73, "supportID":"40B90647358CA7818C58648AFE394632A26BE48B", "rexaID":"ba4305796a9e2896e810fd6cf96fbe11c6d5858b", "author":"Stefan R uping", "title":"A Simple Method For Estimating Conditional Probabilities For SVMs", "venue":"CS Department, AI Unit Dortmund University", "year":"", "window":"from the UCI Repository [9] (covtype, diabetes, digits, digits, ionosphere, liver, <b>mushroom</b>  promoters) and 4 other real-world data sets: a business cycle analysis problem (business), an analysis of a direct mailing application (directmailing), a data set from a life insurance company (insurance) and intensive care patient", "mykey":541},
 {"datasetID":14, "supportID":"40C1586AAECBE5043D9047D4CC05F06623C2A95F", "rexaID":"3ecc983417b61977b8f998e8a843948dad8fa21c", "author":"Andr\u00e1s Antos and Bal\u00e1zs K\u00e9gl and Tam\u00e1s Linder and G\u00e1bor Lugosi", "title":"Data-dependent margin-based generalization bounds for classification", "venue":"Journal of Machine Learning Research, 3", "year":"2002", "window":"attributes were binary coded in a 1-out-of-n fashion. Data points with missing attributes were removed. Each attribute was normalized to have zero mean and 1= p d standard deviation. The four data sets were the Wisconsin <b>breast</b> <b>cancer</b> (n = 683, d = 9), the ionosphere (n = 351, d = 34), the Japanese credit screening (n = 653, d = 42), and the tic-tac-toe endgame (n = 958, d = 27) database. 84", "mykey":542},
 {"datasetID":17, "supportID":"40C1586AAECBE5043D9047D4CC05F06623C2A95F", "rexaID":"3ecc983417b61977b8f998e8a843948dad8fa21c", "author":"Andr\u00e1s Antos and Bal\u00e1zs K\u00e9gl and Tam\u00e1s Linder and G\u00e1bor Lugosi", "title":"Data-dependent margin-based generalization bounds for classification", "venue":"Journal of Machine Learning Research, 3", "year":"2002", "window":"attributes were binary coded in a 1-out-of-n fashion. Data points with missing attributes were removed. Each attribute was normalized to have zero mean and 1= p d standard deviation. The four data sets were the <b>Wisconsin</b> <b>breast</b> <b>cancer</b> (n = 683, d = 9), the ionosphere (n = 351, d = 34), the Japanese credit screening (n = 653, d = 42), and the tic-tac-toe endgame (n = 958, d = 27) database. 84", "mykey":543},
 {"datasetID":15, "supportID":"40C1586AAECBE5043D9047D4CC05F06623C2A95F", "rexaID":"3ecc983417b61977b8f998e8a843948dad8fa21c", "author":"Andr\u00e1s Antos and Bal\u00e1zs K\u00e9gl and Tam\u00e1s Linder and G\u00e1bor Lugosi", "title":"Data-dependent margin-based generalization bounds for classification", "venue":"Journal of Machine Learning Research, 3", "year":"2002", "window":"attributes were binary coded in a 1-out-of-n fashion. Data points with missing attributes were removed. Each attribute was normalized to have zero mean and 1= p d standard deviation. The four data sets were the <b>Wisconsin</b> <b>breast</b> <b>cancer</b> (n = 683, d = 9), the ionosphere (n = 351, d = 34), the Japanese credit screening (n = 653, d = 42), and the tic-tac-toe endgame (n = 958, d = 27) database. 84", "mykey":544},
 {"datasetID":16, "supportID":"40C1586AAECBE5043D9047D4CC05F06623C2A95F", "rexaID":"3ecc983417b61977b8f998e8a843948dad8fa21c", "author":"Andr\u00e1s Antos and Bal\u00e1zs K\u00e9gl and Tam\u00e1s Linder and G\u00e1bor Lugosi", "title":"Data-dependent margin-based generalization bounds for classification", "venue":"Journal of Machine Learning Research, 3", "year":"2002", "window":"attributes were binary coded in a 1-out-of-n fashion. Data points with missing attributes were removed. Each attribute was normalized to have zero mean and 1= p d standard deviation. The four data sets were the <b>Wisconsin</b> <b>breast</b> <b>cancer</b> (n = 683, d = 9), the ionosphere (n = 351, d = 34), the Japanese credit screening (n = 653, d = 42), and the tic-tac-toe endgame (n = 958, d = 27) database. 84", "mykey":545},
 {"datasetID":2, "supportID":"40D384D3E5B95AAA56170E63DAF6B6EE80F14BF2", "rexaID":"47354ca48da5014e0a8f5e4da7f3a7e9aaa6e9e5", "author":"Jie Cheng and Russell Greiner", "title":"Comparing Bayesian Network Classifiers", "venue":"UAI", "year":"1999", "window":"used in the experiments. Instances Dataset Attributes. Classes Train Test <b>Adult</b> 13 2 32561 16281 Nursery 8 5 8640 4320 Mushroom 22 2 5416 2708 Chess 36 2 2130 1066 DNA 60 3 2000 1186 Car 6 4 1728 CV5 Flare 10 3 1066 CV5 Vote 16 2 435 CV5", "mykey":546},
 {"datasetID":19, "supportID":"40D384D3E5B95AAA56170E63DAF6B6EE80F14BF2", "rexaID":"47354ca48da5014e0a8f5e4da7f3a7e9aaa6e9e5", "author":"Jie Cheng and Russell Greiner", "title":"Comparing Bayesian Network Classifiers", "venue":"UAI", "year":"1999", "window":"the value \"?\" in our experiments. Chess: Chess end-game result classification based on board-descriptions. DNA: Recognizing the boundaries between exons and introns given a sequence of DNA. <b>Car</b> dataset: <b>Car evaluation</b> based on the six features of a car. Flare: Classifying the number of times of occurrence of certain type of solar flare. Vote: Using voting records to classify Congressmen as", "mykey":547},
 {"datasetID":20, "supportID":"40D384D3E5B95AAA56170E63DAF6B6EE80F14BF2", "rexaID":"47354ca48da5014e0a8f5e4da7f3a7e9aaa6e9e5", "author":"Jie Cheng and Russell Greiner", "title":"Comparing Bayesian Network Classifiers", "venue":"UAI", "year":"1999", "window":"are given below. Adult dataset: The data was extracted from the <b>census</b> bureau database. Prediction task is to determine whether a person makes over 50K a year. As the discretization process ignores one of the 14 attributes", "mykey":548},
 {"datasetID":67, "supportID":"40D384D3E5B95AAA56170E63DAF6B6EE80F14BF2", "rexaID":"47354ca48da5014e0a8f5e4da7f3a7e9aaa6e9e5", "author":"Jie Cheng and Russell Greiner", "title":"Comparing Bayesian Network Classifiers", "venue":"UAI", "year":"1999", "window":"the value \"?\" in our experiments. Chess: Chess end-game result classification based on board-descriptions. <b>DNA</b>  Recognizing the boundaries between exons and introns given a sequence of DNA. Car dataset: Car evaluation based on the six features of a car. Flare: Classifying the number of times of occurrence of certain type of solar flare. Vote: Using voting records to classify Congressmen as", "mykey":549},
 {"datasetID":76, "supportID":"40D384D3E5B95AAA56170E63DAF6B6EE80F14BF2", "rexaID":"47354ca48da5014e0a8f5e4da7f3a7e9aaa6e9e5", "author":"Jie Cheng and Russell Greiner", "title":"Comparing Bayesian Network Classifiers", "venue":"UAI", "year":"1999", "window":"and GBN and TAN each did best on two of the datasets. On the data sets  <b>Nursery</b>  and \"Car\", the GBN classifier was inferior to the Na\u00efve-Bayes. The reason is, in both cases the GBN actually reduced to the Na\u00efve-Bayes with missing links (the reduced", "mykey":550},
 {"datasetID":34, "supportID":"40ED000B2332D6383529EB2B7A72B54AEC455515", "rexaID":"076c5b4e7bf69148bcde8c83621ecf1140a7d55c", "author":"Michael L. Raymer and Travis E. Doom and Leslie A. Kuhn and William F. Punch", "title":"Knowledge discovery in medical and biological datasets using a hybrid Bayes classifier/evolutionary algorithm", "venue":"IEEE Transactions on Systems, Man, and Cybernetics, Part B, 33", "year":"2003", "window":"with the nonlinear discriminant function and the knn classifier. In all cases the nonlinear discriminant classifier is significantly faster than the EC/knn---in the case of the Pima Indian <b>diabetes</b> data set the difference is nearly tenfold. B. Classification of Medical Data Two additional data sets, also selected from the UCI repository, were employed by [40, 41] in a comparative study of", "mykey":551},
 {"datasetID":46, "supportID":"40ED000B2332D6383529EB2B7A72B54AEC455515", "rexaID":"076c5b4e7bf69148bcde8c83621ecf1140a7d55c", "author":"Michael L. Raymer and Travis E. Doom and Leslie A. Kuhn and William F. Punch", "title":"Knowledge discovery in medical and biological datasets using a hybrid Bayes classifier/evolutionary algorithm", "venue":"IEEE Transactions on Systems, Man, and Cybernetics, Part B, 33", "year":"2003", "window":"used for this evaluation are IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS 7 described in detail in [3], and at the UCI website [33]. A brief synopsis of each data set follows: <b>Hepatitis</b> -- This data consists of 19 descriptive and clinical test result values for 155 hepatitis patients [34, 35]. The two classes, survivors and patients for whom the hepatitis proved", "mykey":552},
 {"datasetID":52, "supportID":"40ED000B2332D6383529EB2B7A72B54AEC455515", "rexaID":"076c5b4e7bf69148bcde8c83621ecf1140a7d55c", "author":"Michael L. Raymer and Travis E. Doom and Leslie A. Kuhn and William F. Punch", "title":"Knowledge discovery in medical and biological datasets using a hybrid Bayes classifier/evolutionary algorithm", "venue":"IEEE Transactions on Systems, Man, and Cybernetics, Part B, 33", "year":"2003", "window":"is thus better for evaluating feature selection capability than classifier accuracy. <b>Ionosphere</b> -- The 34 continuous features in this data set are derived from the signals read by a phased array of 16 highfrequency antennas in Goose Bay, Labrador [39]. These radar signals are designed to recognize structure in the ionosphere. Each reading", "mykey":553},
 {"datasetID":79, "supportID":"40ED000B2332D6383529EB2B7A72B54AEC455515", "rexaID":"076c5b4e7bf69148bcde8c83621ecf1140a7d55c", "author":"Michael L. Raymer and Travis E. Doom and Leslie A. Kuhn and William F. Punch", "title":"Knowledge discovery in medical and biological datasets using a hybrid Bayes classifier/evolutionary algorithm", "venue":"IEEE Transactions on Systems, Man, and Cybernetics, Part B, 33", "year":"2003", "window":"with the nonlinear discriminant function and the knn classifier. In all cases the nonlinear discriminant classifier is significantly faster than the EC/knn---in the case of the <b>Pima</b> Indian <b>diabetes</b> data set the difference is nearly tenfold. B. Classification of Medical Data Two additional data sets, also selected from the UCI repository, were employed by [40, 41] in a comparative study of", "mykey":554},
 {"datasetID":154, "supportID":"40ED000B2332D6383529EB2B7A72B54AEC455515", "rexaID":"076c5b4e7bf69148bcde8c83621ecf1140a7d55c", "author":"Michael L. Raymer and Travis E. Doom and Leslie A. Kuhn and William F. Punch", "title":"Knowledge discovery in medical and biological datasets using a hybrid Bayes classifier/evolutionary algorithm", "venue":"IEEE Transactions on Systems, Man, and Cybernetics, Part B, 33", "year":"2003", "window":"computer-based archival file for macromolecular structures,\" J. Mol. Biol., vol. 112, pp. 535--542, 1977. [46] U. Hobohm, M. Scharf, R. Schneider, and C. Sander, \"Selection of representative <b>Protein</b> data sets,\" Protein Sci., vol. 1, pp. 409--417, 1992. [47] L. A. Kuhn, C. A. Swanson, M. E. Pique, J. A. Tainer, and E. D. Getzoff, \"Atomic and residue hydrophilicity in the context of folded protein", "mykey":555},
 {"datasetID":102, "supportID":"40ED000B2332D6383529EB2B7A72B54AEC455515", "rexaID":"076c5b4e7bf69148bcde8c83621ecf1140a7d55c", "author":"Michael L. Raymer and Travis E. Doom and Leslie A. Kuhn and William F. Punch", "title":"Knowledge discovery in medical and biological datasets using a hybrid Bayes classifier/evolutionary algorithm", "venue":"IEEE Transactions on Systems, Man, and Cybernetics, Part B, 33", "year":"2003", "window":"from the UCI repository, were employed by [40, 41] in a comparative study of classification methods from statistical pattern recognition, neural networks, and machine learning. These two medical data sets, <b>thyroid</b> and appendicitis, are included here to facilitate comparison with these results. The thyroid data consists of 21 clinical test results for a set of patients tested for thyroid dysfunction", "mykey":556},
 {"datasetID":109, "supportID":"40ED000B2332D6383529EB2B7A72B54AEC455515", "rexaID":"076c5b4e7bf69148bcde8c83621ecf1140a7d55c", "author":"Michael L. Raymer and Travis E. Doom and Leslie A. Kuhn and William F. Punch", "title":"Knowledge discovery in medical and biological datasets using a hybrid Bayes classifier/evolutionary algorithm", "venue":"IEEE Transactions on Systems, Man, and Cybernetics, Part B, 33", "year":"2003", "window":"for testing the ability of a classifier and feature extractor to maintain or increase classification accuracy while reducing dimensionality when there are fewer features to work with. <b>Wine</b> -- This data set consists of the results of a chemical analysis of wines derived from three different cultivars [37, 38]. There are 13 continuous features, with no missing values. There are 59, 71, and 48 members of", "mykey":557},
 {"datasetID":107, "supportID":"411AC560C55FD39F7288CCBCA18D4EE4D22D4E31", "rexaID":"b43d0101709b7b7b7b3fdd9fbd2efb9850d65a99", "author":"Pierre Geurts", "title":"Extremely randomized trees", "venue":"Technical report June 2003 University of Li#ege Department of Electrical Engineering and Computer Science Institut Monte#ore", "year":"", "window":"Dataset Nb. attributs Nb. classes #LS #TS <b>Waveform</b> 21 3 4000 1000 Two-norm 20 2 8000 2000 Satellite 36 6 4435 2000 Pendigits 16 10 7494 3498 Dig44 16 10 14000 4000 Letter 16 26 16000 4000 Isolet 617 26 6238", "mykey":558},
 {"datasetID":108, "supportID":"411AC560C55FD39F7288CCBCA18D4EE4D22D4E31", "rexaID":"b43d0101709b7b7b7b3fdd9fbd2efb9850d65a99", "author":"Pierre Geurts", "title":"Extremely randomized trees", "venue":"Technical report June 2003 University of Li#ege Department of Electrical Engineering and Computer Science Institut Monte#ore", "year":"", "window":"Dataset Nb. attributs Nb. classes #LS #TS <b>Waveform</b> 21 3 4000 1000 Two-norm 20 2 8000 2000 Satellite 36 6 4435 2000 Pendigits 16 10 7494 3498 Dig44 16 10 14000 4000 Letter 16 26 16000 4000 Isolet 617 26 6238", "mykey":559},
 {"datasetID":146, "supportID":"41489190B3D94A81B07AB97D29A8EEFB78078783", "rexaID":"e550da2f9c24b03fec3cd94563a837e653db1c60", "author":"Xavier Giannakopoulos and Juha Karhunen and Erkki Oja", "title":"An Experimental Comparison of Neural Algorithms for Independent Component Analysis and Blind Separation", "venue":"Int. J. Neural Syst, 9", "year":"1999", "window":"methods for real-world data For comparing the performance and properties of the studied ICA or BSS algorithms in more practical circumstances, we made experiments with three dioeerent real-world data sets. These data sets, namely crab data, <b>satellite</b> data, and MEG artefact data, will be brieAEy discussed in context with the respective results in the next section. Because we are now dealing with", "mykey":560},
 {"datasetID":42, "supportID":"41581B846BA6DD7BF4F05DEFC37668A334D8A455", "rexaID":"9e2e49a81a0cec7e7872787b2b52ec128f5ebb0d", "author":"Francesco Masulli and Giorgio Valentini", "title":"Effectiveness of Error Correcting Output Codes in Multiclass Learning Problems", "venue":"Multiple Classifier Systems", "year":"2000", "window":"effective for PND classi#ers rather than monolithic MLP classifiers. Hypothesis 2 In PLD error recovering induced by ECOC is counter-balanced by the higher error rate of the dichotomizers. Table 1. Data sets general features. The data sets <b>glass</b>  letter and optdigits data sets are from the UCI repository [16]. Data set Number of Number of Number of Number of attributes classes training samples testing", "mykey":561},
 {"datasetID":148, "supportID":"41764D58770E66878B84E1716D1C32BB05F1F2ED", "rexaID":"78729a95fae0b658d4d1956948c16e81e46d9f8c", "author":"Grigorios Tsoumakas and Ioannis P. Vlahavas", "title":"Effective Stacking of Distributed Classifiers", "venue":"ECAI", "year":"2002", "window":"the complexity of the learning problem at the combination phase is related to the product C # L, where L is the size of the meta-level training set. The variation present in the Letter and <b>Shuttle</b> data sets is due to the fact that the calculation of averages also involves time. This however is very small compared to the time needed for global learning. 0 10 20 30 40 50 60 70 80 90 0 5 10 15 20 25 30", "mykey":562},
 {"datasetID":50, "supportID":"41D7602C950052BA6208E41D333EC0C05F3D0F7A", "rexaID":"c495658301b0128f01553cf69d64ab348a217d67", "author":"James Tin and Yau Kwok", "title":"Moderating the Outputs of Support Vector Machine Classifiers", "venue":"Department of Computer Science Hong Kong Baptist University Hong Kong", "year":"", "window":"versa) and G goes infinite. For the estimate based on the unmoderated output, the value of G obtained is inferior to that based on the moderated output. 5.2 <b>image segmentation</b> Problem The second data set is the image segmentation data from the UCI machine learning repository[1]. Each pattern has 19 continuous attributes and corresponds to a 3 # 3 region of an outdoor image. The problem is to", "mykey":563},
 {"datasetID":147, "supportID":"41D7602C950052BA6208E41D333EC0C05F3D0F7A", "rexaID":"c495658301b0128f01553cf69d64ab348a217d67", "author":"James Tin and Yau Kwok", "title":"Moderating the Outputs of Support Vector Machine Classifiers", "venue":"Department of Computer Science Hong Kong Baptist University Hong Kong", "year":"", "window":"versa) and G goes infinite. For the estimate based on the unmoderated output, the value of G obtained is inferior to that based on the moderated output. 5.2 <b>image segmentation</b> Problem The second data set is the image segmentation data from the UCI machine learning repository[1]. Each pattern has 19 continuous attributes and corresponds to a 3 # 3 region of an outdoor image. The problem is to", "mykey":564},
 {"datasetID":59, "supportID":"427E9BA53A8FCD4543DC3C2E83595A1E72F1B8EB", "rexaID":"ba8302e5db2921603c34aee0c738b103d1948f5f", "author":"Jaakko Peltonen and Arto Klami and Samuel Kaski", "title":"Learning Metrics for Information Visualization", "venue":"Neural Networks Research Centre Helsinki University of Technology", "year":"", "window":"(Table 3). Computing accurate global distances with the graph search (column 'Graph' in Table 3) further improved performance significantly on the Landsat and <b>Letter recognition</b> data sets. On the two other sets the difference between the Sammon-L variants was not significant. The difference between Sammon-L with the graph approximation and Sammon-E is illustrated in Figure 2 on", "mykey":565},
 {"datasetID":146, "supportID":"427E9BA53A8FCD4543DC3C2E83595A1E72F1B8EB", "rexaID":"ba8302e5db2921603c34aee0c738b103d1948f5f", "author":"Jaakko Peltonen and Arto Klami and Samuel Kaski", "title":"Learning Metrics for Information Visualization", "venue":"Neural Networks Research Centre Helsinki University of Technology", "year":"", "window":"estimates of auxiliary data, at the winner units of test samples. This measure has a slightly unintuitive corollary: since it requires a density estimator, even though traditional SOMs are not Data set n C N <b>Landsat</b> <b>Satellite</b> Data [1] 36 6 6435 Letter Recognition Data [1] 16 26 20000 LVQ PAK (Phoneme) [6] 20 13 3656 TIMIT Data [11] 12 41 14994 Table 1: The data sets and their dimensionality (n),", "mykey":566},
 {"datasetID":54, "supportID":"42B4109A50E1A36C8BE990B2BA4DCAD306B9553C", "rexaID":"80b881ba423bc64d836bde049f1e5def83c1d5f8", "author":"Erin L. Allwein and Robert E. Schapire and Yoram Singer", "title":"Reducing Multiclass to Binary: A Unifying Approach for Margin Classifiers", "venue":"ICML", "year":"2000", "window":"according to the number of classes): Dermatology, Satimage, Glass, Segmentation, E-coli, Pendigits, Yeast, Vowel, Soybean-large, Thyroid, Audiology, <b>Isolet</b>  Letterrecognition. The proprties of the datasets are summarized in Table 1. In the SVM experiments, we 22 REDUCING MULTICLASS TO BINARY Hamming Decoding Problem One-vs-all Complete All-Pairs Dense Sparse dermatology 5.0 4.2 3.1 3.9 3.6 satimage", "mykey":567},
 {"datasetID":102, "supportID":"42B4109A50E1A36C8BE990B2BA4DCAD306B9553C", "rexaID":"80b881ba423bc64d836bde049f1e5def83c1d5f8", "author":"Erin L. Allwein and Robert E. Schapire and Yoram Singer", "title":"Reducing Multiclass to Binary: A Unifying Approach for Margin Classifiers", "venue":"ICML", "year":"2000", "window":"tested were used and then evaluated with Hamming decoding and the appropriate loss-based decoding for SVM. skipped Audiology, Isolet, Letter-recognition, Segmentation, and <b>Thyroid</b>  because these datasets were either too big to be handled by our current implementation of SVM or contained many nominal features with missing values which are problematic for SVM. All datasets have at least six classes.", "mykey":568},
 {"datasetID":110, "supportID":"42B4109A50E1A36C8BE990B2BA4DCAD306B9553C", "rexaID":"80b881ba423bc64d836bde049f1e5def83c1d5f8", "author":"Erin L. Allwein and Robert E. Schapire and Yoram Singer", "title":"Reducing Multiclass to Binary: A Unifying Approach for Margin Classifiers", "venue":"ICML", "year":"2000", "window":"the bars in the top row of Figure 6 correspond to large positive values.) One-against-all often results in error rates that are much higher than the error rates of other codes. For instance, for the dataset <b>Yeast</b>  the one-against-all code has an error rate of 72% while the error rate of all the other codes is no more than 47:1% (random sparse) and can be as low as 39:6% (random dense). On the very few", "mykey":569},
 {"datasetID":18, "supportID":"42D776BDD251CFF564FF8C3D1B08882FBD05E261", "rexaID":"5eeec5743b3bcc32f80f4f6ee849c0a777f73480", "author":"Ljupco Todorovski and Saso Dzeroski", "title":"Experiments in Meta-level Learning with ILP", "venue":"PKDD", "year":"1999", "window":"C4.5 CN2 k-NN Dataset C4.5 CN2 k-NN australian p <b>bridges</b> td p p bridges-type p p p chess p diabetes p p echocardiogram p german p glass p p heart p hepatitis p p hypothyroid p image p p iris p p p labor p lenses p", "mykey":570},
 {"datasetID":98, "supportID":"42D776BDD251CFF564FF8C3D1B08882FBD05E261", "rexaID":"5eeec5743b3bcc32f80f4f6ee849c0a777f73480", "author":"Ljupco Todorovski and Saso Dzeroski", "title":"Experiments in Meta-level Learning with ILP", "venue":"PKDD", "year":"1999", "window":"used in the experiments are public domain and the experiments can be repeated. This was not the case with the <b>StatLog</b> dataset repository where more then half of the datasets used are not publicly available. Another improvement is the use of a unified methodology for measuring the error rate of different classification", "mykey":571},
 {"datasetID":20, "supportID":"42E00062729B9AB656A7359B295C42875D090C40", "rexaID":"5b07457d7b968260c0c1a712f9120a243bcfbf8c", "author":"Manuel Oliveira", "title":"Library Release Form Name of Author: Stanley Robson de Medeiros Oliveira Title of Thesis: Data Transformation For Privacy-Preserving Data Mining Degree: Doctor of Philosophy Year this Degree Granted", "venue":"University of Alberta Library", "year":"2005", "window":"is also available at the UCI Repository of Machine Learning Databases [17]. 9. Pumsb: The Pumsb dataset contains <b>census</b> data for population and housing. This dataset is available at http://www.almaden.ibm.com/software/quest. There are 49,046 records with 2,113 different data values (distinct items),", "mykey":572},
 {"datasetID":23, "supportID":"42E00062729B9AB656A7359B295C42875D090C40", "rexaID":"5b07457d7b968260c0c1a712f9120a243bcfbf8c", "author":"Manuel Oliveira", "title":"Library Release Form Name of Author: Stanley Robson de Medeiros Oliveira Title of Thesis: Data Transformation For Privacy-Preserving Data Mining Degree: Doctor of Philosophy Year this Degree Granted", "venue":"University of Alberta Library", "year":"2005", "window":"Pumsb (d o = 74); (b) The error produced on the dataset <b>Chess</b> (d o =37). ......................119 7.4 The error produced on the dataset Pumsb over vertically partitioned data. . . 121 List of Tables 6.1 Thread of selecting the attributes on the party k", "mykey":573},
 {"datasetID":21, "supportID":"42E00062729B9AB656A7359B295C42875D090C40", "rexaID":"5b07457d7b968260c0c1a712f9120a243bcfbf8c", "author":"Manuel Oliveira", "title":"Library Release Form Name of Author: Stanley Robson de Medeiros Oliveira Title of Thesis: Data Transformation For Privacy-Preserving Data Mining Degree: Doctor of Philosophy Year this Degree Granted", "venue":"University of Alberta Library", "year":"2005", "window":"Pumsb (d o = 74); (b) The error produced on the dataset <b>Chess</b> (d o =37). ......................119 7.4 The error produced on the dataset Pumsb over vertically partitioned data. . . 121 List of Tables 6.1 Thread of selecting the attributes on the party k", "mykey":574},
 {"datasetID":22, "supportID":"42E00062729B9AB656A7359B295C42875D090C40", "rexaID":"5b07457d7b968260c0c1a712f9120a243bcfbf8c", "author":"Manuel Oliveira", "title":"Library Release Form Name of Author: Stanley Robson de Medeiros Oliveira Title of Thesis: Data Transformation For Privacy-Preserving Data Mining Degree: Doctor of Philosophy Year this Degree Granted", "venue":"University of Alberta Library", "year":"2005", "window":"Pumsb (d o = 74); (b) The error produced on the dataset <b>Chess</b> (d o =37). ......................119 7.4 The error produced on the dataset Pumsb over vertically partitioned data. . . 121 List of Tables 6.1 Thread of selecting the attributes on the party k", "mykey":575},
 {"datasetID":48, "supportID":"42E00062729B9AB656A7359B295C42875D090C40", "rexaID":"5b07457d7b968260c0c1a712f9120a243bcfbf8c", "author":"Manuel Oliveira", "title":"Library Release Form Name of Author: Stanley Robson de Medeiros Oliveira Title of Thesis: Data Transformation For Privacy-Preserving Data Mining Degree: Doctor of Philosophy Year this Degree Granted", "venue":"University of Alberta Library", "year":"2005", "window":"is also available at the UCI Repository of Machine Learning Databases [17]. 9. Pumsb: The Pumsb dataset contains census data for population and <b>housing</b>  This dataset is available at http://www.almaden.ibm.com/software/quest. There are 49,046 records with 2,113 different data values (distinct items),", "mykey":576},
 {"datasetID":53, "supportID":"42E00062729B9AB656A7359B295C42875D090C40", "rexaID":"5b07457d7b968260c0c1a712f9120a243bcfbf8c", "author":"Manuel Oliveira", "title":"Library Release Form Name of Author: Stanley Robson de Medeiros Oliveira Title of Thesis: Data Transformation For Privacy-Preserving Data Mining Degree: Doctor of Philosophy Year this Degree Granted", "venue":"University of Alberta Library", "year":"2005", "window":"(d o =18,d r = 12). 119 7.13 Average of F-measure (10 trials) for the <b>Iris</b> dataset (d o =5,d r =3)......120 7.14 An example of partitioning for the Pumsb dataset. . . . . . . . . . . . . . . . 120 7.15 Average of F-measure (10 trials) for the Pumsb dataset over vertically", "mykey":577},
 {"datasetID":73, "supportID":"42E00062729B9AB656A7359B295C42875D090C40", "rexaID":"5b07457d7b968260c0c1a712f9120a243bcfbf8c", "author":"Manuel Oliveira", "title":"Library Release Form Name of Author: Stanley Robson de Medeiros Oliveira Title of Thesis: Data Transformation For Privacy-Preserving Data Mining Degree: Doctor of Philosophy Year this Degree Granted", "venue":"University of Alberta Library", "year":"2005", "window":"(d o =37). ..............166 C.2 The error produced on the <b>Mushroom</b> dataset (d o =23).............166 C.3 The error produced on the Pumsb dataset (d o =74)...............166 C.4 The error produced on the Connect dataset (d o =43). .............166 C.5 The error produced on", "mykey":578},
 {"datasetID":137, "supportID":"42E00062729B9AB656A7359B295C42875D090C40", "rexaID":"5b07457d7b968260c0c1a712f9120a243bcfbf8c", "author":"Manuel Oliveira", "title":"Library Release Form Name of Author: Stanley Robson de Medeiros Oliveira Title of Thesis: Data Transformation For Privacy-Preserving Data Mining Degree: Doctor of Philosophy Year this Degree Granted", "venue":"University of Alberta Library", "year":"2005", "window":"Retail for condition C1. . . . . . . . . . 148 B.20 Results of misses cost on the dataset <b>Reuters</b> for condition C1. . . . . . . . . 148 B.21 Results of misses cost on the dataset BMS-1 for condition C1. . . . . . . . . . 148 B.22 Results of misses cost on the Kosarark dataset for", "mykey":579},
 {"datasetID":53, "supportID":"4338C8EA1612D07F9044EB38BC431221B179721B", "rexaID":"b7d95df073db5bba49052a1801fdc7deb9edd091", "author":"Zoubin Ghahramani and Michael I. Jordan", "title":"Learning from incomplete data", "venue":"MASSACHUSETTS INSTITUTE OF TECHNOLOGY ARTIFICIAL INTELLIGENCE LABORATORY and CENTER FOR BIOLOGICAL AND COMPUTATIONAL LEARNING DEPARTMENT OF BRAIN AND COGNITIVE SCIENCES", "year":"1994", "window":"stochastic estimator. 4.4 Classification Classification with missing inputs 0 20 40 60 80 100 20 40 60 80 100 % missing features EM % correct classification MI Figure 3: Classification of the <b>iris</b> data set. 100 data points were used for training and 50 for testing. Each data point consisted of 4 real-valued attributes and one of three class labels. The figure shows classification performance Sigma 1", "mykey":580},
 {"datasetID":151, "supportID":"4357E3C12134AD70F70300666459DD3D97DE399A", "rexaID":"319529c0338699404ff7d7f9d7e5c900e5294959", "author":"Art B. Owen", "title":"Tubular neighbors for regression and classification", "venue":"Stanford University", "year":"1999", "window":"data translates into a mean absolute error of about 0:58, and suggests that the recorded results really are better. 7.4 <b>sonar</b> data This data is from the Irvine repository. The predictors in this data set are 60 values in a sonar spectrum reflected by an object. The response is a 1 if the object is a metal cylinder, and 0 if it is a cylindrical rock. The objective was to distinguish underwater mines", "mykey":581},
 {"datasetID":34, "supportID":"4357E3C12134AD70F70300666459DD3D97DE399A", "rexaID":"319529c0338699404ff7d7f9d7e5c900e5294959", "author":"Art B. Owen", "title":"Tubular neighbors for regression and classification", "venue":"Stanford University", "year":"1999", "window":"10 runs of 13-fold cross-validation using 60 inputs and may not compare directly to the one run of 13-fold cross-validation used here on the first 10 principal components. 7.5 <b>Diabetes</b> data This data set is from the Irvine repository. The response variable is a determination of whether a given woman is diabetic. There are 8 predictors, 26 Model ` k CV-13 Acc. f SqrLin 54 ffi 81 25:10 88:0 f Sqr2 45", "mykey":582},
 {"datasetID":52, "supportID":"4357E3C12134AD70F70300666459DD3D97DE399A", "rexaID":"319529c0338699404ff7d7f9d7e5c900e5294959", "author":"Art B. Owen", "title":"Tubular neighbors for regression and classification", "venue":"Stanford University", "year":"1999", "window":"part of this. The tubular neighbor model performs better than polymars on this data. It is only slightly better than a global additive model with linear and quadratic terms. 7.2 <b>Ionosphere</b> data This data set comes from the Irvine repository. The goal is to separate ``good'' from ``bad'' radar returns based on 34 predictors. The first 200 observations constitute the training data and have 101 ``good''", "mykey":583},
 {"datasetID":82, "supportID":"4357E3C12134AD70F70300666459DD3D97DE399A", "rexaID":"319529c0338699404ff7d7f9d7e5c900e5294959", "author":"Art B. Owen", "title":"Tubular neighbors for regression and classification", "venue":"Stanford University", "year":"1999", "window":"The model it chose ignored three of the input variables, was linear in four of the others and piece-wise linear in one input variable. 29 7.6 Other data 7.6.1 <b>post-operative</b> data This post-operative data set comes from the Irvine repository. The goal is to predict whether a patient will be sent to the hospital floor or sent home. The training data also include two patients who were sent to the intensive", "mykey":584},
 {"datasetID":87, "supportID":"4357E3C12134AD70F70300666459DD3D97DE399A", "rexaID":"319529c0338699404ff7d7f9d7e5c900e5294959", "author":"Art B. Owen", "title":"Tubular neighbors for regression and classification", "venue":"Stanford University", "year":"1999", "window":"and IB3 got 96:7% accuracy (5 errors). Both IB3 and the neural networks have parameters to set, while tubular neighbors makes an automatic choice based on cross-validation. 7.3 <b>Servo</b> data This data set is from the Irvine repository. The response is the rise time for a servo mechanism. There are two integer valued predictors taking 4 and 5 consecutive levels and two categorical predictors each", "mykey":585},
 {"datasetID":98, "supportID":"4357E3C12134AD70F70300666459DD3D97DE399A", "rexaID":"319529c0338699404ff7d7f9d7e5c900e5294959", "author":"Art B. Owen", "title":"Tubular neighbors for regression and classification", "venue":"Stanford University", "year":"1999", "window":"neighbors and slightly worse (three more errors) than global logistic regression. Cross-validation did not identify the best performing method on the test cases. This diabetes data is one of the data sets in the <b>statlog</b> study (Michie et al. 1995). In the statlog study, the best method was global logistic regression, for which they report an accuracy of 77:73%. This differs from the logistic", "mykey":586},
 {"datasetID":53, "supportID":"43E0A7051D99C2DF9F55D8C842922837FBCAABB3", "rexaID":"91c9c07720ed14d74e8cda2ec09b5b6789dda2b2", "author":"Daniel C. St and Ralph W. Wilkerson and Cihan H. Dagli", "title":"RULE SET QUALITY MEASURES FOR INDUCTIVE LEARNING ALGORITHMS", "venue":"proceedings of the Artificial Neural Networks In Engineering Conference 1996 (ANNIE", "year":"1996", "window":"distribution of the 148 instances among the four classes \"normal\" with 2 instances, \"metastases\" with 81 instances, \"malign\" with 61 instances, and \"fibrosis\" with 4 instances. The <b>Iris</b> data set, developed by R. A. Fisher (1936), lists the measurements of four characteristics of Iris flowers: petal length, petal width, sepal length, and sepal width. The set includes the measurements of 50", "mykey":587},
 {"datasetID":151, "supportID":"4417538E07B45AF48E34652A486B6EA8807F952F", "rexaID":"b525865598a98b326df461e4d9d2647543347eb4", "author":"Jianbin Tan and David L. Dowe", "title":"MML Inference of Oblique Decision Trees", "venue":"Australian Conference on Artificial Intelligence", "year":"2004", "window":"and medical data, such as Bupa, Breast Cancer, Wisconsin, Lung Cancer, and Cleveland. The nine UCI Repository [1] data-sets used are these five, Balance, Credit, <b>Sonar</b> and Wine. For each of the nine data sets, 100 independent tests were done by randomly sampling 90% of the data as training data and testing on the remaining 10%. 4 Discussion We compare the MML oblique tree scheme to C4.5 and C5. The", "mykey":588},
 {"datasetID":109, "supportID":"4417538E07B45AF48E34652A486B6EA8807F952F", "rexaID":"b525865598a98b326df461e4d9d2647543347eb4", "author":"Jianbin Tan and David L. Dowe", "title":"MML Inference of Oblique Decision Trees", "venue":"Australian Conference on Artificial Intelligence", "year":"2004", "window":"and medical data, such as Bupa, Breast Cancer, Wisconsin, Lung Cancer, and Cleveland. The nine UCI Repository [1] data-sets used are these five, Balance, Credit, Sonar and <b>Wine</b>  For each of the nine data sets, 100 independent tests were done by randomly sampling 90% of the data as training data and testing on the remaining 10%. 4 Discussion We compare the MML oblique tree scheme to C4.5 and C5. The", "mykey":589},
 {"datasetID":2, "supportID":"4419EF2F549EAEE567BF235D8858FF3CC3489047", "rexaID":"fc0a66d3a7336b6eabad919a7389c68cc37f2564", "author":"Ayhan Demiriz and Kristin P. Bennett and John Shawe and I. Nouretdinov V.", "title":"Linear Programming Boosting via Column Generation", "venue":"Dept. of Decision Sciences and Eng. Systems, Rensselaer Polytechnic Institute", "year":"", "window":"of the final set of weak hypotheses. This is just a very simple method of boosting multiclass problems. Further investigation of LP multiclass approaches is needed. We ran experiments on larger datasets: Forest, <b>Adult</b>  USPS, and Optdigits from UCI(Murphy & Aha, 1992). Forest is a 54-dimension dataset with seven possible classes. The data are divided into 11340 training, 3780 validation, and 565892", "mykey":590},
 {"datasetID":14, "supportID":"4419EF2F549EAEE567BF235D8858FF3CC3489047", "rexaID":"fc0a66d3a7336b6eabad919a7389c68cc37f2564", "author":"Ayhan Demiriz and Kristin P. Bennett and John Shawe and I. Nouretdinov V.", "title":"Linear Programming Boosting via Column Generation", "venue":"Dept. of Decision Sciences and Eng. Systems, Rensselaer Polytechnic Institute", "year":"", "window":"criterion for stopping when an optimal ensemble is found that is reached in relatively few iterations. It uses few weak hypotheses. There are only 81 possible stumps on the <b>Breast</b> <b>Cancer</b> dataset (nine attributes having nine possible values), so clearly AdaBoost may require the same tree to be generated multiple times. LPBoost generates a weak hypothesis only once and can alter the weight on", "mykey":591},
 {"datasetID":45, "supportID":"4419EF2F549EAEE567BF235D8858FF3CC3489047", "rexaID":"fc0a66d3a7336b6eabad919a7389c68cc37f2564", "author":"Ayhan Demiriz and Kristin P. Bennett and John Shawe and I. Nouretdinov V.", "title":"Linear Programming Boosting via Column Generation", "venue":"Dept. of Decision Sciences and Eng. Systems, Rensselaer Polytechnic Institute", "year":"", "window":"and in the stopping criteria. Both methods were allowed the same maximum number of iterations. 8.1. Boosting Decision Tree Stumps We used decision tree stumps as base hypotheses on the following six datasets: Cancer (9,699), Diagnostic (30,569), <b>Heart</b> (13,297), Ionosphere (34,351), Musk (166,476), and Sonar (60,208). The number of features and number of points in each dataset are shown, respectively,", "mykey":592},
 {"datasetID":48, "supportID":"4419EF2F549EAEE567BF235D8858FF3CC3489047", "rexaID":"fc0a66d3a7336b6eabad919a7389c68cc37f2564", "author":"Ayhan Demiriz and Kristin P. Bennett and John Shawe and I. Nouretdinov V.", "title":"Linear Programming Boosting via Column Generation", "venue":"Dept. of Decision Sciences and Eng. Systems, Rensselaer Polytechnic Institute", "year":"", "window":"used in decision tree stumps experiments, we use four additional UCI datasets here. These are the House(16,435), <b>Housing</b> 13,506) 3 , Pima(8,768), and Spam(57,4601) datasets. As in the decision tree stumps experiments, we report results from 10-fold CV. Since the best # value", "mykey":593},
 {"datasetID":80, "supportID":"4419EF2F549EAEE567BF235D8858FF3CC3489047", "rexaID":"fc0a66d3a7336b6eabad919a7389c68cc37f2564", "author":"Ayhan Demiriz and Kristin P. Bennett and John Shawe and I. Nouretdinov V.", "title":"Linear Programming Boosting via Column Generation", "venue":"Dept. of Decision Sciences and Eng. Systems, Rensselaer Polytechnic Institute", "year":"", "window":"with missing values. The default handling in C4.5 has been used for missing values. USPS and Optdigits are <b>optical</b> character <b>recognition</b> datasets. USPS has 256 dimensions without missing values. Out of 7291 original training points, we use 1822 points as training data and the other 5469 as validation data. There are 2007 test points.", "mykey":594},
 {"datasetID":67, "supportID":"451B4188AA6DC0812D2468476B9A56FDC6182668", "rexaID":"144fcc2dcfad39ba4047467d4ce38139bc284f01", "author":"Daphne Koller and Mehran Sahami", "title":"Toward Optimal Feature Selection", "venue":"ICML", "year":"1996", "window":"include: the Corral data which was artificially constructed by John et al (1994) specifically for research in feature selection; the LED24, Vote, and <b>DNA</b> datasets from the UCI repository (Murphy & Aha 1995); and two datasets which are a subset of the Reuters document collection (Reuters 1995). These datasets are detailed in Table 1. We selected these", "mykey":595},
 {"datasetID":137, "supportID":"451B4188AA6DC0812D2468476B9A56FDC6182668", "rexaID":"144fcc2dcfad39ba4047467d4ce38139bc284f01", "author":"Daphne Koller and Mehran Sahami", "title":"Toward Optimal Feature Selection", "venue":"ICML", "year":"1996", "window":"from the UCI repository (Murphy & Aha 1995); and two datasets which are a subset of the <b>Reuters</b> document collection (Reuters 1995). These datasets are detailed in Table 1. We selected these datasets as they are either well understood in terms of feature", "mykey":596},
 {"datasetID":20, "supportID":"458D2E8276AA663BD64BA092F763AC6BAE3FF69A", "rexaID":"6c5ed6cbf453e8eaad5400424c931537c71a5e3e", "author":"Ke Wang and Shiyu Zhou and Ada Wai-Chee Fu and Jeffrey Xu Yu", "title":"Mining Changes of Classification by Correspondence Tracing", "venue":"SDM", "year":"2003", "window":"German Credit Data from the UCI Repository of Machine Learning Databases [14], and IPUMS <b>Census</b> Data from [1]. These data sets were chosen because no special knowledge is required to understand the addressed applications. To verify if the proposed method finds the changes that are supposed to be found, we need to know such", "mykey":597},
 {"datasetID":127, "supportID":"458D2E8276AA663BD64BA092F763AC6BAE3FF69A", "rexaID":"6c5ed6cbf453e8eaad5400424c931537c71a5e3e", "author":"Ke Wang and Shiyu Zhou and Ada Wai-Chee Fu and Jeffrey Xu Yu", "title":"Mining Changes of Classification by Correspondence Tracing", "venue":"SDM", "year":"2003", "window":"German Credit Data from the UCI Repository of Machine Learning Databases [14], and <b>IPUMS</b> Census Data from [1]. These data sets were chosen because no special knowledge is required to understand the addressed applications. To verify if the proposed method finds the changes that are supposed to be found, we need to know such", "mykey":598},
 {"datasetID":58, "supportID":"458D2E8276AA663BD64BA092F763AC6BAE3FF69A", "rexaID":"6c5ed6cbf453e8eaad5400424c931537c71a5e3e", "author":"Ke Wang and Shiyu Zhou and Ada Wai-Chee Fu and Jeffrey Xu Yu", "title":"Mining Changes of Classification by Correspondence Tracing", "venue":"SDM", "year":"2003", "window":"from old ones. Our discussion focuses on forward change mining, but it is equally applicable to backward change mining with the roles of old and new rules exchanged. Example 3.1. We use the  <b>Lenses</b>  data set from the UCI repository [14] to illustrate our approach. There are four attributes, three classes, and 18 examples: Attributes: A 1 : Age: 1, 2, 3 A 2 : Spectacle Prescription: 1, 2 A 3 :", "mykey":599},
 {"datasetID":144, "supportID":"458D2E8276AA663BD64BA092F763AC6BAE3FF69A", "rexaID":"6c5ed6cbf453e8eaad5400424c931537c71a5e3e", "author":"Ke Wang and Shiyu Zhou and Ada Wai-Chee Fu and Jeffrey Xu Yu", "title":"Mining Changes of Classification by Correspondence Tracing", "venue":"SDM", "year":"2003", "window":"for each small interval to have a separate classification characteristics, either having a different class or having higher accuracy. 4 Experiments We evaluated the proposed method on two real-life data sets, <b>German Credit</b> Data from the UCI Repository of Machine Learning Databases [14], and IPUMS Census Data from [1]. These data sets were chosen because no special knowledge is required to understand", "mykey":600},
 {"datasetID":34, "supportID":"45E9563187F82E664D7AA4C35BF2B2EF69696C8F", "rexaID":"c4bbcd894d98036f58a23f51ef83d1860d43021c", "author":"YongSeog Kim and W. Nick Street and Filippo Menczer", "title":"Optimal Ensemble Construction via Meta-Evolutionary Ensembles", "venue":"Business Information Systems, Utah State University", "year":"", "window":"and slightly better performance in the other data sets: <b>diabetes</b>  votes-84, and hypo. Compared to the traditional ensembles (Bagging and Boosting), MEE also shows superior performance. In comparison to Bagging, MEE demonstrates significantly better", "mykey":601},
 {"datasetID":53, "supportID":"45E9563187F82E664D7AA4C35BF2B2EF69696C8F", "rexaID":"c4bbcd894d98036f58a23f51ef83d1860d43021c", "author":"YongSeog Kim and W. Nick Street and Filippo Menczer", "title":"Optimal Ensemble Construction via Meta-Evolutionary Ensembles", "venue":"Business Information Systems, Utah State University", "year":"", "window":"with detailed information from most of input features to learn multiple patterns. Therefore, classifiers with information from few projected variables will not perform well. Note that, among 15 data sets, there are four multi-class data sets  <b>iris</b>  hypo, segment, and soybean) while the remaining 11 data sets are bi-class data sets. Out of four multi-class data sets, MEE shows consistently worse", "mykey":602},
 {"datasetID":56, "supportID":"45E9563187F82E664D7AA4C35BF2B2EF69696C8F", "rexaID":"c4bbcd894d98036f58a23f51ef83d1860d43021c", "author":"YongSeog Kim and W. Nick Street and Filippo Menczer", "title":"Optimal Ensemble Construction via Meta-Evolutionary Ensembles", "venue":"Business Information Systems, Utah State University", "year":"", "window":"It is interesting that MEE performs worse performance in two data sets, <b>labor</b> and segment, compared to both ordinary ensemble methods. Note also that MEE shows comparable performance compared to GEFS with a win-loss-tie score (4-5-6). However, we note that such", "mykey":603},
 {"datasetID":90, "supportID":"45E9563187F82E664D7AA4C35BF2B2EF69696C8F", "rexaID":"c4bbcd894d98036f58a23f51ef83d1860d43021c", "author":"YongSeog Kim and W. Nick Street and Filippo Menczer", "title":"Optimal Ensemble Construction via Meta-Evolutionary Ensembles", "venue":"Business Information Systems, Utah State University", "year":"", "window":"there are four multi-class data sets (iris, hypo, segment, and <b>soybean</b>  while the remaining 11 data sets are bi-class data sets. Out of four multi-class data sets, MEE shows consistently worse performance on \"segment\" data compared to", "mykey":604},
 {"datasetID":91, "supportID":"45E9563187F82E664D7AA4C35BF2B2EF69696C8F", "rexaID":"c4bbcd894d98036f58a23f51ef83d1860d43021c", "author":"YongSeog Kim and W. Nick Street and Filippo Menczer", "title":"Optimal Ensemble Construction via Meta-Evolutionary Ensembles", "venue":"Business Information Systems, Utah State University", "year":"", "window":"there are four multi-class data sets (iris, hypo, segment, and <b>soybean</b>  while the remaining 11 data sets are bi-class data sets. Out of four multi-class data sets, MEE shows consistently worse performance on \"segment\" data compared to", "mykey":605},
 {"datasetID":121, "supportID":"465A4BEAE130CAF080C34F8DE0884A27D16D123B", "rexaID":"c58fd4c0c5b8fefc00686150d5af26f6966807ef", "author":"Stephen D. Bay and Dennis F. Kibler and Michael J. Pazzani and Padhraic Smyth", "title":"The UCI KDD Archive of Large Data Sets for Data Mining Research and Experimentation", "venue":"SIGKDD Explorations, 2", "year":"2000", "window":"dealing with four different topic areas (music bands, bio-medical, goats, and sheep). Time series and Sequence data which consists of a consecutively ordered set of observations, such as the <b>EEG</b> data set in the archive. Time series measure changes in the value of a continuous variable such as stock prices or economic indicators whereas sequence data records an ordered set of categorical variables", "mykey":606},
 {"datasetID":122, "supportID":"465A4BEAE130CAF080C34F8DE0884A27D16D123B", "rexaID":"c58fd4c0c5b8fefc00686150d5af26f6966807ef", "author":"Stephen D. Bay and Dennis F. Kibler and Michael J. Pazzani and Padhraic Smyth", "title":"The UCI KDD Archive of Large Data Sets for Data Mining Research and Experimentation", "venue":"SIGKDD Explorations, 2", "year":"2000", "window":"table in more detail (i.e., the casts and people involved with making the movie). Spatial data which represents a set of observations located on a 2 or 3 dimensional grid. For example, the <b>El</b> <b>Nino</b> Dataset in the archive contains oceanographic and surface meteorological readings taken from a series of buoys positioned throughout the equatorial Pacific. Text Data suchaswebpages or newspaper articles.", "mykey":607},
 {"datasetID":125, "supportID":"465A4BEAE130CAF080C34F8DE0884A27D16D123B", "rexaID":"c58fd4c0c5b8fefc00686150d5af26f6966807ef", "author":"Stephen D. Bay and Dennis F. Kibler and Michael J. Pazzani and Padhraic Smyth", "title":"The UCI KDD Archive of Large Data Sets for Data Mining Research and Experimentation", "venue":"SIGKDD Explorations, 2", "year":"2000", "window":"currently in the archive. Classification: predict the value of a categorical target variable. For example, the <b>Insurance</b> <b>Benchmark</b> data set was used to predict which customers were interested in buying an insurance policy based on product usage data and demographic information. Regression: predict the value of a continuous target", "mykey":608},
 {"datasetID":130, "supportID":"465A4BEAE130CAF080C34F8DE0884A27D16D123B", "rexaID":"c58fd4c0c5b8fefc00686150d5af26f6966807ef", "author":"Stephen D. Bay and Dennis F. Kibler and Michael J. Pazzani and Padhraic Smyth", "title":"The UCI KDD Archive of Large Data Sets for Data Mining Research and Experimentation", "venue":"SIGKDD Explorations, 2", "year":"2000", "window":"Fan, Lee, Stolfo, and Miller [2] worked on the problem of reducing operational costs (the cost of running the system) for a real-time network intrusion detection system. They used the <b>KDD</b> <b>CUP</b> <b>1999</b> data set which is based on processed tcpdump files and includes intrusions such as probing, denial of service, illegal local access, and illegal root access. They developed multiple rule sets to identify", "mykey":609},
 {"datasetID":140, "supportID":"465A4BEAE130CAF080C34F8DE0884A27D16D123B", "rexaID":"c58fd4c0c5b8fefc00686150d5af26f6966807ef", "author":"Stephen D. Bay and Dennis F. Kibler and Michael J. Pazzani and Padhraic Smyth", "title":"The UCI KDD Archive of Large Data Sets for Data Mining Research and Experimentation", "venue":"SIGKDD Explorations, 2", "year":"2000", "window":"and surface meteorological readings taken from a series of buoys positioned throughout the equatorial Pacific. Text Data suchaswebpages or newspaper articles. For example, the <b>Syskill</b> and <b>Webert</b> data set contains webpages dealing with four different topic areas (music bands, bio-medical, goats, and sheep). Time series and Sequence data which consists of a consecutively ordered set of observations,", "mykey":610},
 {"datasetID":144, "supportID":"46C9D50C8B5A15A56C1290626C644CC44557F8E5", "rexaID":"acecd401473cf6df8234b59ef16f347cb77f6853", "author":"Paul O' Dea and Josephine Griffith and Colm O' Riordan", "title":"Combining Feature Selection and Neural Networks for Solving Classification Problems", "venue":"Information Technology Department, National University of Ireland", "year":"", "window":"network (as described earlier), using back-propagation as a learning algorithm, is used to classify the tuples \u00d1\u00b7\u00c4#\u00c7\u00ae\u00c7\u00ab\u00c7}\u00d1 \u00de based on the attributes \u00dc\u00c4#\u00c7\u00ab\u00c7\u00ae\u00c7\u00b7\u00dc \u00de . 5 Results 5.1 The <b>German Credit</b> Data Set In order to facilitate testing of the developed approach, experiments were conducted using the german credit data set 1 . The german credit data set contains information on 1000 loan applicants.", "mykey":611},
 {"datasetID":42, "supportID":"4706480C9E7BF0F35D0816255195ED2523352D95", "rexaID":"341c37581773a2a38cecc290e6272ca752b9df84", "author":"Suresh K. Choubey and Jitender S. Deogun and Vijay V. Raghavan and Hayri Sever", "title":"A comparison of feature selection algorithms in the context of rough classifiers", "venue":"", "year":"", "window":"different and zero otherwise, and the difference between two quantitative values is normalized into the interval [0,1]. We first consider results from Table 2. Except for <b>Glass</b>  Monks, and Hepatitis data sets, the performance obtained in Predictive Experiments approach those in the case of Upperbound Experiments. This suggests that for Glass, Monks, and Hepatitis data data set Size No. of Attributes", "mykey":612},
 {"datasetID":46, "supportID":"4706480C9E7BF0F35D0816255195ED2523352D95", "rexaID":"341c37581773a2a38cecc290e6272ca752b9df84", "author":"Suresh K. Choubey and Jitender S. Deogun and Vijay V. Raghavan and Hayri Sever", "title":"A comparison of feature selection algorithms in the context of rough classifiers", "venue":"", "year":"", "window":"different and zero otherwise, and the difference between two quantitative values is normalized into the interval [0,1]. We first consider results from Table 2. Except for Glass, Monks, and <b>Hepatitis</b> data sets, the performance obtained in Predictive Experiments approach those in the case of Upperbound Experiments. This suggests that for Glass, Monks, and Hepatitis data data set Size No. of Attributes", "mykey":613},
 {"datasetID":90, "supportID":"4706480C9E7BF0F35D0816255195ED2523352D95", "rexaID":"341c37581773a2a38cecc290e6272ca752b9df84", "author":"Suresh K. Choubey and Jitender S. Deogun and Vijay V. Raghavan and Hayri Sever", "title":"A comparison of feature selection algorithms in the context of rough classifiers", "venue":"", "year":"", "window":"for which BFS + UC performed worse than UC were small <b>soybean</b>  and glass data sets. AHS + UC, and HHS + UC performed worse than UC on glass, monks1, and monks2 data sets. KBS+UC performed poorly only in the case of glass data set. In the case of Upperbound Experiments, only monk1", "mykey":614},
 {"datasetID":91, "supportID":"4706480C9E7BF0F35D0816255195ED2523352D95", "rexaID":"341c37581773a2a38cecc290e6272ca752b9df84", "author":"Suresh K. Choubey and Jitender S. Deogun and Vijay V. Raghavan and Hayri Sever", "title":"A comparison of feature selection algorithms in the context of rough classifiers", "venue":"", "year":"", "window":"for which BFS + UC performed worse than UC were small <b>soybean</b>  and glass data sets. AHS + UC, and HHS + UC performed worse than UC on glass, monks1, and monks2 data sets. KBS+UC performed poorly only in the case of glass data set. In the case of Upperbound Experiments, only monk1", "mykey":615},
 {"datasetID":14, "supportID":"476B9E3931A5617DF185F87DEDC773054A700D8F", "rexaID":"d342517262ff52ffd3566bd8f520b36723486aa3", "author":"M. A. Galway and Michael G. Madden", "title":"DEPARTMENT OF INFORMATION TECHNOLOGY technical report NUIG-IT-011002 Evaluation of the Performance of the Markov Blanket Bayesian Classifier Algorithm", "venue":"Department of Information Technology National University of Ireland, Galway", "year":"", "window":"and all four are equally good on the <b>Breast</b> <b>Cancer</b> dataset. Na\u00efve TAN K2 MBBC Chess 87.63\u00b1 1.61 91.68\u00b1 1.09 94.03\u00b1 0.87 97.03\u00b1 0.54 WBCD 97.81\u00b1 0.51 97.47\u00b1 0.68 97.17\u00b1 1.05 97.30\u00b1 1.01 LED-24 73.28\u00b1 0.70 73.18\u00b1 0.63 73.14\u00b1 0.73 73.14\u00b1 0.73 DNA 94.80\u00b1 0.44", "mykey":616},
 {"datasetID":23, "supportID":"476B9E3931A5617DF185F87DEDC773054A700D8F", "rexaID":"d342517262ff52ffd3566bd8f520b36723486aa3", "author":"M. A. Galway and Michael G. Madden", "title":"DEPARTMENT OF INFORMATION TECHNOLOGY technical report NUIG-IT-011002 Evaluation of the Performance of the Markov Blanket Bayesian Classifier Algorithm", "venue":"Department of Information Technology National University of Ireland, Galway", "year":"", "window":"were selected that had these characteristics and that were not very small. The datasets are listed in Table 1. Dataset #I #A <b>Chess</b> (King & Rook vs King & Pawn) 3196 32 Wisconsin Breast Cancer Diagnosis 699 9 LED-24 (17 irrelevant attributes) 3200 24 DNA: Splice Junction Gene Sequences", "mykey":617},
 {"datasetID":21, "supportID":"476B9E3931A5617DF185F87DEDC773054A700D8F", "rexaID":"d342517262ff52ffd3566bd8f520b36723486aa3", "author":"M. A. Galway and Michael G. Madden", "title":"DEPARTMENT OF INFORMATION TECHNOLOGY technical report NUIG-IT-011002 Evaluation of the Performance of the Markov Blanket Bayesian Classifier Algorithm", "venue":"Department of Information Technology National University of Ireland, Galway", "year":"", "window":"were selected that had these characteristics and that were not very small. The datasets are listed in Table 1. Dataset #I #A <b>Chess</b> (King & Rook vs King & Pawn) 3196 32 Wisconsin Breast Cancer Diagnosis 699 9 LED-24 (17 irrelevant attributes) 3200 24 DNA: Splice Junction Gene Sequences", "mykey":618},
 {"datasetID":22, "supportID":"476B9E3931A5617DF185F87DEDC773054A700D8F", "rexaID":"d342517262ff52ffd3566bd8f520b36723486aa3", "author":"M. A. Galway and Michael G. Madden", "title":"DEPARTMENT OF INFORMATION TECHNOLOGY technical report NUIG-IT-011002 Evaluation of the Performance of the Markov Blanket Bayesian Classifier Algorithm", "venue":"Department of Information Technology National University of Ireland, Galway", "year":"", "window":"were selected that had these characteristics and that were not very small. The datasets are listed in Table 1. Dataset #I #A <b>Chess</b> (King & Rook vs King & Pawn) 3196 32 Wisconsin Breast Cancer Diagnosis 699 9 LED-24 (17 irrelevant attributes) 3200 24 DNA: Splice Junction Gene Sequences", "mykey":619},
 {"datasetID":63, "supportID":"476B9E3931A5617DF185F87DEDC773054A700D8F", "rexaID":"d342517262ff52ffd3566bd8f520b36723486aa3", "author":"M. A. Galway and Michael G. Madden", "title":"DEPARTMENT OF INFORMATION TECHNOLOGY technical report NUIG-IT-011002 Evaluation of the Performance of the Markov Blanket Bayesian Classifier Algorithm", "venue":"Department of Information Technology National University of Ireland, Galway", "year":"", "window":"with the fewest instances, this procedure was repeated 10 times. For the SPECT and <b>Lymphography</b> datasets, the procedure was repeated 50 times to reduce variability. Prediction accuracy results and standard deviations are reported in Table 2. Following usual conventions, for each dataset the algorithm", "mykey":620},
 {"datasetID":67, "supportID":"476B9E3931A5617DF185F87DEDC773054A700D8F", "rexaID":"d342517262ff52ffd3566bd8f520b36723486aa3", "author":"M. A. Galway and Michael G. Madden", "title":"DEPARTMENT OF INFORMATION TECHNOLOGY technical report NUIG-IT-011002 Evaluation of the Performance of the Markov Blanket Bayesian Classifier Algorithm", "venue":"Department of Information Technology National University of Ireland, Galway", "year":"", "window":"(based on a paired T-test at the 99% confidence level) and they outperform the other algorithms, they are both highlighted in bold. For example, K2 and MBBC are both best on the <b>DNA</b> Splice dataset and all four are equally good on the Breast Cancer dataset. Na\u00efve TAN K2 MBBC Chess 87.63\u00b1 1.61 91.68\u00b1 1.09 94.03\u00b1 0.87 97.03\u00b1 0.54 WBCD 97.81\u00b1 0.51 97.47\u00b1 0.68 97.17\u00b1 1.05 97.30\u00b1 1.01 LED-24 73.28\u00b1", "mykey":621},
 {"datasetID":69, "supportID":"476B9E3931A5617DF185F87DEDC773054A700D8F", "rexaID":"d342517262ff52ffd3566bd8f520b36723486aa3", "author":"M. A. Galway and Michael G. Madden", "title":"DEPARTMENT OF INFORMATION TECHNOLOGY technical report NUIG-IT-011002 Evaluation of the Performance of the Markov Blanket Bayesian Classifier Algorithm", "venue":"Department of Information Technology National University of Ireland, Galway", "year":"", "window":"(based on a paired T-test at the 99% confidence level) and they outperform the other algorithms, they are both highlighted in bold. For example, K2 and MBBC are both best on the DNA <b>Splice</b> dataset and all four are equally good on the Breast Cancer dataset. Na\u00efve TAN K2 MBBC Chess 87.63\u00b1 1.61 91.68\u00b1 1.09 94.03\u00b1 0.87 97.03\u00b1 0.54 WBCD 97.81\u00b1 0.51 97.47\u00b1 0.68 97.17\u00b1 1.05 97.30\u00b1 1.01 LED-24 73.28\u00b1", "mykey":622},
 {"datasetID":76, "supportID":"476B9E3931A5617DF185F87DEDC773054A700D8F", "rexaID":"d342517262ff52ffd3566bd8f520b36723486aa3", "author":"M. A. Galway and Michael G. Madden", "title":"DEPARTMENT OF INFORMATION TECHNOLOGY technical report NUIG-IT-011002 Evaluation of the Performance of the Markov Blanket Bayesian Classifier Algorithm", "venue":"Department of Information Technology National University of Ireland, Galway", "year":"", "window":"one for each of the analyses described above in Section 4.2. ROC graphs are best suited to two-class problems, which all but one of the datasets are. For the <b>Nursery</b> dataset, the ROC curve is for the prediction of the `Priority' class. On a ROC graph, the point (0 0) represents the strategy of never returning a positive classification, no", "mykey":623},
 {"datasetID":95, "supportID":"476B9E3931A5617DF185F87DEDC773054A700D8F", "rexaID":"d342517262ff52ffd3566bd8f520b36723486aa3", "author":"M. A. Galway and Michael G. Madden", "title":"DEPARTMENT OF INFORMATION TECHNOLOGY technical report NUIG-IT-011002 Evaluation of the Performance of the Markov Blanket Bayesian Classifier Algorithm", "venue":"Department of Information Technology National University of Ireland, Galway", "year":"", "window":"with the fewest instances, this procedure was repeated 10 times. For the <b>SPECT</b> and Lymphography datasets, the procedure was repeated 50 times to reduce variability. Prediction accuracy results and standard deviations are reported in Table 2. Following usual conventions, for each dataset the algorithm", "mykey":624},
 {"datasetID":45, "supportID":"47E3FA7EEB0A88D64A9510C33AE256A35616EDAC", "rexaID":"193231e78c226995eee9f66bf9a4177b8416daf4", "author":"H. -T Lin and C. -J Lin", "title":"A Study on Sigmoid Kernels for SVM and the Training of non-PSD Kernels by SMO-type Methods", "venue":"Department of Computer Science and Information Engineering National Taiwan University", "year":"", "window":"with ~ C = \u00af C 2a = decision value at x using the linear kernel with \u00af C. We can observe the result of Theorems 8 and 9 from Figure 1. The contours show five-fold cross-validation accuracy of the data set <b>heart</b> in different r and C. The contours with a = 1 are on the left-hand side, while those with a = 0.01 are on the right-hand side. Other parameters considered here are log 2 C fromlog 2 (- r) from", "mykey":625},
 {"datasetID":98, "supportID":"47E3FA7EEB0A88D64A9510C33AE256A35616EDAC", "rexaID":"193231e78c226995eee9f66bf9a4177b8416daf4", "author":"H. -T Lin and C. -J Lin", "title":"A Study on Sigmoid Kernels for SVM and the Training of non-PSD Kernels by SMO-type Methods", "venue":"Department of Computer Science and Information Engineering National Taiwan University", "year":"", "window":"D. J. Spiegelhalter, and C. C. Taylor (1994). Machine Learning, Neural and Statistical Classification. Englewood Cliffs, N.J.: Prentice Hall. Data available at http://www.ncc.up.pt/liacc/ML <b>statlog</b> datasets.html. Nash, S. G. and A. Sofer (1996). Linear and Nonlinear Programming. McGraw-Hill. Osuna, E., R. Freund, and F. Girosi (1997). Training support vector machines: An application to face detection.", "mykey":626},
 {"datasetID":1, "supportID":"4827BE00C6A808214167F30BDDA4368A8D0AE258", "rexaID":"351e173bc2176dbf14635cc5471660c911f8e79c", "author":"Edward Snelson and Carl Edward Rasmussen and Zoubin Ghahramani", "title":"Warped Gaussian Processes", "venue":"NIPS", "year":"2003", "window":"predict the the age of <b>abalone</b> from various physical inputs [9]. ailerons is a simulated control problem, with the aim to predict the control action on the ailerons of an F16 aircraft [10, 11]. For datasets creep and abalone, which consist of positive observations only, standard practice may be to model the log of the data with a GP. So for these datasets we have compared three models: a GP directly", "mykey":627},
 {"datasetID":53, "supportID":"48520C7B8FFCAED34318562A748E4B486DC24CBE", "rexaID":"545d588c45ddf6031f6472c1573778506fbae9cf", "author":"Dick de Ridder and Olga Kouropteva and Oleg Okun and Matti Pietik\u00e4inen and Robert P W Duin", "title":"Supervised Locally Linear Embedding", "venue":"ICANN", "year":"2003", "window":"retained in the remaining M dimensions [3]. This local intrinsic dimensionality estimate is denoted by ML . The feature extraction process is illustrated in Figure 1: the C = 3 classes in the <b>iris</b> data set [1] are mapped onto single points by 1-SLLE. #-SLLE retains some of the class structure, but reduces within-class dispersion compared to LLE. Clearly, SLLE is suitable as a feature extraction step", "mykey":628},
 {"datasetID":45, "supportID":"487E52D69FABD2A08021D43233E0EAD5E74F9BD2", "rexaID":"cb68337ad074a5f5ce7d8ca8a7a0b7bad1931070", "author":"Rudy Setiono and Wee Kheng Leow", "title":"Generating rules from trained network using fast pruning", "venue":"School of Computing National University of Singapore", "year":"", "window":"decision nodes may also improve the accuracy of the tree because samples from real world problems may be better separated by oblique hyperplanes. This is the case with the <b>heart</b> disease data set (HeartD in Table 2) where significant improvement is achieved by the neural network methods over C4.5. There is no significant difference in the accuracy and size of the decision trees generated by", "mykey":629},
 {"datasetID":5, "supportID":"48B67DF0A0EF7EA8BB970DB0BB645C0B5CDFF7A2", "rexaID":"e3fb3565af764647303c58f32b57a23f9eae18c0", "author":"Krista Lagus and Esa Alhoniemi and Jeremias Seppa and Antti Honkela and Arno Wagner", "title":"INDEPENDENT VARIABLE GROUP ANALYSIS IN LEARNING COMPACT REPRESENTATIONS FOR DATA", "venue":"Neural Networks Research Centre, Helsinki University of Technology", "year":"", "window":"models optimized carefully using the IVGA implementation. The model search of our IVGA implementation was able to discover the best grouping, i.e., the one with the smallest cost. 3.2. <b>Arrhythmia</b> data set The identification of different types of heart problems, namely cardiac arrhythmias, is carried out based on electrocardiography measurings from a large number of electrodes. We used a freely", "mykey":630},
 {"datasetID":45, "supportID":"48B67DF0A0EF7EA8BB970DB0BB645C0B5CDFF7A2", "rexaID":"e3fb3565af764647303c58f32b57a23f9eae18c0", "author":"Krista Lagus and Esa Alhoniemi and Jeremias Seppa and Antti Honkela and Arno Wagner", "title":"INDEPENDENT VARIABLE GROUP ANALYSIS IN LEARNING COMPACT REPRESENTATIONS FOR DATA", "venue":"Neural Networks Research Centre, Helsinki University of Technology", "year":"", "window":"models optimized carefully using the IVGA implementation. The model search of our IVGA implementation was able to discover the best grouping, i.e., the one with the smallest cost. 3.2. Arrhythmia data set The identification of different types of <b>heart</b> problems, namely cardiac arrhythmias, is carried out based on electrocardiography measurings from a large number of electrodes. We used a freely", "mykey":631},
 {"datasetID":34, "supportID":"48E3C24E1A609BE8ECC7908EC173EDC1665F4720", "rexaID":"08bad2c42799dc0f04d6729f069239fba413cb8f", "author":"Jan C. Bioch and D. Meer and Rob Potharst", "title":"Bivariate Decision Trees", "venue":"PKDD", "year":"1997", "window":"with the standard error. From these table we can conclude 10 name cases attr classes glass 214 9 6 <b>diabetes</b> pima) 768 8 2 breast cancer 699 9 2 heart 270 13 2 wave 300 21 3 Table 1: Summary of the Datasets method glass diabetes cancer heart wave BIT1 65.3Sigma1:1 74.3Sigma0:7 95.4Sigma0:3 78.5Sigma0:3 76.1Sigma1:3 6.2Sigma2:1 5.2Sigma2:5 2.8Sigma0:2 4.1Sigma0:5 5.0Sigma1:6 BIT2", "mykey":632},
 {"datasetID":42, "supportID":"48E3C24E1A609BE8ECC7908EC173EDC1665F4720", "rexaID":"08bad2c42799dc0f04d6729f069239fba413cb8f", "author":"Jan C. Bioch and D. Meer and Rob Potharst", "title":"Bivariate Decision Trees", "venue":"PKDD", "year":"1997", "window":"with the standard error. From these table we can conclude 10 name cases attr classes <b>glass</b> 214 9 6 diabetes(pima) 768 8 2 breast cancer 699 9 2 heart 270 13 2 wave 300 21 3 Table 1: Summary of the Datasets method glass diabetes cancer heart wave BIT1 65.3Sigma1:1 74.3Sigma0:7 95.4Sigma0:3 78.5Sigma0:3 76.1Sigma1:3 6.2Sigma2:1 5.2Sigma2:5 2.8Sigma0:2 4.1Sigma0:5 5.0Sigma1:6 BIT2", "mykey":633},
 {"datasetID":45, "supportID":"48E3C24E1A609BE8ECC7908EC173EDC1665F4720", "rexaID":"08bad2c42799dc0f04d6729f069239fba413cb8f", "author":"Jan C. Bioch and D. Meer and Rob Potharst", "title":"Bivariate Decision Trees", "venue":"PKDD", "year":"1997", "window":"with the standard error. From these table we can conclude 10 name cases attr classes glass 214 9 6 diabetes(pima) 768 8 2 breast cancer 699 9 2 <b>heart</b> 270 13 2 wave 300 21 3 Table 1: Summary of the Datasets method glass diabetes cancer heart wave BIT1 65.3Sigma1:1 74.3Sigma0:7 95.4Sigma0:3 78.5Sigma0:3 76.1Sigma1:3 6.2Sigma2:1 5.2Sigma2:5 2.8Sigma0:2 4.1Sigma0:5 5.0Sigma1:6 BIT2", "mykey":634},
 {"datasetID":70, "supportID":"48E3C24E1A609BE8ECC7908EC173EDC1665F4720", "rexaID":"08bad2c42799dc0f04d6729f069239fba413cb8f", "author":"Jan C. Bioch and D. Meer and Rob Potharst", "title":"Bivariate Decision Trees", "venue":"PKDD", "year":"1997", "window":"in which interactions between two variables occur. We will test our method on two artificial data sets. The first is the <b>monk</b> 1 data set [Thr91]. This data set contains 6 attributes and two classes. The rule that generates the data is: if (x 1 = x 2 or x 5 = 1) then yes else no. Note that x 5 takes", "mykey":635},
 {"datasetID":52, "supportID":"491268B8CE84430AC311F09243CC318DC2CA8CD5", "rexaID":"a3fca86cb1f854c7fd7975e66aae2a5ba3710f8a", "author":"Mikhail Bilenko and Sugato Basu and Raymond J. Mooney", "title":"Integrating constraints and metric learning in semi-supervised clustering", "venue":"ICML", "year":"2004", "window":"from the UCI repository: Iris, Wine, and <b>Ionosphere</b> (Blake & Merz, 1998); the Protein dataset used by Xing et al. (2003) and Bar-Hillel et al. (2003), and randomly sampled subsets from the Digits and Letters handwritten character recognition datasets, also from the UCI repository. For Digits", "mykey":636},
 {"datasetID":53, "supportID":"491268B8CE84430AC311F09243CC318DC2CA8CD5", "rexaID":"a3fca86cb1f854c7fd7975e66aae2a5ba3710f8a", "author":"Mikhail Bilenko and Sugato Basu and Raymond J. Mooney", "title":"Integrating constraints and metric learning in semi-supervised clustering", "venue":"ICML", "year":"2004", "window":"Experiments were conducted on three datasets from the UCI repository: <b>Iris</b>  Wine, and Ionosphere (Blake & Merz, 1998); the Protein dataset used by Xing et al. (2003) and Bar-Hillel et al. (2003), and randomly sampled subsets from the Digits", "mykey":637},
 {"datasetID":81, "supportID":"491268B8CE84430AC311F09243CC318DC2CA8CD5", "rexaID":"a3fca86cb1f854c7fd7975e66aae2a5ba3710f8a", "author":"Mikhail Bilenko and Sugato Basu and Raymond J. Mooney", "title":"Integrating constraints and metric learning in semi-supervised clustering", "venue":"ICML", "year":"2004", "window":"used by Xing et al. (2003) and Bar-Hillel et al. (2003), and randomly sampled subsets from the <b>Digits</b> and Letters <b>handwritten</b> character <b>recognition</b> datasets, also from the UCI repository. For Digits and Letters, we chose two sets of three classes: {I, J, L} from Letters and {3, 8, 9} from Digits, sampling 10% of the data points from the original", "mykey":638},
 {"datasetID":154, "supportID":"491268B8CE84430AC311F09243CC318DC2CA8CD5", "rexaID":"a3fca86cb1f854c7fd7975e66aae2a5ba3710f8a", "author":"Mikhail Bilenko and Sugato Basu and Raymond J. Mooney", "title":"Integrating constraints and metric learning in semi-supervised clustering", "venue":"ICML", "year":"2004", "window":"from the UCI repository: Iris, Wine, and Ionosphere (Blake & Merz, 1998); the <b>Protein</b> dataset used by Xing et al. (2003) and Bar-Hillel et al. (2003), and randomly sampled subsets from the Digits and Letters handwritten character recognition datasets, also from the UCI repository. For Digits", "mykey":639},
 {"datasetID":109, "supportID":"491268B8CE84430AC311F09243CC318DC2CA8CD5", "rexaID":"a3fca86cb1f854c7fd7975e66aae2a5ba3710f8a", "author":"Mikhail Bilenko and Sugato Basu and Raymond J. Mooney", "title":"Integrating constraints and metric learning in semi-supervised clustering", "venue":"ICML", "year":"2004", "window":"Experiments were conducted on three datasets from the UCI repository: Iris, <b>Wine</b>  and Ionosphere (Blake & Merz, 1998); the Protein dataset used by Xing et al. (2003) and Bar-Hillel et al. (2003), and randomly sampled subsets from the Digits", "mykey":640},
 {"datasetID":73, "supportID":"497B9C56D7A4C64C5D3162F7CC285F0162927CC4", "rexaID":"fe5a0c8561af1bb1c51d048bdde1e2b9e8570dae", "author":"Seth Bullock and Peter M. Todd", "title":"Made to Measure: Ecological Rationality in Structured Environments", "venue":"Center for Adaptive Behavior and Cognition Max Planck Institute for Human Development", "year":"1999", "window":"and/or its absence to be an indicator of toxicity, and those for which the presence or absence of the cue indicates the opposite. One might expect that since, on average across the <b>Mushroom</b> Problem data set, the presence of each cue tends to indicate edibility, rules of the former kind might be more useful and hence better represented in the set of elite strategies. Fig. 15 shows that this is indeed", "mykey":641},
 {"datasetID":14, "supportID":"49FDF822B2C887B00F7868C231AC09B9110E04FA", "rexaID":"3a500f9d0b3bfdadc810cde1043178b2d127888e", "author":"Hussein A. Abbass", "title":"An evolutionary artificial neural networks approach for breast cancer diagnosis", "venue":"Artificial Intelligence in Medicine, 25", "year":"2002", "window":"well, compared to the previous studies. In another study, Setiono [26] used his rule extraction from ANNs algorithm [28, 29] to extract useful rules that can predict <b>breast</b> <b>cancer</b> from the Wisconsin dataset. He needed first to train an ANN using BP and achieved an accuracy level on the test data of approximately 94%. After applying his rule extraction technique, the accuracy of the extracted rule set", "mykey":642},
 {"datasetID":17, "supportID":"49FDF822B2C887B00F7868C231AC09B9110E04FA", "rexaID":"3a500f9d0b3bfdadc810cde1043178b2d127888e", "author":"Hussein A. Abbass", "title":"An evolutionary artificial neural networks approach for breast cancer diagnosis", "venue":"Artificial Intelligence in Medicine, 25", "year":"2002", "window":"well, compared to the previous studies. In another study, Setiono [26] used his rule extraction from ANNs algorithm [28, 29] to extract useful rules that can predict <b>breast</b> <b>cancer</b> from the <b>Wisconsin</b> dataset. He needed first to train an ANN using BP and achieved an accuracy level on the test data of approximately 94%. After applying his rule extraction technique, the accuracy of the extracted rule set", "mykey":643},
 {"datasetID":15, "supportID":"49FDF822B2C887B00F7868C231AC09B9110E04FA", "rexaID":"3a500f9d0b3bfdadc810cde1043178b2d127888e", "author":"Hussein A. Abbass", "title":"An evolutionary artificial neural networks approach for breast cancer diagnosis", "venue":"Artificial Intelligence in Medicine, 25", "year":"2002", "window":"well, compared to the previous studies. In another study, Setiono [26] used his rule extraction from ANNs algorithm [28, 29] to extract useful rules that can predict <b>breast</b> <b>cancer</b> from the <b>Wisconsin</b> dataset. He needed first to train an ANN using BP and achieved an accuracy level on the test data of approximately 94%. After applying his rule extraction technique, the accuracy of the extracted rule set", "mykey":644},
 {"datasetID":16, "supportID":"49FDF822B2C887B00F7868C231AC09B9110E04FA", "rexaID":"3a500f9d0b3bfdadc810cde1043178b2d127888e", "author":"Hussein A. Abbass", "title":"An evolutionary artificial neural networks approach for breast cancer diagnosis", "venue":"Artificial Intelligence in Medicine, 25", "year":"2002", "window":"well, compared to the previous studies. In another study, Setiono [26] used his rule extraction from ANNs algorithm [28, 29] to extract useful rules that can predict <b>breast</b> <b>cancer</b> from the <b>Wisconsin</b> dataset. He needed first to train an ANN using BP and achieved an accuracy level on the test data of approximately 94%. After applying his rule extraction technique, the accuracy of the extracted rule set", "mykey":645},
 {"datasetID":14, "supportID":"4A57C0FFEEC2665CAF8E11574CE5A9618304B979", "rexaID":"de9b2ac4ef9c6449914cdc375972f722190fee7a", "author":"Chris Drummond and Robert C. Holte", "title":"C4.5, Class Imbalance, and Cost Sensitivity: Why Under-Sampling beats Over-Sampling", "venue":"Institute for Information Technology, National Research Council Canada", "year":"", "window":"Cost Function PCF(+) Normalized Expected Cost 0.4 0.6 0.8 1.0 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 \u00c5 \u00c5 \u00c5 \u00c5 \u00c5 \u00c5 \u00c5 \u00c5 \u00c5 \u00c5 \u00c5 \u00c5 0.0 \u00c5 0.2 Figure 3. Credit: Comparing Sampling Schemes <b>breast</b> <b>cancer</b> data set from the Institute of Oncology, Ljubljana. It has 286 instances, 201 non-recurrences and 85 recurrences, with 9 nominal attributes. For this data set, C4.5 only marginally outperforms the cheapest", "mykey":646},
 {"datasetID":151, "supportID":"4A57C0FFEEC2665CAF8E11574CE5A9618304B979", "rexaID":"de9b2ac4ef9c6449914cdc375972f722190fee7a", "author":"Chris Drummond and Robert C. Holte", "title":"C4.5, Class Imbalance, and Cost Sensitivity: Why Under-Sampling beats Over-Sampling", "venue":"Institute for Information Technology, National Research Council Canada", "year":"", "window":"The bold dashed curve in Figure 2 shows the performance of C4.5 using under-sampling on the <b>Sonar</b> data set. Sonar has 208 instances, 111 <b>mines</b> and 97 <b>rocks</b>  with 60 real valued attributes. Undersampling produces a cost curve that is reasonably cost sensitive, it is quite smooth and largely within the", "mykey":647},
 {"datasetID":46, "supportID":"4A57C0FFEEC2665CAF8E11574CE5A9618304B979", "rexaID":"de9b2ac4ef9c6449914cdc375972f722190fee7a", "author":"Chris Drummond and Robert C. Holte", "title":"C4.5, Class Imbalance, and Cost Sensitivity: Why Under-Sampling beats Over-Sampling", "venue":"Institute for Information Technology, National Research Council Canada", "year":"", "window":"as they produced cost curves that captured all the qualitative features we observed in a larger set of experiments (including other UCI data sets: vote, <b>hepatitis</b>  labor, letter-k and glass2). For these data sets, under-sampling combined with C4.5 is a useful baseline to evaluate other algorithms. Over-sampling, on the other hand, is not to", "mykey":648},
 {"datasetID":28, "supportID":"4A57C0FFEEC2665CAF8E11574CE5A9618304B979", "rexaID":"de9b2ac4ef9c6449914cdc375972f722190fee7a", "author":"Chris Drummond and Robert C. Holte", "title":"C4.5, Class Imbalance, and Cost Sensitivity: Why Under-Sampling beats Over-Sampling", "venue":"Institute for Information Technology, National Research Council Canada", "year":"", "window":"Expected Cost 0.4 0.6 0.8 1.0 0.0 0.1 0.2 0.3 0.4 \u00c5 \u00c5 \u00c5 \u00c5 \u00c5 \u00c5 \u00c5 \u00c5 \u00c5 \u00c5 \u00c5 \u00c5 0.0 \u00c5 0.2 Figure 2. Sonar: Comparing Sampling Schemes Figure 3 shows the different sampling schemes for the <b>Japanese</b> <b>credit</b> data set. It has 690 instances, 307 positive and 383 negative, with 15 attributes, 6 real and 9 nominal. Again for under-sampling, the curve is reasonably smooth and this time remains completely within the", "mykey":649},
 {"datasetID":56, "supportID":"4A57C0FFEEC2665CAF8E11574CE5A9618304B979", "rexaID":"de9b2ac4ef9c6449914cdc375972f722190fee7a", "author":"Chris Drummond and Robert C. Holte", "title":"C4.5, Class Imbalance, and Cost Sensitivity: Why Under-Sampling beats Over-Sampling", "venue":"Institute for Information Technology, National Research Council Canada", "year":"", "window":"as they produced cost curves that captured all the qualitative features we observed in a larger set of experiments (including other UCI data sets: vote, hepatitis, <b>labor</b>  letter-k and glass2). For these data sets, under-sampling combined with C4.5 is a useful baseline to evaluate other algorithms. Over-sampling, on the other hand, is not to", "mykey":650},
 {"datasetID":19, "supportID":"4A695C60BD2F814C0633767185624621A6FE72EC", "rexaID":"217beab6a7a7b64dc929c3c5fdb42e812f8b2431", "author":"Daniel J. Lizotte", "title":"Library Release Form Name of Author", "venue":"Budgeted Learning of Naive Bayes Classifiers", "year":"", "window":"than when it is assumed to be 300 (i.e., when it looks ahead farther than it should). Other policies do not take the budget into account. We have observed the same overall patterns on several other datasets that we have tested the policies on so far  <b>CAR</b>  DIABETES, CHESS, BREAST): the performance of SFL is superior or comparable to the performance of other policies, and Biased-Robin is the best 36", "mykey":651},
 {"datasetID":105, "supportID":"4A695C60BD2F814C0633767185624621A6FE72EC", "rexaID":"217beab6a7a7b64dc929c3c5fdb42e812f8b2431", "author":"Daniel J. Lizotte", "title":"Library Release Form Name of Author", "venue":"Budgeted Learning of Naive Bayes Classifiers", "year":"", "window":"75 times, while a non-discriminative feature such as feature 18 is bought an average of only 2 times. For some budgets, the 0/1 error of SFL is nearly half that generated by Round-Robin. The <b>votes dataset</b> (Figures 3.7(a) and 3.7(b)) is a binary class problem (democrat vs. republican), with 16 binary features, 435 instances, and a positive class probability of 0.61. In the votes dataset, there is a", "mykey":652},
 {"datasetID":73, "supportID":"4A695C60BD2F814C0633767185624621A6FE72EC", "rexaID":"217beab6a7a7b64dc929c3c5fdb42e812f8b2431", "author":"Daniel J. Lizotte", "title":"Library Release Form Name of Author", "venue":"Budgeted Learning of Naive Bayes Classifiers", "year":"", "window":"from the UCI Machine Learning Repository [BM98]. These plots show averaged validation error of the policies on a holdout set (20% of the dataset) on the <b>mushroom</b>  nursery, and votes datasets. Each point is an average of 50 trials where in each trial a random balanced partition of classes was made for training and validation. The five-fold", "mykey":653},
 {"datasetID":76, "supportID":"4A695C60BD2F814C0633767185624621A6FE72EC", "rexaID":"217beab6a7a7b64dc929c3c5fdb42e812f8b2431", "author":"Daniel J. Lizotte", "title":"Library Release Form Name of Author", "venue":"Budgeted Learning of Naive Bayes Classifiers", "year":"", "window":"from the UCI Machine Learning Repository [BM98]. These plots show averaged validation error of the policies on a holdout set (20% of the dataset) on the mushroom, <b>nursery</b>  and votes datasets. Each point is an average of 50 trials where in each trial a random balanced partition of classes was made for training and validation. The five-fold", "mykey":654},
 {"datasetID":42, "supportID":"4A7FADAA8DF9B482A723551E6611E531FB3ED8C1", "rexaID":"932cec9f3183bf03e366c0b093984ca256a4c3e4", "author":"D. I. S I and Francesco Masulli and Giorgio Valentini and D. I. S", "title":"Universit#a di Genova", "venue":"Dipartimento di Informatica e Scienze dell' Informazione", "year":"2001", "window":"machine, varying the number of hidden units between 5 to 50, yielding to 11 # 20 = 220 evaluations of I E ; I SE ; #R and # S both for ECOC monolithic and ECOC PND learning machines. On the UCI data sets <b>glass</b>  letter and 18 optdigits we have used only 2 different structures, using, respectively, 5 and 9, 120 and 140, 60 and 70 hidden units, yielding to 2 # 20 = 40 evaluations of the mutual", "mykey":655},
 {"datasetID":56, "supportID":"4AE20E99F31E61F1B4C35E978E29A8BD535D8AA5", "rexaID":"96b3a7e6a59457734cf527c895053fb4af3141cb", "author":"Huan Liu and Rudy Setiono", "title":"To appear in Proceedings of IEA-AIE96 FEATURE SELECTION AND CLASSIFICATION -- A PROBABILISTIC WRAPPER APPROACH", "venue":"Department of Information Systems and Computer Science National University of Singapore", "year":"", "window":"selection is conducted, which includes CorrAL [JKP94], Monks13 [TBB + 91], and Parity5+5. The other type is real-world data including Credit, Vote, and <b>Labor</b> [Qui93, MA94]. The choice of these datasets can simplify the comparison of this work with some published work. These datasets were used in [JKP94] in which comparisons with different methods were described. For the artificial datasets, no", "mykey":656},
 {"datasetID":103, "supportID":"4B2F13FA93C33F07A6EA8C7B311E6E7672D56605", "rexaID":"d8d6151f296628418779cbc9a42cdba8ee766415", "author":"Daan Fierens and Jan Ramon and Hendrik Blockeel and Maurice Bruynooghe", "title":"A Comparison of Approaches for Learning Probability Trees", "venue":"Department of Computer Science", "year":"", "window":"16 yeast 1484 10 45 N NbClasses NbTests biodegradability 328 2 47 carcinogenesis 330 2 305 diterpenes 1504 23 210 hiv 41768 2 49 mutagenesis 230 2 139 <b>trains</b> 25000 2 73 3.1 Experiments on Benchmark Datasets: Setup and Results 3.1.1 Experimental Setup Table 2 gives an overview of the datasets used. All non-relational datasets are from the UCI-repository [10], except asm [8]. All relational datasets are", "mykey":657},
 {"datasetID":50, "supportID":"4B689FBDC5FF3A51AB71705380E5A70E2B3AE01F", "rexaID":"e65031293b77aadf555fd232a960cbd8a3fe739a", "author":"Nikos A. Vlassis and Aristidis Likas", "title":"A greedy EM algorithm for Gaussian mixture", "venue":"Intelligent Autonomous Systems, IAS", "year":"", "window":"this becomes clearer as the degree of separation c of the components increases. In the second part of the experiments we applied the greedy EM and the regular EM algorithm on an <b>image segmentation</b> data set available from the UCI repository [1]. This data set contains 210 training patterns and 2100 test patterns of 19 features. Although this is a supervised data set, in the conducted experiments we", "mykey":658},
 {"datasetID":147, "supportID":"4B689FBDC5FF3A51AB71705380E5A70E2B3AE01F", "rexaID":"e65031293b77aadf555fd232a960cbd8a3fe739a", "author":"Nikos A. Vlassis and Aristidis Likas", "title":"A greedy EM algorithm for Gaussian mixture", "venue":"Intelligent Autonomous Systems, IAS", "year":"", "window":"this becomes clearer as the degree of separation c of the components increases. In the second part of the experiments we applied the greedy EM and the regular EM algorithm on an <b>image segmentation</b> data set available from the UCI repository [1]. This data set contains 210 training patterns and 2100 test patterns of 19 features. Although this is a supervised data set, in the conducted experiments we", "mykey":659},
 {"datasetID":74, "supportID":"4B79C231DD6215FD4237361C956CB215356942FB", "rexaID":"476d22372039041e04dd57d8eb40a2666f04fb50", "author":"Qingping Tao and Stephen Scott and N. V. Vinodchandran and Thomas T. Osugi", "title":"SVM-based generalized multiple-instance learning via approximate box counting", "venue":"ICML", "year":"2004", "window":"results of our new kernel on applications such as content-based image retrieval, prediction of drug affinity to bind to multiple sites simultaneously, protein sequence identification, and the <b>Musk</b> data sets. Finally, we conclude in Section 7. 2. Notation and Definitions Let X denote {0, . . . , s} d (though our results trivially generalize to X = Q d i=1 {0, . . . , s i }). Let BX denote the set of", "mykey":660},
 {"datasetID":75, "supportID":"4B79C231DD6215FD4237361C956CB215356942FB", "rexaID":"476d22372039041e04dd57d8eb40a2666f04fb50", "author":"Qingping Tao and Stephen Scott and N. V. Vinodchandran and Thomas T. Osugi", "title":"SVM-based generalized multiple-instance learning via approximate box counting", "venue":"ICML", "year":"2004", "window":"results of our new kernel on applications such as content-based image retrieval, prediction of drug affinity to bind to multiple sites simultaneously, protein sequence identification, and the <b>Musk</b> data sets. Finally, we conclude in Section 7. 2. Notation and Definitions Let X denote {0, . . . , s} d (though our results trivially generalize to X = Q d i=1 {0, . . . , s i }). Let BX denote the set of", "mykey":661},
 {"datasetID":154, "supportID":"4B79C231DD6215FD4237361C956CB215356942FB", "rexaID":"476d22372039041e04dd57d8eb40a2666f04fb50", "author":"Qingping Tao and Stephen Scott and N. V. Vinodchandran and Thomas T. Osugi", "title":"SVM-based generalized multiple-instance learning via approximate box counting", "venue":"ICML", "year":"2004", "window":"results of our new kernel on applications such as content-based image retrieval, prediction of drug affinity to bind to multiple sites simultaneously, <b>protein</b> sequence identification, and the Musk data sets. Finally, we conclude in Section 7. 2. Notation and Definitions Let X denote {0, . . . , s} d (though our results trivially generalize to X = Q d i=1 {0, . . . , s i }). Let BX denote the set of", "mykey":662},
 {"datasetID":44, "supportID":"4BA93F4D9B7EA9CDB1E504AB05EB0153CB696AC5", "rexaID":"43adf91a23b576706c55888c9f46d07d424a7938", "author":"Anthony D. Griffiths and Derek Bridge", "title":"A Yardstick for the Evaluation of Case-Based Classifiers", "venue":"Department of Computer Science, University of York", "year":"", "window":"used in Figures 5 and 4 The data set in the UCI repository for the  <b>Hayes</b> <b>Roth</b>  target function described above is incomplete in that it contains instances for only some of the possible descriptions, contains duplications, and contains", "mykey":663},
 {"datasetID":58, "supportID":"4BA93F4D9B7EA9CDB1E504AB05EB0153CB696AC5", "rexaID":"43adf91a23b576706c55888c9f46d07d424a7938", "author":"Anthony D. Griffiths and Derek Bridge", "title":"A Yardstick for the Evaluation of Case-Based Classifiers", "venue":"Department of Computer Science, University of York", "year":"", "window":"giving 24 possible customer descriptions. These descriptions are classified into three classes according to the suitability of different types of contact <b>lens</b>  The documentation for the data set indicates that a correct description of the target function requires 9 production rules. The distribution of the classification values is highly skewed, giving a relative frequency for the majority", "mykey":664},
 {"datasetID":7, "supportID":"4BADC3F0AD169ED9EC7D073375E9B168FA9F6C8F", "rexaID":"660eddc9c8a809e94ad8d5837952f89d8997ab92", "author":"Mohammed Waleed Kadous and Claude Sammut", "title":"The University of New South Wales School of Computer Science and Engineering Temporal Classification: Extending the Classification Paradigm to Multivariate Time Series", "venue":"", "year":"", "window":"Saito [Sai94], and further worked on by Manganaris [Man97]. The task is to classify a stream as one of three classes, cylinder (c), bell (b) or funnel (f ). Samples are generated as follows: 2 These datasets are: arrythmia, <b>audiology</b>  bach chorales, echocardiogram, isolet, mobile robots, waveform. 6. Experimental Evaluation 161 c(t) = (6 + #) \u00b7 # [a,b] (t) + #(t) b(t) = (6 + #) \u00b7 # [a,b] (t) \u00b7 (t -", "mykey":665},
 {"datasetID":8, "supportID":"4BADC3F0AD169ED9EC7D073375E9B168FA9F6C8F", "rexaID":"660eddc9c8a809e94ad8d5837952f89d8997ab92", "author":"Mohammed Waleed Kadous and Claude Sammut", "title":"The University of New South Wales School of Computer Science and Engineering Temporal Classification: Extending the Classification Paradigm to Multivariate Time Series", "venue":"", "year":"", "window":"Saito [Sai94], and further worked on by Manganaris [Man97]. The task is to classify a stream as one of three classes, cylinder (c), bell (b) or funnel (f ). Samples are generated as follows: 2 These datasets are: arrythmia, <b>audiology</b>  bach chorales, echocardiogram, isolet, mobile robots, waveform. 6. Experimental Evaluation 161 c(t) = (6 + #) \u00b7 # [a,b] (t) + #(t) b(t) = (6 + #) \u00b7 # [a,b] (t) \u00b7 (t -", "mykey":666},
 {"datasetID":114, "supportID":"4BADC3F0AD169ED9EC7D073375E9B168FA9F6C8F", "rexaID":"660eddc9c8a809e94ad8d5837952f89d8997ab92", "author":"Mohammed Waleed Kadous and Claude Sammut", "title":"The University of New South Wales School of Computer Science and Engineering Temporal Classification: Extending the Classification Paradigm to Multivariate Time Series", "venue":"", "year":"", "window":"Mark Peters for sharing their experiences and for proofreading early drafts; and also to two other proofreaders: Peter Gammie and Peter Rickwood. My thanks also extend to those who helped with the datasets: with the <b>Sign</b> <b>Language</b> dataset, my thanks go particularly to Todd Wright, but also to those who helped with the undergraduate work: Adam Schembri and Adam Young. Also Philip de Chazal and Branko", "mykey":667},
 {"datasetID":115, "supportID":"4BADC3F0AD169ED9EC7D073375E9B168FA9F6C8F", "rexaID":"660eddc9c8a809e94ad8d5837952f89d8997ab92", "author":"Mohammed Waleed Kadous and Claude Sammut", "title":"The University of New South Wales School of Computer Science and Engineering Temporal Classification: Extending the Classification Paradigm to Multivariate Time Series", "venue":"", "year":"", "window":"Mark Peters for sharing their experiences and for proofreading early drafts; and also to two other proofreaders: Peter Gammie and Peter Rickwood. My thanks also extend to those who helped with the datasets: with the <b>Sign</b> <b>Language</b> dataset, my thanks go particularly to Todd Wright, but also to those who helped with the undergraduate work: Adam Schembri and Adam Young. Also Philip de Chazal and Branko", "mykey":668},
 {"datasetID":25, "supportID":"4BADC3F0AD169ED9EC7D073375E9B168FA9F6C8F", "rexaID":"660eddc9c8a809e94ad8d5837952f89d8997ab92", "author":"Mohammed Waleed Kadous and Claude Sammut", "title":"The University of New South Wales School of Computer Science and Engineering Temporal Classification: Extending the Classification Paradigm to Multivariate Time Series", "venue":"", "year":"", "window":"Saito [Sai94], and further worked on by Manganaris [Man97]. The task is to classify a stream as one of three classes, cylinder (c), bell (b) or funnel (f ). Samples are generated as follows: 2 These datasets are: arrythmia, audiology, <b>bach</b> <b>chorales</b>  echocardiogram, isolet, mobile robots, waveform. 6. Experimental Evaluation 161 c(t) = (6 + #) \u00b7 # [a,b] (t) + #(t) b(t) = (6 + #) \u00b7 # [a,b] (t) \u00b7 (t -", "mykey":669},
 {"datasetID":92, "supportID":"4BADC3F0AD169ED9EC7D073375E9B168FA9F6C8F", "rexaID":"660eddc9c8a809e94ad8d5837952f89d8997ab92", "author":"Mohammed Waleed Kadous and Claude Sammut", "title":"The University of New South Wales School of Computer Science and Engineering Temporal Classification: Extending the Classification Paradigm to Multivariate Time Series", "venue":"", "year":"", "window":"analysis technique (i.e. a technique that allows the system to cope with the problem that patterns occur at different temporal scales) and applies them to <b>space</b> <b>shuttle</b> data as well as an artificial dataset. Mannila et al [MTV95] have also been looking at temporal classification problems, in particular applying it to network traffic analysis. In their model, streams are a sequence of time-labelled", "mykey":670},
 {"datasetID":148, "supportID":"4BADC3F0AD169ED9EC7D073375E9B168FA9F6C8F", "rexaID":"660eddc9c8a809e94ad8d5837952f89d8997ab92", "author":"Mohammed Waleed Kadous and Claude Sammut", "title":"The University of New South Wales School of Computer Science and Engineering Temporal Classification: Extending the Classification Paradigm to Multivariate Time Series", "venue":"", "year":"", "window":"analysis technique (i.e. a technique that allows the system to cope with the problem that patterns occur at different temporal scales) and applies them to space <b>shuttle</b> data as well as an artificial dataset. Mannila et al [MTV95] have also been looking at temporal classification problems, in particular applying it to network traffic analysis. In their model, streams are a sequence of time-labelled", "mykey":671},
 {"datasetID":54, "supportID":"4C2F166E8384723CD8E5E872FFCF52859F95F6BB", "rexaID":"480c0ae46486d7a7881f88224c5216ea0b7da839", "author":"David Littau and Daniel Boley", "title":"Using Low-Memory Representations to Cluster Very Large Data Sets", "venue":"SDM", "year":"2003", "window":"to 0 centroids are a PDDP clustering of a low-memory representation using the centroid in C closest to a data item to approximate that data item. For the data examined, the results are better if the dataset <b>isolet</b> k1 reuters forest m 7997 2340 9494 581012 n 617 21839 19968 54 categories 26 20 66 7 # dense 0.68% 0.20% dense k s 5 5 5 5 k c 150 50 100 500 k z 5 5 5 1 k f 150 50 100 500 Table 3: Datasets", "mykey":672},
 {"datasetID":137, "supportID":"4C2F166E8384723CD8E5E872FFCF52859F95F6BB", "rexaID":"480c0ae46486d7a7881f88224c5216ea0b7da839", "author":"David Littau and Daniel Boley", "title":"Using Low-Memory Representations to Cluster Very Large Data Sets", "venue":"SDM", "year":"2003", "window":"which are already sparse to begin with, such as the document datasets k1 and <b>reuters</b>  However, a larger document data set would probably see a higher percentage of memory savings, since the number of possible attributes in document data sets is limited by the number", "mykey":673},
 {"datasetID":109, "supportID":"4CBDD239A1E3ABF3515B43DB3DE1AF919F5B824F", "rexaID":"b08db10a1ffecdd5c69a23dc9ba5c0699c2b7db2", "author":"Abdelhamid Bouchachia", "title":"RBF Networks for Learning from Partially Labeled Data", "venue":"Department of Informatics, University of Klagenfurt", "year":"", "window":"data points. Once selected, these data points and the given labeled data points are used to train the neural network. 13 4. Numerical Evaluation To evaluate the approach presented here, two data sets are used: the cancer and the <b>wine</b> data set (Hettich et al., 1998). The cancer data consists of 683 instances with 9 features, while the wine data set consists of 178 instances with 13 features.", "mykey":674},
 {"datasetID":151, "supportID":"4D259A4113D23F61585386EB7C46A16989AE0BD4", "rexaID":"df4390b4164d70e186a080009bf9b23fb367478c", "author":"Michail Vlachos and Carlotta Domeniconi and Dimitrios Gunopulos and George Kollios and Nick Koudas", "title":"Non-linear dimensionality reduction techniques for classification and visualization", "venue":"KDD", "year":"2002", "window":"The average error rates for the smaller data sets (i.e., Iris, <b>Sonar</b>  Glass, Liver, and Lung) were based on leave-oneout cross-validation, and the error rates for Image and Vowel were based on ten two-fold-cross-validation, as summarized in Table", "mykey":675},
 {"datasetID":42, "supportID":"4D259A4113D23F61585386EB7C46A16989AE0BD4", "rexaID":"df4390b4164d70e186a080009bf9b23fb367478c", "author":"Michail Vlachos and Carlotta Domeniconi and Dimitrios Gunopulos and George Kollios and Nick Koudas", "title":"Non-linear dimensionality reduction techniques for classification and visualization", "venue":"KDD", "year":"2002", "window":"The average error rates for the smaller data sets (i.e., Iris, Sonar, <b>Glass</b>  Liver, and Lung) were based on leave-oneout cross-validation, and the error rates for Image and Vowel were based on ten two-fold-cross-validation, as summarized in Table", "mykey":676},
 {"datasetID":53, "supportID":"4D259A4113D23F61585386EB7C46A16989AE0BD4", "rexaID":"df4390b4164d70e186a080009bf9b23fb367478c", "author":"Michail Vlachos and Carlotta Domeniconi and Dimitrios Gunopulos and George Kollios and Nick Koudas", "title":"Non-linear dimensionality reduction techniques for classification and visualization", "venue":"KDD", "year":"2002", "window":"used in our experiments Dataset ] data ] dims ] classes experiment <b>Iris</b> 100 4 2 leave 1 out c-v Sonar 208 60 2 leave 1 out c-v Glass 214 9 6 leave 1 out c-v Liver 345 6 2 leave 1 out c-v Lung 32 56 3 leave 1 out c-v Image 640 16", "mykey":677},
 {"datasetID":60, "supportID":"4D259A4113D23F61585386EB7C46A16989AE0BD4", "rexaID":"df4390b4164d70e186a080009bf9b23fb367478c", "author":"Michail Vlachos and Carlotta Domeniconi and Dimitrios Gunopulos and George Kollios and Nick Koudas", "title":"Non-linear dimensionality reduction techniques for classification and visualization", "venue":"KDD", "year":"2002", "window":"The average error rates for the smaller data sets (i.e., Iris, Sonar, Glass, <b>Liver</b>  and Lung) were based on leave-oneout cross-validation, and the error rates for Image and Vowel were based on ten two-fold-cross-validation, as summarized in Table", "mykey":678},
 {"datasetID":58, "supportID":"4D3D8E45CF085ED273D12BD421C1E9B19E9C8D30", "rexaID":"efbc8c38877792e6ade1634043235d238ac9ec8d", "author":"J. Kent Martin and Daniel S. Hirschberg", "title":"Small Sample Statistics for Classification Error Rates I: Error Rate Measurements", "venue":"Department of Information and Computer Science University of California, Irvine", "year":"1996", "window":"attributes. To explore the behavior of resampling estimators for non-numeric attributes in other inference environments, a series of experiments was conducted using the contact <b>lens</b> prescription data set [42]. In this artificial problem, patients are classified into 3 categories (hard, soft, none) based on the values of 4 attributes (1 tertiary and 3 binary). The 24 instances given cover all cases", "mykey":679},
 {"datasetID":2, "supportID":"4D6A96B3C5E714A2BD3F35087D918D0064AD5AB5", "rexaID":"4dbb0e14d9556fd1099e129462d0bedcf35bd82b", "author":"Nitesh V. Chawla and Kevin W. Bowyer and Lawrence O. Hall and W. Philip Kegelmeyer", "title":"SMOTE: Synthetic Minority Over-sampling Technique", "venue":"J. Artif. Intell. Res. (JAIR, 16", "year":"2002", "window":"is to distinguish between nasal (class 0) and oral sounds (class 1). There are 5 features. The class distribution is 3,818 samples in class 0 and 1,586 samples in class 1. 3. The <b>Adult</b> dataset (Blake & Merz, 1998) has 48,842 samples with 11,687 samples belonging to the minority class. This dataset has 6 continuous features and 8 nominal features. SMOTE and SMOTE-NC (see Section 6.1)", "mykey":680},
 {"datasetID":110, "supportID":"4D6A96B3C5E714A2BD3F35087D918D0064AD5AB5", "rexaID":"4dbb0e14d9556fd1099e129462d0bedcf35bd82b", "author":"Nitesh V. Chawla and Kevin W. Bowyer and Lawrence O. Hall and W. Philip Kegelmeyer", "title":"SMOTE: Synthetic Minority Over-sampling Technique", "venue":"J. Artif. Intell. Res. (JAIR, 16", "year":"2002", "window":"solvent) was measured. The activity classes are either active --- at least one single <b>yeast</b> strain was inhibited more than 70%, or inactive --- no yeast strain was inhibited more than 70%. The dataset has 53,220 samples with 6,351 samples of active compounds. 5. The Satimage dataset (Blake & Merz, 1998) has 6 classes originally. We chose the smallest class as the minority class and collapsed the", "mykey":681},
 {"datasetID":39, "supportID":"4D89623FF93B50F186907F7CE541E5FF9395860B", "rexaID":"f78dcf21b618acf09345e59967f008e96094c072", "author":"Xiaoyong Chai and Li Deng and Qiang Yang and Charles X. Ling", "title":"Test-Cost Sensitive Naive Bayes Classification", "venue":"ICDM", "year":"2004", "window":"were discretized using minimal entropy method as in [8]. Name of No. of Name No. of datasets attributes of datasets attributes <b>Ecoli</b> 6 Breast 9 Heart 8 Thyroid 24 Australia 15 Cars 6 Voting 16 Mushroom 22 Table 2. Datasets used in the experiments We ran a 3-fold cross validation on these", "mykey":682},
 {"datasetID":45, "supportID":"4D89623FF93B50F186907F7CE541E5FF9395860B", "rexaID":"f78dcf21b618acf09345e59967f008e96094c072", "author":"Xiaoyong Chai and Li Deng and Qiang Yang and Charles X. Ling", "title":"Test-Cost Sensitive Naive Bayes Classification", "venue":"ICDM", "year":"2004", "window":"attributes of datasets attributes Ecoli 6 Breast 9 <b>Heart</b> 8 Thyroid 24 Australia 15 Cars 6 Voting 16 Mushroom 22 Table 2. Datasets used in the experiments We ran a 3-fold cross validation on these data sets. In the", "mykey":683},
 {"datasetID":73, "supportID":"4D89623FF93B50F186907F7CE541E5FF9395860B", "rexaID":"f78dcf21b618acf09345e59967f008e96094c072", "author":"Xiaoyong Chai and Li Deng and Qiang Yang and Charles X. Ling", "title":"Test-Cost Sensitive Naive Bayes Classification", "venue":"ICDM", "year":"2004", "window":"attributes Ecoli 6 Breast 9 Heart 8 Thyroid 24 Australia 15 Cars 6 Voting 16 <b>Mushroom</b> 22 Table 2. Datasets used in the experiments We ran a 3-fold cross validation on these data sets. In the experiments, no missing value is assigned in the training examples and for the testing examples, a certain", "mykey":684},
 {"datasetID":102, "supportID":"4D89623FF93B50F186907F7CE541E5FF9395860B", "rexaID":"f78dcf21b618acf09345e59967f008e96094c072", "author":"Xiaoyong Chai and Li Deng and Qiang Yang and Charles X. Ling", "title":"Test-Cost Sensitive Naive Bayes Classification", "venue":"ICDM", "year":"2004", "window":"attributes of datasets attributes Ecoli 6 Breast 9 Heart 8 <b>Thyroid</b> 24 Australia 15 Cars 6 Voting 16 Mushroom 22 Table 2. Datasets used in the experiments We ran a 3-fold cross validation on these data sets. In the", "mykey":685},
 {"datasetID":151, "supportID":"4DB568F72E4E2CA69BF9F228F5D9B537E75BEFD6", "rexaID":"412e0644abd8fccf9fb195db3ed361c2ffd3695c", "author":"Xavier Llor and David E. Goldberg and Ivan Traus and Ester Bernad i Mansilla", "title":"Accuracy, Parsimony, and Generality in Evolutionary Learning Systems via Multiobjective Selection", "venue":"IWLCS", "year":"2002", "window":"Bupa Liver Disorders (bpa), Wisconsin Breast Cancer (bre), Glass (gls), Ionosphere (ion), Iris (irs), Primary Tumor (prt), and <b>Sonar</b> (son). These data sets contain categorical and numeric attributes, as well as binary and n-ary classification tasks. We also run several evolutionary and non-evolutionary classifier schemes on the previous data sets. The", "mykey":686},
 {"datasetID":57, "supportID":"4DB568F72E4E2CA69BF9F228F5D9B537E75BEFD6", "rexaID":"412e0644abd8fccf9fb195db3ed361c2ffd3695c", "author":"Xavier Llor and David E. Goldberg and Ivan Traus and Ester Bernad i Mansilla", "title":"Accuracy, Parsimony, and Generality in Evolutionary Learning Systems via Multiobjective Selection", "venue":"IWLCS", "year":"2002", "window":"(mux and led). Initially, we used a version of the <b>led data</b> set free of noise, leaving noise considerations for the next subsection. In order to identify the optimal Pareto front, we analyze first the optimal solutions that should be obtained in each problem.", "mykey":687},
 {"datasetID":60, "supportID":"4DB568F72E4E2CA69BF9F228F5D9B537E75BEFD6", "rexaID":"412e0644abd8fccf9fb195db3ed361c2ffd3695c", "author":"Xavier Llor and David E. Goldberg and Ivan Traus and Ester Bernad i Mansilla", "title":"Accuracy, Parsimony, and Generality in Evolutionary Learning Systems via Multiobjective Selection", "venue":"IWLCS", "year":"2002", "window":"were obtained from the UCI repository (Merz & Murphy, 1998). We chose seven data sets: Bupa <b>Liver</b> Disorders (bpa), Wisconsin Breast Cancer (bre), Glass (gls), Ionosphere (ion), Iris (irs), Primary Tumor (prt), and Sonar (son). These data sets contain categorical and numeric", "mykey":688},
 {"datasetID":83, "supportID":"4DB568F72E4E2CA69BF9F228F5D9B537E75BEFD6", "rexaID":"412e0644abd8fccf9fb195db3ed361c2ffd3695c", "author":"Xavier Llor and David E. Goldberg and Ivan Traus and Ester Bernad i Mansilla", "title":"Accuracy, Parsimony, and Generality in Evolutionary Learning Systems via Multiobjective Selection", "venue":"IWLCS", "year":"2002", "window":"Bupa Liver Disorders (bpa), Wisconsin Breast Cancer (bre), Glass (gls), Ionosphere (ion), Iris (irs), <b>Primary</b> <b>Tumor</b> (prt), and Sonar (son). These data sets contain categorical and numeric attributes, as well as binary and n-ary classification tasks. We also run several evolutionary and non-evolutionary classifier schemes on the previous data sets. The", "mykey":689},
 {"datasetID":1, "supportID":"4DE199CE503C4775916F40852080F315E53F12CE", "rexaID":"b2d8d3d5275f9de64f7d1d58ed346fc673f94065", "author":"Edward Snelson and Carl Edward Rasmussen and Zoubin Ghahramani", "title":"Draft version; accepted for NIPS*03 Warped Gaussian Processes", "venue":"Gatsby Computational Neuroscience Unit University College London", "year":"", "window":"predict the the age of <b>abalone</b> from various physical inputs [9]. ailerons is a simulated control problem, with the aim to predict the control action on the ailerons of an F16 aircraft [10, 11]. For datasets creep and abalone, which consist of positive observations only, standard practice may be to model the ##### of the data with a GP. So for these datasets we have compared three models: a GP directly", "mykey":690},
 {"datasetID":45, "supportID":"4E24FF9151CE162293B6C54AA6B6CE7754CD035D", "rexaID":"a11c91c5cf02794b7d1d4bdc0222b3c11dce9cac", "author":"David Page and Soumya Ray", "title":"Skewing: An Efficient Alternative to Lookahead for Decision Tree Induction", "venue":"IJCAI", "year":"2003", "window":"Skewing ID3, No Skewing Figure 8: Five-Variable Hard Targets 50 60 70 80 90 100 200 400 600 800 1000 Accuracy (%) Sample Size ID3 with Skewing ID3, No Skewing Figure 9: Six-Variable Hard Targets Data Set Standard ID3 ID3 with Skewing <b>Heart</b> 71.9 74.5 Voting 94.0 94.2 Voting-2 87.4 88.6 Contra 60.4 61.5 Monks-1 92.6 100.0 Monks-2 86.5 89.3 Monks-3 89.8 91.7 Table 4: Accuracies of ID3 and ID3 with", "mykey":691},
 {"datasetID":107, "supportID":"4EC7581789C208DF0A928FA1A781C1F2D3650FEF", "rexaID":"aa89144bb32b6cbc2ad3000adba9cefe83bf54d6", "author":"Zhi-Hua Zhou and W-D Wei and Gang Li and Honghua Dai", "title":"On the Size of Training Set and the Benefit from Ensemble", "venue":"PAKDD", "year":"2004", "window":"3,772 22 7 2 kr-vs-kp 3,196 36 0 2 led7 2,000 7 0 10 led24 2,000 24 0 10 sat 6,435 0 36 6 segment 2,310 0 19 7 sick 3,772 22 7 2 sick-euthyroid 3,156 22 7 2 <b>waveform</b> 5,000 0 21 3 Each original data set is partitioned into ten subsets with similar distributions. At the first time, only one subset is used; at the second time, two subsets are used; and so on. The earlier generated data sets are", "mykey":692},
 {"datasetID":108, "supportID":"4EC7581789C208DF0A928FA1A781C1F2D3650FEF", "rexaID":"aa89144bb32b6cbc2ad3000adba9cefe83bf54d6", "author":"Zhi-Hua Zhou and W-D Wei and Gang Li and Honghua Dai", "title":"On the Size of Training Set and the Benefit from Ensemble", "venue":"PAKDD", "year":"2004", "window":"3,772 22 7 2 kr-vs-kp 3,196 36 0 2 led7 2,000 7 0 10 led24 2,000 24 0 10 sat 6,435 0 36 6 segment 2,310 0 19 7 sick 3,772 22 7 2 sick-euthyroid 3,156 22 7 2 <b>waveform</b> 5,000 0 21 3 Each original data set is partitioned into ten subsets with similar distributions. At the first time, only one subset is used; at the second time, two subsets are used; and so on. The earlier generated data sets are", "mykey":693},
 {"datasetID":74, "supportID":"4F24F1B2ED03DB59BE47F0474D7B695108BC8E9F", "rexaID":"7025b49e49b58acbb1881ee289e9d719a528d505", "author":"Giorgio Valentini and Thomas G. Dietterich", "title":"Low Bias Bagged Support Vector Machines", "venue":"ICML", "year":"2003", "window":"Spam Linear 0.1356 0.1340 0.1627 0-4-1 5-0-0 5-0-0 Polyn. 0.1309 0.1338 0.1388 1-4-0 2-3-0 2-2-1 Gauss. 0.1239 0.1349 0.1407 3-2-0 3-2-0 2-3-0 Data set <b>Musk</b> Linear 0.1244 0.1247 0.1415 0-5-0 4-1-0 4-1-0 Polyn. 0.1039 0.1193 0.1192 4-1-0 4-0-1 2-2-1 Gauss. 0.0872 0.0972 0.0920 4-1-0 2-2-1 1-0-4 6.1. Experimental setup We employed two synthetic data", "mykey":694},
 {"datasetID":75, "supportID":"4F24F1B2ED03DB59BE47F0474D7B695108BC8E9F", "rexaID":"7025b49e49b58acbb1881ee289e9d719a528d505", "author":"Giorgio Valentini and Thomas G. Dietterich", "title":"Low Bias Bagged Support Vector Machines", "venue":"ICML", "year":"2003", "window":"Spam Linear 0.1356 0.1340 0.1627 0-4-1 5-0-0 5-0-0 Polyn. 0.1309 0.1338 0.1388 1-4-0 2-3-0 2-2-1 Gauss. 0.1239 0.1349 0.1407 3-2-0 3-2-0 2-3-0 Data set <b>Musk</b> Linear 0.1244 0.1247 0.1415 0-5-0 4-1-0 4-1-0 Polyn. 0.1039 0.1193 0.1192 4-1-0 4-0-1 2-2-1 Gauss. 0.0872 0.0972 0.0920 4-1-0 2-2-1 1-0-4 6.1. Experimental setup We employed two synthetic data", "mykey":695},
 {"datasetID":146, "supportID":"4F24F1B2ED03DB59BE47F0474D7B695108BC8E9F", "rexaID":"7025b49e49b58acbb1881ee289e9d719a528d505", "author":"Giorgio Valentini and Thomas G. Dietterich", "title":"Low Bias Bagged Support Vector Machines", "venue":"ICML", "year":"2003", "window":"Waveform Linear 0.0811 0.0821 0.0955 2-3-0 5-0-0 5-0-0 Polyn. 0.0625 0.0677 0.0698 2-3-0 2-3-0 3-2-0 Gauss. 0.0574 0.0653 0.0666 4-1-0 4-1-0 2-3-0 Data set Grey <b>Landsat</b> Linear 0.0508 0.0510 0.0601 0-5-0 3-2-0 3-2-0 Polyn. 0.0432 0.0493 0.0535 1-4-0 2-3-0 1-4-0 Gauss. 0.0475 0.0486 0.0483 1-3-1 1-3-1 0-5-0 Data set Letter-Two Linear 0.0832 0.0864 0.1011", "mykey":696},
 {"datasetID":107, "supportID":"4F24F1B2ED03DB59BE47F0474D7B695108BC8E9F", "rexaID":"7025b49e49b58acbb1881ee289e9d719a528d505", "author":"Giorgio Valentini and Thomas G. Dietterich", "title":"Low Bias Bagged Support Vector Machines", "venue":"ICML", "year":"2003", "window":"P2 Polyn. 0.1687 0.1863 0.1892 4-1-0 4-1-0 1-4-0 Gauss. 0.1429 0.1534 0.1605 4-1-0 5-0-0 3-2-0 Data set <b>Waveform</b> Linear 0.0811 0.0821 0.0955 2-3-0 5-0-0 5-0-0 Polyn. 0.0625 0.0677 0.0698 2-3-0 2-3-0 3-2-0 Gauss. 0.0574 0.0653 0.0666 4-1-0 4-1-0 2-3-0 Data set Grey-Landsat Linear 0.0508 0.0510 0.0601", "mykey":697},
 {"datasetID":108, "supportID":"4F24F1B2ED03DB59BE47F0474D7B695108BC8E9F", "rexaID":"7025b49e49b58acbb1881ee289e9d719a528d505", "author":"Giorgio Valentini and Thomas G. Dietterich", "title":"Low Bias Bagged Support Vector Machines", "venue":"ICML", "year":"2003", "window":"P2 Polyn. 0.1687 0.1863 0.1892 4-1-0 4-1-0 1-4-0 Gauss. 0.1429 0.1534 0.1605 4-1-0 5-0-0 3-2-0 Data set <b>Waveform</b> Linear 0.0811 0.0821 0.0955 2-3-0 5-0-0 5-0-0 Polyn. 0.0625 0.0677 0.0698 2-3-0 2-3-0 3-2-0 Gauss. 0.0574 0.0653 0.0666 4-1-0 4-1-0 2-3-0 Data set Grey-Landsat Linear 0.0508 0.0510 0.0601", "mykey":698},
 {"datasetID":67, "supportID":"4F7B8693AA194F6404D868962DF65BC4997C9029", "rexaID":"30ef69908e0a0792007e4a8b95c7c422b7e0ff99", "author":"Zoubin Ghahramani and Hyun-Chul Kim", "title":"Bayesian Classifier Combination", "venue":"Gatsby Computational Neuroscience Unit University College London", "year":"2003", "window":"and using different component classifiers. We used Satellite and <b>DNA</b> data sets from the Statlog project([8]) and the UCI digit data set ([1]) 3 . Our goal was not to obtain the best classifier performance---for this we would have paid very careful attention to the component", "mykey":699},
 {"datasetID":146, "supportID":"4F7B8693AA194F6404D868962DF65BC4997C9029", "rexaID":"30ef69908e0a0792007e4a8b95c7c422b7e0ff99", "author":"Zoubin Ghahramani and Hyun-Chul Kim", "title":"Bayesian Classifier Combination", "venue":"Gatsby Computational Neuroscience Unit University College London", "year":"2003", "window":"and using different component classifiers. We used <b>Satellite</b> and DNA data sets from the Statlog project([8]) and the UCI digit data set ([1]) 3 . Our goal was not to obtain the best classifier performance---for this we would have paid very careful attention to the component", "mykey":700},
 {"datasetID":98, "supportID":"4F7B8693AA194F6404D868962DF65BC4997C9029", "rexaID":"30ef69908e0a0792007e4a8b95c7c422b7e0ff99", "author":"Zoubin Ghahramani and Hyun-Chul Kim", "title":"Bayesian Classifier Combination", "venue":"Gatsby Computational Neuroscience Unit University College London", "year":"2003", "window":"and using different component classifiers. We used Satellite and DNA data sets from the <b>Statlog</b> project([8]) and the UCI digit data set ([1]) 3 . Our goal was not to obtain the best classifier performance---for this we would have paid very careful attention to the component", "mykey":701},
 {"datasetID":50, "supportID":"4FDC539AAFC82DD12574F4E0B1A763129B750A51", "rexaID":"426e712ef75473927e172cd8d75299a5c9037bd6", "author":"Xiaoli Z. Fern and Carla Brodley", "title":"Cluster Ensembles for High Dimensional Clustering: An Empirical Study", "venue":"Journal of Machine Learning Research n, a", "year":"2004", "window":"(6 letters only) UCI ML archive mfeat Handwritten digits represented by Fourier coefficients (Blake and Merz, 1998) satimage StatLog Satellite image data set (training set) segmentation <b>Image segmentation</b> data In contrast, HBGF allows the similarity of instances and the similarity of clusters to be considered simultaneously in producing the final", "mykey":702},
 {"datasetID":59, "supportID":"4FDC539AAFC82DD12574F4E0B1A763129B750A51", "rexaID":"426e712ef75473927e172cd8d75299a5c9037bd6", "author":"Xiaoli Z. Fern and Carla Brodley", "title":"Cluster Ensembles for High Dimensional Clustering: An Empirical Study", "venue":"Journal of Machine Learning Research n, a", "year":"2004", "window":"High resolution computed tomography lung image data Dy et al. (1999) chart Synthetically generated control chart time series UCI KDD archive (Hettich and Bay, 1999) isolet6 Spoken <b>letter recognition</b> data set (6 letters only) UCI ML archive mfeat Handwritten digits represented by Fourier coefficients (Blake and Merz, 1998) satimage StatLog Satellite image data set (training set) segmentation Image", "mykey":703},
 {"datasetID":72, "supportID":"4FDC539AAFC82DD12574F4E0B1A763129B750A51", "rexaID":"426e712ef75473927e172cd8d75299a5c9037bd6", "author":"Xiaoli Z. Fern and Carla Brodley", "title":"Cluster Ensembles for High Dimensional Clustering: An Empirical Study", "venue":"Journal of Machine Learning Research n, a", "year":"2004", "window":"High resolution computed tomography lung image data Dy et al. (1999) chart Synthetically generated control chart time series UCI KDD archive (Hettich and Bay, 1999) isolet6 Spoken letter recognition data set (6 letters only) UCI ML archive <b>mfeat</b> Handwritten digits represented by Fourier coefficients (Blake and Merz, 1998) satimage StatLog Satellite image data set (training set) segmentation Image", "mykey":704},
 {"datasetID":147, "supportID":"4FDC539AAFC82DD12574F4E0B1A763129B750A51", "rexaID":"426e712ef75473927e172cd8d75299a5c9037bd6", "author":"Xiaoli Z. Fern and Carla Brodley", "title":"Cluster Ensembles for High Dimensional Clustering: An Empirical Study", "venue":"Journal of Machine Learning Research n, a", "year":"2004", "window":"(6 letters only) UCI ML archive mfeat Handwritten digits represented by Fourier coefficients (Blake and Merz, 1998) satimage StatLog Satellite image data set (training set) segmentation <b>Image segmentation</b> data In contrast, HBGF allows the similarity of instances and the similarity of clusters to be considered simultaneously in producing the final", "mykey":705},
 {"datasetID":146, "supportID":"4FDC539AAFC82DD12574F4E0B1A763129B750A51", "rexaID":"426e712ef75473927e172cd8d75299a5c9037bd6", "author":"Xiaoli Z. Fern and Carla Brodley", "title":"Cluster Ensembles for High Dimensional Clustering: An Empirical Study", "venue":"Journal of Machine Learning Research n, a", "year":"2004", "window":"(6 letters only) UCI ML archive mfeat Handwritten digits represented by Fourier coefficients (Blake and Merz, 1998) satimage StatLog <b>Satellite</b> image data set (training set) segmentation Image segmentation data In contrast, HBGF allows the similarity of instances and the similarity of clusters to be considered simultaneously in producing the final", "mykey":706},
 {"datasetID":98, "supportID":"4FDC539AAFC82DD12574F4E0B1A763129B750A51", "rexaID":"426e712ef75473927e172cd8d75299a5c9037bd6", "author":"Xiaoli Z. Fern and Carla Brodley", "title":"Cluster Ensembles for High Dimensional Clustering: An Empirical Study", "venue":"Journal of Machine Learning Research n, a", "year":"2004", "window":"(6 letters only) UCI ML archive mfeat Handwritten digits represented by Fourier coefficients (Blake and Merz, 1998) satimage <b>StatLog</b> Satellite image data set (training set) segmentation Image segmentation data In contrast, HBGF allows the similarity of instances and the similarity of clusters to be considered simultaneously in producing the final", "mykey":707},
 {"datasetID":111, "supportID":"507C9B4917638D77B0BAD5DF47EDFF92F2FF229A", "rexaID":"51abae4db521e2ea17ab25892535bba817de5edc", "author":"Mikko Koivisto and Kismat Sood", "title":"Exact Bayesian Structure Discovery in Bayesian Networks", "venue":"Journal of Machine Learning Research, 5", "year":"2004", "window":"contain discrete variables only and no values are missing. The <b>Zoo</b> data set is available from the UCI Machine Learning Repository (Blake and Merz, 1998, the data set contributed by Richard Forsyth). It contains 17 variables and 101 records. The Alarm data set built by", "mykey":708},
 {"datasetID":13, "supportID":"5084909AA758ACF14841A1DDAF0DFE88CDDCEA6A", "rexaID":"3f9e71cd1f71f5f65b712d5cc5c29045d0193711", "author":"Ron Kohavi and George H. John and Richard Long and David Manley and Karl Pfleger", "title":"MLC++: A Machine Learning Library in C", "venue":"ICTAI", "year":"1994", "window":"octag yes no yes no yes no yes no yes no yes no yes no yes no yes no yes no yes no yes no red yellow green blue red yellow green blue red yellow green blue sword <b>balloon</b> flag Figure 5: The Monk3 dataset (top), the concept predicted by ID3 (center), and the errors in black (bottom). 14 0 100 200 300 400 TS size Monk1 0.7 0.75 0.8 0.85 0.9 0.95 1 Accuracy HOODG ID3 Figure 6: Learning curves generated", "mykey":709},
 {"datasetID":105, "supportID":"5084909AA758ACF14841A1DDAF0DFE88CDDCEA6A", "rexaID":"3f9e71cd1f71f5f65b712d5cc5c29045d0193711", "author":"Ron Kohavi and George H. John and Richard Long and David Manley and Karl Pfleger", "title":"MLC++: A Machine Learning Library in C", "venue":"ICTAI", "year":"1994", "window":"n y u n y u n y u n y u n y u n y u n y u n y u n y u n y u n y u n y u n y u n y u n y u n y u n y u n y u n y u n y u n y u n y u n y u n y u n y u n y u n y u n y u n y u n y u Figure 4: The <b>vote dataset</b> (top), the concept induced by ID3 (center), and the concept induced by HOODG (bottom). 13 Body Shape Head Shape Is Smiling yes no yes no yes no yes no yes no yes no yes no yes no yes no round squar", "mykey":710},
 {"datasetID":40, "supportID":"5084909AA758ACF14841A1DDAF0DFE88CDDCEA6A", "rexaID":"3f9e71cd1f71f5f65b712d5cc5c29045d0193711", "author":"Ron Kohavi and George H. John and Richard Long and David Manley and Karl Pfleger", "title":"MLC++: A Machine Learning Library in C", "venue":"ICTAI", "year":"1994", "window":"octag yes no yes no yes no yes no yes no yes no yes no yes no yes no yes no yes no yes no red yellow green blue red yellow green blue red yellow green blue sword balloon <b>flag</b> Figure 5: The Monk3 dataset (top), the concept predicted by ID3 (center), and the errors in black (bottom). 14 0 100 200 300 400 TS size Monk1 0.7 0.75 0.8 0.85 0.9 0.95 1 Accuracy HOODG ID3 Figure 6: Learning curves generated", "mykey":711},
 {"datasetID":98, "supportID":"5084909AA758ACF14841A1DDAF0DFE88CDDCEA6A", "rexaID":"3f9e71cd1f71f5f65b712d5cc5c29045d0193711", "author":"Ron Kohavi and George H. John and Richard Long and David Manley and Karl Pfleger", "title":"MLC++: A Machine Learning Library in C", "venue":"ICTAI", "year":"1994", "window":"rules in Consultant. 3.3 Large-Scale Comparisons: <b>StatLog</b> and MLToolbox StatLog [TMS94] is an ESPRIT project studying the behavior of over twenty algorithms (mostly in the MLToolbox), on over twenty datasets. The StatLog book gives a nice introduction to a wide variety of statistical and machine learning algorithms and an interesting discussion of the results. MLC ++ can be used to generate such", "mykey":712},
 {"datasetID":45, "supportID":"508571DB5D2F77C17D2829878BB1DC645010DDA8", "rexaID":"8cf017f4c8d633c1694c2ea3fa2a165e0d8d35f1", "author":"Floriana Esposito and Donato Malerba and Giovanni Semeraro", "title":"A Comparative Analysis of Methods for Pruning Decision Trees", "venue":"IEEE Trans. Pattern Anal. Mach. Intell, 19", "year":"1997", "window":"available in the UCI Machine Learning Repository 2 [21], and some of them have even been used to compare different pruning methods [25], [20], [3]. The database <b>heart</b> is actually the union of four data sets on heart diseases, with the same number of attributes but collected in four distinct places (Hungary, Switzerland, Cleveland, and Long Beach). 3 Of the 76 original attributes, only 14 have been", "mykey":713},
 {"datasetID":46, "supportID":"508571DB5D2F77C17D2829878BB1DC645010DDA8", "rexaID":"8cf017f4c8d633c1694c2ea3fa2a165e0d8d35f1", "author":"Floriana Esposito and Donato Malerba and Giovanni Semeraro", "title":"A Comparative Analysis of Methods for Pruning Decision Trees", "venue":"IEEE Trans. Pattern Anal. Mach. Intell, 19", "year":"1997", "window":"This means that methods requiring a pruning set labor under a disadvantage. Nonetheless, the misclassification rate of the OPTT is not always lower than the error rate of the OPGT. Hence, in some data sets, like <b>Hepatitis</b>  Hungary, and Switzerland above, grown trees can be better starting points for pruning processes than trained trees. Finally, the standard error reported in Table 3 confirms the", "mykey":714},
 {"datasetID":1, "supportID":"511C102F54FF40CC57450EF0AA424A468447F7B0", "rexaID":"1b7ec8555d756f0b0f3899a066abb09329234c5b", "author":"Christopher K I Williams and Carl Edward Rasmussen and Anton Schwaighofer and Volker Tresp", "title":"Observations on the Nystrom Method for Gaussian Process Prediction", "venue":"Division of Informatics Gatsby Computational Neuroscience Unit University of Edinburgh University College London", "year":"2002", "window":"the drop at index 100 for the Nystrom case, due to the rank-100 approximation. The horizontial line in the dashed plot is at log e # 2 # . The Nystrom method was originally tested on the UCI <b>abalone</b> data set using # 2 # = 0:05. Analysis shows that for the kernel parameters used, 112 eigenvalues in K were larger than # 2 # . This is in good agreement with the experimental results that values of m of 250", "mykey":715},
 {"datasetID":48, "supportID":"511C102F54FF40CC57450EF0AA424A468447F7B0", "rexaID":"1b7ec8555d756f0b0f3899a066abb09329234c5b", "author":"Christopher K I Williams and Carl Edward Rasmussen and Anton Schwaighofer and Volker Tresp", "title":"Observations on the Nystrom Method for Gaussian Process Prediction", "venue":"Division of Informatics Gatsby Computational Neuroscience Unit University of Edinburgh University College London", "year":"2002", "window":"0.0885 # 0.0073 0.1171 # 0.0222 0.0846 400 0.0871 # 0.0071 0.0843 # 0.0026 0.0922 # 0.0193 0.0845 Table 1: Comparison of the Nystrom, SR, just-m and m-eigenvectors methods on the Boston <b>housing</b> data set for values of m of 100; 200; 300; 400. For the first three methods ten replications were used, with random choice of the x points; each entry shows the mean and standard deviation of the 10 MSE", "mykey":716},
 {"datasetID":98, "supportID":"513C2BF352E529C935DCE22FE157142BF0478552", "rexaID":"5fb9f4bc7a3d1365076eca93c3ffae6e628a8926", "author":"Wl odzisl and aw Duch", "title":"Control and Cybernetics", "venue":"Department of Computer Methods, Nicholas Copernicus University", "year":"", "window":"and thus also belong to the SBM. All of these methods may be useful in control problems. A review of many approaches to classification and comparison of performance of 20 methods on 20 real world datasets has been done within the <b>StatLog</b> European Community project (Michie et al. 1994). More recently the accuracy of 24 neuralbased, pattern recognition and statistical classification systems has been", "mykey":717},
 {"datasetID":151, "supportID":"51EEBC1145475FA3893496C4B13949DDBC37701F", "rexaID":"df4390b4164d70e186a080009bf9b23fb367478c", "author":"Michail Vlachos and Carlotta Domeniconi and Dimitrios Gunopulos and George Kollios and Nick Koudas", "title":"Non-linear dimensionality reduction techniques for classification and visualization", "venue":"KDD", "year":"2002", "window":"The average error rates for the smaller data sets (i.e., Iris, <b>Sonar</b>  Glass, Liver, and Lung) were based on leave-oneout cross-validation, and the error rates for Image and Vowel were based on ten two-fold-cross-validation, as summarized in Table", "mykey":718},
 {"datasetID":42, "supportID":"51EEBC1145475FA3893496C4B13949DDBC37701F", "rexaID":"df4390b4164d70e186a080009bf9b23fb367478c", "author":"Michail Vlachos and Carlotta Domeniconi and Dimitrios Gunopulos and George Kollios and Nick Koudas", "title":"Non-linear dimensionality reduction techniques for classification and visualization", "venue":"KDD", "year":"2002", "window":"The average error rates for the smaller data sets (i.e., Iris, Sonar, <b>Glass</b>  Liver, and Lung) were based on leave-oneout cross-validation, and the error rates for Image and Vowel were based on ten two-fold-cross-validation, as summarized in Table", "mykey":719},
 {"datasetID":53, "supportID":"51EEBC1145475FA3893496C4B13949DDBC37701F", "rexaID":"df4390b4164d70e186a080009bf9b23fb367478c", "author":"Michail Vlachos and Carlotta Domeniconi and Dimitrios Gunopulos and George Kollios and Nick Koudas", "title":"Non-linear dimensionality reduction techniques for classification and visualization", "venue":"KDD", "year":"2002", "window":"used in our experiments Dataset ] data ] dims ] classes experiment <b>Iris</b> 100 4 2 leave 1 out c-v Sonar 208 60 2 leave 1 out c-v Glass 214 9 6 leave 1 out c-v Liver 345 6 2 leave 1 out c-v Lung 32 56 3 leave 1 out c-v Image 640 16", "mykey":720},
 {"datasetID":60, "supportID":"51EEBC1145475FA3893496C4B13949DDBC37701F", "rexaID":"df4390b4164d70e186a080009bf9b23fb367478c", "author":"Michail Vlachos and Carlotta Domeniconi and Dimitrios Gunopulos and George Kollios and Nick Koudas", "title":"Non-linear dimensionality reduction techniques for classification and visualization", "venue":"KDD", "year":"2002", "window":"The average error rates for the smaller data sets (i.e., Iris, Sonar, Glass, <b>Liver</b>  and Lung) were based on leave-oneout cross-validation, and the error rates for Image and Vowel were based on ten two-fold-cross-validation, as summarized in Table", "mykey":721},
 {"datasetID":120, "supportID":"520ED04C93149D14B3E77588906F3CB7EED2927A", "rexaID":"f006e50cdba5e00538e2b48452e1d0bab8ed02b9", "author":"Mukund Deshpande and George Karypis", "title":"Evaluation of Techniques for Classifying Biological Sequences", "venue":"PAKDD", "year":"2002", "window":"and contains 10,918 sequences of average length 361.6 symbols, it will be referred as M-EI. The <b>E</b> <b>Coli</b> Genome dataset contains sequences from the genome of prokaryote organism Escherichia coli (ecoli) [NTPP01]. To build this dataset the e-coli genome was first split into two sets of sequences, one containing", "mykey":722},
 {"datasetID":39, "supportID":"520ED04C93149D14B3E77588906F3CB7EED2927A", "rexaID":"f006e50cdba5e00538e2b48452e1d0bab8ed02b9", "author":"Mukund Deshpande and George Karypis", "title":"Evaluation of Techniques for Classifying Biological Sequences", "venue":"PAKDD", "year":"2002", "window":"contains sequences from the genome of prokaryote organism Escherichia <b>coli</b>  <b>ecoli</b>  [NTPP01]. To build this dataset the e-coli genome was first split into two sets of sequences, one containing sequences making up the coding region whereas other containing sequences making up the non-coding region. Next a fixed", "mykey":723},
 {"datasetID":67, "supportID":"520ED04C93149D14B3E77588906F3CB7EED2927A", "rexaID":"f006e50cdba5e00538e2b48452e1d0bab8ed02b9", "author":"Mukund Deshpande and George Karypis", "title":"Evaluation of Techniques for Classifying Biological Sequences", "venue":"PAKDD", "year":"2002", "window":"to the alphabet, which is assumed to be present at the beginning of each sequences [DEKG98]. Equation 3 contains these extra states. Figure 1 illustrates an example of sequence classification on a dataset of <b>DNA</b> sequences. Figure 1(a) shows the training sequences with their respective class labels, these sequences are first split into two parts for computing the TPM associated with each class label.", "mykey":724},
 {"datasetID":68, "supportID":"520ED04C93149D14B3E77588906F3CB7EED2927A", "rexaID":"f006e50cdba5e00538e2b48452e1d0bab8ed02b9", "author":"Mukund Deshpande and George Karypis", "title":"Evaluation of Techniques for Classifying Biological Sequences", "venue":"PAKDD", "year":"2002", "window":"contains a total of 3,370 sequences, which is equal to the number of coding and non-coding regions in the e-coli genome. The <b>Protein Structure</b> dataset is made of amino acid sequences and addresses the problem of assigning a secondary structure to a protein sequence. The dataset was created by processing the SWISS-PROT [BA99] database to obtain", "mykey":725},
 {"datasetID":69, "supportID":"520ED04C93149D14B3E77588906F3CB7EED2927A", "rexaID":"f006e50cdba5e00538e2b48452e1d0bab8ed02b9", "author":"Mukund Deshpande and George Karypis", "title":"Evaluation of Techniques for Classifying Biological Sequences", "venue":"PAKDD", "year":"2002", "window":"# Sessions Avg. Length Dataset # Sessions Avg. Length <b>Splice</b> (S-EI) 1, 527 60.0 Peptidias (P-) 1,584 511.3 Exon 762 60.0 cysteine 416 854.3 Intron 765 60.0 metallo 580 512.6 Mouse Genome (MG-GN) 10,918 361.6 serine 775 500.5 exon", "mykey":726},
 {"datasetID":154, "supportID":"520ED04C93149D14B3E77588906F3CB7EED2927A", "rexaID":"f006e50cdba5e00538e2b48452e1d0bab8ed02b9", "author":"Mukund Deshpande and George Karypis", "title":"Evaluation of Techniques for Classifying Biological Sequences", "venue":"PAKDD", "year":"2002", "window":"contains a total of 3,370 sequences, which is equal to the number of coding and non-coding regions in the e-coli genome. The <b>Protein</b> Structure dataset is made of amino acid sequences and addresses the problem of assigning a secondary structure to a protein sequence. The dataset was created by processing the SWISS-PROT [BA99] database to obtain", "mykey":727},
 {"datasetID":70, "supportID":"521CD1DBC85B792C9F98061C1A9616F9B932EEE7", "rexaID":"95e24786ffc779c3b7d5a2d838391b5d0252a15c", "author":"Wl odzisl/aw Duch and Rafal Adamczak and Krzysztof Grabczewski and Norbert Jankowski", "title":"Control and Cybernetics", "venue":"Department of Computer Methods, Nicholas Copernicus University", "year":"", "window":"classifiers used in such problems, but they have an opinion of being opaque black boxes. Several neural methods have been compared experimentally on the mushroom and the 3 <b>Monk</b> problems benchmark datasets (Andrews et al. 1995), and recently comparison with some machine learning methods has been given (Duch et al. 2000). There is no reason why a simple classification model based on logical rules", "mykey":728},
 {"datasetID":73, "supportID":"521CD1DBC85B792C9F98061C1A9616F9B932EEE7", "rexaID":"95e24786ffc779c3b7d5a2d838391b5d0252a15c", "author":"Wl odzisl/aw Duch and Rafal Adamczak and Krzysztof Grabczewski and Norbert Jankowski", "title":"Control and Cybernetics", "venue":"Department of Computer Methods, Nicholas Copernicus University", "year":"", "window":"classifiers used in such problems, but they have an opinion of being opaque black boxes. Several neural methods have been compared experimentally on the <b>mushroom</b> and the 3 Monk problems benchmark datasets (Andrews et al. 1995), and recently comparison with some machine learning methods has been given (Duch et al. 2000). There is no reason why a simple classification model based on logical rules", "mykey":729},
 {"datasetID":2, "supportID":"522872ED3A98BCE23FC75E6F307FAA6117AF329D", "rexaID":"6bacbb21f7a1d6dee581f756cf326e0932446698", "author":"Zhiyuan Chen and Johannes Gehrke and Flip Korn", "title":"Query Optimization In Compressed Database Systems", "venue":"SIGMOD Conference", "year":"2001", "window":"are not compressed. TPC-H data contains 8 tables and 61 attributes, 23 of which are string-valued. The string attributes account for about 60% of the total database size. We also used a 4MB of dataset with US census data, the <b>adult</b> data set [5] for experiments on compression strategies. The adult dataset contains a single table with 14 attributes, 8 of them string-valued, accounting for about 80%", "mykey":730},
 {"datasetID":20, "supportID":"522872ED3A98BCE23FC75E6F307FAA6117AF329D", "rexaID":"6bacbb21f7a1d6dee581f756cf326e0932446698", "author":"Zhiyuan Chen and Johannes Gehrke and Flip Korn", "title":"Query Optimization In Compressed Database Systems", "venue":"SIGMOD Conference", "year":"2001", "window":"are not compressed. TPC-H data contains 8 tables and 61 attributes, 23 of which are string-valued. The string attributes account for about 60% of the total database size. We also used a 4MB of dataset with US <b>census</b> data, the adult data set [5] for experiments on compression strategies. The adult dataset contains a single table with 14 attributes, 8 of them string-valued, accounting for about 80%", "mykey":731},
 {"datasetID":116, "supportID":"522872ED3A98BCE23FC75E6F307FAA6117AF329D", "rexaID":"6bacbb21f7a1d6dee581f756cf326e0932446698", "author":"Zhiyuan Chen and Johannes Gehrke and Flip Korn", "title":"Query Optimization In Compressed Database Systems", "venue":"SIGMOD Conference", "year":"2001", "window":"are not compressed. TPC-H data contains 8 tables and 61 attributes, 23 of which are string-valued. The string attributes account for about 60% of the total database size. We also used a 4MB of dataset with <b>US census</b> data, the adult data set [5] for experiments on compression strategies. The adult dataset contains a single table with 14 attributes, 8 of them string-valued, accounting for about 80%", "mykey":732},
 {"datasetID":14, "supportID":"53561D3A6F34CA7B6B5884466236FD6A5C45B5FB", "rexaID":"19e1b6e0932bbe665a2c4a069a0636d8d5cf0c6f", "author":"Jennifer A. Blue and Kristin P. Bennett", "title":"Hybrid Extreme Point Tabu Search", "venue":"Department of Mathematical Sciences Rensselaer Polytechnic Institute", "year":"1996", "window":"(Liver); the PIMA Indians Diabetes dataset (Diabetes), the Wisconsin <b>Breast</b> <b>Cancer</b> Database (Cancer) [23], and the Cleveland Heart Disease Database (Heart) [9]. We used 5-fold cross validation. Each dataset was divided into 5 parts. The", "mykey":733},
 {"datasetID":17, "supportID":"53561D3A6F34CA7B6B5884466236FD6A5C45B5FB", "rexaID":"19e1b6e0932bbe665a2c4a069a0636d8d5cf0c6f", "author":"Jennifer A. Blue and Kristin P. Bennett", "title":"Hybrid Extreme Point Tabu Search", "venue":"Department of Mathematical Sciences Rensselaer Polytechnic Institute", "year":"1996", "window":"(Liver); the PIMA Indians Diabetes dataset (Diabetes), the <b>Wisconsin</b> <b>Breast</b> <b>Cancer</b> Database (Cancer) [23], and the Cleveland Heart Disease Database (Heart) [9]. We used 5-fold cross validation. Each dataset was divided into 5 parts. The", "mykey":734},
 {"datasetID":15, "supportID":"53561D3A6F34CA7B6B5884466236FD6A5C45B5FB", "rexaID":"19e1b6e0932bbe665a2c4a069a0636d8d5cf0c6f", "author":"Jennifer A. Blue and Kristin P. Bennett", "title":"Hybrid Extreme Point Tabu Search", "venue":"Department of Mathematical Sciences Rensselaer Polytechnic Institute", "year":"1996", "window":"(Liver); the PIMA Indians Diabetes dataset (Diabetes), the <b>Wisconsin</b> <b>Breast</b> <b>Cancer</b> Database (Cancer) [23], and the Cleveland Heart Disease Database (Heart) [9]. We used 5-fold cross validation. Each dataset was divided into 5 parts. The", "mykey":735},
 {"datasetID":16, "supportID":"53561D3A6F34CA7B6B5884466236FD6A5C45B5FB", "rexaID":"19e1b6e0932bbe665a2c4a069a0636d8d5cf0c6f", "author":"Jennifer A. Blue and Kristin P. Bennett", "title":"Hybrid Extreme Point Tabu Search", "venue":"Department of Mathematical Sciences Rensselaer Polytechnic Institute", "year":"1996", "window":"(Liver); the PIMA Indians Diabetes dataset (Diabetes), the <b>Wisconsin</b> <b>Breast</b> <b>Cancer</b> Database (Cancer) [23], and the Cleveland Heart Disease Database (Heart) [9]. We used 5-fold cross validation. Each dataset was divided into 5 parts. The", "mykey":736},
 {"datasetID":34, "supportID":"53561D3A6F34CA7B6B5884466236FD6A5C45B5FB", "rexaID":"19e1b6e0932bbe665a2c4a069a0636d8d5cf0c6f", "author":"Jennifer A. Blue and Kristin P. Bennett", "title":"Hybrid Extreme Point Tabu Search", "venue":"Department of Mathematical Sciences Rensselaer Polytechnic Institute", "year":"1996", "window":"are: the BUPA Liver Disease dataset (Liver); the PIMA Indians <b>Diabetes</b> dataset (Diabetes), the Wisconsin Breast Cancer Database (Cancer) [23], and the Cleveland Heart Disease Database (Heart) [9]. We used 5-fold cross validation. Each", "mykey":737},
 {"datasetID":60, "supportID":"53561D3A6F34CA7B6B5884466236FD6A5C45B5FB", "rexaID":"19e1b6e0932bbe665a2c4a069a0636d8d5cf0c6f", "author":"Jennifer A. Blue and Kristin P. Bennett", "title":"Hybrid Extreme Point Tabu Search", "venue":"Department of Mathematical Sciences Rensselaer Polytechnic Institute", "year":"1996", "window":"available via anonymous ftp from the Machine Learning Repository at the University of California at Irvine [16]. The datasets are: the BUPA <b>Liver</b> Disease dataset (Liver); the PIMA Indians Diabetes dataset (Diabetes), the Wisconsin Breast Cancer Database (Cancer) [23], and the Cleveland Heart Disease Database (Heart) [9].", "mykey":738},
 {"datasetID":79, "supportID":"53561D3A6F34CA7B6B5884466236FD6A5C45B5FB", "rexaID":"19e1b6e0932bbe665a2c4a069a0636d8d5cf0c6f", "author":"Jennifer A. Blue and Kristin P. Bennett", "title":"Hybrid Extreme Point Tabu Search", "venue":"Department of Mathematical Sciences Rensselaer Polytechnic Institute", "year":"1996", "window":"are: the BUPA Liver Disease dataset (Liver); the <b>PIMA</b> <b>Indians</b> <b>Diabetes</b> dataset (Diabetes), the Wisconsin Breast Cancer Database (Cancer) [23], and the Cleveland Heart Disease Database (Heart) [9]. We used 5-fold cross validation. Each", "mykey":739},
 {"datasetID":42, "supportID":"536B979DB33793D8525FBBB573D7E6B4FDCF4960", "rexaID":"f51be2f205fbe38436ed150a6627139668eb002a", "author":"Thierry Denoeux", "title":"A neural network classifier based on Dempster-Shafer theory", "venue":"IEEE Transactions on Systems, Man, and Cybernetics, Part A, 30", "year":"2000", "window":"analysis techniques [13]. As shown in Table I, our approach with at least three prototypes per class dominates the other techniques for this classification task. 2) Forensic <b>Glass</b> Data: This data set contains the description of 214 fragments of glass [17] originally collected for a study in the context of criminal investigation. Each fragment has a measured reflectivity index and chemical", "mykey":740},
 {"datasetID":149, "supportID":"536B979DB33793D8525FBBB573D7E6B4FDCF4960", "rexaID":"f51be2f205fbe38436ed150a6627139668eb002a", "author":"Thierry Denoeux", "title":"A neural network classifier based on Dempster-Shafer theory", "venue":"IEEE Transactions on Systems, Man, and Cybernetics, Part A, 30", "year":"2000", "window":"by Ripley [20], 29 instances were discarded, and the remaining 185 were re-grouped in four classes: window float glass (70), window nonfloat glass (76), <b>vehicle</b> window glass (17) and other (22). The data set was split randomly in a training set of size 89 and a test set of size 96. Our method (with normalized outputs) was compared to three neural network classifiers: learning vector quantization (LVQ)", "mykey":741},
 {"datasetID":1, "supportID":"538E5168E724B47D2B8A6B2FD5CF528BB6EECEEB", "rexaID":"70172e511a3bc27c7927119a3b2a3405fbad99e0", "author":"Kai Ming Ting and Ian H. Witten", "title":"Issues in Stacked Generalization", "venue":"J. Artif. Intell. Res. (JAIR, 10", "year":"1999", "window":"can easily be interpreted. Examples of the combination weights it derives (for the probabilitybased model ~ M 0 ) appear in Table 5 for the Horse, Credit, Splice, <b>Abalone</b>  Waveform, Led24 and Vowel datasets. The weights indicate the relative importance of the level-0 generalizers for each prediction class. For example, in the Splice dataset (in Table 5(b)), NB is the dominant generalizer for", "mykey":742},
 {"datasetID":14, "supportID":"538E5168E724B47D2B8A6B2FD5CF528BB6EECEEB", "rexaID":"70172e511a3bc27c7927119a3b2a3405fbad99e0", "author":"Kai Ming Ting and Ian H. Witten", "title":"Issues in Stacked Generalization", "venue":"J. Artif. Intell. Res. (JAIR, 10", "year":"1999", "window":"are given in Table 10, and indicate that the three methods are very competitive. 4 Stacking performs better than both arcing and bagging in three datasets (Waveform, Soybean and <b>Breast</b> <b>Cancer</b> , and is better than arcing but worse than bagging in the Diabetes dataset. Note that stacking performs very poorly on Glass and Ionosphere, two small", "mykey":743},
 {"datasetID":150, "supportID":"538E5168E724B47D2B8A6B2FD5CF528BB6EECEEB", "rexaID":"70172e511a3bc27c7927119a3b2a3405fbad99e0", "author":"Kai Ming Ting and Ian H. Witten", "title":"Issues in Stacked Generalization", "venue":"J. Artif. Intell. Res. (JAIR, 10", "year":"1999", "window":"networks for this purpose and found that they have a much slower learning rate than MLR. For example, MLR only took 2.9 seconds as compare to 4790 seconds for the neural network in the <b>nettalk</b> dataset; while both have the same error rate. Other possible candidates are the multinomial logit model (Jordan & Jacobs, 1994), which is a special case of generalized linear models (McCullagh & Nelder,", "mykey":744},
 {"datasetID":34, "supportID":"538E5168E724B47D2B8A6B2FD5CF528BB6EECEEB", "rexaID":"70172e511a3bc27c7927119a3b2a3405fbad99e0", "author":"Kai Ming Ting and Ian H. Witten", "title":"Issues in Stacked Generalization", "venue":"J. Artif. Intell. Res. (JAIR, 10", "year":"1999", "window":"(Waveform, Soybean and Breast Cancer), and is better than arcing but worse than bagging in the <b>Diabetes</b> dataset. Note that stacking performs very poorly on Glass and Ionosphere, two small real-world datasets. This is not surprising, because cross-validation inevitably produces poor estimates for small", "mykey":745},
 {"datasetID":42, "supportID":"538E5168E724B47D2B8A6B2FD5CF528BB6EECEEB", "rexaID":"70172e511a3bc27c7927119a3b2a3405fbad99e0", "author":"Kai Ming Ting and Ian H. Witten", "title":"Issues in Stacked Generalization", "venue":"J. Artif. Intell. Res. (JAIR, 10", "year":"1999", "window":"Note that stacking performs very poorly on <b>Glass</b> and Ionosphere, two small real-world datasets. This is not surprising, because cross-validation inevitably produces poor estimates for small datasets. 4.2 Discussion Like bagging, stacking is ideal for parallel computation. The construction of", "mykey":746},
 {"datasetID":45, "supportID":"538E5168E724B47D2B8A6B2FD5CF528BB6EECEEB", "rexaID":"70172e511a3bc27c7927119a3b2a3405fbad99e0", "author":"Kai Ming Ting and Ian H. Witten", "title":"Issues in Stacked Generalization", "venue":"J. Artif. Intell. Res. (JAIR, 10", "year":"1999", "window":"situation. The performance variation among the member models in bagging is rather small because they are derived from the same learning algorithm using bootstrap samples. Section 3.3 4. The <b>heart</b> dataset used by Breiman (1996b; 1996c) is omitted because it was very much modified from the original one. 284 Issues in Stacked Generalization shows that a small performance variation among member models", "mykey":747},
 {"datasetID":47, "supportID":"538E5168E724B47D2B8A6B2FD5CF528BB6EECEEB", "rexaID":"70172e511a3bc27c7927119a3b2a3405fbad99e0", "author":"Kai Ming Ting and Ian H. Witten", "title":"Issues in Stacked Generalization", "venue":"J. Artif. Intell. Res. (JAIR, 10", "year":"1999", "window":"C4.5 NB IB1 1 0.36 0.20 0.42 0.63 0.30 0.04 2 0.39 0.19 0.41 0.65 0.28 0.07 C4.5 for ff 1 ; NB for ff 2 ; IB1 for ff 3 . Table 5: (a) Weights generated by MLR (model ~ M 0 ) for the <b>Horse</b> and Credit datasets. Splice Abalone Waveform Class C4.5 NB IB1 C4.5 NB IB1 C4.5 NB IB1 1 0.23 0.43 0.36 0.25 0.25 0.39 0.16 0.59 0.34 2 0.15 0.72 0.12 0.27 0.20 0.25 0.14 0.72 0.07 3 0.08 0.52 0.40 0.30 0.18 0.39 0.04", "mykey":748},
 {"datasetID":52, "supportID":"538E5168E724B47D2B8A6B2FD5CF528BB6EECEEB", "rexaID":"70172e511a3bc27c7927119a3b2a3405fbad99e0", "author":"Kai Ming Ting and Ian H. Witten", "title":"Issues in Stacked Generalization", "venue":"J. Artif. Intell. Res. (JAIR, 10", "year":"1999", "window":"Note that stacking performs very poorly on Glass and <b>Ionosphere</b>  two small real-world datasets. This is not surprising, because cross-validation inevitably produces poor estimates for small datasets. 4.2 Discussion Like bagging, stacking is ideal for parallel computation. The construction of", "mykey":749},
 {"datasetID":69, "supportID":"538E5168E724B47D2B8A6B2FD5CF528BB6EECEEB", "rexaID":"70172e511a3bc27c7927119a3b2a3405fbad99e0", "author":"Kai Ming Ting and Ian H. Witten", "title":"Issues in Stacked Generalization", "venue":"J. Artif. Intell. Res. (JAIR, 10", "year":"1999", "window":"can easily be interpreted. Examples of the combination weights it derives (for the probabilitybased model ~ M 0 ) appear in Table 5 for the Horse, Credit, <b>Splice</b>  Abalone, Waveform, Led24 and Vowel datasets. The weights indicate the relative importance of the level-0 generalizers for each prediction class. For example, in the Splice dataset (in Table 5(b)), NB is the dominant generalizer for", "mykey":750},
 {"datasetID":90, "supportID":"538E5168E724B47D2B8A6B2FD5CF528BB6EECEEB", "rexaID":"70172e511a3bc27c7927119a3b2a3405fbad99e0", "author":"Kai Ming Ting and Ian H. Witten", "title":"Issues in Stacked Generalization", "venue":"J. Artif. Intell. Res. (JAIR, 10", "year":"1999", "window":"are given in Table 10, and indicate that the three methods are very competitive. 4 Stacking performs better than both arcing and bagging in three datasets (Waveform, <b>Soybean</b> and Breast Cancer), and is better than arcing but worse than bagging in the Diabetes dataset. Note that stacking performs very poorly on Glass and Ionosphere, two small", "mykey":751},
 {"datasetID":91, "supportID":"538E5168E724B47D2B8A6B2FD5CF528BB6EECEEB", "rexaID":"70172e511a3bc27c7927119a3b2a3405fbad99e0", "author":"Kai Ming Ting and Ian H. Witten", "title":"Issues in Stacked Generalization", "venue":"J. Artif. Intell. Res. (JAIR, 10", "year":"1999", "window":"are given in Table 10, and indicate that the three methods are very competitive. 4 Stacking performs better than both arcing and bagging in three datasets (Waveform, <b>Soybean</b> and Breast Cancer), and is better than arcing but worse than bagging in the Diabetes dataset. Note that stacking performs very poorly on Glass and Ionosphere, two small", "mykey":752},
 {"datasetID":107, "supportID":"538E5168E724B47D2B8A6B2FD5CF528BB6EECEEB", "rexaID":"70172e511a3bc27c7927119a3b2a3405fbad99e0", "author":"Kai Ming Ting and Ian H. Witten", "title":"Issues in Stacked Generalization", "venue":"J. Artif. Intell. Res. (JAIR, 10", "year":"1999", "window":"from the UCI Repository of machine learning databases (Blake, Keogh & Merz, 1998). Details of these are given in Table 1. For the artificial datasets---Led24 and <b>Waveform</b> --each training dataset L of size 200 and 300, respectively, is generated using a different seed. The algorithms used for the experiments are then tested on a separate dataset", "mykey":753},
 {"datasetID":108, "supportID":"538E5168E724B47D2B8A6B2FD5CF528BB6EECEEB", "rexaID":"70172e511a3bc27c7927119a3b2a3405fbad99e0", "author":"Kai Ming Ting and Ian H. Witten", "title":"Issues in Stacked Generalization", "venue":"J. Artif. Intell. Res. (JAIR, 10", "year":"1999", "window":"from the UCI Repository of machine learning databases (Blake, Keogh & Merz, 1998). Details of these are given in Table 1. For the artificial datasets---Led24 and <b>Waveform</b> --each training dataset L of size 200 and 300, respectively, is generated using a different seed. The algorithms used for the experiments are then tested on a separate dataset", "mykey":754},
 {"datasetID":67, "supportID":"5395CD50070B1BCED4781448DAEFC764A795AD89", "rexaID":"af5f79a0f4d68c9ff526c960428c114e8e0fa4a5", "author":"Warodom Geamsakul and Takashi Matsuda and Tetsuya Yoshida and Hiroshi Motoda and Takashi Washio", "title":"Constructing a Decision Tree for Graph Structured Data", "venue":"Institute of Scientific and Industrial Research, Osaka University", "year":"", "window":"is employed to extract good enough discriminative patterns within the greedy search framework. Pessimistic pruning is incorporated to avoid overfitting to the training data. Experiments using a <b>DNA</b> dataset were conducted to see the effect of the beam width, the number of chunking at each node of a decision tree, and the pruning. The results indicate that DT-GBI that does not use any prior domain", "mykey":755},
 {"datasetID":42, "supportID":"53ADE64EE305D2C1E6B394CC55D8650ABC932AAD", "rexaID":"27e33344198975ea1b20e02c8f0fce01cd29f6e5", "author":"Eibe Frank and Ian H. Witten", "title":"Generating Accurate Rule Sets Without Global Optimization", "venue":"ICML", "year":"1998", "window":"listed in Table 2. They give the percentage of correct classifications, averaged over ten ten-fold cross-validation runs, and standard 3 Following Holte (Holte, 1993), the G2 variant of the <b>glass</b> dataset has classes 1 and 3 combined and classes 4 to 7 deleted, and the horse-colic dataset has attributes 3, 25, 26, 27, 28 deleted with attribute 24 being used as the class. We also deleted all", "mykey":756},
 {"datasetID":47, "supportID":"53ADE64EE305D2C1E6B394CC55D8650ABC932AAD", "rexaID":"27e33344198975ea1b20e02c8f0fce01cd29f6e5", "author":"Eibe Frank and Ian H. Witten", "title":"Generating Accurate Rule Sets Without Global Optimization", "venue":"ICML", "year":"1998", "window":"has classes 1 and 3 combined and classes 4 to 7 deleted, and the <b>horse</b> <b>colic</b> dataset has attributes 3, 25, 26, 27, 28 deleted with attribute 24 being used as the class. We also deleted all identifier attributes from the datasets. 4 We used Revision 8 of C4.5. Table 2: Experimental", "mykey":757},
 {"datasetID":42, "supportID":"53BE2C9C67B260720C526879C0004AD76388148F", "rexaID":"8be22c7cb4b6b5d2147c5376492279b13ba12e07", "author":"", "title":"Eectiveness of Error Correcting Output Coding methods in ensemble and monolithic learning machines", "venue":"Dipartimento di Informatica, Universitdi Pisa", "year":"", "window":"and composed by normal distributed clusters of data. The set p6 contains 6 classes with no overlapping regions, while the regions of the 9 classes of p9 hardly overlap. <b>Glass</b>  letter and optdigits data sets are from the UCI repository [42]. In the experimentation we have used exhaustive [17] and BCH ECOC generation algorithms [8]. ECOC exhaustive algorithms select among all possible 2 K dichotomies", "mykey":758},
 {"datasetID":80, "supportID":"53C136F820E1D0FB928FA08420A3E4265128598D", "rexaID":"4ce0d7e58fb5a580416fc6aa07ca54a2829a0091", "author":"Ethem Alpaydin", "title":"Combined 5 x 2 cv F Test for Comparing Supervised Classification Learning Algorithms", "venue":"Neural Computation, 11", "year":"1999", "window":"2 no VOWEL 2 no ODR 8 yes DIGIT 7 yes PEN 10 yes changing the numerator where we compare a single layer perceptron (LP) with a multilayer perceptron with one hidden layer (MLP). ODR, DIGIT are two datasets on <b>optical</b> handwritten digit <b>recognition</b> and PEN is on pen-based handwritten digit recognition. These three datasets are available from the author. The other datasets are from the UCI repository", "mykey":759},
 {"datasetID":81, "supportID":"53C136F820E1D0FB928FA08420A3E4265128598D", "rexaID":"4ce0d7e58fb5a580416fc6aa07ca54a2829a0091", "author":"Ethem Alpaydin", "title":"Combined 5 x 2 cv F Test for Comparing Supervised Classification Learning Algorithms", "venue":"Neural Computation, 11", "year":"1999", "window":"2 no VOWEL 2 no ODR 8 yes DIGIT 7 yes PEN 10 yes changing the numerator where we compare a single layer perceptron (LP) with a multilayer perceptron with one hidden layer (MLP). ODR, DIGIT are two datasets on optical <b>handwritten</b> digit <b>recognition</b> and PEN is on pen-based handwritten digit recognition. These three datasets are available from the author. The other datasets are from the UCI repository", "mykey":760},
 {"datasetID":7, "supportID":"53DFABDDB60022FB5D0D4AE073A86B8B6B9C9C1A", "rexaID":"12fe4ef85ae4ef89437a10d0a7b0daf32af2be21", "author":"Jerome H. Friedman and Ron Kohavi and Youngkeol Yun", "title":"To appear in AAAI-96 Lazy Decision Trees", "venue":"Statistics Department and Stanford Linear Accelerator Center Stanford University", "year":"", "window":"that have large differences. The LazyDT's average error rate is 1.9% lower, which is a relative improvement in error of 10.6% over C4.5's 17.9% average error rate. Three datasets deserve special discussion: anneal, <b>audiology</b>  and the monk2 problem. Anneal is interesting because ID3 manages so well. An investigation of the problem shows that the main difference stems from", "mykey":761},
 {"datasetID":8, "supportID":"53DFABDDB60022FB5D0D4AE073A86B8B6B9C9C1A", "rexaID":"12fe4ef85ae4ef89437a10d0a7b0daf32af2be21", "author":"Jerome H. Friedman and Ron Kohavi and Youngkeol Yun", "title":"To appear in AAAI-96 Lazy Decision Trees", "venue":"Statistics Department and Stanford Linear Accelerator Center Stanford University", "year":"", "window":"that have large differences. The LazyDT's average error rate is 1.9% lower, which is a relative improvement in error of 10.6% over C4.5's 17.9% average error rate. Three datasets deserve special discussion: anneal, <b>audiology</b>  and the monk2 problem. Anneal is interesting because ID3 manages so well. An investigation of the problem shows that the main difference stems from", "mykey":762},
 {"datasetID":23, "supportID":"53DFABDDB60022FB5D0D4AE073A86B8B6B9C9C1A", "rexaID":"12fe4ef85ae4ef89437a10d0a7b0daf32af2be21", "author":"Jerome H. Friedman and Ron Kohavi and Youngkeol Yun", "title":"To appear in AAAI-96 Lazy Decision Trees", "venue":"Statistics Department and Stanford Linear Accelerator Center Stanford University", "year":"", "window":"fast algorithm. The largest running time by far was for mushroom with 8.4 Sparc-10 cpu minutes per crossvalidation fold (equivalent to a run), followed by <b>chess</b> with 1.59 cpu minutes. These datasets have 8124 instances and 3196 instances, respectively. From the table we can see that simple ID3 is generally inferior, as is C4.5 without pruning. Pruning improves C4.5-NP's performance, except for", "mykey":763},
 {"datasetID":21, "supportID":"53DFABDDB60022FB5D0D4AE073A86B8B6B9C9C1A", "rexaID":"12fe4ef85ae4ef89437a10d0a7b0daf32af2be21", "author":"Jerome H. Friedman and Ron Kohavi and Youngkeol Yun", "title":"To appear in AAAI-96 Lazy Decision Trees", "venue":"Statistics Department and Stanford Linear Accelerator Center Stanford University", "year":"", "window":"fast algorithm. The largest running time by far was for mushroom with 8.4 Sparc-10 cpu minutes per crossvalidation fold (equivalent to a run), followed by <b>chess</b> with 1.59 cpu minutes. These datasets have 8124 instances and 3196 instances, respectively. From the table we can see that simple ID3 is generally inferior, as is C4.5 without pruning. Pruning improves C4.5-NP's performance, except for", "mykey":764},
 {"datasetID":22, "supportID":"53DFABDDB60022FB5D0D4AE073A86B8B6B9C9C1A", "rexaID":"12fe4ef85ae4ef89437a10d0a7b0daf32af2be21", "author":"Jerome H. Friedman and Ron Kohavi and Youngkeol Yun", "title":"To appear in AAAI-96 Lazy Decision Trees", "venue":"Statistics Department and Stanford Linear Accelerator Center Stanford University", "year":"", "window":"fast algorithm. The largest running time by far was for mushroom with 8.4 Sparc-10 cpu minutes per crossvalidation fold (equivalent to a run), followed by <b>chess</b> with 1.59 cpu minutes. These datasets have 8124 instances and 3196 instances, respectively. From the table we can see that simple ID3 is generally inferior, as is C4.5 without pruning. Pruning improves C4.5-NP's performance, except for", "mykey":765},
 {"datasetID":44, "supportID":"53DFABDDB60022FB5D0D4AE073A86B8B6B9C9C1A", "rexaID":"12fe4ef85ae4ef89437a10d0a7b0daf32af2be21", "author":"Jerome H. Friedman and Ron Kohavi and Youngkeol Yun", "title":"To appear in AAAI-96 Lazy Decision Trees", "venue":"Statistics Department and Stanford Linear Accelerator Center Stanford University", "year":"", "window":"monk1, monk2, and monk3; and pseudo-artificial datasets: tic-tac-toe, and chess. <b>Hayes</b> <b>roth</b> and glass2 also have large differences probably because they have many strongly relevant features and few weakly relevant features (John, Kohavi, & Pfleger", "mykey":766},
 {"datasetID":101, "supportID":"53DFABDDB60022FB5D0D4AE073A86B8B6B9C9C1A", "rexaID":"12fe4ef85ae4ef89437a10d0a7b0daf32af2be21", "author":"Jerome H. Friedman and Ron Kohavi and Youngkeol Yun", "title":"To appear in AAAI-96 Lazy Decision Trees", "venue":"Statistics Department and Stanford Linear Accelerator Center Stanford University", "year":"", "window":"monk1, monk2, and monk3; and pseudo-artificial datasets: <b>tic-tac-toe</b>  and chess. Hayes-roth and glass2 also have large differences probably because they have many strongly relevant features and few weakly relevant features (John, Kohavi, & Pfleger", "mykey":767},
 {"datasetID":7, "supportID":"53DFBC477FFB016E36D54F98E48695902D8EB566", "rexaID":"944b9d70eb0a01d18c91109dfeb566936461a194", "author":"Alexander K. Seewald", "title":"Meta-Learning for Stacked Classification", "venue":"Austrian Research Institute for Artificial Intelligence", "year":"", "window":"with number of classes and examples, discrete and continuous attributes, baseline accuracy (%) and entropy in bits per example (Kononenko & Bratko, 1991). Dataset cl Inst disc cont bL E <b>audiology</b> 24 226 69 0 25.22 3.51 autos 7 205 10 16 32.68 2.29 balance-scale 3 625 0 4 45.76 1.32 breast-cancer 2 286 10 0 70.28 0.88 breast-w 2 699 0 9 65.52 0.93 colic 2 368", "mykey":768},
 {"datasetID":8, "supportID":"53DFBC477FFB016E36D54F98E48695902D8EB566", "rexaID":"944b9d70eb0a01d18c91109dfeb566936461a194", "author":"Alexander K. Seewald", "title":"Meta-Learning for Stacked Classification", "venue":"Austrian Research Institute for Artificial Intelligence", "year":"", "window":"with number of classes and examples, discrete and continuous attributes, baseline accuracy (%) and entropy in bits per example (Kononenko & Bratko, 1991). Dataset cl Inst disc cont bL E <b>audiology</b> 24 226 69 0 25.22 3.51 autos 7 205 10 16 32.68 2.29 balance-scale 3 625 0 4 45.76 1.32 breast-cancer 2 286 10 0 70.28 0.88 breast-w 2 699 0 9 65.52 0.93 colic 2 368", "mykey":769},
 {"datasetID":12, "supportID":"53DFBC477FFB016E36D54F98E48695902D8EB566", "rexaID":"944b9d70eb0a01d18c91109dfeb566936461a194", "author":"Alexander K. Seewald", "title":"Meta-Learning for Stacked Classification", "venue":"Austrian Research Institute for Artificial Intelligence", "year":"", "window":"the baseline accuracy is already 66.7%. Interestingly in this case the best model is from DecisionStump which learns a single J48 node, obtaining 88.9% accuracy, corresponding to a single error on dataset <b>balance</b> <b>scale</b>  It seems J48 is prone to overfitting on this meta-dataset. The training set model is based on meanAbsSkew. The models from the nine folds are more diverse: seven times, the following", "mykey":770},
 {"datasetID":56, "supportID":"53DFBC477FFB016E36D54F98E48695902D8EB566", "rexaID":"944b9d70eb0a01d18c91109dfeb566936461a194", "author":"Alexander K. Seewald", "title":"Meta-Learning for Stacked Classification", "venue":"Austrian Research Institute for Artificial Intelligence", "year":"", "window":"using only seven folds shows the exact same result. When removing the base-classifier dependent features, IBk is still the best classifier with an additional error on <b>labor</b>  the smallest dataset. In this case MLR which is also a global learner is equally good. So we may tentatively conclude that for this meta-dataset, there seems to be no single feature which can predict the significant", "mykey":771},
 {"datasetID":98, "supportID":"53DFBC477FFB016E36D54F98E48695902D8EB566", "rexaID":"944b9d70eb0a01d18c91109dfeb566936461a194", "author":"Alexander K. Seewald", "title":"Meta-Learning for Stacked Classification", "venue":"Austrian Research Institute for Artificial Intelligence", "year":"", "window":"features which uniquely characterize the dataset. These were inspired by the <b>StatLOG</b> project (Brazdil, Gama & Henery, 1994) and reimplemented in WEKA. # Inst, the number of examples. # log(Inst) which is the natural logarithm of Inst. # Classes,", "mykey":772},
 {"datasetID":53, "supportID":"54008F57C35B26D84A889A6717D4DFA9C35339AA", "rexaID":"b7408989feafd783e1ebdcdac949100e6d133b9e", "author":"David Horn and A. Gottlieb", "title":"The Method of Quantum Clustering", "venue":"NIPS", "year":"2001", "window":"minima appear, as seen in Fig. 3. Nonetheless, they lie high and contain only a few data points. The major minima are the same as in Fig. 2. 3.2 <b>iris</b> Data Our second example consists of the iris data set [10], which is a standard benchmark obtainable from the UCI repository [11]. Here we use the first two principal components to define the two dimensions in which we apply our method. Fig. 4, which", "mykey":773},
 {"datasetID":54, "supportID":"548FA7D2C256E37A0B9916BCEE71A42BBCED20FD", "rexaID":"4f52834d0595282e8454f617706fbec7589c8de6", "author":"Jaakko Peltonen and Samuel Kaski", "title":"Discriminative Components of Data", "venue":"IEEE", "year":"2004", "window":"whose properties are summarized in Table I. The Landsat, <b>Isolet</b>  and Multiple Features (MFeat) data sets are from UCI Machine Learning Repository [31], LVQ PAK refers to the Finnish acoustic phoneme data distributed with the LVQ-PAK [32], and TIMIT refers to phoneme data from the Darpa TIMIT acoustic", "mykey":774},
 {"datasetID":72, "supportID":"548FA7D2C256E37A0B9916BCEE71A42BBCED20FD", "rexaID":"4f52834d0595282e8454f617706fbec7589c8de6", "author":"Jaakko Peltonen and Samuel Kaski", "title":"Discriminative Components of Data", "venue":"IEEE", "year":"2004", "window":"whose properties are summarized in Table I. The Landsat, Isolet, and <b>Multiple Features</b>  <b>MFeat</b>  data sets are from UCI Machine Learning Repository [31], LVQ PAK refers to the Finnish acoustic phoneme data distributed with the LVQ-PAK [32], and TIMIT refers to phoneme data from the Darpa TIMIT acoustic", "mykey":775},
 {"datasetID":146, "supportID":"548FA7D2C256E37A0B9916BCEE71A42BBCED20FD", "rexaID":"4f52834d0595282e8454f617706fbec7589c8de6", "author":"Jaakko Peltonen and Samuel Kaski", "title":"Discriminative Components of Data", "venue":"IEEE", "year":"2004", "window":"Number of Number of Dataset Dimensionality classes samples <b>Landsat</b> 36 6 4435 LVQ PAK 20 13 3656 Isolet 30 26 3742 MFeat 76 10 1500 TIMIT 12 41 14994 to indirect measures; we will make sure, however, that the comparisons", "mykey":776},
 {"datasetID":2, "supportID":"54C021ADACFD1C7A6C54534278E5FC62466B35A4", "rexaID":"92270daca0b7d369e25eb444e918bcb74b18d682", "author":"Saharon Rosset", "title":"Model selection via the AUC", "venue":"ICML", "year":"2004", "window":"to prefer the K-NN model most of the time. This illustrates the \"bias\" in using AUC to select classi#cation models, which we discuss in section 3.2 Finally, we performed experiments on a real-life data set. We used the  <b>Adult</b>  data-set available from the UCI repository (Blake & Merz, 1998). We used only the first ten variables in this data-set, to make a largescale experiment feasible, and compared", "mykey":777},
 {"datasetID":53, "supportID":"54C813567D0F4B1FA498A8F90FD9B20D3C4FE580", "rexaID":"030d9becf3287c5da5f82e2c2da60937121b3ce5", "author":"Wl odzisl/aw Duch and Rafal Adamczak and Norbert Jankowski", "title":"Initialization of adaptive parameters in density networks", "venue":"Department of Computer Methods, Nicholas Copernicus University", "year":"", "window":"network parameters, but it is interesting to note that these results are frequently already of rather high quality. Except for galaxies all other data was obtained from the UCI repository [13]. <b>Iris</b> dataset contains 150 cases in 3 classes. After initialization with Gaussian functions including rotations only 4 classification errors are made (97.3% accuracy), which is a better results than many", "mykey":778},
 {"datasetID":74, "supportID":"55CA7A12A4E48E0101D240880BB8C025D62E2253", "rexaID":"c2fee20b32fddc929738addedddbe57ccb74800e", "author":"Hendrik Blockeel and Luc De Raedt", "title":"Top-down Induction of Logical Decision Trees", "venue":"Katholieke Universiteit Leuven Department of Computer Science", "year":"", "window":"Dougherty's [DKS95] work, and as such is capable of handling numerical data. For more details see [VLDDR96, BDR97b]. 5 Experimental Evaluation Experiments have been performed on several benchmark datasets: Mutagenesis [SMSK96], <b>Musk</b> [DLLP97, MM96], and Diterpenes [DSKH + 96]. For all the experiments, Tilde's default parameters were used; only the choice of the number of thresholds for discretization", "mykey":779},
 {"datasetID":75, "supportID":"55CA7A12A4E48E0101D240880BB8C025D62E2253", "rexaID":"c2fee20b32fddc929738addedddbe57ccb74800e", "author":"Hendrik Blockeel and Luc De Raedt", "title":"Top-down Induction of Logical Decision Trees", "venue":"Katholieke Universiteit Leuven Department of Computer Science", "year":"", "window":"Dougherty's [DKS95] work, and as such is capable of handling numerical data. For more details see [VLDDR96, BDR97b]. 5 Experimental Evaluation Experiments have been performed on several benchmark datasets: Mutagenesis [SMSK96], <b>Musk</b> [DLLP97, MM96], and Diterpenes [DSKH + 96]. For all the experiments, Tilde's default parameters were used; only the choice of the number of thresholds for discretization", "mykey":780},
 {"datasetID":81, "supportID":"561C40AC0091D15185FF9435B69D6C1FEB299749", "rexaID":"e76ede4369e0fb1cdbdae67f131c89b40f76e74c", "author":"Georg Thimm and Emile Fiesler", "title":"High Order and Multilayer Perceptron Initialization", "venue":"", "year":"", "window":"in the same row or the same column in the image. This configuration should allow the extraction of sufficient features to learn the <b>digits</b>  Training sessions on the in section IV-A described digits data set gave an acceptable <b>recognition</b> of untrained digits, despite the small training set used. The three different initial random weight distributions used are: uniform on the interval [Gamma a; a] (with", "mykey":781},
 {"datasetID":87, "supportID":"561C40AC0091D15185FF9435B69D6C1FEB299749", "rexaID":"e76ede4369e0fb1cdbdae67f131c89b40f76e74c", "author":"Georg Thimm and Emile Fiesler", "title":"High Order and Multilayer Perceptron Initialization", "venue":"", "year":"", "window":"itself has a large influence on the optimal initial weight variance: for the solar, wine, and <b>servo</b> data sets, the networks have about the same size for the same order, but the optimal value for the weight variance differs a lot for the network with the logistic activation function. Further, the optimal", "mykey":782},
 {"datasetID":109, "supportID":"561C40AC0091D15185FF9435B69D6C1FEB299749", "rexaID":"e76ede4369e0fb1cdbdae67f131c89b40f76e74c", "author":"Georg Thimm and Emile Fiesler", "title":"High Order and Multilayer Perceptron Initialization", "venue":"", "year":"", "window":"itself has a large influence on the optimal initial weight variance: for the solar, <b>wine</b>  and servo data sets, the networks have about the same size for the same order, but the optimal value for the weight variance differs a lot for the network with the logistic activation function. Further, the optimal", "mykey":783},
 {"datasetID":53, "supportID":"5620F776DF74AA076EDC3E8AFEB60EDDFB37E139", "rexaID":"cde93f878ecbecffbd0b885d62b8fcd32fcabdb5", "author":"Ke Wang and Han Chong Goh", "title":"Minimum Splits Based Discretization for Continuous Features", "venue":"IJCAI (2)", "year":"1997", "window":"but never explored multi-way split of a continuous feature, making the simple structure disappear. Consider the following two decision trees built in one of the 10fold cross validation on <b>Iris</b> dataset. The first tree is produced by the multi-way split proposed in this paper, and the second by C4.5. Though both trees have the same size and same error rate on test data, the first tree classifies", "mykey":784},
 {"datasetID":14, "supportID":"56CE04B8F5426AFE91E53D474B5BE5876F4266E2", "rexaID":"ca3e1e0bf335a97cedb76be7b64610181e0f6684", "author":"Kristin P. Bennett and Ayhan Demiriz and Richard Maclin", "title":"Exploiting unlabeled data in ensemble methods", "venue":"KDD", "year":"2002", "window":"experiments we used simple multilayer perceptrons with a single layer of hidden units. The networks were trained using backpropagation with a learning rate of 0.15 and a momentum value of 0.90. The datasets for the experiments are <b>breast</b> <b>cancer</b> wisconsin, pima-indians diabetes, and letter-recognition drawn from the UCI Machine Learning repository [3]. The number of units in the hidden layer for the", "mykey":785},
 {"datasetID":17, "supportID":"56CE04B8F5426AFE91E53D474B5BE5876F4266E2", "rexaID":"ca3e1e0bf335a97cedb76be7b64610181e0f6684", "author":"Kristin P. Bennett and Ayhan Demiriz and Richard Maclin", "title":"Exploiting unlabeled data in ensemble methods", "venue":"KDD", "year":"2002", "window":"experiments we used simple multilayer perceptrons with a single layer of hidden units. The networks were trained using backpropagation with a learning rate of 0.15 and a momentum value of 0.90. The datasets for the experiments are <b>breast</b> <b>cancer</b> <b>wisconsin</b>  pima-indians diabetes, and letter-recognition drawn from the UCI Machine Learning repository [3]. The number of units in the hidden layer for the", "mykey":786},
 {"datasetID":15, "supportID":"56CE04B8F5426AFE91E53D474B5BE5876F4266E2", "rexaID":"ca3e1e0bf335a97cedb76be7b64610181e0f6684", "author":"Kristin P. Bennett and Ayhan Demiriz and Richard Maclin", "title":"Exploiting unlabeled data in ensemble methods", "venue":"KDD", "year":"2002", "window":"experiments we used simple multilayer perceptrons with a single layer of hidden units. The networks were trained using backpropagation with a learning rate of 0.15 and a momentum value of 0.90. The datasets for the experiments are <b>breast</b> <b>cancer</b> <b>wisconsin</b>  pima-indians diabetes, and letter-recognition drawn from the UCI Machine Learning repository [3]. The number of units in the hidden layer for the", "mykey":787},
 {"datasetID":16, "supportID":"56CE04B8F5426AFE91E53D474B5BE5876F4266E2", "rexaID":"ca3e1e0bf335a97cedb76be7b64610181e0f6684", "author":"Kristin P. Bennett and Ayhan Demiriz and Richard Maclin", "title":"Exploiting unlabeled data in ensemble methods", "venue":"KDD", "year":"2002", "window":"experiments we used simple multilayer perceptrons with a single layer of hidden units. The networks were trained using backpropagation with a learning rate of 0.15 and a momentum value of 0.90. The datasets for the experiments are <b>breast</b> <b>cancer</b> <b>wisconsin</b>  pima-indians diabetes, and letter-recognition drawn from the UCI Machine Learning repository [3]. The number of units in the hidden layer for the", "mykey":788},
 {"datasetID":34, "supportID":"56CE04B8F5426AFE91E53D474B5BE5876F4266E2", "rexaID":"ca3e1e0bf335a97cedb76be7b64610181e0f6684", "author":"Kristin P. Bennett and Ayhan Demiriz and Richard Maclin", "title":"Exploiting unlabeled data in ensemble methods", "venue":"KDD", "year":"2002", "window":"for the experiments are breast-cancer-wisconsin, pima-indians <b>diabetes</b>  and letter-recognition drawn from the UCI Machine Learning repository [3]. The number of units in the hidden layer for the datasets was 5 for the breast-cancer and diabetes datasets and 40 in the letter-recognition dataset. The number of training epochs was set to 20 for breastcancer, 30 for diabetes, and 30 for letter", "mykey":789},
 {"datasetID":59, "supportID":"56CE04B8F5426AFE91E53D474B5BE5876F4266E2", "rexaID":"ca3e1e0bf335a97cedb76be7b64610181e0f6684", "author":"Kristin P. Bennett and Ayhan Demiriz and Richard Maclin", "title":"Exploiting unlabeled data in ensemble methods", "venue":"KDD", "year":"2002", "window":"ufraction 0.10 superv ufraction 0.10 semi_sup ufraction 0.25 superv ufraction 0.25 semi_sup ufraction 0.5 superv ufraction 0.5 semi_sup Figure 1: Neural network results for the <b>Letter Recognition</b> dataset using networks with 5, 10 and 20 hidden units. Results shown are for 10, 25, and 50 percent of the data marked as unlabeled (ufractions of 0.10, 0.25, and 0.5) for AdaBoost (superv) and ASSEMBLE", "mykey":790},
 {"datasetID":7, "supportID":"572B0456280CD4137BC104D60AED08DEAC6AAEC5", "rexaID":"7125dc87ceddf952f1840074b40ac0526dcc03dc", "author":"Marcus Hutter and Marco Zaffalon", "title":"Distribution of Mutual Information from Complete and Incomplete Data", "venue":"CoRR, csLG/0403025", "year":"2004", "window":"the traditional filter F, as well as the naive Bayes classifier, are now computed using the empirical probabilities (20). The remaining implementation details are as in the case of complete data. Data set #feat. FF F BF <b>Audiology</b> 69 64.3 68.0 68.7 Crx 15 9.7 12.6 13.8 Horse-colic 18 11.8 16.1 17.4 Hypothyroidloss 23 4.3 8.3 13.2 Soybean-large 35 34.2 35.0 35.0 Table 4: Average number of attributes", "mykey":791},
 {"datasetID":8, "supportID":"572B0456280CD4137BC104D60AED08DEAC6AAEC5", "rexaID":"7125dc87ceddf952f1840074b40ac0526dcc03dc", "author":"Marcus Hutter and Marco Zaffalon", "title":"Distribution of Mutual Information from Complete and Incomplete Data", "venue":"CoRR, csLG/0403025", "year":"2004", "window":"the traditional filter F, as well as the naive Bayes classifier, are now computed using the empirical probabilities (20). The remaining implementation details are as in the case of complete data. Data set #feat. FF F BF <b>Audiology</b> 69 64.3 68.0 68.7 Crx 15 9.7 12.6 13.8 Horse-colic 18 11.8 16.1 17.4 Hypothyroidloss 23 4.3 8.3 13.2 Soybean-large 35 34.2 35.0 35.0 Table 4: Average number of attributes", "mykey":792},
 {"datasetID":23, "supportID":"572B0456280CD4137BC104D60AED08DEAC6AAEC5", "rexaID":"7125dc87ceddf952f1840074b40ac0526dcc03dc", "author":"Marcus Hutter and Marco Zaffalon", "title":"Distribution of Mutual Information from Complete and Incomplete Data", "venue":"CoRR, csLG/0403025", "year":"2004", "window":"600 900 1200 1500 1800 2100 2400 2700 3000 Instance number Prediction accuracy  <b>Chess</b>  FF F Figure 2: Comparison of the prediction accuracies of the naive Bayes with filters F and FF on the Chess data set. The gray area denotes di\u00aeerences that are not statistically significant. The remaining cases are described by means of the following figures. Figure 2 shows that FF allowed the naive Bayes to", "mykey":793},
 {"datasetID":21, "supportID":"572B0456280CD4137BC104D60AED08DEAC6AAEC5", "rexaID":"7125dc87ceddf952f1840074b40ac0526dcc03dc", "author":"Marcus Hutter and Marco Zaffalon", "title":"Distribution of Mutual Information from Complete and Incomplete Data", "venue":"CoRR, csLG/0403025", "year":"2004", "window":"600 900 1200 1500 1800 2100 2400 2700 3000 Instance number Prediction accuracy  <b>Chess</b>  FF F Figure 2: Comparison of the prediction accuracies of the naive Bayes with filters F and FF on the Chess data set. The gray area denotes di\u00aeerences that are not statistically significant. The remaining cases are described by means of the following figures. Figure 2 shows that FF allowed the naive Bayes to", "mykey":794},
 {"datasetID":22, "supportID":"572B0456280CD4137BC104D60AED08DEAC6AAEC5", "rexaID":"7125dc87ceddf952f1840074b40ac0526dcc03dc", "author":"Marcus Hutter and Marco Zaffalon", "title":"Distribution of Mutual Information from Complete and Incomplete Data", "venue":"CoRR, csLG/0403025", "year":"2004", "window":"600 900 1200 1500 1800 2100 2400 2700 3000 Instance number Prediction accuracy  <b>Chess</b>  FF F Figure 2: Comparison of the prediction accuracies of the naive Bayes with filters F and FF on the Chess data set. The gray area denotes di\u00aeerences that are not statistically significant. The remaining cases are described by means of the following figures. Figure 2 shows that FF allowed the naive Bayes to", "mykey":795},
 {"datasetID":63, "supportID":"572B0456280CD4137BC104D60AED08DEAC6AAEC5", "rexaID":"7125dc87ceddf952f1840074b40ac0526dcc03dc", "author":"Marcus Hutter and Marco Zaffalon", "title":"Distribution of Mutual Information from Complete and Incomplete Data", "venue":"CoRR, csLG/0403025", "year":"2004", "window":"on a number of di\u00aeerent domains. For example, Shuttle-small reports data on diagnosing failures of the space shuttle; <b>Lymphography</b> and Hypothyroid are medical data sets; Spam is a body of e-mails that can be spam or non-spam; etc. The data sets presenting non-categorical features have been pre-discretized by MLC++ [KJL + 94], default options, i.e. by the common", "mykey":796},
 {"datasetID":23, "supportID":"5762BBBC00F70B47EEF26D212556BC8A1B94193F", "rexaID":"111c890abe7dc9f26523d1c59b7a75a04b69256b", "author":"Jinyan Li and Guozhu Dong and Kotagiri Ramamohanarao", "title":"Instance-Based Classification by Emerging Patterns", "venue":"PKDD", "year":"2000", "window":"(as explained in [10]). Note that for the datasets such as <b>chess</b>  flare, nursery, splice, mushroom, voting, soybean-l, t-t-t, and zoo which do not contain any continuous attributes, DeEPs does not need ff. Columns 5, 6, 7, 8, and 9 give the", "mykey":797},
 {"datasetID":21, "supportID":"5762BBBC00F70B47EEF26D212556BC8A1B94193F", "rexaID":"111c890abe7dc9f26523d1c59b7a75a04b69256b", "author":"Jinyan Li and Guozhu Dong and Kotagiri Ramamohanarao", "title":"Instance-Based Classification by Emerging Patterns", "venue":"PKDD", "year":"2000", "window":"(as explained in [10]). Note that for the datasets such as <b>chess</b>  flare, nursery, splice, mushroom, voting, soybean-l, t-t-t, and zoo which do not contain any continuous attributes, DeEPs does not need ff. Columns 5, 6, 7, 8, and 9 give the", "mykey":798},
 {"datasetID":22, "supportID":"5762BBBC00F70B47EEF26D212556BC8A1B94193F", "rexaID":"111c890abe7dc9f26523d1c59b7a75a04b69256b", "author":"Jinyan Li and Guozhu Dong and Kotagiri Ramamohanarao", "title":"Instance-Based Classification by Emerging Patterns", "venue":"PKDD", "year":"2000", "window":"(as explained in [10]). Note that for the datasets such as <b>chess</b>  flare, nursery, splice, mushroom, voting, soybean-l, t-t-t, and zoo which do not contain any continuous attributes, DeEPs does not need ff. Columns 5, 6, 7, 8, and 9 give the", "mykey":799},
 {"datasetID":69, "supportID":"5762BBBC00F70B47EEF26D212556BC8A1B94193F", "rexaID":"111c890abe7dc9f26523d1c59b7a75a04b69256b", "author":"Jinyan Li and Guozhu Dong and Kotagiri Ramamohanarao", "title":"Instance-Based Classification by Emerging Patterns", "venue":"PKDD", "year":"2000", "window":"(as explained in [10]). Note that for the datasets such as chess, flare, nursery, <b>splice</b>  mushroom, voting, soybean-l, t-t-t, and zoo which do not contain any continuous attributes, DeEPs does not need ff. Columns 5, 6, 7, 8, and 9 give the", "mykey":800},
 {"datasetID":73, "supportID":"5762BBBC00F70B47EEF26D212556BC8A1B94193F", "rexaID":"111c890abe7dc9f26523d1c59b7a75a04b69256b", "author":"Jinyan Li and Guozhu Dong and Kotagiri Ramamohanarao", "title":"Instance-Based Classification by Emerging Patterns", "venue":"PKDD", "year":"2000", "window":"substantially reduced since itemsets T `` P i are frequently contained in some other itemsets T `` P j . Then, R p can be viewed as a compressed D p , and Rn a compressed Dn . We use the <b>mushroom</b> dataset to demonstrate this point. The original mushroom data has a volume of 3788 edible training instances, with 22 attributes per instance. The average number of items (or length) of the 3788 processed", "mykey":801},
 {"datasetID":76, "supportID":"5762BBBC00F70B47EEF26D212556BC8A1B94193F", "rexaID":"111c890abe7dc9f26523d1c59b7a75a04b69256b", "author":"Jinyan Li and Guozhu Dong and Kotagiri Ramamohanarao", "title":"Instance-Based Classification by Emerging Patterns", "venue":"PKDD", "year":"2000", "window":"(as explained in [10]). Note that for the datasets such as chess, flare, <b>nursery</b>  splice, mushroom, voting, soybean-l, t-t-t, and zoo which do not contain any continuous attributes, DeEPs does not need ff. Columns 5, 6, 7, 8, and 9 give the", "mykey":802},
 {"datasetID":89, "supportID":"5762BBBC00F70B47EEF26D212556BC8A1B94193F", "rexaID":"111c890abe7dc9f26523d1c59b7a75a04b69256b", "author":"Jinyan Li and Guozhu Dong and Kotagiri Ramamohanarao", "title":"Instance-Based Classification by Emerging Patterns", "venue":"PKDD", "year":"2000", "window":"(as explained in [10]). Note that for the datasets such as chess, <b>flare</b>  nursery, splice, mushroom, voting, soybean-l, t-t-t, and zoo which do not contain any continuous attributes, DeEPs does not need ff. Columns 5, 6, 7, 8, and 9 give the", "mykey":803},
 {"datasetID":52, "supportID":"589FD8767056ED210C05CF7BC5605F108C695CCD", "rexaID":"fe52c7b7465f378427ac09aa5480dd559ff5f569", "author":"Christos Emmanouilidis and Anthony Hunter", "title":"A Comparison of Crossover Operators in Neural Network Feature Selection with Multiobjective Evolutionary Algorithms", "venue":"Centre for Adaptive Systems, School of Computing, Engineering and Technology University of Sunderland", "year":"", "window":"it reduces the effect of the noise in fitness evaluation. 4 EXPERIMENTAL INVESTIGATION We compare the performance of the SSOCF operator against that of standard n-point crossover on a benchmarking data set of considerable dimensionality, the <b>ionosphere</b> dataset [13]. It consists of 351 patterns, with 34 attributes and one output with two classes. Ten random permutations of this data set are employed.", "mykey":804},
 {"datasetID":51, "supportID":"58E03FDBC096684DD3185EC538E082DC6CC6A464", "rexaID":"1ddaa5b4eba8093faad88a5426ec586c6ac63f6e", "author":"Dmitriy Fradkin and David Madigan", "title":"Experiments with random projections for machine learning", "venue":"KDD", "year":"2003", "window":"that we have used in our experiments. Ionosphere, Spambase and <b>Internet</b> <b>Ads</b> were taken from UCI repository [5]. Datasets Colon and Leukemia were first used in [3] and [10] respectfully. Datasets are used without modifications, except for the Ads dataset that originally contained 3 more attributes with missing values.", "mykey":805},
 {"datasetID":52, "supportID":"58E03FDBC096684DD3185EC538E082DC6CC6A464", "rexaID":"1ddaa5b4eba8093faad88a5426ec586c6ac63f6e", "author":"Dmitriy Fradkin and David Madigan", "title":"Experiments with random projections for machine learning", "venue":"KDD", "year":"2003", "window":"bounds on the quality of randomized dimensionality reduction. Engebretsen, Indyk and O'Donnell [9] present a deterministic algorithm for constructing mappings of the type Table 1: Description of Datasets Name # Instances # Attributes <b>Ionosphere</b> 351 34 Colon 62 2000 Leukemia 72 3571 Spam 4601 57 Ads 3279 1554 described in the JL lemma (by use of the method of conditional probabilities) and use it to", "mykey":806},
 {"datasetID":14, "supportID":"596CB9844E5321DD0023BCCD9C0B978555A3E9FE", "rexaID":"c3f9c3303aa080beec901b74703cef88ee2b2f24", "author":"Wl odzisl and Rafal Adamczak and Krzysztof Grabczewski and Grzegorz Zal", "title":"A hybrid method for extraction of logical rules from data", "venue":"Department of Computer Methods, Nicholas Copernicus University", "year":"", "window":"obtained from the UCI repository [14]. A. Wisconsin <b>breast</b> <b>cancer</b> data. The Wisconsin cancer dataset [17] contains 699 instances, with 458 benign (65.5%) and 241 (34.5%) malignant cases. Each instance is described by the case number, 9 attributes with integer value in the range 1-10 (for example,", "mykey":807},
 {"datasetID":17, "supportID":"596CB9844E5321DD0023BCCD9C0B978555A3E9FE", "rexaID":"c3f9c3303aa080beec901b74703cef88ee2b2f24", "author":"Wl odzisl and Rafal Adamczak and Krzysztof Grabczewski and Grzegorz Zal", "title":"A hybrid method for extraction of logical rules from data", "venue":"Department of Computer Methods, Nicholas Copernicus University", "year":"", "window":"obtained from the UCI repository [14]. A. <b>Wisconsin</b> <b>breast</b> <b>cancer</b> data. The Wisconsin cancer dataset [17] contains 699 instances, with 458 benign (65.5%) and 241 (34.5%) malignant cases. Each instance is described by the case number, 9 attributes with integer value in the range 1-10 (for example,", "mykey":808},
 {"datasetID":15, "supportID":"596CB9844E5321DD0023BCCD9C0B978555A3E9FE", "rexaID":"c3f9c3303aa080beec901b74703cef88ee2b2f24", "author":"Wl odzisl and Rafal Adamczak and Krzysztof Grabczewski and Grzegorz Zal", "title":"A hybrid method for extraction of logical rules from data", "venue":"Department of Computer Methods, Nicholas Copernicus University", "year":"", "window":"obtained from the UCI repository [14]. A. <b>Wisconsin</b> <b>breast</b> <b>cancer</b> data. The Wisconsin cancer dataset [17] contains 699 instances, with 458 benign (65.5%) and 241 (34.5%) malignant cases. Each instance is described by the case number, 9 attributes with integer value in the range 1-10 (for example,", "mykey":809},
 {"datasetID":16, "supportID":"596CB9844E5321DD0023BCCD9C0B978555A3E9FE", "rexaID":"c3f9c3303aa080beec901b74703cef88ee2b2f24", "author":"Wl odzisl and Rafal Adamczak and Krzysztof Grabczewski and Grzegorz Zal", "title":"A hybrid method for extraction of logical rules from data", "venue":"Department of Computer Methods, Nicholas Copernicus University", "year":"", "window":"obtained from the UCI repository [14]. A. <b>Wisconsin</b> <b>breast</b> <b>cancer</b> data. The Wisconsin cancer dataset [17] contains 699 instances, with 458 benign (65.5%) and 241 (34.5%) malignant cases. Each instance is described by the case number, 9 attributes with integer value in the range 1-10 (for example,", "mykey":810},
 {"datasetID":45, "supportID":"596CB9844E5321DD0023BCCD9C0B978555A3E9FE", "rexaID":"c3f9c3303aa080beec901b74703cef88ee2b2f24", "author":"Wl odzisl and Rafal Adamczak and Krzysztof Grabczewski and Grzegorz Zal", "title":"A hybrid method for extraction of logical rules from data", "venue":"Department of Computer Methods, Nicholas Copernicus University", "year":"", "window":"2 ! 9 ^ f 5 ! 6 ^ f 7 ! 9 ^ f 8 ! 5 R 7 ) f 2 ! 9 ^ f 4 ! 6 ^ f 5 ! 8 ^ f 7 ! 9 R 8 ) f 2 =6^ f 4 ! 10 ^ f 5 ! 10 ^ f 7 ! 2 ^ f 8 ! 9 B. The Cleveland <b>heart</b> disease data. The Cleveland heart disease dataset [14] (collected at V.A. Medical Center, Long Beach and Cleveland Clinic Foundation by R. Detrano) contains 303 instances, with 164 healthy (54.1%) instances, the rest are heart disease instances of", "mykey":811},
 {"datasetID":53, "supportID":"596CB9844E5321DD0023BCCD9C0B978555A3E9FE", "rexaID":"c3f9c3303aa080beec901b74703cef88ee2b2f24", "author":"Wl odzisl and Rafal Adamczak and Krzysztof Grabczewski and Grzegorz Zal", "title":"A hybrid method for extraction of logical rules from data", "venue":"Department of Computer Methods, Nicholas Copernicus University", "year":"", "window":"for benchmark applications were taken from the UCI machine learning repository [14]. Application of the constructive MLP2LN approach to the classical <b>Iris</b> dataset was already presented in detail [15], therefore only new aspects related to the hybrid method are discussed here. The Iris data has 150 vectors evenly distributed in three iris-setosa,", "mykey":812},
 {"datasetID":70, "supportID":"596CB9844E5321DD0023BCCD9C0B978555A3E9FE", "rexaID":"c3f9c3303aa080beec901b74703cef88ee2b2f24", "author":"Wl odzisl and Rafal Adamczak and Krzysztof Grabczewski and Grzegorz Zal", "title":"A hybrid method for extraction of logical rules from data", "venue":"Department of Computer Methods, Nicholas Copernicus University", "year":"", "window":"this hybrid method the simplest logical description for several benchmark problems (Iris, mushroom) has been found. Very good solutions were obtained for the three <b>monk</b> problems. For many medical datasets (only 3 were shown here) very simple and highly accurate results were obtained. It is not quite clear why logical rules work so well, for example in the hypothyroidor the Wisconsin breast cancer", "mykey":813},
 {"datasetID":73, "supportID":"596CB9844E5321DD0023BCCD9C0B978555A3E9FE", "rexaID":"c3f9c3303aa080beec901b74703cef88ee2b2f24", "author":"Wl odzisl and Rafal Adamczak and Krzysztof Grabczewski and Grzegorz Zal", "title":"A hybrid method for extraction of logical rules from data", "venue":"Department of Computer Methods, Nicholas Copernicus University", "year":"", "window":"be replaced by \"population=clustered\". This is the simplest systematic logical description (some of these rules have probably been also found by the RULEX and TREX algorithms [3]) of the <b>mushroom</b> dataset that we know of and therefore should be used as benchmark for other rule extraction methods. We have also solved the three monk problems [16]. For the Monks 1 problem one additional neuron handling", "mykey":814},
 {"datasetID":2, "supportID":"59FEEC238B33E967FA09143593E7701F5B7D491E", "rexaID":"94cf84d5a454c97d326efdb24b2b96e0eac25c33", "author":"Kristin P. Bennett and Ayhan Demiriz and John Shawe-Taylor", "title":"A Column Generation Algorithm For Boosting", "venue":"ICML", "year":"2000", "window":"all points and # i measures the additional margin obtained by each point. AdaBoost also minimizes a margin cost function based on the margin obtained by each point. We ran experiments on two larger datasets: Forest and <b>Adult</b>  from UCI(Murphy & Aha, 1992). Forest is a 54-dimension dataset with 7 possible classes. The data are divided into 11340 training, 3780 validation and 565892 testing instances.", "mykey":815},
 {"datasetID":14, "supportID":"59FEEC238B33E967FA09143593E7701F5B7D491E", "rexaID":"94cf84d5a454c97d326efdb24b2b96e0eac25c33", "author":"Kristin P. Bennett and Ayhan Demiriz and John Shawe-Taylor", "title":"A Column Generation Algorithm For Boosting", "venue":"ICML", "year":"2000", "window":"LPBoost has a well defined stopping criterion that is reached in a few iterations. It uses few weak learners. There are only 81 possible stumps on the <b>Breast</b> <b>Cancer</b> dataset (9 attributes having 9 possible values), so clearly AdaBoost may require the same tree to be generated multiple times. LPBoost generates a weak learner only once and can alter the weight on that", "mykey":816},
 {"datasetID":151, "supportID":"59FEEC238B33E967FA09143593E7701F5B7D491E", "rexaID":"94cf84d5a454c97d326efdb24b2b96e0eac25c33", "author":"Kristin P. Bennett and Ayhan Demiriz and John Shawe-Taylor", "title":"A Column Generation Algorithm For Boosting", "venue":"ICML", "year":"2000", "window":"where the base learner solves (6) exactly, then to examine LPBoost in a more realistic environment. 5.1 Boosting Decision Tree Stumps We used decision tree stumps as a base learner on six UCI datasets: Cancer (9,699), Heart (13,297), <b>Sonar</b> (60,208), Ionosphere (34,351), Diagnostic (30,569), and Musk (166,476). The number of features and number of points in each dataset are shown in parentheses", "mykey":817},
 {"datasetID":45, "supportID":"59FEEC238B33E967FA09143593E7701F5B7D491E", "rexaID":"94cf84d5a454c97d326efdb24b2b96e0eac25c33", "author":"Kristin P. Bennett and Ayhan Demiriz and John Shawe-Taylor", "title":"A Column Generation Algorithm For Boosting", "venue":"ICML", "year":"2000", "window":"where the base learner solves (6) exactly, then to examine LPBoost in a more realistic environment. 5.1 Boosting Decision Tree Stumps We used decision tree stumps as a base learner on six UCI datasets: Cancer (9,699), <b>Heart</b> (13,297), Sonar (60,208), Ionosphere (34,351), Diagnostic (30,569), and Musk (166,476). The number of features and number of points in each dataset are shown in parentheses", "mykey":818},
 {"datasetID":14, "supportID":"5A10009D3F14FB95905A28D4E0929F10ABADED75", "rexaID":"7265efd898e4c045ff078fcb63fec9fbde4b1249", "author":"Endre Boros and Peter Hammer and Toshihide Ibaraki and Alexander Kogan and Eddy Mayoraz and Ilya B. Muchnik", "title":"An Implementation of Logical Analysis of Data", "venue":"IEEE Trans. Knowl. Data Eng, 12", "year":"2000", "window":"the housing value is above or below the median. Using training sets of 80% of the observations, [16] reports correct prediction rates ranging from 82% to 83.2%. <b>Breast</b> <b>Cancer</b> (Wisconsin). The dataset, compiled by O. Mangasarian and K.P. Bennett, is widely used in the machine learning community for comparing learning algorithms. It is, however, difficult to use it for rigorous comparisons since", "mykey":819},
 {"datasetID":17, "supportID":"5A10009D3F14FB95905A28D4E0929F10ABADED75", "rexaID":"7265efd898e4c045ff078fcb63fec9fbde4b1249", "author":"Endre Boros and Peter Hammer and Toshihide Ibaraki and Alexander Kogan and Eddy Mayoraz and Ilya B. Muchnik", "title":"An Implementation of Logical Analysis of Data", "venue":"IEEE Trans. Knowl. Data Eng, 12", "year":"2000", "window":"the housing value is above or below the median. Using training sets of 80% of the observations, [16] reports correct prediction rates ranging from 82% to 83.2%. <b>Breast</b> <b>Cancer</b>  <b>Wisconsin</b> . The dataset, compiled by O. Mangasarian and K.P. Bennett, is widely used in the machine learning community for comparing learning algorithms. It is, however, difficult to use it for rigorous comparisons since", "mykey":820},
 {"datasetID":15, "supportID":"5A10009D3F14FB95905A28D4E0929F10ABADED75", "rexaID":"7265efd898e4c045ff078fcb63fec9fbde4b1249", "author":"Endre Boros and Peter Hammer and Toshihide Ibaraki and Alexander Kogan and Eddy Mayoraz and Ilya B. Muchnik", "title":"An Implementation of Logical Analysis of Data", "venue":"IEEE Trans. Knowl. Data Eng, 12", "year":"2000", "window":"the housing value is above or below the median. Using training sets of 80% of the observations, [16] reports correct prediction rates ranging from 82% to 83.2%. <b>Breast</b> <b>Cancer</b>  <b>Wisconsin</b> . The dataset, compiled by O. Mangasarian and K.P. Bennett, is widely used in the machine learning community for comparing learning algorithms. It is, however, difficult to use it for rigorous comparisons since", "mykey":821},
 {"datasetID":16, "supportID":"5A10009D3F14FB95905A28D4E0929F10ABADED75", "rexaID":"7265efd898e4c045ff078fcb63fec9fbde4b1249", "author":"Endre Boros and Peter Hammer and Toshihide Ibaraki and Alexander Kogan and Eddy Mayoraz and Ilya B. Muchnik", "title":"An Implementation of Logical Analysis of Data", "venue":"IEEE Trans. Knowl. Data Eng, 12", "year":"2000", "window":"the housing value is above or below the median. Using training sets of 80% of the observations, [16] reports correct prediction rates ranging from 82% to 83.2%. <b>Breast</b> <b>Cancer</b>  <b>Wisconsin</b> . The dataset, compiled by O. Mangasarian and K.P. Bennett, is widely used in the machine learning community for comparing learning algorithms. It is, however, difficult to use it for rigorous comparisons since", "mykey":822},
 {"datasetID":34, "supportID":"5A10009D3F14FB95905A28D4E0929F10ABADED75", "rexaID":"7265efd898e4c045ff078fcb63fec9fbde4b1249", "author":"Endre Boros and Peter Hammer and Toshihide Ibaraki and Alexander Kogan and Eddy Mayoraz and Ilya B. Muchnik", "title":"An Implementation of Logical Analysis of Data", "venue":"IEEE Trans. Knowl. Data Eng, 12", "year":"2000", "window":"consistent. According to [12], correct predictions rates reported in the literature about this dataset range from 84% to 95.6%. <b>Diabetes</b>  This dataset, compiled by the National Institute of Diabetes and Digestive and Kidney Diseases, was contributed to the repository by V. Sigillito. The dataset", "mykey":823},
 {"datasetID":45, "supportID":"5A10009D3F14FB95905A28D4E0929F10ABADED75", "rexaID":"7265efd898e4c045ff078fcb63fec9fbde4b1249", "author":"Endre Boros and Peter Hammer and Toshihide Ibaraki and Alexander Kogan and Eddy Mayoraz and Ilya B. Muchnik", "title":"An Implementation of Logical Analysis of Data", "venue":"IEEE Trans. Knowl. Data Eng, 12", "year":"2000", "window":"rates ranging from 71.4% to 74.4%. Further, [19] reports a 76% correct prediction rate using 75% of the data for training. <b>heart</b> Disease (Cleveland). The Cleveland Clinic Foundation heart disease dataset, contributed to the repository by R. Detrano, contains 303 observations, 165 of which describe healthy people and 138 sick ones; 7 observations are incomplete, and 2 of the observations of healthy", "mykey":824},
 {"datasetID":48, "supportID":"5A10009D3F14FB95905A28D4E0929F10ABADED75", "rexaID":"7265efd898e4c045ff078fcb63fec9fbde4b1249", "author":"Endre Boros and Peter Hammer and Toshihide Ibaraki and Alexander Kogan and Eddy Mayoraz and Ilya B. Muchnik", "title":"An Implementation of Logical Analysis of Data", "venue":"IEEE Trans. Knowl. Data Eng, 12", "year":"2000", "window":"in 85.51% of the cases. Furthermore, it is interesting to notice that the inclusion of additional patterns in the discriminant does not seem to improve the prediction accuracy. Boston <b>Housing</b>  This dataset was created by D. Harrison and D. Rubinfeld in 1978 and contains 506 records describing housing values in the suburbs of Boston, depending on observations consisting of one binary and 12 continuous", "mykey":825},
 {"datasetID":56, "supportID":"5A10009D3F14FB95905A28D4E0929F10ABADED75", "rexaID":"7265efd898e4c045ff078fcb63fec9fbde4b1249", "author":"Endre Boros and Peter Hammer and Toshihide Ibaraki and Alexander Kogan and Eddy Mayoraz and Ilya B. Muchnik", "title":"An Implementation of Logical Analysis of Data", "venue":"IEEE Trans. Knowl. Data Eng, 12", "year":"2000", "window":"several equivalent tests involving different items, as a way of verifying the validity of that patient's answers. 9.3 <b>labor</b> Productivity in China 4 Using the official China Statistical Yearbooks, a dataset was compiled in [8, 7] for the analysis of labor productivity in the developing Chinese economy. The dataset provides 290 annual observations for the 29 Chinese provinces for the period 1985 --", "mykey":826},
 {"datasetID":143, "supportID":"5A10009D3F14FB95905A28D4E0929F10ABADED75", "rexaID":"7265efd898e4c045ff078fcb63fec9fbde4b1249", "author":"Endre Boros and Peter Hammer and Toshihide Ibaraki and Alexander Kogan and Eddy Mayoraz and Ilya B. Muchnik", "title":"An Implementation of Logical Analysis of Data", "venue":"IEEE Trans. Knowl. Data Eng, 12", "year":"2000", "window":"from the Irvine repository which were used in our experiments. <b>Australian Credit</b> Card. The dataset, submitted to the repository by J. Quinlan, consists of 690 records of MasterCard applicants, 307 of which are classified as positive and 383 as negative. While 37 records have some missing data,", "mykey":827},
 {"datasetID":14, "supportID":"5A13D7532F186BB8D3B04A802BCD657FD4B2A027", "rexaID":"021a415dcb129689411af1d4b233ec48a090aae6", "author":"Sally A. Goldman and Yan Zhou", "title":"Enhancing Supervised Learning with Unlabeled Data", "venue":"ICML", "year":"2000", "window":"just the initial labeled data (i.e. round 0). Our cotraining procedure helped both algorithms to improve their performance. Figure 2 shows the results from one of our runs using the <b>breast</b> <b>cancer</b> data set. In this data set ID3 had the better performance. Again (as we generally see), both hypotheses were improved by the co-training. 0 1 2 3 Number of co-training rounds 0.21 0.22 0.23 0.24 0.25 Error", "mykey":828},
 {"datasetID":89, "supportID":"5A13D7532F186BB8D3B04A802BCD657FD4B2A027", "rexaID":"021a415dcb129689411af1d4b233ec48a090aae6", "author":"Sally A. Goldman and Yan Zhou", "title":"Enhancing Supervised Learning with Unlabeled Data", "venue":"ICML", "year":"2000", "window":"we used for our empirical tests as well as a summary of our results. Figure 1 shows the results from one of our runs using the <b>Flare</b> data in graphical form. For this data set HOODG performed better than ID3 when using just the initial labeled data (i.e. round 0). Our cotraining procedure helped both algorithms to improve their performance. Figure 2 shows the results from", "mykey":829},
 {"datasetID":80, "supportID":"5A5E6B5CC1E62AC7AEABFF6F4DC1AA7E1CCB0AE9", "rexaID":"b60b36ac9d879270bfd990b761897f1f168612b7", "author":"Claudio Gentile", "title":"A New Approximate Maximal Margin Classification Algorithm", "venue":"NIPS", "year":"2000", "window":"214 Approximate Maximal Margin Classification The real-world datasets are well-known <b>Optical</b> Character <b>Recognition</b> (OCR) benchmarks. On these datasets we followed the experimental setting described by Cortes and Vapnik (1995), Freund and Schapire (1999), Li and Long", "mykey":830},
 {"datasetID":7, "supportID":"5A771DA565C38950842ACD1B5DAA4EE9225CAE25", "rexaID":"b6069e488f10098d50e684d245a5574aa0c50c58", "author":"D. Randall Wilson and Roel Martinez", "title":"Improved Center Point Selection for Probabilistic Neural Networks", "venue":"Proceedings of the International Conference on Artificial Neural Networks and Genetic Algorithms", "year":"", "window":"reduction in size can be even more dramatic when there are more instances available. This is especially true when the number of instances is large compared to the complexity of the decision surface. Dataset Anneal <b>Audiology</b> Australian Breast Cancer (WI) Bridges Crx Echocardiogram Flag Heart (Hungarian) Heart (More) Heart Heart (Swiss) Hepatitis Horse-Colic Iris Liver-Bupa Pima-Indians-Diabetes", "mykey":831},
 {"datasetID":8, "supportID":"5A771DA565C38950842ACD1B5DAA4EE9225CAE25", "rexaID":"b6069e488f10098d50e684d245a5574aa0c50c58", "author":"D. Randall Wilson and Roel Martinez", "title":"Improved Center Point Selection for Probabilistic Neural Networks", "venue":"Proceedings of the International Conference on Artificial Neural Networks and Genetic Algorithms", "year":"", "window":"reduction in size can be even more dramatic when there are more instances available. This is especially true when the number of instances is large compared to the complexity of the decision surface. Dataset Anneal <b>Audiology</b> Australian Breast Cancer (WI) Bridges Crx Echocardiogram Flag Heart (Hungarian) Heart (More) Heart Heart (Swiss) Hepatitis Horse-Colic Iris Liver-Bupa Pima-Indians-Diabetes", "mykey":832},
 {"datasetID":14, "supportID":"5A771DA565C38950842ACD1B5DAA4EE9225CAE25", "rexaID":"b6069e488f10098d50e684d245a5574aa0c50c58", "author":"D. Randall Wilson and Roel Martinez", "title":"Improved Center Point Selection for Probabilistic Neural Networks", "venue":"Proceedings of the International Conference on Artificial Neural Networks and Genetic Algorithms", "year":"", "window":"reduction in size can be even more dramatic when there are more instances available. This is especially true when the number of instances is large compared to the complexity of the decision surface. Dataset Anneal Audiology Australian <b>Breast</b> <b>Cancer</b> (WI) Bridges Crx Echocardiogram Flag Heart (Hungarian) Heart (More) Heart Heart (Swiss) Hepatitis Horse-Colic Iris Liver-Bupa Pima-Indians-Diabetes", "mykey":833},
 {"datasetID":38, "supportID":"5A771DA565C38950842ACD1B5DAA4EE9225CAE25", "rexaID":"b6069e488f10098d50e684d245a5574aa0c50c58", "author":"D. Randall Wilson and Roel Martinez", "title":"Improved Center Point Selection for Probabilistic Neural Networks", "venue":"Proceedings of the International Conference on Artificial Neural Networks and Genetic Algorithms", "year":"", "window":"it retained almost two-thirds of the instances while suffering a large drop in accuracy compared to the other two models. However, in the <b>Echocardiogram</b> dataset, the RPNN used only 9% of the data while improving generalization accuracy by over 12%. Future research will focus on identifying characteristics of applications that help determine whether the RPNN", "mykey":834},
 {"datasetID":45, "supportID":"5A94C9B8DE69C5A208F01804F7AE925600E0D012", "rexaID":"a11c91c5cf02794b7d1d4bdc0222b3c11dce9cac", "author":"David Page and Soumya Ray", "title":"Skewing: An Efficient Alternative to Lookahead for Decision Tree Induction", "venue":"IJCAI", "year":"2003", "window":"Skewing ID3, No Skewing Figure 8: Five-Variable Hard Targets 50 60 70 80 90 100 200 400 600 800 1000 Accuracy (%) Sample Size ID3 with Skewing ID3, No Skewing Figure 9: Six-Variable Hard Targets Data Set Standard ID3 ID3 with Skewing <b>Heart</b> 71.9 74.5 Voting 94.0 94.2 Voting-2 87.4 88.6 Contra 60.4 61.5 Monks-1 92.6 100.0 Monks-2 86.5 89.3 Monks-3 89.8 91.7 Table 4: Accuracies of ID3 and ID3 with", "mykey":835},
 {"datasetID":23, "supportID":"5AA16679C7A3EB570FEC55489097B9A2E7A2AEE2", "rexaID":"287a92657decc181f6842e52b05c1fa17e5ab277", "author":"Hankil Yoon and Khaled A. Alsabti and Sanjay Ranka", "title":"Tree-based Incremental Classification for Large Datasets", "venue":"CISE Department, University of Florida", "year":"", "window":"very quickly and identify a method that better suits to the application. 5.2 Experimental results with <b>chess</b> dataset The chess dataset obtained from [BKM98] contains 25,000 training records and 3,056 test records. There are 18 class values and each record consists of 6 attributes, all of which are numeric", "mykey":836},
 {"datasetID":21, "supportID":"5AA16679C7A3EB570FEC55489097B9A2E7A2AEE2", "rexaID":"287a92657decc181f6842e52b05c1fa17e5ab277", "author":"Hankil Yoon and Khaled A. Alsabti and Sanjay Ranka", "title":"Tree-based Incremental Classification for Large Datasets", "venue":"CISE Department, University of Florida", "year":"", "window":"very quickly and identify a method that better suits to the application. 5.2 Experimental results with <b>chess</b> dataset The chess dataset obtained from [BKM98] contains 25,000 training records and 3,056 test records. There are 18 class values and each record consists of 6 attributes, all of which are numeric", "mykey":837},
 {"datasetID":22, "supportID":"5AA16679C7A3EB570FEC55489097B9A2E7A2AEE2", "rexaID":"287a92657decc181f6842e52b05c1fa17e5ab277", "author":"Hankil Yoon and Khaled A. Alsabti and Sanjay Ranka", "title":"Tree-based Incremental Classification for Large Datasets", "venue":"CISE Department, University of Florida", "year":"", "window":"very quickly and identify a method that better suits to the application. 5.2 Experimental results with <b>chess</b> dataset The chess dataset obtained from [BKM98] contains 25,000 training records and 3,056 test records. There are 18 class values and each record consists of 6 attributes, all of which are numeric", "mykey":838},
 {"datasetID":34, "supportID":"5B44200FEAAA607ECBE2282B7C43B5686FE832B1", "rexaID":"5f8eb537fc397ad5e506b2eae4f6676b48990ba6", "author":"Thomas G. Dietterich", "title":"Approximate Statistical Test For Comparing Supervised Classification Learning Algorithms", "venue":"Neural Computation, 10", "year":"1998", "window":"measured on the 10,000 calibration examples) matched the average performance of C4.5 to within 0.1%. For the Pima Indians <b>Diabetes</b> data set, we drew 1000 data sets of size 300 from the 768 available examples. For each of these data sets, the remaining 468 examples were retained for calibration. Each of the 1000 data sets of size 300 was", "mykey":839},
 {"datasetID":59, "supportID":"5B44200FEAAA607ECBE2282B7C43B5686FE832B1", "rexaID":"5f8eb537fc397ad5e506b2eae4f6676b48990ba6", "author":"Thomas G. Dietterich", "title":"Approximate Statistical Test For Comparing Supervised Classification Learning Algorithms", "venue":"Neural Computation, 10", "year":"1998", "window":"1 (Quinlan, 1993) and the first nearest-neighbor (NN) algorithm (Dasarathy, 1991). We then selected three difficult problems: the EXP6 problem developed by Kong (1995), the <b>Letter Recognition</b> data set (Frey & Slate, 1991), and the Pima Indians Diabetes Task (Merz & Murphy, 1996). Of course, C4.5 and NN do not have the same performance on these data sets. In EXP6 and Letter Recognition, NN", "mykey":840},
 {"datasetID":79, "supportID":"5B44200FEAAA607ECBE2282B7C43B5686FE832B1", "rexaID":"5f8eb537fc397ad5e506b2eae4f6676b48990ba6", "author":"Thomas G. Dietterich", "title":"Approximate Statistical Test For Comparing Supervised Classification Learning Algorithms", "venue":"Neural Computation, 10", "year":"1998", "window":"measured on the 10,000 calibration examples) matched the average performance of C4.5 to within 0.1%. For the <b>Pima</b> <b>Indians</b> <b>Diabetes</b> data set, we drew 1000 data sets of size 300 from the 768 available examples. For each of these data sets, the remaining 468 examples were retained for calibration. Each of the 1000 data sets of size 300 was", "mykey":841},
 {"datasetID":50, "supportID":"5B4B162BB62A5B5F8485DC741342F7445956954A", "rexaID":"d8a0c5fd34dede91f5241e9b9dca0626f6969c0d", "author":"Je Scott and Mahesan Niranjan and Richard W. Prager", "title":"Realisable Classifiers: Improving Operating Performance on Variable Cost Problems", "venue":"Cambridge University Department of Engineering", "year":"", "window":"for improving classification systems in problem domains within which classification costs may not be known apriori. Empirical results are presented for artificial data, and for two real world data sets: an <b>image segmentation</b> task and the diagnosis of abnormal thyroid condition. 1 Introduction A large fraction of decision support systems, particularly those used in medical diagnostics (e.g.", "mykey":842},
 {"datasetID":147, "supportID":"5B4B162BB62A5B5F8485DC741342F7445956954A", "rexaID":"d8a0c5fd34dede91f5241e9b9dca0626f6969c0d", "author":"Je Scott and Mahesan Niranjan and Richard W. Prager", "title":"Realisable Classifiers: Improving Operating Performance on Variable Cost Problems", "venue":"Cambridge University Department of Engineering", "year":"", "window":"for improving classification systems in problem domains within which classification costs may not be known apriori. Empirical results are presented for artificial data, and for two real world data sets: an <b>image segmentation</b> task and the diagnosis of abnormal thyroid condition. 1 Introduction A large fraction of decision support systems, particularly those used in medical diagnostics (e.g.", "mykey":843},
 {"datasetID":146, "supportID":"5B4B162BB62A5B5F8485DC741342F7445956954A", "rexaID":"d8a0c5fd34dede91f5241e9b9dca0626f6969c0d", "author":"Je Scott and Mahesan Niranjan and Richard W. Prager", "title":"Realisable Classifiers: Improving Operating Performance on Variable Cost Problems", "venue":"Cambridge University Department of Engineering", "year":"", "window":"the convex hull over the Test data ROC curves. The ROC curves for System 1 and System 2 using the Unseen data are plotted for comparison with the MRROC. 3.3 <b>LandSat</b> data A LandSat image segmentation dataset, originally used in the Statlog project, was obtained from the UCI repository [13, 12]. The data consisted of multi-spectral values of pixels in 3 # 3 neighbourhoods in a satellite image. A", "mykey":844},
 {"datasetID":98, "supportID":"5B4B162BB62A5B5F8485DC741342F7445956954A", "rexaID":"d8a0c5fd34dede91f5241e9b9dca0626f6969c0d", "author":"Je Scott and Mahesan Niranjan and Richard W. Prager", "title":"Realisable Classifiers: Improving Operating Performance on Variable Cost Problems", "venue":"Cambridge University Department of Engineering", "year":"", "window":"the convex hull over the Test data ROC curves. The ROC curves for System 1 and System 2 using the Unseen data are plotted for comparison with the MRROC. 3.3 LandSat data A LandSat image segmentation dataset, originally used in the <b>Statlog</b> project, was obtained from the UCI repository [13, 12]. The data consisted of multi-spectral values of pixels in 3 # 3 neighbourhoods in a satellite image. A", "mykey":845},
 {"datasetID":102, "supportID":"5B4B162BB62A5B5F8485DC741342F7445956954A", "rexaID":"d8a0c5fd34dede91f5241e9b9dca0626f6969c0d", "author":"Je Scott and Mahesan Niranjan and Richard W. Prager", "title":"Realisable Classifiers: Improving Operating Performance on Variable Cost Problems", "venue":"Cambridge University Department of Engineering", "year":"", "window":"nature of the classifiers used to form it. The classifiers are random variables, whose central tendency will be to lie on the MRROC. 312 British Machine Vision Conference 3.2 <b>thyroid</b> data A medical data set describing patients with abnormal thyroid conditions was obtained from the UCI machine learning repository [12]. The data was originally contained 7200 instances, with had 3 classes, hyperthyroid,", "mykey":846},
 {"datasetID":102, "supportID":"5B9F86CB73E928810AA19489B0A8F36408033FED", "rexaID":"02ececaa586677e52299e3881239ace545c6ec98", "author":"Qiang Yang and Jing Wu", "title":"Enhancing the Effectiveness of Interactive Case-Based Reasoning with Clustering and Decision Forests", "venue":"Appl. Intell, 14", "year":"2001", "window":"from the UCI Repository of Machine Learning Databases and Domain Theories [Keogh et al. 1998] at the University of California at Irvine. Two data sets are first used for our experiments (the <b>Thyroid</b> Disease Database and the Mushroom (agaricus-lepiota) database). In addition, in order to test the system under the condition that there are missing", "mykey":847},
 {"datasetID":20, "supportID":"5BA3AA18D3DA933C6CA297903FE7914FE7D041CE", "rexaID":"57df1fe4db314b15e12a8036d1c677dd1b9633af", "author":"Aristides Gionis and Heikki Mannila and Panayiotis Tsaparas", "title":"Clustering Aggregation", "venue":"ICDE", "year":"2005", "window":"by 22 categorical attributes, such as shape, color, odor, etc. There is a class label describing if a mushroom is poisonous or edible, and there are 2,480 missing values in total. Finally, the third dataset, <b>census</b>  has been extracted from the census bureau database, and it contains demographic information on 32,561 people in the US. There are 8 categorical attributes (such as education, occupation,", "mykey":848},
 {"datasetID":105, "supportID":"5BA3AA18D3DA933C6CA297903FE7914FE7D041CE", "rexaID":"57df1fe4db314b15e12a8036d1c677dd1b9633af", "author":"Aristides Gionis and Heikki Mannila and Panayiotis Tsaparas", "title":"Clustering Aggregation", "venue":"ICDE", "year":"2005", "window":"completely parameter-free! Neither a threshold nor the number of clusters need to be specified. The number of clusters discovered by our algorithms seem to be very reasonable choices: for the <b>Votes dataset</b>  most people vote according to the official position of their political parties, so having two clusters is natural; for the Mushrooms dataset, notice that both ROCK and LIMBO achieve much better", "mykey":849},
 {"datasetID":48, "supportID":"5BBD858C7382665CC8CC4039236FC5F6C7C582B8", "rexaID":"220290933c845bcae1348931e08740d3e16b5360", "author":"David Hershberger and Hillol Kargupta", "title":"Distributed Multivariate Regression Using Wavelet-Based Collective Data Mining", "venue":"J. Parallel Distrib. Comput, 61", "year":"2001", "window":"use to generate a regression model may not generate the MSE model for that amount of information transfer. The result also supports the MSE model result for the wavelet basis. The second benchmark data set we employ is the Boston <b>Housing</b> data set created by Harrison and Rubinfeld [18]. This data set consists of 506 samples with 13 independent variables, 12 of which are real-valued, and one real-valued", "mykey":850},
 {"datasetID":53, "supportID":"5BBD858C7382665CC8CC4039236FC5F6C7C582B8", "rexaID":"220290933c845bcae1348931e08740d3e16b5360", "author":"David Hershberger and Hillol Kargupta", "title":"Distributed Multivariate Regression Using Wavelet-Based Collective Data Mining", "venue":"J. Parallel Distrib. Comput, 61", "year":"2001", "window":"Application of this method to Linear Discriminant Analysis, which is related to parametric multivariate regression, produced classification results on the <b>Iris</b> data set that are comparable to those obtained with centralized data analysis. Key Words: data mining, distributed data mining, collective data mining, knowledge discovery, wavelets, regression 1.", "mykey":851},
 {"datasetID":25, "supportID":"5C6D1AA078B4372CB0D8B465C4A8F11279629C72", "rexaID":"cac8f7b952b6dc917dd68834c825e7f48373cafa", "author":"Zoubin Ghahramani and Michael I. Jordan", "title":"Factorial Hidden Markov Models", "venue":"Machine Learning, 29", "year":"1997", "window":"EM). Factorial HMMs of varying sizes (K ranging from 2 to 6; M ranging from 2 to 9) were also trained on the same data. To 262 Z. GHAHRAMANI AND M.I. JORDAN Table 2. Attributes in the <b>Bach</b> chorale data set. The key signature and time signature attributes were constant over the duration of the chorale. All attributes were treated as real numbers and modeled as linear-Gaussian observations (4a).", "mykey":852},
 {"datasetID":14, "supportID":"5D6E0C594CEFE5604A3472F48C7D3D23A0E3CC78", "rexaID":"f4405e32dbb5dea3ece303e2a5b3edb6b413271e", "author":"Rafael S. Parpinelli and Heitor S. Lopes and Alex Alves Freitas", "title":"PART FOUR: ANT COLONY OPTIMIZATION AND IMMUNE SYSTEMS Chapter X An Ant Colony Algorithm for Classification Rule Discovery", "venue":"CEFET-PR, Curitiba", "year":"", "window":"2. The numbers after the \"\u00b1\" symbol are the standard deviations of the corresponding accuracy rates. As shown in this table, Ant-Miner discovered rules with a better accuracy rate than C4.5 in four data sets, namely Ljubljana <b>breast</b> <b>cancer</b>  Wisconsin breast cancer, Hepatitis and Heart disease. In two data sets, Ljubljana breast cancer and Heart disease, the difference was quite small. In the other two", "mykey":853},
 {"datasetID":17, "supportID":"5D6E0C594CEFE5604A3472F48C7D3D23A0E3CC78", "rexaID":"f4405e32dbb5dea3ece303e2a5b3edb6b413271e", "author":"Rafael S. Parpinelli and Heitor S. Lopes and Alex Alves Freitas", "title":"PART FOUR: ANT COLONY OPTIMIZATION AND IMMUNE SYSTEMS Chapter X An Ant Colony Algorithm for Classification Rule Discovery", "venue":"CEFET-PR, Curitiba", "year":"", "window":"2. The numbers after the \"\u00b1\" symbol are the standard deviations of the corresponding accuracy rates. As shown in this table, Ant-Miner discovered rules with a better accuracy rate than C4.5 in four data sets, namely Ljubljana <b>breast</b> <b>cancer</b>  <b>Wisconsin</b> breast cancer, Hepatitis and Heart disease. In two data sets, Ljubljana breast cancer and Heart disease, the difference was quite small. In the other two", "mykey":854},
 {"datasetID":15, "supportID":"5D6E0C594CEFE5604A3472F48C7D3D23A0E3CC78", "rexaID":"f4405e32dbb5dea3ece303e2a5b3edb6b413271e", "author":"Rafael S. Parpinelli and Heitor S. Lopes and Alex Alves Freitas", "title":"PART FOUR: ANT COLONY OPTIMIZATION AND IMMUNE SYSTEMS Chapter X An Ant Colony Algorithm for Classification Rule Discovery", "venue":"CEFET-PR, Curitiba", "year":"", "window":"2. The numbers after the \"\u00b1\" symbol are the standard deviations of the corresponding accuracy rates. As shown in this table, Ant-Miner discovered rules with a better accuracy rate than C4.5 in four data sets, namely Ljubljana <b>breast</b> <b>cancer</b>  <b>Wisconsin</b> breast cancer, Hepatitis and Heart disease. In two data sets, Ljubljana breast cancer and Heart disease, the difference was quite small. In the other two", "mykey":855},
 {"datasetID":16, "supportID":"5D6E0C594CEFE5604A3472F48C7D3D23A0E3CC78", "rexaID":"f4405e32dbb5dea3ece303e2a5b3edb6b413271e", "author":"Rafael S. Parpinelli and Heitor S. Lopes and Alex Alves Freitas", "title":"PART FOUR: ANT COLONY OPTIMIZATION AND IMMUNE SYSTEMS Chapter X An Ant Colony Algorithm for Classification Rule Discovery", "venue":"CEFET-PR, Curitiba", "year":"", "window":"2. The numbers after the \"\u00b1\" symbol are the standard deviations of the corresponding accuracy rates. As shown in this table, Ant-Miner discovered rules with a better accuracy rate than C4.5 in four data sets, namely Ljubljana <b>breast</b> <b>cancer</b>  <b>Wisconsin</b> breast cancer, Hepatitis and Heart disease. In two data sets, Ljubljana breast cancer and Heart disease, the difference was quite small. In the other two", "mykey":856},
 {"datasetID":33, "supportID":"5D6E0C594CEFE5604A3472F48C7D3D23A0E3CC78", "rexaID":"f4405e32dbb5dea3ece303e2a5b3edb6b413271e", "author":"Rafael S. Parpinelli and Heitor S. Lopes and Alex Alves Freitas", "title":"PART FOUR: ANT COLONY OPTIMIZATION AND IMMUNE SYSTEMS Chapter X An Ant Colony Algorithm for Classification Rule Discovery", "venue":"CEFET-PR, Curitiba", "year":"", "window":"represents a reduction of 20% in the error rate of C4.5. ((96.04 -- 95.02)/(100 -- 95.02) = 0.20) On the other hand, C4.5 discovered rules with a better accuracy rate than AntMiner in the other two data sets. In one data set, <b>Dermatology</b>  the difference was quite small, whereas in the Tic-tac-toe the difference was relatively large. (This result will be revisited later.) Overall one can conclude that", "mykey":857},
 {"datasetID":45, "supportID":"5D6E0C594CEFE5604A3472F48C7D3D23A0E3CC78", "rexaID":"f4405e32dbb5dea3ece303e2a5b3edb6b413271e", "author":"Rafael S. Parpinelli and Heitor S. Lopes and Alex Alves Freitas", "title":"PART FOUR: ANT COLONY OPTIMIZATION AND IMMUNE SYSTEMS Chapter X An Ant Colony Algorithm for Classification Rule Discovery", "venue":"CEFET-PR, Curitiba", "year":"", "window":"namely Ljubljana breast cancer, Wisconsin breast cancer, Hepatitis and <b>Heart</b> disease. In two data sets, Ljubljana breast cancer and Heart disease, the difference was quite small. In the other two data sets, Wisconsin breast cancer and Hepatitis, the difference was more relevant. Note that although", "mykey":858},
 {"datasetID":46, "supportID":"5D6E0C594CEFE5604A3472F48C7D3D23A0E3CC78", "rexaID":"f4405e32dbb5dea3ece303e2a5b3edb6b413271e", "author":"Rafael S. Parpinelli and Heitor S. Lopes and Alex Alves Freitas", "title":"PART FOUR: ANT COLONY OPTIMIZATION AND IMMUNE SYSTEMS Chapter X An Ant Colony Algorithm for Classification Rule Discovery", "venue":"CEFET-PR, Curitiba", "year":"", "window":"namely Ljubljana breast cancer, Wisconsin breast cancer, <b>Hepatitis</b> and Heart disease. In two data sets, Ljubljana breast cancer and Heart disease, the difference was quite small. In the other two data sets, Wisconsin breast cancer and Hepatitis, the difference was more relevant. Note that although", "mykey":859},
 {"datasetID":101, "supportID":"5D6E0C594CEFE5604A3472F48C7D3D23A0E3CC78", "rexaID":"f4405e32dbb5dea3ece303e2a5b3edb6b413271e", "author":"Rafael S. Parpinelli and Heitor S. Lopes and Alex Alves Freitas", "title":"PART FOUR: ANT COLONY OPTIMIZATION AND IMMUNE SYSTEMS Chapter X An Ant Colony Algorithm for Classification Rule Discovery", "venue":"CEFET-PR, Curitiba", "year":"", "window":"the difference between the number of rules discovered by Ant-Miner and C4.5 is quite large, as follows. In the <b>Tic-tac-toe</b> and Dermatology data sets Ant-Miner discovered 8.5 and 7.0 rules, respectively, whereas C4.5 discovered 83 and 23.2 rules, respectively. In both data sets C4.5 achieved a better accuracy rate. So, in these two data sets", "mykey":860},
 {"datasetID":47, "supportID":"5D8EA619F80D1A065E482737F77A27F5716C514B", "rexaID":"a32ab1b3da96c9ae515a685b4fcf50e857708f24", "author":"Mukund Deshpande and George Karypis", "title":"Using conjunction of attribute values for classification", "venue":"CIKM", "year":"2002", "window":"We performed our experiments using a 10 way cross validation scheme and computed average accuracy across different runs. We ran our experiments using a support threshold of 1.0% for all the datasets, except hepati, <b>horse</b> where we used a support threshold of 2.0% and for lymph and zoo we used the support threshold of 5.0%. This was done to ensure that the composite features generated are", "mykey":861},
 {"datasetID":109, "supportID":"5D8EA619F80D1A065E482737F77A27F5716C514B", "rexaID":"a32ab1b3da96c9ae515a685b4fcf50e857708f24", "author":"Mukund Deshpande and George Karypis", "title":"Using conjunction of attribute values for classification", "venue":"CIKM", "year":"2002", "window":"7 214 heart 13 0 2 270 hepati 6 13 2 155 horse 7 15 2 368 iris 4 0 3 150 labor 8 8 2 57 led7 0 7 10 3200 lymph 0 18 4 148 pima 8 0 2 768 tic-tac 0 9 2 958 <b>wine</b> 13 0 3 178 zoo 0 16 7 101 Table 1: UCI dataset statistics. We performed our experiments using a 10 way cross validation scheme and computed average accuracy across different runs. We ran our experiments using a support threshold of 1.0% for all", "mykey":862},
 {"datasetID":111, "supportID":"5D8EA619F80D1A065E482737F77A27F5716C514B", "rexaID":"a32ab1b3da96c9ae515a685b4fcf50e857708f24", "author":"Mukund Deshpande and George Karypis", "title":"Using conjunction of attribute values for classification", "venue":"CIKM", "year":"2002", "window":"7 214 heart 13 0 2 270 hepati 6 13 2 155 horse 7 15 2 368 iris 4 0 3 150 labor 8 8 2 57 led7 0 7 10 3200 lymph 0 18 4 148 pima 8 0 2 768 tic-tac 0 9 2 958 wine 13 0 3 178 <b>zoo</b> 0 16 7 101 Table 1: UCI dataset statistics. We performed our experiments using a 10 way cross validation scheme and computed average accuracy across different runs. We ran our experiments using a support threshold of 1.0% for all", "mykey":863},
 {"datasetID":14, "supportID":"5D935F69244FA68C8E46435B9F03BAB03BEB06AB", "rexaID":"73cfbd8185405d37df94492642c9ffdb3b48c37f", "author":"Huan Liu and Hiroshi Motoda and Manoranjan Dash", "title":"A Monotonic Measure for Optimal Feature Selection", "venue":"ECML", "year":"1998", "window":"with unknown relevant attributes, consists of WBC - the Wisconsin <b>Breast</b> <b>Cancer</b> data set, LED-7 - data with 7 Boolean attributes and 10 classes, the set of decimal digits (0..9), Letter - the letter image recognition data, LYM - the lymphography data, and Vote - the U.S. House of", "mykey":864},
 {"datasetID":17, "supportID":"5D935F69244FA68C8E46435B9F03BAB03BEB06AB", "rexaID":"73cfbd8185405d37df94492642c9ffdb3b48c37f", "author":"Huan Liu and Hiroshi Motoda and Manoranjan Dash", "title":"A Monotonic Measure for Optimal Feature Selection", "venue":"ECML", "year":"1998", "window":"with unknown relevant attributes, consists of WBC - the <b>Wisconsin</b> <b>Breast</b> <b>Cancer</b> data set, LED-7 - data with 7 Boolean attributes and 10 classes, the set of decimal digits (0..9), Letter - the letter image recognition data, LYM - the lymphography data, and Vote - the U.S. House of", "mykey":865},
 {"datasetID":15, "supportID":"5D935F69244FA68C8E46435B9F03BAB03BEB06AB", "rexaID":"73cfbd8185405d37df94492642c9ffdb3b48c37f", "author":"Huan Liu and Hiroshi Motoda and Manoranjan Dash", "title":"A Monotonic Measure for Optimal Feature Selection", "venue":"ECML", "year":"1998", "window":"with unknown relevant attributes, consists of WBC - the <b>Wisconsin</b> <b>Breast</b> <b>Cancer</b> data set, LED-7 - data with 7 Boolean attributes and 10 classes, the set of decimal digits (0..9), Letter - the letter image recognition data, LYM - the lymphography data, and Vote - the U.S. House of", "mykey":866},
 {"datasetID":16, "supportID":"5D935F69244FA68C8E46435B9F03BAB03BEB06AB", "rexaID":"73cfbd8185405d37df94492642c9ffdb3b48c37f", "author":"Huan Liu and Hiroshi Motoda and Manoranjan Dash", "title":"A Monotonic Measure for Optimal Feature Selection", "venue":"ECML", "year":"1998", "window":"with unknown relevant attributes, consists of WBC - the <b>Wisconsin</b> <b>Breast</b> <b>Cancer</b> data set, LED-7 - data with 7 Boolean attributes and 10 classes, the set of decimal digits (0..9), Letter - the letter image recognition data, LYM - the lymphography data, and Vote - the U.S. House of", "mykey":867},
 {"datasetID":45, "supportID":"5DB332AAD676E48D55D15331FA47E86A3CDBC9F0", "rexaID":"58642563848aaece947320829da7d338224cb8b3", "author":"Yoav Freund and Lorne Mason", "title":"The Alternating Decision Tree Learning Algorithm", "venue":"ICML", "year":"1999", "window":"representation. To demonstrate our interpretation, we consider the alternating tree presented in Figure 4. This tree is the result of running our learning algorithm for six iterations on the cleve data set from Irvine. This is a data set of <b>heart</b> disease diagnostics for which the goal is to discriminate between sick and healthy people 3 In our mapping positive classification correspond to healthy and", "mykey":868},
 {"datasetID":69, "supportID":"5DB332AAD676E48D55D15331FA47E86A3CDBC9F0", "rexaID":"58642563848aaece947320829da7d338224cb8b3", "author":"Yoav Freund and Lorne Mason", "title":"The Alternating Decision Tree Learning Algorithm", "venue":"ICML", "year":"1999", "window":"1 Proportion +ve Prediction <b>splice</b> train test 0 0.2 0.4 0.6 0.8 1 -1 -0.5 0 0.5 1 Proportion +ve Prediction sick-euthyroid train test Figure 7: Calibration graphs for the splice and sick-euthyroid data sets for train and test after 100 rounds of ADTree. In addition to justifying our interpretation, the calibration graphs can potentially be used to improve our performance situation where our", "mykey":869},
 {"datasetID":45, "supportID":"5F07C4598F9B5EA3E302F4F333FE2BFEEF17E562", "rexaID":"0f1456aba1cd88fe698ef134fa761a3ee2fc03f2", "author":"Wl/odzisl/aw Duch and Karol Grudzinski and Geerd H. F Diercksen", "title":"Minimal distance neural methods", "venue":"Department of Computer Methods, Nicholas Copernicus University", "year":"", "window":"of the number of neighbors and for vowel the r-NN method gives 57.8% accuracy, but in both TABLE I The appendicitis, Wisconsin breast cancer data, hepatitis and the Cleveland <b>heart</b> data. Dataset and method Leave-one-out % The appendicitis data Bayes rule (statistical) 83.0 CART, C4.5 (dec. trees) 84.9 MLP+backpropagation 85.8 RIAC (prob. inductive) 86.9 9-NN 89.6 PVM, C-MLP2LN (logical", "mykey":870},
 {"datasetID":46, "supportID":"5F07C4598F9B5EA3E302F4F333FE2BFEEF17E562", "rexaID":"0f1456aba1cd88fe698ef134fa761a3ee2fc03f2", "author":"Wl/odzisl/aw Duch and Karol Grudzinski and Geerd H. F Diercksen", "title":"Minimal distance neural methods", "venue":"Department of Computer Methods, Nicholas Copernicus University", "year":"", "window":"of the number of neighbors and for vowel the r-NN method gives 57.8% accuracy, but in both TABLE I The appendicitis, Wisconsin breast cancer data, <b>hepatitis</b> and the Cleveland heart data. Dataset and method Leave-one-out % The appendicitis data Bayes rule (statistical) 83.0 CART, C4.5 (dec. trees) 84.9 MLP+backpropagation 85.8 RIAC (prob. inductive) 86.9 9-NN 89.6 PVM, C-MLP2LN (logical", "mykey":871},
 {"datasetID":52, "supportID":"5F07C4598F9B5EA3E302F4F333FE2BFEEF17E562", "rexaID":"0f1456aba1cd88fe698ef134fa761a3ee2fc03f2", "author":"Wl/odzisl/aw Duch and Karol Grudzinski and Geerd H. F Diercksen", "title":"Minimal distance neural methods", "venue":"Department of Computer Methods, Nicholas Copernicus University", "year":"", "window":"optimized radius but the results were not significantly better. The failure of the minimum distance method with global parameters for this case is surprising and requires further study. Non-medical datasets included the <b>ionosphere</b> (350 cases, 34 attributes, 2 classes), satimage (4435 cases, 36 attributes, 6 classes), sonar (208 cases, 60 attributes, 2 classes) and vovel (528 training and 462 test", "mykey":872},
 {"datasetID":14, "supportID":"5F16AAFCE2B0C777DE3F25FCFB5185FC32D3D5D0", "rexaID":"c5ac7c07f095dd7068c3ab10d4e7615b6d1564d5", "author":"Liping Wei and Russ B. Altman", "title":"An Automated System for Generating Comparative Disease Profiles and Making Diagnoses", "venue":"Section on Medical Informatics Stanford University School of Medicine, MSOB X215", "year":"", "window":"profile instead of using all attributes in the original clinical data. The results remain the same. RESULTS We evaluated the system by applying it to heart disease, diabetes, and <b>breast</b> <b>cancer</b>  All data sets were obtained from the UCI Repository of Machine Learning databases and domain theories. 7 Heart Disease Four clinical data sets were used. These sets consists of patients who had been referred for", "mykey":873},
 {"datasetID":34, "supportID":"5F16AAFCE2B0C777DE3F25FCFB5185FC32D3D5D0", "rexaID":"c5ac7c07f095dd7068c3ab10d4e7615b6d1564d5", "author":"Liping Wei and Russ B. Altman", "title":"An Automated System for Generating Comparative Disease Profiles and Making Diagnoses", "venue":"Section on Medical Informatics Stanford University School of Medicine, MSOB X215", "year":"", "window":"profile instead of using all attributes in the original clinical data. The results remain the same. RESULTS We evaluated the system by applying it to heart disease, <b>diabetes</b>  and breast cancer. All data sets were obtained from the UCI Repository of Machine Learning databases and domain theories. 7 Heart Disease Four clinical data sets were used. These sets consists of patients who had been referred for", "mykey":874},
 {"datasetID":45, "supportID":"5F16AAFCE2B0C777DE3F25FCFB5185FC32D3D5D0", "rexaID":"c5ac7c07f095dd7068c3ab10d4e7615b6d1564d5", "author":"Liping Wei and Russ B. Altman", "title":"An Automated System for Generating Comparative Disease Profiles and Making Diagnoses", "venue":"Section on Medical Informatics Stanford University School of Medicine, MSOB X215", "year":"", "window":"profile instead of using all attributes in the original clinical data. The results remain the same. RESULTS We evaluated the system by applying it to <b>heart</b> disease, diabetes, and breast cancer. All data sets were obtained from the UCI Repository of Machine Learning databases and domain theories. 7 Heart Disease Four clinical data sets were used. These sets consists of patients who had been referred for", "mykey":875},
 {"datasetID":79, "supportID":"5F16AAFCE2B0C777DE3F25FCFB5185FC32D3D5D0", "rexaID":"c5ac7c07f095dd7068c3ab10d4e7615b6d1564d5", "author":"Liping Wei and Russ B. Altman", "title":"An Automated System for Generating Comparative Disease Profiles and Making Diagnoses", "venue":"Section on Medical Informatics Stanford University School of Medicine, MSOB X215", "year":"", "window":"diagnosis. This finding is consistent with the study by Ohno-Machado. 9 We realize that the difference may not be statistically significant and that further studies are needed. <b>diabetes</b> The diabetes data set consisted of 768 females patients of <b>Pima</b> Indian heritage who were at least 21 years old. Eight attributes were collected for each patient. A class variable was also documented, 1 as \"having", "mykey":876},
 {"datasetID":23, "supportID":"5F27ED82C52339124AA368507D66B71D96862CB7", "rexaID":"9b1522e8a25d84453b24d3891ac99e165d3eebaa", "author":"Ira Cohen and Fabio Gagliardi Cozman and Nicu Sebe and Marcelo Cesar Cirelo and Thomas S. Huang", "title":"Semisupervised Learning of Classifiers: Theory, Algorithms, and Their Application to Human-Computer Interaction", "venue":"IEEE Trans. Pattern Anal. Mach. Intell, 26", "year":"2004", "window":"EMTAN can sometimes improve performance over TAN with just labeled data (Shuttle). With the <b>Chess</b> dataset, discarding the unlabeled data and using only TAN seems the best approach. We have compared two likelihood based structure learning methods (K2 and MCMC) on the same datasets as well [34], showing", "mykey":877},
 {"datasetID":21, "supportID":"5F27ED82C52339124AA368507D66B71D96862CB7", "rexaID":"9b1522e8a25d84453b24d3891ac99e165d3eebaa", "author":"Ira Cohen and Fabio Gagliardi Cozman and Nicu Sebe and Marcelo Cesar Cirelo and Thomas S. Huang", "title":"Semisupervised Learning of Classifiers: Theory, Algorithms, and Their Application to Human-Computer Interaction", "venue":"IEEE Trans. Pattern Anal. Mach. Intell, 26", "year":"2004", "window":"EMTAN can sometimes improve performance over TAN with just labeled data (Shuttle). With the <b>Chess</b> dataset, discarding the unlabeled data and using only TAN seems the best approach. We have compared two likelihood based structure learning methods (K2 and MCMC) on the same datasets as well [34], showing", "mykey":878},
 {"datasetID":22, "supportID":"5F27ED82C52339124AA368507D66B71D96862CB7", "rexaID":"9b1522e8a25d84453b24d3891ac99e165d3eebaa", "author":"Ira Cohen and Fabio Gagliardi Cozman and Nicu Sebe and Marcelo Cesar Cirelo and Thomas S. Huang", "title":"Semisupervised Learning of Classifiers: Theory, Algorithms, and Their Application to Human-Computer Interaction", "venue":"IEEE Trans. Pattern Anal. Mach. Intell, 26", "year":"2004", "window":"EMTAN can sometimes improve performance over TAN with just labeled data (Shuttle). With the <b>Chess</b> dataset, discarding the unlabeled data and using only TAN seems the best approach. We have compared two likelihood based structure learning methods (K2 and MCMC) on the same datasets as well [34], showing", "mykey":879},
 {"datasetID":148, "supportID":"5F27ED82C52339124AA368507D66B71D96862CB7", "rexaID":"9b1522e8a25d84453b24d3891ac99e165d3eebaa", "author":"Ira Cohen and Fabio Gagliardi Cozman and Nicu Sebe and Marcelo Cesar Cirelo and Thomas S. Huang", "title":"Semisupervised Learning of Classifiers: Theory, Algorithms, and Their Application to Human-Computer Interaction", "venue":"IEEE Trans. Pattern Anal. Mach. Intell, 26", "year":"2004", "window":"EMTAN can sometimes improve performance over TAN with just labeled data  <b>Shuttle</b> . With the Chess dataset, discarding the unlabeled data and using only TAN seems the best approach. We have compared two likelihood based structure learning methods (K2 and MCMC) on the same datasets as well [34], showing", "mykey":880},
 {"datasetID":19, "supportID":"5F3CFBF60AB9A004E5E65FB0E4E3CEEA124DFE4F", "rexaID":"217beab6a7a7b64dc929c3c5fdb42e812f8b2431", "author":"Daniel J. Lizotte", "title":"Library Release Form Name of Author", "venue":"Budgeted Learning of Naive Bayes Classifiers", "year":"", "window":"than when it is assumed to be 300 (i.e., when it looks ahead farther than it should). Other policies do not take the budget into account. We have observed the same overall patterns on several other datasets that we have tested the policies on so far  <b>CAR</b>  DIABETES, CHESS, BREAST): the performance of SFL is superior or comparable to the performance of other policies, and Biased-Robin is the best 36", "mykey":881},
 {"datasetID":105, "supportID":"5F3CFBF60AB9A004E5E65FB0E4E3CEEA124DFE4F", "rexaID":"217beab6a7a7b64dc929c3c5fdb42e812f8b2431", "author":"Daniel J. Lizotte", "title":"Library Release Form Name of Author", "venue":"Budgeted Learning of Naive Bayes Classifiers", "year":"", "window":"75 times, while a non-discriminative feature such as feature 18 is bought an average of only 2 times. For some budgets, the 0/1 error of SFL is nearly half that generated by Round-Robin. The <b>votes dataset</b> (Figures 3.7(a) and 3.7(b)) is a binary class problem (democrat vs. republican), with 16 binary features, 435 instances, and a positive class probability of 0.61. In the votes dataset, there is a", "mykey":882},
 {"datasetID":73, "supportID":"5F3CFBF60AB9A004E5E65FB0E4E3CEEA124DFE4F", "rexaID":"217beab6a7a7b64dc929c3c5fdb42e812f8b2431", "author":"Daniel J. Lizotte", "title":"Library Release Form Name of Author", "venue":"Budgeted Learning of Naive Bayes Classifiers", "year":"", "window":"from the UCI Machine Learning Repository [BM98]. These plots show averaged validation error of the policies on a holdout set (20% of the dataset) on the <b>mushroom</b>  nursery, and votes datasets. Each point is an average of 50 trials where in each trial a random balanced partition of classes was made for training and validation. The five-fold", "mykey":883},
 {"datasetID":76, "supportID":"5F3CFBF60AB9A004E5E65FB0E4E3CEEA124DFE4F", "rexaID":"217beab6a7a7b64dc929c3c5fdb42e812f8b2431", "author":"Daniel J. Lizotte", "title":"Library Release Form Name of Author", "venue":"Budgeted Learning of Naive Bayes Classifiers", "year":"", "window":"from the UCI Machine Learning Repository [BM98]. These plots show averaged validation error of the policies on a holdout set (20% of the dataset) on the mushroom, <b>nursery</b>  and votes datasets. Each point is an average of 50 trials where in each trial a random balanced partition of classes was made for training and validation. The five-fold", "mykey":884},
 {"datasetID":20, "supportID":"5F6472F0C8B67ED76F4B12F29F10E625C9F9F467", "rexaID":"287f8092a743949a6e0151893b9e3bc4d03466ed", "author":"Gabor Melli", "title":"A Lazy Model-Based Approach to On-Line Classification", "venue":"University of British Columbia", "year":"1989", "window":"........................104 8.1 Number of classification performed against the <b>census</b> year dataset by DBPredictor before C4.5 returns its first classification. . . . . . ............110 8.2 Number of classification performed against the census-year dataset by IB1 before DBPredictor returns its", "mykey":885},
 {"datasetID":38, "supportID":"5F6472F0C8B67ED76F4B12F29F10E625C9F9F467", "rexaID":"287f8092a743949a6e0151893b9e3bc4d03466ed", "author":"Gabor Melli", "title":"A Lazy Model-Based Approach to On-Line Classification", "venue":"University of British Columbia", "year":"1989", "window":"The five selected datasets were: <b>echocardiogram</b>  hayes-roth, heart, horse-colic,andiris datasets. These datasets (marked in Table 7.1 with a * symbol beside their name) contain a sampling of attribute types and domains. For", "mykey":886},
 {"datasetID":44, "supportID":"5F6472F0C8B67ED76F4B12F29F10E625C9F9F467", "rexaID":"287f8092a743949a6e0151893b9e3bc4d03466ed", "author":"Gabor Melli", "title":"A Lazy Model-Based Approach to On-Line Classification", "venue":"University of British Columbia", "year":"1989", "window":"The five selected datasets were: echocardiogram, <b>hayes</b> <b>roth</b>  heart, horse-colic,andiris datasets. These datasets (marked in Table 7.1 with a * symbol beside their name) contain a sampling of attribute types and domains. For", "mykey":887},
 {"datasetID":45, "supportID":"5F6472F0C8B67ED76F4B12F29F10E625C9F9F467", "rexaID":"287f8092a743949a6e0151893b9e3bc4d03466ed", "author":"Gabor Melli", "title":"A Lazy Model-Based Approach to On-Line Classification", "venue":"University of British Columbia", "year":"1989", "window":"The five selected datasets were: echocardiogram, hayes-roth, <b>heart</b>  horse-colic,andiris datasets. These datasets (marked in Table 7.1 with a * symbol beside their name) contain a sampling of attribute types and domains. For", "mykey":888},
 {"datasetID":46, "supportID":"5F6472F0C8B67ED76F4B12F29F10E625C9F9F467", "rexaID":"287f8092a743949a6e0151893b9e3bc4d03466ed", "author":"Gabor Melli", "title":"A Lazy Model-Based Approach to On-Line Classification", "venue":"University of British Columbia", "year":"1989", "window":"DBPredictor achieved a higher error rate on five datasets: liver-disease, <b>hepatitis</b>  heart-c, credit-g,andechocardiogram. Based on this evidence pruning appears to significantly lower DBPredictor's vulnerability to overspecialization. CHAPTER 7. EMPIRICAL", "mykey":889},
 {"datasetID":47, "supportID":"5F6472F0C8B67ED76F4B12F29F10E625C9F9F467", "rexaID":"287f8092a743949a6e0151893b9e3bc4d03466ed", "author":"Gabor Melli", "title":"A Lazy Model-Based Approach to On-Line Classification", "venue":"University of British Columbia", "year":"1989", "window":"were: echocardiogram, hayes-roth, heart, <b>horse</b> <b>colic</b> andiris datasets. These datasets (marked in Table 7.1 with a * symbol beside their name) contain a sampling of attribute types and domains. For this initial study however the datasets needed to be small enough (#", "mykey":890},
 {"datasetID":53, "supportID":"5F6472F0C8B67ED76F4B12F29F10E625C9F9F467", "rexaID":"287f8092a743949a6e0151893b9e3bc4d03466ed", "author":"Gabor Melli", "title":"A Lazy Model-Based Approach to On-Line Classification", "venue":"University of British Columbia", "year":"1989", "window":"................ 88 7.2 Example of one algorithm (A 1 ) being more accurate than another (A 2 ). . . . 90 7.3 Accuracy performance on the <b>iris</b> dataset for several parameter combinations of the DI n ()basedalgorithm............................ 93 7.4 Parameter settings for the DI n () based algorithm that achieve the lowest", "mykey":891},
 {"datasetID":60, "supportID":"5F6472F0C8B67ED76F4B12F29F10E625C9F9F467", "rexaID":"287f8092a743949a6e0151893b9e3bc4d03466ed", "author":"Gabor Melli", "title":"A Lazy Model-Based Approach to On-Line Classification", "venue":"University of British Columbia", "year":"1989", "window":"DBPredictor achieved a higher error rate on five datasets: <b>liver</b> disease, hepatitis, heart-c, credit-g,andechocardiogram. Based on this evidence pruning appears to significantly lower DBPredictor's vulnerability to overspecialization. CHAPTER 7. EMPIRICAL", "mykey":892},
 {"datasetID":20, "supportID":"5FD5E3B53713B79DDF8025A6AB53839182E82924", "rexaID":"6c5ed6cbf453e8eaad5400424c931537c71a5e3e", "author":"Ke Wang and Shiyu Zhou and Ada Wai-Chee Fu and Jeffrey Xu Yu", "title":"Mining Changes of Classification by Correspondence Tracing", "venue":"SDM", "year":"2003", "window":"German Credit Data from the UCI Repository of Machine Learning Databases [14], and IPUMS <b>Census</b> Data from [1]. These data sets were chosen because no special knowledge is required to understand the addressed applications. To verify if the proposed method finds the changes that are supposed to be found, we need to know such", "mykey":893},
 {"datasetID":127, "supportID":"5FD5E3B53713B79DDF8025A6AB53839182E82924", "rexaID":"6c5ed6cbf453e8eaad5400424c931537c71a5e3e", "author":"Ke Wang and Shiyu Zhou and Ada Wai-Chee Fu and Jeffrey Xu Yu", "title":"Mining Changes of Classification by Correspondence Tracing", "venue":"SDM", "year":"2003", "window":"German Credit Data from the UCI Repository of Machine Learning Databases [14], and <b>IPUMS</b> Census Data from [1]. These data sets were chosen because no special knowledge is required to understand the addressed applications. To verify if the proposed method finds the changes that are supposed to be found, we need to know such", "mykey":894},
 {"datasetID":58, "supportID":"5FD5E3B53713B79DDF8025A6AB53839182E82924", "rexaID":"6c5ed6cbf453e8eaad5400424c931537c71a5e3e", "author":"Ke Wang and Shiyu Zhou and Ada Wai-Chee Fu and Jeffrey Xu Yu", "title":"Mining Changes of Classification by Correspondence Tracing", "venue":"SDM", "year":"2003", "window":"from old ones. Our discussion focuses on forward change mining, but it is equally applicable to backward change mining with the roles of old and new rules exchanged. Example 3.1. We use the  <b>Lenses</b>  data set from the UCI repository [14] to illustrate our approach. There are four attributes, three classes, and 18 examples: Attributes: A 1 : Age: 1, 2, 3 A 2 : Spectacle Prescription: 1, 2 A 3 :", "mykey":895},
 {"datasetID":144, "supportID":"5FD5E3B53713B79DDF8025A6AB53839182E82924", "rexaID":"6c5ed6cbf453e8eaad5400424c931537c71a5e3e", "author":"Ke Wang and Shiyu Zhou and Ada Wai-Chee Fu and Jeffrey Xu Yu", "title":"Mining Changes of Classification by Correspondence Tracing", "venue":"SDM", "year":"2003", "window":"for each small interval to have a separate classification characteristics, either having a different class or having higher accuracy. 4 Experiments We evaluated the proposed method on two real-life data sets, <b>German Credit</b> Data from the UCI Repository of Machine Learning Databases [14], and IPUMS Census Data from [1]. These data sets were chosen because no special knowledge is required to understand", "mykey":896},
 {"datasetID":105, "supportID":"5FD66E44BB96F8919F46F1E20FB1F1CBECB487F3", "rexaID":"632fe4095475bee152843a02969ade56a290db39", "author":"Igor Kononenko and Edvard Simec and Marko Robnik-Sikonja", "title":"Overcoming the Myopia of Inductive Learning Algorithms with RELIEFF", "venue":"Appl. Intell, 7", "year":"1997", "window":"Assistant-R and LFC achieve significantly better result (99.95% confidence level). This result confirms that RELIEFF estimates the quality of attributes better than the information gain. On the <b>VOTE data</b> set the naive Bayesian classifier is the worst, while both versions of Assistant are comparable to the rule based classifier by Smyth et al. [31]. The most interesting results appear in the MESH", "mykey":897},
 {"datasetID":45, "supportID":"5FD66E44BB96F8919F46F1E20FB1F1CBECB487F3", "rexaID":"632fe4095475bee152843a02969ade56a290db39", "author":"Igor Kononenko and Edvard Simec and Marko Robnik-Sikonja", "title":"Overcoming the Myopia of Inductive Learning Algorithms with RELIEFF", "venue":"Appl. Intell, 7", "year":"1997", "window":"obtained from the StatLog database[18]: diagnosis of diabetes (DIAB) and diagnosis of <b>HEART</b> diseases (HEART). For the DIAB data set, Ragavan & Rendell [27]report 78.8% classification accuracy with their LFC algorithm. They also report poor performance of 12 THE AUTHORS??? Table 8 Basic description of the medical data sets domain", "mykey":898},
 {"datasetID":53, "supportID":"5FD66E44BB96F8919F46F1E20FB1F1CBECB487F3", "rexaID":"632fe4095475bee152843a02969ade56a290db39", "author":"Igor Kononenko and Edvard Simec and Marko Robnik-Sikonja", "title":"Overcoming the Myopia of Inductive Learning Algorithms with RELIEFF", "venue":"Appl. Intell, 7", "year":"1997", "window":"We compared the performance of the algorithms also on the following non-medical real world data sets (SOYB, <b>IRIS</b>  and VOTE are obtained from the Irvine database[21], SAT is obtained from the StatLog database [18]): SOYB: The famous soybean data set used by Michalski & Chilausky [17]. IRIS: The", "mykey":899},
 {"datasetID":83, "supportID":"5FD66E44BB96F8919F46F1E20FB1F1CBECB487F3", "rexaID":"632fe4095475bee152843a02969ade56a290db39", "author":"Igor Kononenko and Edvard Simec and Marko Robnik-Sikonja", "title":"Overcoming the Myopia of Inductive Learning Algorithms with RELIEFF", "venue":"Appl. Intell, 7", "year":"1997", "window":"This corresponds to the estimates by RELIEFF with very large number of nearest hits/misses. To test the effect of the normalization factor in eq. (3) we run RELIEFF also on one well known medical data set, ` <b>primary</b> <b>tumor</b> ', described in 6 THE AUTHORS??? Section 5.3. The major difference between the estimates by impurity functions and the estimates by RELIEFF in the ``primary tumor'' problem is in", "mykey":900},
 {"datasetID":90, "supportID":"5FD66E44BB96F8919F46F1E20FB1F1CBECB487F3", "rexaID":"632fe4095475bee152843a02969ade56a290db39", "author":"Igor Kononenko and Edvard Simec and Marko Robnik-Sikonja", "title":"Overcoming the Myopia of Inductive Learning Algorithms with RELIEFF", "venue":"Appl. Intell, 7", "year":"1997", "window":"(SOYB, IRIS, and VOTE are obtained from the Irvine database[21], SAT is obtained from the StatLog database [18]): SOYB: The famous <b>soybean</b> data set used by Michalski & Chilausky [17]. IRIS: The well known Fisher's problem of determining the type of iris flower. MESH3,MESH15: The problem of determining the number of elements for each of the", "mykey":901},
 {"datasetID":91, "supportID":"5FD66E44BB96F8919F46F1E20FB1F1CBECB487F3", "rexaID":"632fe4095475bee152843a02969ade56a290db39", "author":"Igor Kononenko and Edvard Simec and Marko Robnik-Sikonja", "title":"Overcoming the Myopia of Inductive Learning Algorithms with RELIEFF", "venue":"Appl. Intell, 7", "year":"1997", "window":"(SOYB, IRIS, and VOTE are obtained from the Irvine database[21], SAT is obtained from the StatLog database [18]): SOYB: The famous <b>soybean</b> data set used by Michalski & Chilausky [17]. IRIS: The well known Fisher's problem of determining the type of iris flower. MESH3,MESH15: The problem of determining the number of elements for each of the", "mykey":902},
 {"datasetID":145, "supportID":"5FD66E44BB96F8919F46F1E20FB1F1CBECB487F3", "rexaID":"632fe4095475bee152843a02969ade56a290db39", "author":"Igor Kononenko and Edvard Simec and Marko Robnik-Sikonja", "title":"Overcoming the Myopia of Inductive Learning Algorithms with RELIEFF", "venue":"Appl. Intell, 7", "year":"1997", "window":"Milan Soklic for LYMP, Gail Gong for HEPA, Padhraic Smyth for BOOL and LED, Saso Dzeroski for KRK1 and MESH, Bob Henery for the DIAB, <b>HEART</b>  and SAT data sets from the <b>StatLog</b> database at Strathclyde University, and Patrick Murphy and David Aha for the data sets from the Irvine database. We are grateful to our colleagues Saso Dzeroski, Matevz Kovacic,", "mykey":903},
 {"datasetID":98, "supportID":"5FD66E44BB96F8919F46F1E20FB1F1CBECB487F3", "rexaID":"632fe4095475bee152843a02969ade56a290db39", "author":"Igor Kononenko and Edvard Simec and Marko Robnik-Sikonja", "title":"Overcoming the Myopia of Inductive Learning Algorithms with RELIEFF", "venue":"Appl. Intell, 7", "year":"1997", "window":"(LYMP), and diagnosis in rheumatology (RHEU). ffl HEPA: prognostics of survival for patients suffering from hepatitis. The data was provided by Gail Gong from Carnegie-Mellon University. ffl Data sets obtained from the <b>StatLog</b> database[18]: diagnosis of diabetes (DIAB) and diagnosis of heart diseases (HEART). For the DIAB data set, Ragavan & Rendell [27]report 78.8% classification accuracy with", "mykey":904},
 {"datasetID":12, "supportID":"609CF16352322905E6716CA1B1F41E06DEC1DBAE", "rexaID":"faf03ce4427609aa6db25e3d0e6479fb2ae153a7", "author":"Zhi-Hua Zhou and Yuan Jiang and Shifu Chen", "title":"Extracting symbolic rules from trained neural network ensembles", "venue":"AI Commun, 16", "year":"2003", "window":"is tidied and the extraction process terminates. In summary, the flowchart of REFNE is depicted in Fig. 1. 4. Experiments 4.1. Summary In this section, we report the experiments on REFNE on six UCI data sets [4] including <b>balance</b> <b>scale</b>  congressional voting records, hepatitis, iris plant, statlog australian credit approval, and statlog german credit. Since we do not want to test the ability of REFNE in", "mykey":905},
 {"datasetID":46, "supportID":"609CF16352322905E6716CA1B1F41E06DEC1DBAE", "rexaID":"faf03ce4427609aa6db25e3d0e6479fb2ae153a7", "author":"Zhi-Hua Zhou and Yuan Jiang and Shifu Chen", "title":"Extracting symbolic rules from trained neural network ensembles", "venue":"AI Commun, 16", "year":"2003", "window":"80 2 19 13 6 iris plant iris 150 3 4 0 4 statlog australian credit approval credit-a 690 2 15 9 6 statlog german credit credit-g 1,000 2 20 13 7 Table 2 Fidelity of rules extracted via REFNE data set balance voting <b>hepatitis</b> iris credit-a credit-g average fidelity 87.88% 89.26% 84.50% 96.25% 84.13% 74.10% 86.02% Table 3 Comparison of generalization error data set REFNE ensemble single NN C4.5", "mykey":906},
 {"datasetID":53, "supportID":"609CF16352322905E6716CA1B1F41E06DEC1DBAE", "rexaID":"faf03ce4427609aa6db25e3d0e6479fb2ae153a7", "author":"Zhi-Hua Zhou and Yuan Jiang and Shifu Chen", "title":"Extracting symbolic rules from trained neural network ensembles", "venue":"AI Commun, 16", "year":"2003", "window":"80 2 19 13 6 <b>iris</b> plant iris 150 3 4 0 4 statlog australian credit approval credit-a 690 2 15 9 6 statlog german credit credit-g 1,000 2 20 13 7 Table 2 Fidelity of rules extracted via REFNE data set balance voting hepatitis iris credit-a credit-g average fidelity 87.88% 89.26% 84.50% 96.25% 84.13% 74.10% 86.02% Table 3 Comparison of generalization error data set REFNE ensemble single NN C4.5", "mykey":907},
 {"datasetID":56, "supportID":"611C70E221B480ED148AAA9193EB0116E6C917F0", "rexaID":"e5a02b311c1ae7f6cf526e09f08165e731ee35d1", "author":"Huan Liu and Rudy Setiono", "title":"A Probabilistic Approach to Feature Selection - A Filter Solution", "venue":"ICML", "year":"1996", "window":"was divided by Quinlan [ Quinlan, 1993 ] into 490 training instances and 200 test instances. 6. <b>Labor</b> The dataset contains instances for acceptable and unacceptable contracts. It is a small dataset with 16 features, a training set of 40 instances, and a testing set of 17 instances. 7. Mushroom The dataset has a", "mykey":908},
 {"datasetID":73, "supportID":"611C70E221B480ED148AAA9193EB0116E6C917F0", "rexaID":"e5a02b311c1ae7f6cf526e09f08165e731ee35d1", "author":"Huan Liu and Rudy Setiono", "title":"A Probabilistic Approach to Feature Selection - A Filter Solution", "venue":"ICML", "year":"1996", "window":"simplifies the comparison of this work with some published work. These datasets except <b>Mushroom</b> were used in [ John et al., 1994 ] in which comparisons with different methods were described. Nevertheless, the experiments here can alone demonstrate the effectiveness of LVF", "mykey":909},
 {"datasetID":105, "supportID":"61F0B4767D9D78BDFB19DB9B81F02241F2B8A2A4", "rexaID":"09b1c64b200c3b3acff18a3e45a2d75ba0aef2b7", "author":"Julie Greensmith", "title":"New Frontiers For An Artificial Immune System", "venue":"Digital Media Systems Laboratory HP Laboratories Bristol", "year":"2003", "window":"have been created from the #-informative words, and have been labelled, the classifier is all set and ready to go. 5 times 10 fold cross validation was performed in a similar manner to the <b>voting dataset</b> and the results collated. Standard statistical techniques including Wilcoxon Mann-Whitney ranking [33] and Students t-test [30]are used in order to analyse the results. The Wilcoxon ranking was used", "mykey":910},
 {"datasetID":47, "supportID":"61F0B4767D9D78BDFB19DB9B81F02241F2B8A2A4", "rexaID":"09b1c64b200c3b3acff18a3e45a2d75ba0aef2b7", "author":"Julie Greensmith", "title":"New Frontiers For An Artificial Immune System", "venue":"Digital Media Systems Laboratory HP Laboratories Bristol", "year":"2003", "window":"with a 1 and the absence of that word is marked with a 0. This gives rise to the creation of the feature vectors for use within the classifier. For example, our most informative words from the tiny dataset above were <b>horse</b> and monitor. So, the feature vector for the document D containing the words \"I rode a horse today\" would look like \"1 0\", thus denoting the presence of the word horse within the", "mykey":911},
 {"datasetID":53, "supportID":"61F0B4767D9D78BDFB19DB9B81F02241F2B8A2A4", "rexaID":"09b1c64b200c3b3acff18a3e45a2d75ba0aef2b7", "author":"Julie Greensmith", "title":"New Frontiers For An Artificial Immune System", "venue":"Digital Media Systems Laboratory HP Laboratories Bristol", "year":"2003", "window":"using the g++ compiler version 2.96 for Red Hat Linux 7.3 2.96-113, and was run on one out of 4 of Intel Pentium fiff 4 CPU 1.80GHz HP `e-PC's'. On completion of the compilation process, the <b>iris</b> dataset (provided with the source code) was used to perform preliminary testing on the system. Once it was clear on how to use the various parameter settings, and that classification could be performed,", "mykey":912},
 {"datasetID":53, "supportID":"620AFA2CB7E403C38DD504B8B9FD06AA07ACD674", "rexaID":"5047fbe99b73ea1e127150b6688d65effd51f4c1", "author":"Manoranjan Dash and Kiseok Choi and Peter Scheuermann and Huan Liu", "title":"Feature Selection for Clustering - A Filter Solution", "venue":"ICDM", "year":"2002", "window":"are almost correct as well as the selected features are all important and it missed out only one important feature. 5.2 Benchmark and Real Datasets <b>Iris</b> dataset, popularly used for testing clustering and classification algorithms, is taken from UCI ML repository [5]. It contains 3 classes of 50 instances each, where each class refers to a type", "mykey":913},
 {"datasetID":110, "supportID":"620AFA2CB7E403C38DD504B8B9FD06AA07ACD674", "rexaID":"5047fbe99b73ea1e127150b6688d65effd51f4c1", "author":"Manoranjan Dash and Kiseok Choi and Peter Scheuermann and Huan Liu", "title":"Feature Selection for Clustering - A Filter Solution", "venue":"ICDM", "year":"2002", "window":"is taken from the recently publicized clustering software CLUTO available from the web site http://www-users.cs.umn.edu/ ~ karypis/cluto/. In this, there is a dataset called Genes2 which has 99 <b>yeast</b> genes (or data points) described using 7 profiles (or features). When ForwardSelect is run over this data, it shows the minimum entropy for subset fF3,F5g (see", "mykey":914},
 {"datasetID":14, "supportID":"621A925413DA8A0627C5B71FC827E1548825B497", "rexaID":"7265efd898e4c045ff078fcb63fec9fbde4b1249", "author":"Endre Boros and Peter Hammer and Toshihide Ibaraki and Alexander Kogan and Eddy Mayoraz and Ilya B. Muchnik", "title":"An Implementation of Logical Analysis of Data", "venue":"IEEE Trans. Knowl. Data Eng, 12", "year":"2000", "window":"the housing value is above or below the median. Using training sets of 80% of the observations, [16] reports correct prediction rates ranging from 82% to 83.2%. <b>Breast</b> <b>Cancer</b> (Wisconsin). The dataset, compiled by O. Mangasarian and K.P. Bennett, is widely used in the machine learning community for comparing learning algorithms. It is, however, di\u00c6cult to use it for rigorous comparisons since its", "mykey":915},
 {"datasetID":17, "supportID":"621A925413DA8A0627C5B71FC827E1548825B497", "rexaID":"7265efd898e4c045ff078fcb63fec9fbde4b1249", "author":"Endre Boros and Peter Hammer and Toshihide Ibaraki and Alexander Kogan and Eddy Mayoraz and Ilya B. Muchnik", "title":"An Implementation of Logical Analysis of Data", "venue":"IEEE Trans. Knowl. Data Eng, 12", "year":"2000", "window":"the housing value is above or below the median. Using training sets of 80% of the observations, [16] reports correct prediction rates ranging from 82% to 83.2%. <b>Breast</b> <b>Cancer</b>  <b>Wisconsin</b> . The dataset, compiled by O. Mangasarian and K.P. Bennett, is widely used in the machine learning community for comparing learning algorithms. It is, however, di\u00c6cult to use it for rigorous comparisons since its", "mykey":916},
 {"datasetID":15, "supportID":"621A925413DA8A0627C5B71FC827E1548825B497", "rexaID":"7265efd898e4c045ff078fcb63fec9fbde4b1249", "author":"Endre Boros and Peter Hammer and Toshihide Ibaraki and Alexander Kogan and Eddy Mayoraz and Ilya B. Muchnik", "title":"An Implementation of Logical Analysis of Data", "venue":"IEEE Trans. Knowl. Data Eng, 12", "year":"2000", "window":"the housing value is above or below the median. Using training sets of 80% of the observations, [16] reports correct prediction rates ranging from 82% to 83.2%. <b>Breast</b> <b>Cancer</b>  <b>Wisconsin</b> . The dataset, compiled by O. Mangasarian and K.P. Bennett, is widely used in the machine learning community for comparing learning algorithms. It is, however, di\u00c6cult to use it for rigorous comparisons since its", "mykey":917},
 {"datasetID":16, "supportID":"621A925413DA8A0627C5B71FC827E1548825B497", "rexaID":"7265efd898e4c045ff078fcb63fec9fbde4b1249", "author":"Endre Boros and Peter Hammer and Toshihide Ibaraki and Alexander Kogan and Eddy Mayoraz and Ilya B. Muchnik", "title":"An Implementation of Logical Analysis of Data", "venue":"IEEE Trans. Knowl. Data Eng, 12", "year":"2000", "window":"the housing value is above or below the median. Using training sets of 80% of the observations, [16] reports correct prediction rates ranging from 82% to 83.2%. <b>Breast</b> <b>Cancer</b>  <b>Wisconsin</b> . The dataset, compiled by O. Mangasarian and K.P. Bennett, is widely used in the machine learning community for comparing learning algorithms. It is, however, di\u00c6cult to use it for rigorous comparisons since its", "mykey":918},
 {"datasetID":34, "supportID":"621A925413DA8A0627C5B71FC827E1548825B497", "rexaID":"7265efd898e4c045ff078fcb63fec9fbde4b1249", "author":"Endre Boros and Peter Hammer and Toshihide Ibaraki and Alexander Kogan and Eddy Mayoraz and Ilya B. Muchnik", "title":"An Implementation of Logical Analysis of Data", "venue":"IEEE Trans. Knowl. Data Eng, 12", "year":"2000", "window":"consistent. According to [12], correct predictions rates reported in the literature about this dataset range from 84% to 95.6%. <b>Diabetes</b>  This dataset, compiled by the National Institute of Diabetes and Digestive and Kidney Diseases, was contributed to the repository by V. Sigillito. The dataset", "mykey":919},
 {"datasetID":45, "supportID":"621A925413DA8A0627C5B71FC827E1548825B497", "rexaID":"7265efd898e4c045ff078fcb63fec9fbde4b1249", "author":"Endre Boros and Peter Hammer and Toshihide Ibaraki and Alexander Kogan and Eddy Mayoraz and Ilya B. Muchnik", "title":"An Implementation of Logical Analysis of Data", "venue":"IEEE Trans. Knowl. Data Eng, 12", "year":"2000", "window":"rates ranging from 71.4% to 74.4%. Further, [19] reports a 76% correct prediction rate using 75% of the data for training. <b>heart</b> Disease (Cleveland). The Cleveland Clinic Foundation heart disease dataset, contributed to the repository by R. Detrano, contains 303 observations, 165 of which describe healthy people and 138 sick ones; 7 observations are incomplete, and 2 of the observations of healthy", "mykey":920},
 {"datasetID":48, "supportID":"621A925413DA8A0627C5B71FC827E1548825B497", "rexaID":"7265efd898e4c045ff078fcb63fec9fbde4b1249", "author":"Endre Boros and Peter Hammer and Toshihide Ibaraki and Alexander Kogan and Eddy Mayoraz and Ilya B. Muchnik", "title":"An Implementation of Logical Analysis of Data", "venue":"IEEE Trans. Knowl. Data Eng, 12", "year":"2000", "window":"in 85.51% of the cases. Furthermore, it is interesting to notice that the inclusion of additional patterns in the discriminant does not seem to improve the prediction accuracy. Boston <b>Housing</b>  This dataset was created by D. Harrison and D. Rubinfeld in 1978 and contains 506 records describing housing values in the suburbs of Boston, depending on observations consisting of one binary and 12 continuous", "mykey":921},
 {"datasetID":56, "supportID":"621A925413DA8A0627C5B71FC827E1548825B497", "rexaID":"7265efd898e4c045ff078fcb63fec9fbde4b1249", "author":"Endre Boros and Peter Hammer and Toshihide Ibaraki and Alexander Kogan and Eddy Mayoraz and Ilya B. Muchnik", "title":"An Implementation of Logical Analysis of Data", "venue":"IEEE Trans. Knowl. Data Eng, 12", "year":"2000", "window":"tests involving different items, as a way of verifying the validity of that patient's answers. 20 IDIAP{RR 96-05 8.3 <b>labor</b> Productivity in China 4 Using the o\u00c6cial China Statistical Yearbooks, a dataset was compiled in [8, 7] for the analysis of labor productivity in the developing Chinese economy. The dataset provides 290 annual observations for the 29 Chinese provinces for the period 1985 { 1994.", "mykey":922},
 {"datasetID":143, "supportID":"621A925413DA8A0627C5B71FC827E1548825B497", "rexaID":"7265efd898e4c045ff078fcb63fec9fbde4b1249", "author":"Endre Boros and Peter Hammer and Toshihide Ibaraki and Alexander Kogan and Eddy Mayoraz and Ilya B. Muchnik", "title":"An Implementation of Logical Analysis of Data", "venue":"IEEE Trans. Knowl. Data Eng, 12", "year":"2000", "window":"from the Irvine repository which were used in our experiments. <b>Australian Credit</b> Card. The dataset, submitted to the repository by J. Quinlan, consists of 690 records of MasterCard applicants, 307 of which are classified as positive and 383 as negative. While 37 records have some missing data,", "mykey":923},
 {"datasetID":1, "supportID":"62B922714A7AF6A63EF7E7DF484DD91FFEF26FE2", "rexaID":"500f7245b01ee7978bef7ad022c0cbe7eec8eace", "author":"Christopher J. Merz", "title":"Combining Classifiers Using Correspondence Analysis", "venue":"NIPS", "year":"1997", "window":"the direct comparison of SCANN with the S-BP and S-Bayes, SCANN posts 5 and 4 signifcant wins, respectively, and no losses. The most dramatic improvement of the combiners over PV came in the <b>abalone</b> data set. A closer look at the result revealed that 7 of the 8 learned models were very poor classifiers with error rates around 80 percent. This empirically demonstrates PV's known sensitivity to learned", "mykey":924},
 {"datasetID":42, "supportID":"62B922714A7AF6A63EF7E7DF484DD91FFEF26FE2", "rexaID":"500f7245b01ee7978bef7ad022c0cbe7eec8eace", "author":"Christopher J. Merz", "title":"Combining Classifiers Using Correspondence Analysis", "venue":"NIPS", "year":"1997", "window":"with error rates around 80 percent. This empirically demonstrates PV's known sensitivity to learned models with highly correlated errors. On the other hand, PV performs well on the <b>glass</b> and wave data sets where the errors of the learned models are measured to be fairly uncorrelated. Here, SCANN performs similarly to PV, but S-BP and S-Bayes appear to be overfitting by making erroneous predictions", "mykey":925},
 {"datasetID":14, "supportID":"62F9594F8F2D9E2742D3EABB99CC7227469F46E3", "rexaID":"2d05e6777bcfde449fc35a3a95dde0697a5c49ac", "author":"David W. Opitz and Richard Maclin", "title":"Popular Ensemble Methods: An Empirical Study", "venue":"J. Artif. Intell. Res. (JAIR, 11", "year":"1999", "window":"ensemble. Also shown (results column 3) is the \"best\" result produced from all of the single network results run using all of the training data. 197 Opitz & Maclin Single Bagging Arcing Boosting Data Set Err SD Best Err SD Err SD Err SD <b>breast</b> <b>cancer</b> w 5.0 0.7 4.0 3.7 0.5 3.5 0.6 3.5 0.3 credit-a 14.9 0.8 14.2 13.4 0.5 14.0 0.9 13.7 0.5 credit-g 29.6 1.0 28.7 25.2 0.7 25.9 1.0 26.7 0.4 diabetes 27.8", "mykey":926},
 {"datasetID":46, "supportID":"62F9594F8F2D9E2742D3EABB99CC7227469F46E3", "rexaID":"2d05e6777bcfde449fc35a3a95dde0697a5c49ac", "author":"David W. Opitz and Richard Maclin", "title":"Popular Ensemble Methods: An Empirical Study", "venue":"J. Artif. Intell. Res. (JAIR, 11", "year":"1999", "window":"then the decision-tree ensemble methods also had lower (or higher) error than their neural network counterpart. The exceptions to this rule generally happened on the same data set for all three ensemble methods (e.g., <b>hepatitis</b>  soybean, satellite, credit-a, and heart-cleveland). These results suggest that (a) the performance of the ensemble methods is dependent on both the", "mykey":927},
 {"datasetID":1, "supportID":"62F990B49134CD6D18256372320D07EBDF4846AB", "rexaID":"a8617c8faca95eaf149b490cac7eebacb2af28ea", "author":"Christian Borgelt and Rudolf Kruse", "title":"Speeding Up Fuzzy Clustering with Neural Network Techniques", "venue":"Research Group Neural Networks and Fuzzy Systems Dept. of Knowledge Processing and Language Engineering, School of Computer Science Otto-von-Guericke-University of Magdeburg", "year":"", "window":"FCM a.p. GK GK dataset exp. mom. exp. mom. exp. mom. <b>abalone</b> 3 1.50 0.30 1.40 0.20 1.90 0.65 abalone 6 1.60 0.50 1.40 0.25 1.90 0.70 breast 2 1.20 0.05 1.30 0.10 1.80 0.55 iris 3 1.40 0.15 1.20 0.05 1.80 0.50 wine 3 1.40", "mykey":928},
 {"datasetID":109, "supportID":"62F990B49134CD6D18256372320D07EBDF4846AB", "rexaID":"a8617c8faca95eaf149b490cac7eebacb2af28ea", "author":"Christian Borgelt and Rudolf Kruse", "title":"Speeding Up Fuzzy Clustering with Neural Network Techniques", "venue":"Research Group Neural Networks and Fuzzy Systems Dept. of Knowledge Processing and Language Engineering, School of Computer Science Otto-von-Guericke-University of Magdeburg", "year":"", "window":"the number of clusters to find (abalone: 3, breast: 2, iris: 3, <b>wine</b>  3), so we ran the clustering using these numbers. In addition, we ran the algorithm with 6 clusters for the abalone and the wine dataset. The clustering process was terminated when a normal update step changed no center coordinate by more than 10 6 . That is, regardless of the modification employed, we used the normal update step to", "mykey":929},
 {"datasetID":151, "supportID":"630C070E5F44F43845A52E5C1ACA3A77F5CF8ACA", "rexaID":"36d1c58be0e3c0c1d57492d050d8a71a04a72858", "author":"Ayhan Demiriz and Kristin P. Bennett", "title":"Chapter 1 OPTIMIZATIONAPPROACHESTOSEMI-SUPERVISED LEARNING", "venue":"Department of Decision Sciences and Engineering Systems & Department of Mathematical Sciences, Rensselaer Polytechnic Institute", "year":"", "window":"as in the previous section. Due to the long computational times for S \u00b5 VM-IQP and transductive SVM-Light, we limit our experiments to only the Heart, Housing, Ionosphere, and <b>Sonar</b> datasets. Linear kernel functions are used for all methods used in this section. The results given in Table 1.3 show that using unlabeled data in the case of datasets Heart and Ionosphere affects", "mykey":930},
 {"datasetID":45, "supportID":"630C070E5F44F43845A52E5C1ACA3A77F5CF8ACA", "rexaID":"36d1c58be0e3c0c1d57492d050d8a71a04a72858", "author":"Ayhan Demiriz and Kristin P. Bennett", "title":"Chapter 1 OPTIMIZATIONAPPROACHESTOSEMI-SUPERVISED LEARNING", "venue":"Department of Decision Sciences and Engineering Systems & Department of Mathematical Sciences, Rensselaer Polytechnic Institute", "year":"", "window":"an action reduces the overall error. Like S \u00b5 VM-IQP, SVM-Light alternates 16 APPLICATIONS AND ALGORITHMS OF COMPLEMENTARITY Table 1.3 Average Error Results for Transductive and Inductive Methods Data Set SVM-QP SVM-Light S \u00b5 VM-IQP <b>Heart</b> 0.16 0.163 0.1966 Housing 0.1804 0.1608 0.1647 Ionosphere 0.0857 0.1572 0.0943 Sonar 0.1762 0.2524 0.1572 the labels to avoid local minima. The primary difference", "mykey":931},
 {"datasetID":48, "supportID":"630C070E5F44F43845A52E5C1ACA3A77F5CF8ACA", "rexaID":"36d1c58be0e3c0c1d57492d050d8a71a04a72858", "author":"Ayhan Demiriz and Kristin P. Bennett", "title":"Chapter 1 OPTIMIZATIONAPPROACHESTOSEMI-SUPERVISED LEARNING", "venue":"Department of Decision Sciences and Engineering Systems & Department of Mathematical Sciences, Rensselaer Polytechnic Institute", "year":"", "window":"of the working set is set to 50 points and rest of the data are used as the training set. We use the following formula to pick the penalty parameter: 1 The continuous response variable in <b>Housing</b> dataset was categorized at 21.5 12 APPLICATIONS AND ALGORITHMS OF COMPLEMENTARITY Table 1.2 Average Error Results for Inductive and Transductive SVM Methods Data Set SVM-RLP \u00d3\u00d4\u00b5\u00d6\u00d5\u00c9\u00d1 Local SVM Local \u00d3\u00ce\u00b5S\u00d5\u00d7\u00d1", "mykey":932},
 {"datasetID":52, "supportID":"630C070E5F44F43845A52E5C1ACA3A77F5CF8ACA", "rexaID":"36d1c58be0e3c0c1d57492d050d8a71a04a72858", "author":"Ayhan Demiriz and Kristin P. Bennett", "title":"Chapter 1 OPTIMIZATIONAPPROACHESTOSEMI-SUPERVISED LEARNING", "venue":"Department of Decision Sciences and Engineering Systems & Department of Mathematical Sciences, Rensselaer Polytechnic Institute", "year":"", "window":"as in the previous section. Due to the long computational times for S \u00b5 VM-IQP and transductive SVM-Light, we limit our experiments to only the Heart, Housing, <b>Ionosphere</b>  and Sonar datasets. Linear kernel functions are used for all methods used in this section. The results given in Table 1.3 show that using unlabeled data in the case of datasets Heart and Ionosphere affects", "mykey":933},
 {"datasetID":2, "supportID":"632FB7A92DF490C156A254C0A4B5464EE191C1FB", "rexaID":"94cf84d5a454c97d326efdb24b2b96e0eac25c33", "author":"Kristin P. Bennett and Ayhan Demiriz and John Shawe-Taylor", "title":"A Column Generation Algorithm For Boosting", "venue":"ICML", "year":"2000", "window":"all points and # i measures the additional margin obtained by each point. AdaBoost also minimizes a margin cost function based on the margin obtained by each point. We ran experiments on two larger datasets: Forest and <b>Adult</b>  from UCI(Murphy & Aha, 1992). Forest is a 54-dimension dataset with 7 possible classes. The data are divided into 11340 training, 3780 validation and 565892 testing instances.", "mykey":934},
 {"datasetID":14, "supportID":"632FB7A92DF490C156A254C0A4B5464EE191C1FB", "rexaID":"94cf84d5a454c97d326efdb24b2b96e0eac25c33", "author":"Kristin P. Bennett and Ayhan Demiriz and John Shawe-Taylor", "title":"A Column Generation Algorithm For Boosting", "venue":"ICML", "year":"2000", "window":"LPBoost has a well defined stopping criterion that is reached in a few iterations. It uses few weak learners. There are only 81 possible stumps on the <b>Breast</b> <b>Cancer</b> dataset (9 attributes having 9 possible values), so clearly AdaBoost may require the same tree to be generated multiple times. LPBoost generates a weak learner only once and can alter the weight on that", "mykey":935},
 {"datasetID":151, "supportID":"632FB7A92DF490C156A254C0A4B5464EE191C1FB", "rexaID":"94cf84d5a454c97d326efdb24b2b96e0eac25c33", "author":"Kristin P. Bennett and Ayhan Demiriz and John Shawe-Taylor", "title":"A Column Generation Algorithm For Boosting", "venue":"ICML", "year":"2000", "window":"where the base learner solves (6) exactly, then to examine LPBoost in a more realistic environment. 5.1 Boosting Decision Tree Stumps We used decision tree stumps as a base learner on six UCI datasets: Cancer (9,699), Heart (13,297), <b>Sonar</b> (60,208), Ionosphere (34,351), Diagnostic (30,569), and Musk (166,476). The number of features and number of points in each dataset are shown in parentheses", "mykey":936},
 {"datasetID":45, "supportID":"632FB7A92DF490C156A254C0A4B5464EE191C1FB", "rexaID":"94cf84d5a454c97d326efdb24b2b96e0eac25c33", "author":"Kristin P. Bennett and Ayhan Demiriz and John Shawe-Taylor", "title":"A Column Generation Algorithm For Boosting", "venue":"ICML", "year":"2000", "window":"where the base learner solves (6) exactly, then to examine LPBoost in a more realistic environment. 5.1 Boosting Decision Tree Stumps We used decision tree stumps as a base learner on six UCI datasets: Cancer (9,699), <b>Heart</b> (13,297), Sonar (60,208), Ionosphere (34,351), Diagnostic (30,569), and Musk (166,476). The number of features and number of points in each dataset are shown in parentheses", "mykey":937},
 {"datasetID":94, "supportID":"6347E71BF8FD792FE39465F7D322E40FA5523268", "rexaID":"a2734ae038cae7393159934e860c24a52dc2754d", "author":"Don R. Hush and Clint Scovel and Ingo Steinwart", "title":"Los Alamos National Laboratory Stability of Unstable Learning Algorithms", "venue":"Modeling, Algorithms and Informatics Group, CCS-3", "year":"2003", "window":"are synthetically generated according to Fukunaga's so-called I-4I and I-# distributions (Fukunaga, 1990), and the third is the <b>Spambase</b> data set from the UCI repository (Blake & Merz, 1998). For the synthetic data sets we set d = 8 and generate samples from R d # f 1; 1g according to the I-4I and I-# distributions. For both distributions the", "mykey":938},
 {"datasetID":53, "supportID":"63D83289E3183E39B73E260F4C0131ACE7C4BE9A", "rexaID":"8a6877d257fd7de465ca5f47834294629105e92c", "author":"Michael P. Cummings and Daniel S. Myers and Marci Mangelson", "title":"Applying Permuation Tests to Tree-Based Statistical Models: Extending the R Package rpart", "venue":"Center for Bioinformatics and Computational Biology, Institute for Advanced Computer Studies, University of Maryland", "year":"", "window":"In this section we show several examples of the application of permutation tests to tree-based statistical models. We begin by permutation testing a classification tree built on the famous <b>Iris</b> dataset setosa: 50 versicolor: 50 virginica: 50 virginica: 0 versiolor: 0 setosa: 0 versicolor: 50 virginica: 50 virginica: 45 versicolor: 1 virginica: 5 versicolor: 49 setosa: 0 petal length < 2.45 cm", "mykey":939},
 {"datasetID":4, "supportID":"64096AF17E6F1B54B5C4C2DA1ADA4CE2C0BAA972", "rexaID":"b1f8720873793cfc73b06213300b3bdb1ad5e8ef", "author":"Kristin P. Bennett and Erin J. Bredensteiner", "title":"Geometry in Learning", "venue":"Department of Mathematical Sciences Rensselaer Polytechnic Institute", "year":"", "window":"that are publicly available via the World Wide <b>Web</b>  All of the above datasets are available via <b>anonymous</b> file transfer protocol (ftp) from the UCI Repository of Machine Learning Databases and Domain Theories [24] at ftp://ftp.ics.uci.edu/pub/machine-learning-databases. The", "mykey":940},
 {"datasetID":151, "supportID":"64096AF17E6F1B54B5C4C2DA1ADA4CE2C0BAA972", "rexaID":"b1f8720873793cfc73b06213300b3bdb1ad5e8ef", "author":"Kristin P. Bennett and Erin J. Bredensteiner", "title":"Geometry in Learning", "venue":"Department of Mathematical Sciences Rensselaer Polytechnic Institute", "year":"", "window":"signals off a metal cylinder. The <b>Sonar</b> signal is transmitted at various angles with rises in frequency. A similar procedure is performed to obtain the rock attributes. The publicly available Sonar dataset represents 208 <b>mines</b> and <b>rocks</b> [12]. Sixty real-valued attributes between 0.0 and 1.0 are collected for each mine or rock. The value of the attribute represents the amount of energy within a", "mykey":941},
 {"datasetID":45, "supportID":"64096AF17E6F1B54B5C4C2DA1ADA4CE2C0BAA972", "rexaID":"b1f8720873793cfc73b06213300b3bdb1ad5e8ef", "author":"Kristin P. Bennett and Erin J. Bredensteiner", "title":"Geometry in Learning", "venue":"Department of Mathematical Sciences Rensselaer Polytechnic Institute", "year":"", "window":"<b>Heart</b> disease status is known. By evaluating a new patient's attributes with respect to the separating plane a diagnosis is made. The Cleveland Heart Disease Database (Heart) is a publicly available dataset that contains information on 297 patients using 13 attributes [6]. A second application, as discussed previously, is the diagnosis of breast cancer. To evaluate whether a tumor is benign or", "mykey":942},
 {"datasetID":151, "supportID":"642E47B14E7BCD832996218E268B19AFA35984CF", "rexaID":"f15ee3b90f8d2dd6218e7cffa513b49d9f6dda1c", "author":"Jing Peng and Bir Bhanu", "title":"Feature Relevance Estimation for Image Databases", "venue":"Multimedia Information Systems", "year":"1999", "window":"significant performance improvement across the tasks. In general, PFRL seems to outperform MARS on all the tasks. Superior performance by PFRL is particularly pronounced on the SegData and <b>Sonar</b> data sets. Moreover, the results show that PFRL can handle large problems with high dimensionality well by its superb performance on the 60 dimensional Sonar data set. Table 1: Average retrieval precision", "mykey":943},
 {"datasetID":34, "supportID":"6488EB1A04F3F3E1DB148DD78B60025C6B1487D2", "rexaID":"05c326dc479677ca55b6b6e260c8622539762b62", "author":"Hussein A. Abbass", "title":"Pareto Neuro-Evolution: Constructing Ensemble of Neural Networks Using Multi-objective Optimization", "venue":"Artificial Life and Adaptive Robotics (A.L.A.R.) Lab, School of Information Technology and Electrical Engineering, Australian Defence Force Academy", "year":"", "window":"size 25, the learning rate for BP 0.003, the number of hidden units is set to 5, and the number of epochs BP was applied to an individual is set to 5 for each subset incase of Prob1. The <b>diabetes</b> dataset has 768 patterns; 500 belonging to the first class and 268 to the second. It contains 8 attributes. The classification problem is difficult as the class value is a binarized form of another", "mykey":944},
 {"datasetID":143, "supportID":"6488EB1A04F3F3E1DB148DD78B60025C6B1487D2", "rexaID":"05c326dc479677ca55b6b6e260c8622539762b62", "author":"Hussein A. Abbass", "title":"Pareto Neuro-Evolution: Constructing Ensemble of Neural Networks Using Multi-objective Optimization", "venue":"Artificial Life and Adaptive Robotics (A.L.A.R.) Lab, School of Information Technology and Electrical Engineering, Australian Defence Force Academy", "year":"", "window":"tested MPANN on two benchmark problems; the <b>Australian credit</b> card assessment problem and the diabetes problem, available by anonymous ftp from ice.uci.edu [6]. The Australian credit card assessment dataset contains 690 patterns with 14 attributes; 6 numeric and 8 discrete (with 2 to 14 possible values). The predicted class is binary - 1 for awarding the credit and 0 for not. To be consistent with the", "mykey":945},
 {"datasetID":105, "supportID":"649EB4948F8B02309F15F52D9F057793787CD88E", "rexaID":"a3483a19000e691a9fbd69f2b008445f3acd3124", "author":"Blai Bonet and Hector Geffner", "title":"Learning Sorting and Decision Trees with POMDPs", "venue":"ICML", "year":"1998", "window":"takes a few minutes on average and leaves a few thousand entries in the hash table. For the larger <b>Votes dataset</b>  the run takes 24 minutes on average and leaves around 16000 entries in the hash table. During testing, whenever a new belief state b o a was generated that was not in the hash table, b o a was", "mykey":946},
 {"datasetID":70, "supportID":"649EB4948F8B02309F15F52D9F057793787CD88E", "rexaID":"a3483a19000e691a9fbd69f2b008445f3acd3124", "author":"Blai Bonet and Hector Geffner", "title":"Learning Sorting and Decision Trees with POMDPs", "venue":"ICML", "year":"1998", "window":"figures for ID3 and C4.5 were taken from (Friedman, Kohavi, & Yun 1996). The column named `Test' in the table indicates how the generalization performance of the algorithms was measured. The <b>Monk</b> n datasets come with separate training and test data; on the other two problems the test data was generated by 5-fold cross validation: the data were partitioned into five segments, and fives runs were", "mykey":947},
 {"datasetID":52, "supportID":"64C12555FFDFEEDE0F8397C5A668F7BE49554A25", "rexaID":"fe52c7b7465f378427ac09aa5480dd559ff5f569", "author":"Christos Emmanouilidis and Anthony Hunter", "title":"A Comparison of Crossover Operators in Neural Network Feature Selection with Multiobjective Evolutionary Algorithms", "venue":"Centre for Adaptive Systems, School of Computing, Engineering and Technology University of Sunderland", "year":"", "window":"it reduces the effect of the noise in fitness evaluation. 4 EXPERIMENTAL INVESTIGATION We compare the performance of the SSOCF operator against that of standard n-point crossover on a benchmarking data set of considerable dimensionality, the <b>ionosphere</b> dataset [13]. It consists of 351 patterns, with 34 attributes and one output with two classes. Ten random permutations of this data set are employed.", "mykey":948},
 {"datasetID":12, "supportID":"653A012F03822736CA9CF2694CF88AB18E3F5A02", "rexaID":"2ea57412a6ce00f095ef1a23aba00961bb392bf0", "author":"Nir Friedman and Mois\u00e9s Goldszmidt and Thomas J. Lee", "title":"Bayesian Network Classification with Continuous Attributes: Getting the Best of Both Discretization and Parametric Fitting", "venue":"ICML", "year":"1998", "window":"experimental results show, the additional flexibility of the mixture results in drastically improved performance in the cases where the Gaussian TAN did poorly (see, for example, the accuracy of the data sets \"anneal-U\" and  <b>balance</b> <b>scale</b>  in Table 1). In this paper, we learned mixtures only when modeling a continuous feature with discrete parents. We note, however, that learning a mixture of linear", "mykey":949},
 {"datasetID":2, "supportID":"6570985DE2548D526BEB59C8D67AAB5342221CEC", "rexaID":"0ded09c3d904c9f8eeef9ed0bc3445c2b44a6008", "author":"William W. Cohen and Yoram Singer", "title":"A Simple, Fast, and Effective Rule Learner", "venue":"AT&T Labs--Research Shannon Laboratory", "year":"", "window":"average ranks among these three are 1.8, 2.3, and 1.9. The largest ruleset produced by SLIPPER is 49 rules (for coding). Finally, we evaluated the scalability of the rule learners on several large datasets. We used <b>adult</b>  blackjack, with the addition of 20 irrelevant noise variables; and market3, for which many examples were available. C4rules was not run, since it is known to have scalability", "mykey":950},
 {"datasetID":110, "supportID":"65F20FD9350002D8A3011B4BF633843A13CBDD9D", "rexaID":"76b4e0dc34501a39e272d5331e363877c2522d31", "author":"Samuel Kaski and Jaakko Peltonen", "title":"Informative Discriminant Analysis", "venue":"ICML", "year":"2003", "window":"of noise in parameter validation. 5. Analysis of Gene Expression Data In this section we demonstrate one way of using the extracted components for exploratory analysis of <b>yeast</b> gene expression. The data set (Hughes et al., 2000) consists of measurements of the expression of each yeast gene in 300 knock-out mutation experiments. After leaving out all genes and experiments without significant expression,", "mykey":951},
 {"datasetID":48, "supportID":"662A54E93FB90C038071BCAD2CAF74BE013C99E2", "rexaID":"b921d6a9c3c9b7cd79c2e3c070080f7c6770ac28", "author":"S. Sathiya Keerthi", "title":"Improvements to SMO Algorithm for SVM Regression", "venue":"Author for Correspondence: Prof", "year":"", "window":"samples were chosen randomly. The performance of the four algorithms for the polynomial kernel k(x i ; x j ) = (1 + x i Delta x j ) p where p was chosen to be 3, is shown in Fig. 1. The second dataset is the Boston <b>housing</b> dataset which is a standard benchmark for testing regression algorithms. This dataset is available at UCI Repository [1]. The dimension of the input is 13. We used the training", "mykey":952},
 {"datasetID":7, "supportID":"668135C3220E95D3CABB4B212E50612A18B74B9E", "rexaID":"1ef1e2c976bf0ee1cac221de391fcfabe5b10188", "author":"Geoffrey I Webb", "title":"Learning Decision Lists by Prepending Inferred Rules", "venue":"School of Computing and Mathematics Deakin University", "year":"", "window":"were compiled by M. Zwitter and M. Soklic at University Medical Centre, Institute of Oncology, Ljubljana, Yugoslavia. The <b>Audiology</b> data set was compiled by Professor Jergen at Baylor College of Medicine. References Clark, P., & Boswell, R. (1991). Rule induction with CN2: some recent improvements. In Proceedings of the Fifth European", "mykey":953},
 {"datasetID":8, "supportID":"668135C3220E95D3CABB4B212E50612A18B74B9E", "rexaID":"1ef1e2c976bf0ee1cac221de391fcfabe5b10188", "author":"Geoffrey I Webb", "title":"Learning Decision Lists by Prepending Inferred Rules", "venue":"School of Computing and Mathematics Deakin University", "year":"", "window":"were compiled by M. Zwitter and M. Soklic at University Medical Centre, Institute of Oncology, Ljubljana, Yugoslavia. The <b>Audiology</b> data set was compiled by Professor Jergen at Baylor College of Medicine. References Clark, P., & Boswell, R. (1991). Rule induction with CN2: some recent improvements. In Proceedings of the Fifth European", "mykey":954},
 {"datasetID":14, "supportID":"668135C3220E95D3CABB4B212E50612A18B74B9E", "rexaID":"1ef1e2c976bf0ee1cac221de391fcfabe5b10188", "author":"Geoffrey I Webb", "title":"Learning Decision Lists by Prepending Inferred Rules", "venue":"School of Computing and Mathematics Deakin University", "year":"", "window":"supported by the Australian Research Council. I am grateful to Mike Cammeron-Jones for discussions that helped refine the ideas presented herein. The <b>Breast</b> <b>Cancer</b>  Lymphography and Primary Tumor data sets were compiled by M. Zwitter and M. Soklic at University Medical Centre, Institute of Oncology, Ljubljana, Yugoslavia. The Audiology data set was compiled by Professor Jergen at Baylor College of", "mykey":955},
 {"datasetID":63, "supportID":"668135C3220E95D3CABB4B212E50612A18B74B9E", "rexaID":"1ef1e2c976bf0ee1cac221de391fcfabe5b10188", "author":"Geoffrey I Webb", "title":"Learning Decision Lists by Prepending Inferred Rules", "venue":"School of Computing and Mathematics Deakin University", "year":"", "window":"supported by the Australian Research Council. I am grateful to Mike Cammeron-Jones for discussions that helped refine the ideas presented herein. The Breast Cancer, <b>Lymphography</b> and Primary Tumor data sets were compiled by M. Zwitter and M. Soklic at University Medical Centre, Institute of Oncology, Ljubljana, Yugoslavia. The Audiology data set was compiled by Professor Jergen at Baylor College of", "mykey":956},
 {"datasetID":83, "supportID":"668135C3220E95D3CABB4B212E50612A18B74B9E", "rexaID":"1ef1e2c976bf0ee1cac221de391fcfabe5b10188", "author":"Geoffrey I Webb", "title":"Learning Decision Lists by Prepending Inferred Rules", "venue":"School of Computing and Mathematics Deakin University", "year":"", "window":"supported by the Australian Research Council. I am grateful to Mike Cammeron-Jones for discussions that helped refine the ideas presented herein. The Breast Cancer, Lymphography and <b>Primary</b> <b>Tumor</b> data sets were compiled by M. Zwitter and M. Soklic at University Medical Centre, Institute of Oncology, Ljubljana, Yugoslavia. The Audiology data set was compiled by Professor Jergen at Baylor College of", "mykey":957},
 {"datasetID":89, "supportID":"669FE7D8B9B3838B425EEDA626C7B1C523BE8AA9", "rexaID":"377a3c79c5b7a108aa16ed38407c81d035a0740d", "author":"Nir Friedman and Daphne Koller", "title":"Being Bayesian about Network Structure", "venue":"UAI", "year":"2000", "window":"Edges Figure 2: Comparison of posterior probabilities using true posterior over orderings (x-axis) versus ordering-MCMC (y-axis). The figures show Markov features and Edge features in the <b>Flare</b> dataset with 100 samples. ordering obtained by flipping i j and i k . Now, consider the terms in Eq. (6); those terms corresponding to nodes i ` in the ordering # that precede i j or succeed i k do not", "mykey":958},
 {"datasetID":89, "supportID":"66C138148D9FA3AD9528E91D5924EE1D304257C6", "rexaID":"377a3c79c5b7a108aa16ed38407c81d035a0740d", "author":"Nir Friedman and Daphne Koller", "title":"Being Bayesian about Network Structure", "venue":"UAI", "year":"2000", "window":"Edges Figure 2: Comparison of posterior probabilities using true posterior over orderings ( | -axis) versus ordering-MCMC ( \u00da -axis). The figures show Markov features and Edge features in the <b>Flare</b> dataset with 100 samples. ordering obtained by flipping ~#\u00b4 and ~ J . Now, consider the terms in Eq. (6); those terms corresponding to nodes ~\u00e5 in the ordering i that precede ~#\u00b4 or succeed ~ J do not", "mykey":959},
 {"datasetID":14, "supportID":"66CC6A2866834CDCD20DCAD562D43F42E98F955B", "rexaID":"c185d513badef2336ca48f64098d4b5df17bf5a4", "author":"Robert Burbidge and Matthew Trotter and Bernard F. Buxton and Sean B. Holden", "title":"STAR - Sparsity through Automated Rejection", "venue":"IWANN (1)", "year":"2001", "window":"available from the UCI Machine Learning Data Repository [11], are as follows. The <b>breast</b> <b>cancer</b> Wisconsin data set has 699 examples in nine dimensions and is `noise-free', one feature has 16 missing values which are replaced with the feature mean. The ionosphere data set has 351 examples in 33 dimensions and is", "mykey":960},
 {"datasetID":17, "supportID":"66CC6A2866834CDCD20DCAD562D43F42E98F955B", "rexaID":"c185d513badef2336ca48f64098d4b5df17bf5a4", "author":"Robert Burbidge and Matthew Trotter and Bernard F. Buxton and Sean B. Holden", "title":"STAR - Sparsity through Automated Rejection", "venue":"IWANN (1)", "year":"2001", "window":"available from the UCI Machine Learning Data Repository [11], are as follows. The <b>breast</b> <b>cancer</b> <b>Wisconsin</b> data set has 699 examples in nine dimensions and is `noise-free', one feature has 16 missing values which are replaced with the feature mean. The ionosphere data set has 351 examples in 33 dimensions and is", "mykey":961},
 {"datasetID":15, "supportID":"66CC6A2866834CDCD20DCAD562D43F42E98F955B", "rexaID":"c185d513badef2336ca48f64098d4b5df17bf5a4", "author":"Robert Burbidge and Matthew Trotter and Bernard F. Buxton and Sean B. Holden", "title":"STAR - Sparsity through Automated Rejection", "venue":"IWANN (1)", "year":"2001", "window":"available from the UCI Machine Learning Data Repository [11], are as follows. The <b>breast</b> <b>cancer</b> <b>Wisconsin</b> data set has 699 examples in nine dimensions and is `noise-free', one feature has 16 missing values which are replaced with the feature mean. The ionosphere data set has 351 examples in 33 dimensions and is", "mykey":962},
 {"datasetID":16, "supportID":"66CC6A2866834CDCD20DCAD562D43F42E98F955B", "rexaID":"c185d513badef2336ca48f64098d4b5df17bf5a4", "author":"Robert Burbidge and Matthew Trotter and Bernard F. Buxton and Sean B. Holden", "title":"STAR - Sparsity through Automated Rejection", "venue":"IWANN (1)", "year":"2001", "window":"available from the UCI Machine Learning Data Repository [11], are as follows. The <b>breast</b> <b>cancer</b> <b>Wisconsin</b> data set has 699 examples in nine dimensions and is `noise-free', one feature has 16 missing values which are replaced with the feature mean. The ionosphere data set has 351 examples in 33 dimensions and is", "mykey":963},
 {"datasetID":34, "supportID":"66CC6A2866834CDCD20DCAD562D43F42E98F955B", "rexaID":"c185d513badef2336ca48f64098d4b5df17bf5a4", "author":"Robert Burbidge and Matthew Trotter and Bernard F. Buxton and Sean B. Holden", "title":"STAR - Sparsity through Automated Rejection", "venue":"IWANN (1)", "year":"2001", "window":"has 270 examples in 13 dimensions. The Pima Indians <b>diabetes</b> data set has 768 examples in eight dimensions. These last two data sets have a high degree of overlap which leads to a dense model for the standard SVM as many training errors contribute to the solution. The", "mykey":964},
 {"datasetID":45, "supportID":"66CC6A2866834CDCD20DCAD562D43F42E98F955B", "rexaID":"c185d513badef2336ca48f64098d4b5df17bf5a4", "author":"Robert Burbidge and Matthew Trotter and Bernard F. Buxton and Sean B. Holden", "title":"STAR - Sparsity through Automated Rejection", "venue":"IWANN (1)", "year":"2001", "window":"has 351 examples in 33 dimensions and is slightly noisy. The <b>heart</b> data set has 270 examples in 13 dimensions. The Pima Indians diabetes data set has 768 examples in eight dimensions. These last two data sets have a high degree of overlap which leads to a dense model for", "mykey":965},
 {"datasetID":52, "supportID":"66CC6A2866834CDCD20DCAD562D43F42E98F955B", "rexaID":"c185d513badef2336ca48f64098d4b5df17bf5a4", "author":"Robert Burbidge and Matthew Trotter and Bernard F. Buxton and Sean B. Holden", "title":"STAR - Sparsity through Automated Rejection", "venue":"IWANN (1)", "year":"2001", "window":"has 699 examples in nine dimensions and is `noise-free', one feature has 16 missing values which are replaced with the feature mean. The <b>ionosphere</b> data set has 351 examples in 33 dimensions and is slightly noisy. The heart data set has 270 examples in 13 dimensions. The Pima Indians diabetes data set has 768 examples in eight dimensions. These last two", "mykey":966},
 {"datasetID":79, "supportID":"66CC6A2866834CDCD20DCAD562D43F42E98F955B", "rexaID":"c185d513badef2336ca48f64098d4b5df17bf5a4", "author":"Robert Burbidge and Matthew Trotter and Bernard F. Buxton and Sean B. Holden", "title":"STAR - Sparsity through Automated Rejection", "venue":"IWANN (1)", "year":"2001", "window":"has 270 examples in 13 dimensions. The <b>Pima</b> <b>Indians</b> <b>diabetes</b> data set has 768 examples in eight dimensions. These last two data sets have a high degree of overlap which leads to a dense model for the standard SVM as many training errors contribute to the solution. The", "mykey":967},
 {"datasetID":53, "supportID":"66D0757B5B8B019707D2F06E52F97BCDF7F9CF74", "rexaID":"d320ece010630c32341d927b2573372abbbca524", "author":"Stephen D. Bay", "title":"Combining Nearest Neighbor Classifiers Through Multiple Feature Subsets", "venue":"ICML", "year":"1998", "window":"comparison, we used the Wilcoxon signed rank test and found that MFS1 and MFS2 were significantly better than all others with a confidence level greater than 99%. MFS only performed poorly on two datasets: <b>Iris</b> and Tic-Tac-Toe. For Iris, both MFS1 and MFS2 gave the lowest accuracy out of all the classifiers. This can possibly be explained by the small number of features in the Iris dataset. With", "mykey":968},
 {"datasetID":74, "supportID":"66D0757B5B8B019707D2F06E52F97BCDF7F9CF74", "rexaID":"d320ece010630c32341d927b2573372abbbca524", "author":"Stephen D. Bay", "title":"Combining Nearest Neighbor Classifiers Through Multiple Feature Subsets", "venue":"ICML", "year":"1998", "window":"maintain consistency with reported results (Quinlan, 1996). For Satimage, we used the original division into a training and test set, so the results represent one run of each algorithm. For the <b>Musk</b> dataset, which has 166 features, FSS and BSS took too long to run (over 24 hours for a single trial) and no results were obtained. 3.2 ACCURACY The accuracy and parameter selection results (average k or", "mykey":969},
 {"datasetID":75, "supportID":"66D0757B5B8B019707D2F06E52F97BCDF7F9CF74", "rexaID":"d320ece010630c32341d927b2573372abbbca524", "author":"Stephen D. Bay", "title":"Combining Nearest Neighbor Classifiers Through Multiple Feature Subsets", "venue":"ICML", "year":"1998", "window":"maintain consistency with reported results (Quinlan, 1996). For Satimage, we used the original division into a training and test set, so the results represent one run of each algorithm. For the <b>Musk</b> dataset, which has 166 features, FSS and BSS took too long to run (over 24 hours for a single trial) and no results were obtained. 3.2 ACCURACY The accuracy and parameter selection results (average k or", "mykey":970},
 {"datasetID":101, "supportID":"66D0757B5B8B019707D2F06E52F97BCDF7F9CF74", "rexaID":"d320ece010630c32341d927b2573372abbbca524", "author":"Stephen D. Bay", "title":"Combining Nearest Neighbor Classifiers Through Multiple Feature Subsets", "venue":"ICML", "year":"1998", "window":"comparison, we used the Wilcoxon signed rank test and found that MFS1 and MFS2 were significantly better than all others with a confidence level greater than 99%. MFS only performed poorly on two datasets: Iris and <b>Tic-Tac-Toe</b>  For Iris, both MFS1 and MFS2 gave the lowest accuracy out of all the classifiers. This can possibly be explained by the small number of features in the Iris dataset. With", "mykey":971},
 {"datasetID":73, "supportID":"66EB337FC4F545518A5593F47B43CEB83429CC26", "rexaID":"d32b83c84ede5f9ed1b2bda59ec57d68228b6b01", "author":"Venkatesh Ganti and Johannes Gehrke and Raghu Ramakrishnan", "title":"CACTUS - Clustering Categorical Data Using Summaries", "venue":"KDD", "year":"1999", "window":"1 on which distance functions are not naturally defined. Recently, the problem of clustering categorical data started receiving interest [GKR98, GRS99]. As an example, consider the <b>MUSHROOM</b> dataset in the popular UCI Machine Learning repository [CBM98]. Each tuple in the dataset describes a sample of gilled mushrooms using twenty two categorical attributes. For instance, the cap color", "mykey":972},
 {"datasetID":53, "supportID":"66FC0D5F52CD4284E14FDAECE247F6A0D8FB078F", "rexaID":"5047fbe99b73ea1e127150b6688d65effd51f4c1", "author":"Manoranjan Dash and Kiseok Choi and Peter Scheuermann and Huan Liu", "title":"Feature Selection for Clustering - A Filter Solution", "venue":"ICDM", "year":"2002", "window":"are almost correct as well as the selected features are all important and it missed out only one important feature. 5.2 Benchmark and Real Datasets <b>Iris</b> dataset, popularly used for testing clustering and classification algorithms, is taken from UCI ML repository [5]. It contains 3 classes of 50 instances each, where each class refers to a type", "mykey":973},
 {"datasetID":110, "supportID":"66FC0D5F52CD4284E14FDAECE247F6A0D8FB078F", "rexaID":"5047fbe99b73ea1e127150b6688d65effd51f4c1", "author":"Manoranjan Dash and Kiseok Choi and Peter Scheuermann and Huan Liu", "title":"Feature Selection for Clustering - A Filter Solution", "venue":"ICDM", "year":"2002", "window":"is taken from the recently publicized clustering software CLUTO available from the web site http://www-users.cs.umn.edu/ ~ karypis/cluto/. In this, there is a dataset called Genes2 which has 99 <b>yeast</b> genes (or data points) described using 7 profiles (or features). When ForwardSelect is run over this data, it shows the minimum entropy for subset fF3,F5g (see", "mykey":974},
 {"datasetID":34, "supportID":"67282C3CE8BA6668F510EBDD68A87F9EB72B8B57", "rexaID":"7ca6f3f2b00e225b5b648c2998c727d0b7d6cde8", "author":"Krzysztof Krawiec", "title":"Genetic Programming-based Construction of Features for Machine Learning and Knowledge Discovery Tasks", "venue":"Institute of Computing Science, Poznan University of Technology", "year":"2002", "window":"in favor of feature construction is usually statistically relevant. Note also that positive results have been obtained for both real-world problems (Crx, <b>Diabetes</b> and Glass) as well as artificial datasets, which were intentionally designed to test the usefulness of feature construction methods [34]. Although the increases in accuracy of classification are not always impressive, the feature", "mykey":975},
 {"datasetID":42, "supportID":"67282C3CE8BA6668F510EBDD68A87F9EB72B8B57", "rexaID":"7ca6f3f2b00e225b5b648c2998c727d0b7d6cde8", "author":"Krzysztof Krawiec", "title":"Genetic Programming-based Construction of Features for Machine Learning and Knowledge Discovery Tasks", "venue":"Institute of Computing Science, Poznan University of Technology", "year":"2002", "window":"in favor of feature construction is usually statistically relevant. Note also that positive results have been obtained for both real-world problems (Crx, Diabetes and <b>Glass</b>  as well as artificial datasets, which were intentionally designed to test the usefulness of feature construction methods [34]. Although the increases in accuracy of classification are not always impressive, the feature", "mykey":976},
 {"datasetID":59, "supportID":"67B0F4F0EC623D8382C1DC94EA9114FBAAFBC23B", "rexaID":"cf50848a3edbd7599d1ff18d941d332e1fe6a673", "author":"Dmitry Pavlov and Alexandrin Popescul and David M. Pennock and Lyle H. Ungar", "title":"Mixtures of Conditional Maximum Entropy Models", "venue":"ICML", "year":"2003", "window":"actual time complexity strongly depends on the sparsity of the data. By looking only at the complexity terms of Table 1, one could expect that time performance on the <b>Letter Recognition</b>  and Cover data sets would be roughly the same. However, the Cover data set is substantially more sparse and this results in an order of magnitude decrease in actual training time difference. Overall, we conclude that", "mykey":977},
 {"datasetID":149, "supportID":"67B0F4F0EC623D8382C1DC94EA9114FBAAFBC23B", "rexaID":"cf50848a3edbd7599d1ff18d941d332e1fe6a673", "author":"Dmitry Pavlov and Alexandrin Popescul and David M. Pennock and Lyle H. Ungar", "title":"Mixtures of Conditional Maximum Entropy Models", "venue":"ICML", "year":"2003", "window":"from university computer science departments. We used all classes but others and different numbers (up to 1000) of the most frequent words. The Letter recognition, Yeast, MS Web, <b>Vehicle</b> and Vowel data sets were downloaded from the UC Irvine machine learning repository (Blake & Merz, 1998). In the MS Web data set, we predicted whether a user visited the free downloads\" web page, given the rest of his", "mykey":978},
 {"datasetID":110, "supportID":"67B0F4F0EC623D8382C1DC94EA9114FBAAFBC23B", "rexaID":"cf50848a3edbd7599d1ff18d941d332e1fe6a673", "author":"Dmitry Pavlov and Alexandrin Popescul and David M. Pennock and Lyle H. Ungar", "title":"Mixtures of Conditional Maximum Entropy Models", "venue":"ICML", "year":"2003", "window":"from university computer science departments. We used all classes but others and different numbers (up to 1000) of the most frequent words. The Letter recognition, <b>Yeast</b>  MS Web, Vehicle and Vowel data sets were downloaded from the UC Irvine machine learning repository (Blake & Merz, 1998). In the MS Web data set, we predicted whether a user visited the free downloads\" web page, given the rest of his", "mykey":979},
 {"datasetID":43, "supportID":"67B7280E2466BE2AEFDD53215E9680147A9078B1", "rexaID":"bc9fc18577f413262d1fc743ab3438d4b37d689e", "author":"Denver Dash and Gregory F. Cooper", "title":"Model Averaging with Discrete Bayesian Network Classifiers", "venue":"Decision Systems Laboratory Intelligent Systems Program University of Pittsburgh", "year":"", "window":"to emphasize the fact that AMA was typiData set \u00b1 SNN \u00b1 GTT \u00b1 NMA \u00b1 AMA N k Nr <b>haberman</b> 0.35 0.35 0 0 4 4 306 hayes-roth 0.32 0.32 0 0.01 6 6 132 monks-3 0.83 0.24 0.82 0 7 7 552 monks-1 0.98 0 0.98 0 7 7 554 monks-2 0.48 0.48 0.21 0 7 7 600", "mykey":980},
 {"datasetID":109, "supportID":"67B7280E2466BE2AEFDD53215E9680147A9078B1", "rexaID":"bc9fc18577f413262d1fc743ab3438d4b37d689e", "author":"Denver Dash and Gregory F. Cooper", "title":"Model Averaging with Discrete Bayesian Network Classifiers", "venue":"Decision Systems Laboratory Intelligent Systems Program University of Pittsburgh", "year":"", "window":"variable is considered to be the \"positive\" state; therefore, the scores in Table 5 are average scores for all ROC curves associated with a particular classification variable; therefore some data sets (e.g., <b>wine</b>  have no zero entries when two or more classifiers score highest on di\u00aeerent curves. We have underlined the top two scoring classifiers for each data set to emphasize the fact that AMA", "mykey":981},
 {"datasetID":109, "supportID":"67BB83EB88F003C6294A1DEB823054DCD0DBEF1E", "rexaID":"32122524e8078586e382e0b8f07f6a91cc247358", "author":"Agapito Ledezma and Ricardo Aler and Araceli Sanch\u00eds and Daniel Borrajo", "title":"Empirical Evaluation of Optimized Stacking Configurations", "venue":"ICTAI", "year":"2004", "window":"2 hepatitis 19 155 77 78 2 hypo 25 3163 2846 317 2 image 19 2310 1848 462 7 ionosphere 34 351 175 176 2 iris 4 150 75 75 3 soya 35 683 341 342 19 vote 16 435 217 218 2 <b>wine</b> 13 178 89 89 3 Table 1. Datasets descriptions . C4.5 [25]. It generates decision trees . A probabilistic Naive Bayes classifier [19] . IBk. This is Aha's instance based learning algorithm [1] . PART [14]. It forms a decision list", "mykey":982},
 {"datasetID":74, "supportID":"6815E3C80D7948263B01052BD336390E562A8BEE", "rexaID":"c2fee20b32fddc929738addedddbe57ccb74800e", "author":"Hendrik Blockeel and Luc De Raedt", "title":"Top-down Induction of Logical Decision Trees", "venue":"Katholieke Universiteit Leuven Department of Computer Science", "year":"", "window":"tree induction system is used for the actual induction process. Figures for this table were copied from [ Quinlan, 1996 ] (FOIL, FFOIL, FORS) and [ Geibel and Wysotzki, 1996 ] (Indigo). 6.3 <b>musk</b> Dataset The musk dataset was studied by Dietterich et al. [ Dietterich et al., 1996 ] , who donated it to the UCI repository [ Merz and Murphy, 1996 ] . Dietterich used these data to study the so-called", "mykey":983},
 {"datasetID":75, "supportID":"6815E3C80D7948263B01052BD336390E562A8BEE", "rexaID":"c2fee20b32fddc929738addedddbe57ccb74800e", "author":"Hendrik Blockeel and Luc De Raedt", "title":"Top-down Induction of Logical Decision Trees", "venue":"Katholieke Universiteit Leuven Department of Computer Science", "year":"", "window":"tree induction system is used for the actual induction process. Figures for this table were copied from [ Quinlan, 1996 ] (FOIL, FFOIL, FORS) and [ Geibel and Wysotzki, 1996 ] (Indigo). 6.3 <b>musk</b> Dataset The musk dataset was studied by Dietterich et al. [ Dietterich et al., 1996 ] , who donated it to the UCI repository [ Merz and Murphy, 1996 ] . Dietterich used these data to study the so-called", "mykey":984},
 {"datasetID":149, "supportID":"68D6AA6CDDB4F2312DE07372E3ADFE1A2C8EB09F", "rexaID":"66119cd7be7811baab2a4b05715b5eb2f5c358b1", "author":"Robi Polikar and L. Upda and S. S. Upda and Vasant Honavar", "title":"Learn++: an incremental learning algorithm for supervised neural networks", "venue":"IEEE Transactions on Systems, Man, and Cybernetics, Part C, 31", "year":"2001", "window":"combinations were tried. An MLP with 50 hidden layer nodes and a 100 times smaller error goal of 0.001 was able to match (and slightly exceed) Learn++ performance, by classifying 95% of the TEST dataset. B. <b>Vehicle</b> Silhouette Database Also obtained from the UCI depository, the vehicle silhouette database consisted of 18 features from which the type of a vehicle is determined. The database consisted", "mykey":985},
 {"datasetID":7, "supportID":"68E4CBD76EB78057876930BD7362D346BE6BFED4", "rexaID":"cd11168bb19fd462bc59beefbe670bc4eb31e3eb", "author":"Mark A. Hall", "title":"Department of Computer Science Hamilton, NewZealand Correlation-based Feature Selection for Machine Learning", "venue":"Doctor of Philosophy at The University of Waikato", "year":"1999", "window":"algorithm using the features in the subset, and then 111 evaluating its performance on the test set 4 . Figure 6.23 shows plots of CFS-UC's merit versus naive Bayes' accuracy on a selection of datasets (chess end-game, horse colic, <b>audiology</b>  and soybean). Plots for the remaining datasets---and for when IB1 and C4.5 are used to measure accuracy---can be found in appendix E. The first thing that", "mykey":986},
 {"datasetID":8, "supportID":"68E4CBD76EB78057876930BD7362D346BE6BFED4", "rexaID":"cd11168bb19fd462bc59beefbe670bc4eb31e3eb", "author":"Mark A. Hall", "title":"Department of Computer Science Hamilton, NewZealand Correlation-based Feature Selection for Machine Learning", "venue":"Doctor of Philosophy at The University of Waikato", "year":"1999", "window":"algorithm using the features in the subset, and then 111 evaluating its performance on the test set 4 . Figure 6.23 shows plots of CFS-UC's merit versus naive Bayes' accuracy on a selection of datasets (chess end-game, horse colic, <b>audiology</b>  and soybean). Plots for the remaining datasets---and for when IB1 and C4.5 are used to measure accuracy---can be found in appendix E. The first thing that", "mykey":987},
 {"datasetID":24, "supportID":"68E4CBD76EB78057876930BD7362D346BE6BFED4", "rexaID":"cd11168bb19fd462bc59beefbe670bc4eb31e3eb", "author":"Mark A. Hall", "title":"Department of Computer Science Hamilton, NewZealand Correlation-based Feature Selection for Machine Learning", "venue":"Doctor of Philosophy at The University of Waikato", "year":"1999", "window":"2 In comparison, CFS-P using the MDL measure considered on average less than 50 derived attributes on the <b>chess</b> end-game dataset. 141 IB1 <b>Domain</b> CFS-UC CFS-P CFS-RELIEF mu 98.48 \u00b1 0.1 98.64 \u00b1 0.3+ 99.72 \u00b1 0.2+ vo 95.60 \u00b1 1.0 95.60 \u00b1 1.0 95.12 \u00b1 1.2 - v1 88.35 \u00b1 2.1 88.35 \u00b1 2.1 88.17 \u00b1 1.9 cr 85.61 \u00b1 1.0 85.61 \u00b1 1.0 85.61 \u00b1", "mykey":988},
 {"datasetID":23, "supportID":"68E4CBD76EB78057876930BD7362D346BE6BFED4", "rexaID":"cd11168bb19fd462bc59beefbe670bc4eb31e3eb", "author":"Mark A. Hall", "title":"Department of Computer Science Hamilton, NewZealand Correlation-based Feature Selection for Machine Learning", "venue":"Doctor of Philosophy at The University of Waikato", "year":"1999", "window":"without feature selection using merged subsets. . . . . . . . . . . . . . . . . . . . . . . . . . 115 6.9 Top eight feature-class correlations assigned by CFS-UC and CFS-MDL on the <b>chess</b> end-game dataset. . . . . . . . . . . . . . . . . . . . . . . . 116 7.1 Comparison between naive Bayes without feature selection and naive Bayes with feature selection by the wrapper and CFS. . . . . . . . . . . .", "mykey":989},
 {"datasetID":21, "supportID":"68E4CBD76EB78057876930BD7362D346BE6BFED4", "rexaID":"cd11168bb19fd462bc59beefbe670bc4eb31e3eb", "author":"Mark A. Hall", "title":"Department of Computer Science Hamilton, NewZealand Correlation-based Feature Selection for Machine Learning", "venue":"Doctor of Philosophy at The University of Waikato", "year":"1999", "window":"without feature selection using merged subsets. . . . . . . . . . . . . . . . . . . . . . . . . . 115 6.9 Top eight feature-class correlations assigned by CFS-UC and CFS-MDL on the <b>chess</b> end-game dataset. . . . . . . . . . . . . . . . . . . . . . . . 116 7.1 Comparison between naive Bayes without feature selection and naive Bayes with feature selection by the wrapper and CFS. . . . . . . . . . . .", "mykey":990},
 {"datasetID":22, "supportID":"68E4CBD76EB78057876930BD7362D346BE6BFED4", "rexaID":"cd11168bb19fd462bc59beefbe670bc4eb31e3eb", "author":"Mark A. Hall", "title":"Department of Computer Science Hamilton, NewZealand Correlation-based Feature Selection for Machine Learning", "venue":"Doctor of Philosophy at The University of Waikato", "year":"1999", "window":"without feature selection using merged subsets. . . . . . . . . . . . . . . . . . . . . . . . . . 115 6.9 Top eight feature-class correlations assigned by CFS-UC and CFS-MDL on the <b>chess</b> end-game dataset. . . . . . . . . . . . . . . . . . . . . . . . 116 7.1 Comparison between naive Bayes without feature selection and naive Bayes with feature selection by the wrapper and CFS. . . . . . . . . . . .", "mykey":991},
 {"datasetID":120, "supportID":"68E4CBD76EB78057876930BD7362D346BE6BFED4", "rexaID":"cd11168bb19fd462bc59beefbe670bc4eb31e3eb", "author":"Mark A. Hall", "title":"Department of Computer Science Hamilton, NewZealand Correlation-based Feature Selection for Machine Learning", "venue":"Doctor of Philosophy at The University of Waikato", "year":"1999", "window":"is to predict whether cancer will recur in patients. There are 9 nominal attributes describing characteristics such as tumour size and location. There are 286 instances. Dna-promoter (dna) A small dataset containing 53 positive examples of <b>E</b>  <b>coli</b> promoter gene sequences and 53 negative examples. There are 55 nominal attributes representing the gene sequence. Each attribute is a DNA nucleotide", "mykey":992},
 {"datasetID":39, "supportID":"68E4CBD76EB78057876930BD7362D346BE6BFED4", "rexaID":"cd11168bb19fd462bc59beefbe670bc4eb31e3eb", "author":"Mark A. Hall", "title":"Department of Computer Science Hamilton, NewZealand Correlation-based Feature Selection for Machine Learning", "venue":"Doctor of Philosophy at The University of Waikato", "year":"1999", "window":"is to predict whether cancer will recur in patients. There are 9 nominal attributes describing characteristics such as tumour size and location. There are 286 instances. Dna-promoter (dna) A small dataset containing 53 positive examples of E. <b>coli</b> promoter gene sequences and 53 negative examples. There are 55 nominal attributes representing the gene sequence. Each attribute is a DNA nucleotide", "mykey":993},
 {"datasetID":47, "supportID":"68E4CBD76EB78057876930BD7362D346BE6BFED4", "rexaID":"cd11168bb19fd462bc59beefbe670bc4eb31e3eb", "author":"Mark A. Hall", "title":"Department of Computer Science Hamilton, NewZealand Correlation-based Feature Selection for Machine Learning", "venue":"Doctor of Philosophy at The University of Waikato", "year":"1999", "window":"examples described by 35 nominal features. Features measure properties of leaves and various plant abnormalities. There are 19 classes (diseases). <b>Horse</b> <b>colic</b> (hc) There are 368 instances in this dataset, provided by Mary McLeish and Matt Cecile from the University of Guelph. There are 27 attributes, of which 7 are continuous. Features include whether a horse is young or old, whether it had surgery,", "mykey":994},
 {"datasetID":63, "supportID":"68E4CBD76EB78057876930BD7362D346BE6BFED4", "rexaID":"cd11168bb19fd462bc59beefbe670bc4eb31e3eb", "author":"Mark A. Hall", "title":"Department of Computer Science Hamilton, NewZealand Correlation-based Feature Selection for Machine Learning", "venue":"Doctor of Philosophy at The University of Waikato", "year":"1999", "window":"symbols to ensure confidentiality of the data. There are six continuous features and nine nominal. The nominal features range from 2 to 14 values. <b>Lymphography</b> (ly) This is a small medical dataset containing 148 instances. The task is to distinguish healthy patients from those with metastases or malignant lymphoma. All 18 features are nominal. This is the one of three medical domains (the", "mykey":995},
 {"datasetID":67, "supportID":"68E4CBD76EB78057876930BD7362D346BE6BFED4", "rexaID":"cd11168bb19fd462bc59beefbe670bc4eb31e3eb", "author":"Mark A. Hall", "title":"Department of Computer Science Hamilton, NewZealand Correlation-based Feature Selection for Machine Learning", "venue":"Doctor of Philosophy at The University of Waikato", "year":"1999", "window":"is to predict whether cancer will recur in patients. There are 9 nominal attributes describing characteristics such as tumour size and location. There are 286 instances. <b>dna</b> <b>promoter</b> (dna) A small dataset containing 53 positive examples of E. coli promoter gene sequences and 53 negative examples. There are 55 nominal attributes representing the gene sequence. Each attribute is a DNA nucleotide", "mykey":996},
 {"datasetID":70, "supportID":"68E4CBD76EB78057876930BD7362D346BE6BFED4", "rexaID":"cd11168bb19fd462bc59beefbe670bc4eb31e3eb", "author":"Mark A. Hall", "title":"Department of Computer Science Hamilton, NewZealand Correlation-based Feature Selection for Machine Learning", "venue":"Doctor of Philosophy at The University of Waikato", "year":"1999", "window":"a one-third/two-thirds split on credit and one-eighth of the instances were used for training on mushroom (the largest 80 dataset). In the case of the <b>Monk</b> s problems, testing is performed on the full dataset (as was done originally by Thrun et al. [TBB + 91]). Various different train and test set sizes are used with the", "mykey":997},
 {"datasetID":73, "supportID":"68E4CBD76EB78057876930BD7362D346BE6BFED4", "rexaID":"cd11168bb19fd462bc59beefbe670bc4eb31e3eb", "author":"Mark A. Hall", "title":"Department of Computer Science Hamilton, NewZealand Correlation-based Feature Selection for Machine Learning", "venue":"Doctor of Philosophy at The University of Waikato", "year":"1999", "window":"The following is a brief description of the datasets. <b>Mushroom</b> (mu) This dataset contains records drawn from The Audubon Society Field Guide to North American Mushrooms [Lin81]. The task is to distinguish edible from poisonous mushrooms on the basis", "mykey":998},
 {"datasetID":90, "supportID":"68E4CBD76EB78057876930BD7362D346BE6BFED4", "rexaID":"cd11168bb19fd462bc59beefbe670bc4eb31e3eb", "author":"Mark A. Hall", "title":"Department of Computer Science Hamilton, NewZealand Correlation-based Feature Selection for Machine Learning", "venue":"Doctor of Philosophy at The University of Waikato", "year":"1999", "window":"(chess end-game, horse colic, audiology, and <b>soybean</b> . Plots for the remaining datasets---and for when IB1 and C4.5 are used to measure accuracy---can be found in appendix E. The first thing that is apparent from Figure 6.23 is that a correspondence between merit and actual accuracy", "mykey":999},
 {"datasetID":91, "supportID":"68E4CBD76EB78057876930BD7362D346BE6BFED4", "rexaID":"cd11168bb19fd462bc59beefbe670bc4eb31e3eb", "author":"Mark A. Hall", "title":"Department of Computer Science Hamilton, NewZealand Correlation-based Feature Selection for Machine Learning", "venue":"Doctor of Philosophy at The University of Waikato", "year":"1999", "window":"(chess end-game, horse colic, audiology, and <b>soybean</b> . Plots for the remaining datasets---and for when IB1 and C4.5 are used to measure accuracy---can be found in appendix E. The first thing that is apparent from Figure 6.23 is that a correspondence between merit and actual accuracy", "mykey":1000},
 {"datasetID":143, "supportID":"68E4CBD76EB78057876930BD7362D346BE6BFED4", "rexaID":"cd11168bb19fd462bc59beefbe670bc4eb31e3eb", "author":"Mark A. Hall", "title":"Department of Computer Science Hamilton, NewZealand Correlation-based Feature Selection for Machine Learning", "venue":"Doctor of Philosophy at The University of Waikato", "year":"1999", "window":"has the single most predictive attribute (physician-fee-freeze) removed. <b>Australian credit</b> screening (cr) This dataset contains 690 instances from an Australian credit company. The task is to distinguish credit-worthy from non credit-worthy customers. There are 15 attributes whose names and values have been", "mykey":1001},
 {"datasetID":12, "supportID":"692EEDC58F38D88DFED5383FC56AFAFB1983BD0B", "rexaID":"d328ae33fb50756832a1c6cd703f7176c361923f", "author":"Peter Sykacek and Stephen J. Roberts", "title":"Adaptive Classification by Variational Kalman Filtering", "venue":"NIPS", "year":"2002", "window":"sequential variational inference. The probability of the null hypothesis, # ####### , that both classifiers are equal suggests that only the differences for the <b>Balance</b> <b>scale</b> and the Pima Indian data sets are significant, with either method being better in one case. Since the generalization accuracies of both methods are almost identical, we conclude that if applied to 2 Vehicle data was donated to", "mykey":1002},
 {"datasetID":34, "supportID":"692EEDC58F38D88DFED5383FC56AFAFB1983BD0B", "rexaID":"d328ae33fb50756832a1c6cd703f7176c361923f", "author":"Peter Sykacek and Stephen J. Roberts", "title":"Adaptive Classification by Variational Kalman Filtering", "venue":"NIPS", "year":"2002", "window":"which were both used as training and independent test sets respectively. We also use the pima <b>diabetes</b> data set from [16] 3 . Table 1 compares the generalization accuracies (in fractions) obtained with the variational Kalman filter with generalization accuracies obtained with sequential variational inference.", "mykey":1003},
 {"datasetID":121, "supportID":"692EEDC58F38D88DFED5383FC56AFAFB1983BD0B", "rexaID":"d328ae33fb50756832a1c6cd703f7176c361923f", "author":"Peter Sykacek and Stephen J. Roberts", "title":"Adaptive Classification by Variational Kalman Filtering", "venue":"NIPS", "year":"2002", "window":"size 9 # # , because this is a good compromise between fast tracking and high stationary accuracy. We are now ready to compare the algorithm with an equivalent static classifier using several public data sets and classification of single trial <b>EEG</b> which, due to learning effects in humans, is known to be non-stationary. In order to avoid that the model has an influence on 1 This data set can be obtained", "mykey":1004},
 {"datasetID":79, "supportID":"692EEDC58F38D88DFED5383FC56AFAFB1983BD0B", "rexaID":"d328ae33fb50756832a1c6cd703f7176c361923f", "author":"Peter Sykacek and Stephen J. Roberts", "title":"Adaptive Classification by Variational Kalman Filtering", "venue":"NIPS", "year":"2002", "window":"which were both used as training and independent test sets respectively. We also use the <b>pima</b> <b>diabetes</b> data set from [16] 3 . Table 1 compares the generalization accuracies (in fractions) obtained with the variational Kalman filter with generalization accuracies obtained with sequential variational inference.", "mykey":1005},
 {"datasetID":146, "supportID":"692EEDC58F38D88DFED5383FC56AFAFB1983BD0B", "rexaID":"d328ae33fb50756832a1c6cd703f7176c361923f", "author":"Peter Sykacek and Stephen J. Roberts", "title":"Adaptive Classification by Variational Kalman Filtering", "venue":"NIPS", "year":"2002", "window":"ionosphere data, balance scale weight and distance data and the wine recognition database, all taken from the StatLog database which is available at the UCI repository ([4]). The <b>satellite</b> image data set is used as is provided with 4435 samples in the training and 2000 samples in the test set. Vehicle data are merged such that we have 500 samples in the training and 252 in the test set. The other", "mykey":1006},
 {"datasetID":34, "supportID":"6960026BAFD31775813E2512360D9460B75F4A2C", "rexaID":"de7b203eb1605a71f18d23d9368046bfeb381b9f", "author":"Prem Melville and Raymond J. Mooney", "title":"Proceedings of the 21st International Conference on Machine Learning", "venue":"Department of Computer Sciences", "year":"", "window":"In particular, we used a sample size of two for the primary dataset, and three for breast-w, soybean, <b>diabetes</b>  vowel and credit-g. The primary aim of active learning is to reduce the amount of training data needed to induce an accurate model. Toevaluate this, we", "mykey":1007},
 {"datasetID":90, "supportID":"6960026BAFD31775813E2512360D9460B75F4A2C", "rexaID":"de7b203eb1605a71f18d23d9368046bfeb381b9f", "author":"Prem Melville and Raymond J. Mooney", "title":"Proceedings of the 21st International Conference on Machine Learning", "venue":"Department of Computer Sciences", "year":"", "window":"In particular, we used a sample size of two for the primary dataset, and three for breast-w, <b>soybean</b>  diabetes, vowel and credit-g. The primary aim of active learning is to reduce the amount of training data needed to induce an accurate model. Toevaluate this, we", "mykey":1008},
 {"datasetID":91, "supportID":"6960026BAFD31775813E2512360D9460B75F4A2C", "rexaID":"de7b203eb1605a71f18d23d9368046bfeb381b9f", "author":"Prem Melville and Raymond J. Mooney", "title":"Proceedings of the 21st International Conference on Machine Learning", "venue":"Department of Computer Sciences", "year":"", "window":"In particular, we used a sample size of two for the primary dataset, and three for breast-w, <b>soybean</b>  diabetes, vowel and credit-g. The primary aim of active learning is to reduce the amount of training data needed to induce an accurate model. Toevaluate this, we", "mykey":1009},
 {"datasetID":48, "supportID":"699120D8BA13272EB88DD13FCC90648210EBFE77", "rexaID":"45391c9343dc03f6033eba55364366eddbb693db", "author":"Predrag Radivojac and Zoran Obradovic and A. Keith Dunker and Slobodan Vucetic", "title":"Feature Selection Filters Based on the Permutation Test", "venue":"ECML", "year":"2004", "window":"summarized in Table 1. The first nine were downloaded from the UCI repository [38], with dataset <b>HOUSING</b> converted into a binary classification problem according to the mean value of the target. Datasets MAMMOGRAPHY and OIL were constructed in [39] and [40], respectively, and provided to us by", "mykey":1010},
 {"datasetID":52, "supportID":"699120D8BA13272EB88DD13FCC90648210EBFE77", "rexaID":"45391c9343dc03f6033eba55364366eddbb693db", "author":"Predrag Radivojac and Zoran Obradovic and A. Keith Dunker and Slobodan Vucetic", "title":"Feature Selection Filters Based on the Permutation Test", "venue":"ECML", "year":"2004", "window":"basic characteristics. NF and CF indicate the number of numerical and categorical features, respectively. Dataset Size Size of class 1 NF CF <b>IONOSPHERE</b> 351 225 34 0 VOTES 435 267 0 48 GLASS 214 163 9 0 HEART 303 139 6 7 LABOR 57 37 8 21 HOUSING 506 250 13 0 CREDIT 690 307 6 41 PIMA 768 268 9 0 ZOO 78 41 1 15", "mykey":1011},
 {"datasetID":14, "supportID":"69B27D863FA6F1E5B1E4EA35353CF8E93BA7E20E", "rexaID":"3ecc983417b61977b8f998e8a843948dad8fa21c", "author":"Andr\u00e1s Antos and Bal\u00e1zs K\u00e9gl and Tam\u00e1s Linder and G\u00e1bor Lugosi", "title":"Data-dependent margin-based generalization bounds for classification", "venue":"Journal of Machine Learning Research, 3", "year":"2002", "window":"attributes were binary coded in a 1-out-of-n fashion. Data points with missing attributes were removed. Each attribute was normalized to have zero mean and 1= p d standard deviation. The four data sets were the Wisconsin <b>breast</b> <b>cancer</b> (n = 683, d = 9), the ionosphere (n = 351, d = 34), the Japanese credit screening (n = 653, d = 42), and the tic-tac-toe endgame (n = 958, d = 27) database. 84", "mykey":1012},
 {"datasetID":17, "supportID":"69B27D863FA6F1E5B1E4EA35353CF8E93BA7E20E", "rexaID":"3ecc983417b61977b8f998e8a843948dad8fa21c", "author":"Andr\u00e1s Antos and Bal\u00e1zs K\u00e9gl and Tam\u00e1s Linder and G\u00e1bor Lugosi", "title":"Data-dependent margin-based generalization bounds for classification", "venue":"Journal of Machine Learning Research, 3", "year":"2002", "window":"attributes were binary coded in a 1-out-of-n fashion. Data points with missing attributes were removed. Each attribute was normalized to have zero mean and 1= p d standard deviation. The four data sets were the <b>Wisconsin</b> <b>breast</b> <b>cancer</b> (n = 683, d = 9), the ionosphere (n = 351, d = 34), the Japanese credit screening (n = 653, d = 42), and the tic-tac-toe endgame (n = 958, d = 27) database. 84", "mykey":1013},
 {"datasetID":15, "supportID":"69B27D863FA6F1E5B1E4EA35353CF8E93BA7E20E", "rexaID":"3ecc983417b61977b8f998e8a843948dad8fa21c", "author":"Andr\u00e1s Antos and Bal\u00e1zs K\u00e9gl and Tam\u00e1s Linder and G\u00e1bor Lugosi", "title":"Data-dependent margin-based generalization bounds for classification", "venue":"Journal of Machine Learning Research, 3", "year":"2002", "window":"attributes were binary coded in a 1-out-of-n fashion. Data points with missing attributes were removed. Each attribute was normalized to have zero mean and 1= p d standard deviation. The four data sets were the <b>Wisconsin</b> <b>breast</b> <b>cancer</b> (n = 683, d = 9), the ionosphere (n = 351, d = 34), the Japanese credit screening (n = 653, d = 42), and the tic-tac-toe endgame (n = 958, d = 27) database. 84", "mykey":1014},
 {"datasetID":16, "supportID":"69B27D863FA6F1E5B1E4EA35353CF8E93BA7E20E", "rexaID":"3ecc983417b61977b8f998e8a843948dad8fa21c", "author":"Andr\u00e1s Antos and Bal\u00e1zs K\u00e9gl and Tam\u00e1s Linder and G\u00e1bor Lugosi", "title":"Data-dependent margin-based generalization bounds for classification", "venue":"Journal of Machine Learning Research, 3", "year":"2002", "window":"attributes were binary coded in a 1-out-of-n fashion. Data points with missing attributes were removed. Each attribute was normalized to have zero mean and 1= p d standard deviation. The four data sets were the <b>Wisconsin</b> <b>breast</b> <b>cancer</b> (n = 683, d = 9), the ionosphere (n = 351, d = 34), the Japanese credit screening (n = 653, d = 42), and the tic-tac-toe endgame (n = 958, d = 27) database. 84", "mykey":1015},
 {"datasetID":69, "supportID":"6AAE0DC122102693E8136856FFC8B72DF7F78386", "rexaID":"a27103950b8d7098468bd52f2af1d033fed91419", "author":"Gustavo E. A and Gustavo E A P A Batista and Ronaldo C. Prati and Maria Carolina Monard", "title":"A Study of the Behavior of Several Methods for Balancing Machine Learning Training Data", "venue":"Instituto de Ci ^ encias Matem aticas e de Computac~ ao", "year":"", "window":"having more than two classes, we chose the class with fewer examples as the positive class, and collapsed the remainder as the negative class. As the Letter and <b>Splice</b> data sets have a similar number of examples in the minority classes, we created two data sets with each of them: Letter-a and Letter-vowel, Splice-ie and Splice-ei. In our experiments, we used release 8 of", "mykey":1016},
 {"datasetID":76, "supportID":"6AAE0DC122102693E8136856FFC8B72DF7F78386", "rexaID":"a27103950b8d7098468bd52f2af1d033fed91419", "author":"Gustavo E. A and Gustavo E A P A Batista and Ronaldo C. Prati and Maria Carolina Monard", "title":"A Study of the Behavior of Several Methods for Balancing Machine Learning Training Data", "venue":"Instituto de Ci ^ encias Matem aticas e de Computac~ ao", "year":"", "window":"However, in spite of a large degree of imbalance the data sets Letter-a and <b>Nursery</b> obtained almost 100% AUC. The results obtained in the UCI data sets seem to be compatible with previous work of the authors [18] conducted on a series of experiments with", "mykey":1017},
 {"datasetID":151, "supportID":"6B321725CEF45A4EB49663E84250287F41C60299", "rexaID":"179e206cfdc14b74819ce95f39bb68610874578b", "author":"Juan J. Rodr##guez and Carlos J. Alonso and Henrik Bostrom", "title":"Boosting Interval Based Literals", "venue":"", "year":"2000", "window":"in a supervised classification setting from other authors. The results are shown in table 10. All the differences considered are significant, with only one exception. 4.6 <b>Sonar</b> This data set was introduced in [GS88] and it is available at the UCI ML Repository [Bay99]. The task is to discriminate between sonar signals bounced off a metal cylinder and those bounced off a roughly", "mykey":1018},
 {"datasetID":32, "supportID":"6B321725CEF45A4EB49663E84250287F41C60299", "rexaID":"179e206cfdc14b74819ce95f39bb68610874578b", "author":"Juan J. Rodr##guez and Carlos J. Alonso and Henrik Bostrom", "title":"Boosting Interval Based Literals", "venue":"", "year":"2000", "window":".0.048 0.214 4 0.704 0.084 0.781 0.676 .0.002 0.135 0.086 .0.039 0.147 0.192 Signi#c. 5 0.186 .0.005 .0.003 .0.002 .4e-05 .0.001 .0.001 .0.002 .3e-04 .0.002 Table 8: Results for the Shifted Wave data set <b>Cylinder</b> Bell Funnel -2 0 2 4 6 8 20 40 60 80 100 120 -2 0 2 4 6 8 20 40 60 80 100 120 -2 0 2 4 6 8 20 40 60 80 100 120 Figure 8: Examples of the CBF data set. Iter.: 10 20 30 40 50 60 70 80 90 100", "mykey":1019},
 {"datasetID":52, "supportID":"6B321725CEF45A4EB49663E84250287F41C60299", "rexaID":"179e206cfdc14b74819ce95f39bb68610874578b", "author":"Juan J. Rodr##guez and Carlos J. Alonso and Henrik Bostrom", "title":"Boosting Interval Based Literals", "venue":"", "year":"2000", "window":"in [HR99] is an error of 9.96, for the specified partition. Our results for the specified partition, setting 5, 100 iterations is an error of 11.54, our best result is 10.58. 4.7 <b>Ionosphere</b> This data set, also from the ML UCI Repository, contains information collected by a radar system [SWHB89]. The targets were free electrons in the ionosphere. Good\" radar returns are those showing evidence of", "mykey":1020},
 {"datasetID":107, "supportID":"6B321725CEF45A4EB49663E84250287F41C60299", "rexaID":"179e206cfdc14b74819ce95f39bb68610874578b", "author":"Juan J. Rodr##guez and Carlos J. Alonso and Henrik Bostrom", "title":"Boosting Interval Based Literals", "venue":"", "year":"2000", "window":"iterations, and a comparison of the results for settings 2{5 (combinations of interval literals) against the results for setting 1 (point based literals), using the McNemar's test. 4.1 <b>Waveform</b> This data set was introduced by [BFOS93]. The purpose is to distinguish between three classes, defined by the evaluation for i = 1; 2 : : : 21, of the following functions: x 1 (i) = uh 1 (i) + (1 u)h 2 (i) + #(i)", "mykey":1021},
 {"datasetID":108, "supportID":"6B321725CEF45A4EB49663E84250287F41C60299", "rexaID":"179e206cfdc14b74819ce95f39bb68610874578b", "author":"Juan J. Rodr##guez and Carlos J. Alonso and Henrik Bostrom", "title":"Boosting Interval Based Literals", "venue":"", "year":"2000", "window":"iterations, and a comparison of the results for settings 2{5 (combinations of interval literals) against the results for setting 1 (point based literals), using the McNemar's test. 4.1 <b>Waveform</b> This data set was introduced by [BFOS93]. The purpose is to distinguish between three classes, defined by the evaluation for i = 1; 2 : : : 21, of the following functions: x 1 (i) = uh 1 (i) + (1 u)h 2 (i) + #(i)", "mykey":1022},
 {"datasetID":150, "supportID":"6B587A673538A05EAD7839C7BFA107131F4E211C", "rexaID":"348b01031a1751ae4325c18e4dee08777b4cb9ee", "author":"Steven Salzberg", "title":"On Comparing Classifiers: Pitfalls to Avoid and a Recommended Approach", "venue":"Data Min. Knowl. Discov, 1", "year":"1997", "window":"major new results using wellstudied and widely shared data. For example, Fisher's iris data has been around for 60 years and has been used in hundreds (maybe thousands) of studies. The <b>NetTalk</b> dataset of English pronunciation data (introduced by Sejnowski and Rosenberg, [21] has been used in numerous experiments, as has the protein secondary structure data (introduced by Qian and Sejnowski [19]),", "mykey":1023},
 {"datasetID":144, "supportID":"6B97F7C9C8CEFF17179AA8782DBD96FF54AB3AE7", "rexaID":"64b5cc377a38b3f27da4b5ec9de3391f634beb4b", "author":"Avelino J. Gonzalez and Lawrence B. Holder and Diane J. Cook", "title":"Graph-Based Concept Learning", "venue":"FLAIRS Conference", "year":"2001", "window":"Voting Records Database available from the UCI machine learning repository (Keogh et. al 1998). The diabetes domain is the Pima Indians Diabetes Database, and the credit domain is the <b>German Credit</b> Dataset from the Statlog Project Databases (Keogh et. Al 1998). The Tic-Tac-Toe domain consists of 958 exhaustively generated examples. Positive examples are those where \"X\" starts moving and wins the game", "mykey":1024},
 {"datasetID":98, "supportID":"6B97F7C9C8CEFF17179AA8782DBD96FF54AB3AE7", "rexaID":"64b5cc377a38b3f27da4b5ec9de3391f634beb4b", "author":"Avelino J. Gonzalez and Lawrence B. Holder and Diane J. Cook", "title":"Graph-Based Concept Learning", "venue":"FLAIRS Conference", "year":"2001", "window":"Voting Records Database available from the UCI machine learning repository (Keogh et. al 1998). The diabetes domain is the Pima Indians Diabetes Database, and the credit domain is the German Credit Dataset from the <b>Statlog</b> Project Databases (Keogh et. Al 1998). The Tic-Tac-Toe domain consists of 958 exhaustively generated examples. Positive examples are those where \"X\" starts moving and wins the game", "mykey":1025},
 {"datasetID":34, "supportID":"6BB2FF3EE833CE07D3C6B60D064A22F6811244D7", "rexaID":"bf8052c51e6338fa2ab5479c08d657a06e4dbd4a", "author":"Eibe Frank and Mark Hall", "title":"Visualizing Class Probability Estimators", "venue":"PKDD", "year":"2003", "window":"(although they are not explicitly represented in the classifier). To provide a more realistic example Figure 8 shows four visualizations for pairs of attributes from the pima-indians <b>diabetes</b> dataset [1]. This dataset has eight attributes and 768 instances (500 belonging to class tested_negative plas mass <= 127 mass } 127 tested_negative (132.0/3.0) <= 26.4 age } 26.4 tested_negative", "mykey":1026},
 {"datasetID":53, "supportID":"6BB2FF3EE833CE07D3C6B60D064A22F6811244D7", "rexaID":"bf8052c51e6338fa2ab5479c08d657a06e4dbd4a", "author":"Eibe Frank and Mark Hall", "title":"Visualizing Class Probability Estimators", "venue":"PKDD", "year":"2003", "window":"<= 1.7 <b>iris</b> virginica (46.0/1.0) } 1.7 Iris-versicolor (48.0/1.0) <= 4.9 petalwidth } 4.9 Iris-virginica (3.0) <= 1.5 Iris-versicolor (3.0/1.0) } 1.5 Fig. 5. The decision tree for the two-class iris dataset. (a) (b) (c) Fig. 6. Visualizing the decision tree for the two-class iris data using (a) petallength and petalwidth, (b) petallength and sepallength, and (c) sepallength and sepalwidth (with the", "mykey":1027},
 {"datasetID":79, "supportID":"6BB2FF3EE833CE07D3C6B60D064A22F6811244D7", "rexaID":"bf8052c51e6338fa2ab5479c08d657a06e4dbd4a", "author":"Eibe Frank and Mark Hall", "title":"Visualizing Class Probability Estimators", "venue":"PKDD", "year":"2003", "window":"(although they are not explicitly represented in the classifier). To provide a more realistic example Figure 8 shows four visualizations for pairs of attributes from the <b>pima</b> <b>indians</b> <b>diabetes</b> dataset [1]. This dataset has eight attributes and 768 instances (500 belonging to class tested_negative plas mass <= 127 mass } 127 tested_negative (132.0/3.0) <= 26.4 age } 26.4 tested_negative", "mykey":1028},
 {"datasetID":23, "supportID":"6BDC9BB96F5A88726451A936CFC8A3432B60B914", "rexaID":"d1f5f29cee39c05c852c7fe7a1857064bc7957da", "author":"BayesianClassifi552 Pat Langley and Wayne Iba", "title":"In Proceedings of the Tenth National ConferenceonArtifi256 Intelligence( 42840", "venue":"Lambda Kevin Thompson", "year":"", "window":"C4 algorithm (Buntine & Caruana, 1991) and an algorithm that simply predicts the modal class. The five domains, from the UCI database collection (Murphy& Aha, 1992), include the ``small'' soybean dataset, <b>chess</b> end games involving aking-12 ok--126-22 wn confrontatwobiologidata set into 80% training instances and 20% testinpairs of training and test sets. The table shows the mean accuracy and 95%", "mykey":1029},
 {"datasetID":21, "supportID":"6BDC9BB96F5A88726451A936CFC8A3432B60B914", "rexaID":"d1f5f29cee39c05c852c7fe7a1857064bc7957da", "author":"BayesianClassifi552 Pat Langley and Wayne Iba", "title":"In Proceedings of the Tenth National ConferenceonArtifi256 Intelligence( 42840", "venue":"Lambda Kevin Thompson", "year":"", "window":"C4 algorithm (Buntine & Caruana, 1991) and an algorithm that simply predicts the modal class. The five domains, from the UCI database collection (Murphy& Aha, 1992), include the ``small'' soybean dataset, <b>chess</b> end games involving aking-12 ok--126-22 wn confrontatwobiologidata set into 80% training instances and 20% testinpairs of training and test sets. The table shows the mean accuracy and 95%", "mykey":1030},
 {"datasetID":22, "supportID":"6BDC9BB96F5A88726451A936CFC8A3432B60B914", "rexaID":"d1f5f29cee39c05c852c7fe7a1857064bc7957da", "author":"BayesianClassifi552 Pat Langley and Wayne Iba", "title":"In Proceedings of the Tenth National ConferenceonArtifi256 Intelligence( 42840", "venue":"Lambda Kevin Thompson", "year":"", "window":"C4 algorithm (Buntine & Caruana, 1991) and an algorithm that simply predicts the modal class. The five domains, from the UCI database collection (Murphy& Aha, 1992), include the ``small'' soybean dataset, <b>chess</b> end games involving aking-12 ok--126-22 wn confrontatwobiologidata set into 80% training instances and 20% testinpairs of training and test sets. The table shows the mean accuracy and 95%", "mykey":1031},
 {"datasetID":90, "supportID":"6BDC9BB96F5A88726451A936CFC8A3432B60B914", "rexaID":"d1f5f29cee39c05c852c7fe7a1857064bc7957da", "author":"BayesianClassifi552 Pat Langley and Wayne Iba", "title":"In Proceedings of the Tenth National ConferenceonArtifi256 Intelligence( 42840", "venue":"Lambda Kevin Thompson", "year":"", "window":"C4 algorithm (Buntine & Caruana, 1991) and an algorithm that simply predicts the modal class. The five domains, from the UCI database collection (Murphy& Aha, 1992), include the ``small'' <b>soybean</b> dataset, chess end games involving aking-12 ok--126-22 wn confrontatwobiologidata set into 80% training instances and 20% testinpairs of training and test sets. The table shows the mean accuracy and 95%", "mykey":1032},
 {"datasetID":91, "supportID":"6BDC9BB96F5A88726451A936CFC8A3432B60B914", "rexaID":"d1f5f29cee39c05c852c7fe7a1857064bc7957da", "author":"BayesianClassifi552 Pat Langley and Wayne Iba", "title":"In Proceedings of the Tenth National ConferenceonArtifi256 Intelligence( 42840", "venue":"Lambda Kevin Thompson", "year":"", "window":"C4 algorithm (Buntine & Caruana, 1991) and an algorithm that simply predicts the modal class. The five domains, from the UCI database collection (Murphy& Aha, 1992), include the ``small'' <b>soybean</b> dataset, chess end games involving aking-12 ok--126-22 wn confrontatwobiologidata set into 80% training instances and 20% testinpairs of training and test sets. The table shows the mean accuracy and 95%", "mykey":1033},
 {"datasetID":2, "supportID":"6C21FE03EFF8D65B8E0A4050AA13B513EB3A752F", "rexaID":"7de1dca96d7e6789dadc875a7f1b070f9d65fff5", "author":"Grigorios Tsoumakas and Ioannis P. Vlahavas", "title":"Fuzzy Meta-Learning: Preliminary Results", "venue":"Greek Secretariat for Research and Technology", "year":"", "window":"from the Machine Learning Repository at the University of Irvine, California (Blake & Merz, 1998). These were the <b>adult</b> and chess data sets, large enough (} 1000 examples) to simulate distributed environment. Only two domains were selected at this stage of our research to investigate the performance of the suggested methodology. The", "mykey":1034},
 {"datasetID":23, "supportID":"6C21FE03EFF8D65B8E0A4050AA13B513EB3A752F", "rexaID":"7de1dca96d7e6789dadc875a7f1b070f9d65fff5", "author":"Grigorios Tsoumakas and Ioannis P. Vlahavas", "title":"Fuzzy Meta-Learning: Preliminary Results", "venue":"Greek Secretariat for Research and Technology", "year":"", "window":"from the Machine Learning Repository at the University of Irvine, California (Blake & Merz, 1998). These were the adult and <b>chess</b> data sets, large enough (} 1000 examples) to simulate distributed environment. Only two domains were selected at this stage of our research to investigate the performance of the suggested methodology. The", "mykey":1035},
 {"datasetID":21, "supportID":"6C21FE03EFF8D65B8E0A4050AA13B513EB3A752F", "rexaID":"7de1dca96d7e6789dadc875a7f1b070f9d65fff5", "author":"Grigorios Tsoumakas and Ioannis P. Vlahavas", "title":"Fuzzy Meta-Learning: Preliminary Results", "venue":"Greek Secretariat for Research and Technology", "year":"", "window":"from the Machine Learning Repository at the University of Irvine, California (Blake & Merz, 1998). These were the adult and <b>chess</b> data sets, large enough (} 1000 examples) to simulate distributed environment. Only two domains were selected at this stage of our research to investigate the performance of the suggested methodology. The", "mykey":1036},
 {"datasetID":22, "supportID":"6C21FE03EFF8D65B8E0A4050AA13B513EB3A752F", "rexaID":"7de1dca96d7e6789dadc875a7f1b070f9d65fff5", "author":"Grigorios Tsoumakas and Ioannis P. Vlahavas", "title":"Fuzzy Meta-Learning: Preliminary Results", "venue":"Greek Secretariat for Research and Technology", "year":"", "window":"from the Machine Learning Repository at the University of Irvine, California (Blake & Merz, 1998). These were the adult and <b>chess</b> data sets, large enough (} 1000 examples) to simulate distributed environment. Only two domains were selected at this stage of our research to investigate the performance of the suggested methodology. The", "mykey":1037},
 {"datasetID":146, "supportID":"6C21FE03EFF8D65B8E0A4050AA13B513EB3A752F", "rexaID":"7de1dca96d7e6789dadc875a7f1b070f9d65fff5", "author":"Grigorios Tsoumakas and Ioannis P. Vlahavas", "title":"Fuzzy Meta-Learning: Preliminary Results", "venue":"Greek Secretariat for Research and Technology", "year":"", "window":"that have large number of classes. These were the  <b>Satellite</b>  and `Segment' data sets again from the Machine Learning Repository. The setup of the experiments was the same as in the case of testing the meta-fuzzy scheme. Table 2 presents the results. Table 2: Results with the", "mykey":1038},
 {"datasetID":9, "supportID":"6C4D5C33421F2CD1BB4CAC947EC6D58FE27AAC10", "rexaID":"0cedee7ef9325127fb946dcdddf8795644e28335", "author":"Jinyan Li and Kotagiri Ramamohanarao and Guozhu Dong", "title":"Combining the Strength of Pattern Frequency and Distance for Classification", "venue":"PAKDD", "year":"2001", "window":"The accuracy gaps can reach up to 14.93% (in sonar), half of them are around 6.5%. -- Our method is not always better than C5.0. We lose on four data sets, particularly on <b>auto</b>  -- On average over the 30 data sets, our accuracy is 2.18% higher than C5.0, and 7.27% higher than 3-NN. Table 2. Accuracy comparison among our algorithm, C5.0, and k-NN.", "mykey":1039},
 {"datasetID":69, "supportID":"6C4D5C33421F2CD1BB4CAC947EC6D58FE27AAC10", "rexaID":"0cedee7ef9325127fb946dcdddf8795644e28335", "author":"Jinyan Li and Kotagiri Ramamohanarao and Guozhu Dong", "title":"Combining the Strength of Pattern Frequency and Distance for Classification", "venue":"PAKDD", "year":"2001", "window":"containing pure categorical attributes. The accuracy is sometimes better (e.g., on the tic-tac-toe data set), but sometimes worse (e.g., on the <b>splice</b> data set). 4.2 Accuracy Variation among Folds The next set of experimental results are used to demonstrate the accuracy variations among the ten folds. We", "mykey":1040},
 {"datasetID":101, "supportID":"6C4D5C33421F2CD1BB4CAC947EC6D58FE27AAC10", "rexaID":"0cedee7ef9325127fb946dcdddf8795644e28335", "author":"Jinyan Li and Kotagiri Ramamohanarao and Guozhu Dong", "title":"Combining the Strength of Pattern Frequency and Distance for Classification", "venue":"PAKDD", "year":"2001", "window":"containing pure categorical attributes. The accuracy is sometimes better (e.g., on the <b>tic-tac-toe</b> data set), but sometimes worse (e.g., on the splice data set). 4.2 Accuracy Variation among Folds The next set of experimental results are used to demonstrate the accuracy variations among the ten folds. We", "mykey":1041},
 {"datasetID":45, "supportID":"6C73DC740EEBBF4A78556DD5F89FD4E3E29BBFC2", "rexaID":"77b535b98a279e3b1ee9499bead3408bc8d58c08", "author":"Glenn Fung and Sathyakama Sandilya and R. Bharat Rao", "title":"Rule extraction from Linear Support Vector Machines", "venue":"Computer-Aided Diagnosis & Therapy, Siemens Medical Solutions, Inc", "year":"", "window":"from the UCI Machine Learning Repository [13]: Wisconsin Diagnosis Breast Cancer (WDBC), Ionosphere, and Cleveland <b>heart</b>  The fourth dataset is a dataset related to the nontraditional authorship attribution problem related to the federalist papers [7] and the fifth dataset is a dataset used for training in a computer aided detection", "mykey":1042},
 {"datasetID":52, "supportID":"6C73DC740EEBBF4A78556DD5F89FD4E3E29BBFC2", "rexaID":"77b535b98a279e3b1ee9499bead3408bc8d58c08", "author":"Glenn Fung and Sathyakama Sandilya and R. Bharat Rao", "title":"Rule extraction from Linear Support Vector Machines", "venue":"Computer-Aided Diagnosis & Therapy, Siemens Medical Solutions, Inc", "year":"", "window":"from the UCI Machine Learning Repository [13]: Wisconsin Diagnosis Breast Cancer (WDBC), <b>Ionosphere</b>  and Cleveland heart. The fourth dataset is a dataset related to the nontraditional authorship attribution problem related to the federalist papers [7] and the fifth dataset is a dataset used for training in a computer aided detection", "mykey":1043},
 {"datasetID":62, "supportID":"6C73DC740EEBBF4A78556DD5F89FD4E3E29BBFC2", "rexaID":"77b535b98a279e3b1ee9499bead3408bc8d58c08", "author":"Glenn Fung and Sathyakama Sandilya and R. Bharat Rao", "title":"Rule extraction from Linear Support Vector Machines", "venue":"Computer-Aided Diagnosis & Therapy, Siemens Medical Solutions, Inc", "year":"", "window":"including a medical dataset on detection of <b>lung cancer</b> from medical images. The ability to convert SVM's and other \"black-box\" classifiers into a set of human-understandable rules, is critical not only for physician", "mykey":1044},
 {"datasetID":82, "supportID":"6C73DC740EEBBF4A78556DD5F89FD4E3E29BBFC2", "rexaID":"77b535b98a279e3b1ee9499bead3408bc8d58c08", "author":"Glenn Fung and Sathyakama Sandilya and R. Bharat Rao", "title":"Rule extraction from Linear Support Vector Machines", "venue":"Computer-Aided Diagnosis & Therapy, Siemens Medical Solutions, Inc", "year":"", "window":"The first experiment relates to the publicly available WDBC dataset that consists of 683 <b>patient data</b>  The classification task associated with this dataset is to diagnose breast masses based solely on a Fine Needle Aspiration (FNA). Doctors identified nine visually", "mykey":1045},
 {"datasetID":14, "supportID":"6C79B6228AA533774359EB0CBDDB6C7EF0240041", "rexaID":"01ede1d6464c9533cd6ec1a4492d134729ce99bd", "author":"Christophe Giraud and Tony Martinez and Christophe G. Giraud-Carrier", "title":"University of Bristol Department of Computer Science ILA: Combining Inductive Learning with Prior Knowledge and Reasoning", "venue":"", "year":"1995", "window":"Study Algorithm PA GR ILA 82.7 .49 ILA, T=2 73.9 .20 PDL2 79.7 .66 As expected, results with T=2 show a decrease in PA (about 10%), but also a significant decrease in GR (over 59%). For three of the datasets (zoo, <b>breast</b> <b>cancer</b>  and soybean-small), the decrease in PA is less than 1.1% on average, while the decrease in GR is greater than 76%. The threshold T, though not part of the basic model, provides", "mykey":1046},
 {"datasetID":46, "supportID":"6C79B6228AA533774359EB0CBDDB6C7EF0240041", "rexaID":"01ede1d6464c9533cd6ec1a4492d134729ce99bd", "author":"Christophe Giraud and Tony Martinez and Christophe G. Giraud-Carrier", "title":"University of Bristol Department of Computer Science ILA: Combining Inductive Learning with Prior Knowledge and Reasoning", "venue":"", "year":"1995", "window":"1. If representative voted 'no' on the 'physician-fee-freeze' issue, then rep. is a democrat <b>hepatitis</b> dataset: 1. If patient is between 21 and 30, then patient lives 2. If patient is between 51 and 60, is a male, uses steroids, has malaise, has a liver that is big and firm, has high bilirubin and high", "mykey":1047},
 {"datasetID":58, "supportID":"6C79B6228AA533774359EB0CBDDB6C7EF0240041", "rexaID":"01ede1d6464c9533cd6ec1a4492d134729ce99bd", "author":"Christophe Giraud and Tony Martinez and Christophe G. Giraud-Carrier", "title":"University of Bristol Department of Computer Science ILA: Combining Inductive Learning with Prior Knowledge and Reasoning", "venue":"", "year":"1995", "window":"animal has four legs, then animal belongs to class 1 2. If animal has feathers, then animal belongs to class 2 3. If animal lays eggs, is aquatic, and has fins, then animal belongs to class 4 <b>lenses</b> dataset: 1. If patient has low tear production rate, then patient is not fit for contact lenses voting-84 dataset: 1. If representative voted 'no' on the 'physician-fee-freeze' issue, then rep. is a", "mykey":1048},
 {"datasetID":90, "supportID":"6C79B6228AA533774359EB0CBDDB6C7EF0240041", "rexaID":"01ede1d6464c9533cd6ec1a4492d134729ce99bd", "author":"Christophe Giraud and Tony Martinez and Christophe G. Giraud-Carrier", "title":"University of Bristol Department of Computer Science ILA: Combining Inductive Learning with Prior Knowledge and Reasoning", "venue":"", "year":"1995", "window":"are found, ILA degenerates into a restricted form of MBR [21]. This allows ILA to perform well, even with simple (and possibly weak) generalization mechanisms (see for example the <b>soybean</b> small dataset in Section 3). Going further, one may consider extending ILA with some of the well established mechanisms of MBR to further improve ILA's performance in situations where only few rules are", "mykey":1049},
 {"datasetID":91, "supportID":"6C79B6228AA533774359EB0CBDDB6C7EF0240041", "rexaID":"01ede1d6464c9533cd6ec1a4492d134729ce99bd", "author":"Christophe Giraud and Tony Martinez and Christophe G. Giraud-Carrier", "title":"University of Bristol Department of Computer Science ILA: Combining Inductive Learning with Prior Knowledge and Reasoning", "venue":"", "year":"1995", "window":"are found, ILA degenerates into a restricted form of MBR [21]. This allows ILA to perform well, even with simple (and possibly weak) generalization mechanisms (see for example the <b>soybean</b> small dataset in Section 3). Going further, one may consider extending ILA with some of the well established mechanisms of MBR to further improve ILA's performance in situations where only few rules are", "mykey":1050},
 {"datasetID":111, "supportID":"6C79B6228AA533774359EB0CBDDB6C7EF0240041", "rexaID":"01ede1d6464c9533cd6ec1a4492d134729ce99bd", "author":"Christophe Giraud and Tony Martinez and Christophe G. Giraud-Carrier", "title":"University of Bristol Department of Computer Science ILA: Combining Inductive Learning with Prior Knowledge and Reasoning", "venue":"", "year":"1995", "window":"none of the continuous attributes are discretized. Table 1 - Selected Applications and Attributes Dataset #instances #inputs Input space flout. values <b>zoo</b> 90 16 Nominal 7 iris 150 4 Linear 3 lenses 24 4 Lin/Nom 3 hepatitis 155 19 Linear 2 voting-84 435 16 Boolean 2 glass 214 9 Linear 7 breast-cancer 699", "mykey":1051},
 {"datasetID":14, "supportID":"6CEA5EF59096948B99F761531EFC55FEA9819322", "rexaID":"3e9ebff12a232c9f091156827e92c55d259b95f3", "author":"Nikunj C. Oza and Stuart J. Russell", "title":"Online Bagging and Boosting", "venue":"Computer Science Division University of California", "year":"", "window":"Bagging and online bagging performed noticeably better than single decision trees on all except the <b>Breast</b> <b>Cancer</b> dataset. With Naive Bayes, bagging and online bagging never performed noticeably better than Naive Bayes, which we expected because of the stability of Naive Bayes [3]. Boosting and online boosting", "mykey":1052},
 {"datasetID":19, "supportID":"6CEA5EF59096948B99F761531EFC55FEA9819322", "rexaID":"3e9ebff12a232c9f091156827e92c55d259b95f3", "author":"Nikunj C. Oza and Stuart J. Russell", "title":"Online Bagging and Boosting", "venue":"Computer Science Division University of California", "year":"", "window":"ITI online algorithm [14]; batch and online Naive Bayes algorithms are essentially identical. To illustrate the convergence of batch and online learning, we experimented with the <b>Car</b> Evaluation dataset from the UCI Machine Learning Repository [2]. The dataset has 1728 examples, of which we retained 346 (20%) as a test set and used 200, 400, 600, 800, 1000, 1200, and all the remaining 1382 examples", "mykey":1053},
 {"datasetID":23, "supportID":"6CEA5EF59096948B99F761531EFC55FEA9819322", "rexaID":"3e9ebff12a232c9f091156827e92c55d259b95f3", "author":"Nikunj C. Oza and Stuart J. Russell", "title":"Online Bagging and Boosting", "venue":"Computer Science Division University of California", "year":"", "window":"AdaBoost performed significantly worse and online boosting performed marginally worse. On the Car Evaluation and <b>Chess</b> datasets, AdaBoost and online boosting performed significantly better than Naive Bayes. On the Nursery dataset, AdaBoost performed significantly better and online boosting performed marginally better. 5", "mykey":1054},
 {"datasetID":21, "supportID":"6CEA5EF59096948B99F761531EFC55FEA9819322", "rexaID":"3e9ebff12a232c9f091156827e92c55d259b95f3", "author":"Nikunj C. Oza and Stuart J. Russell", "title":"Online Bagging and Boosting", "venue":"Computer Science Division University of California", "year":"", "window":"AdaBoost performed significantly worse and online boosting performed marginally worse. On the Car Evaluation and <b>Chess</b> datasets, AdaBoost and online boosting performed significantly better than Naive Bayes. On the Nursery dataset, AdaBoost performed significantly better and online boosting performed marginally better. 5", "mykey":1055},
 {"datasetID":22, "supportID":"6CEA5EF59096948B99F761531EFC55FEA9819322", "rexaID":"3e9ebff12a232c9f091156827e92c55d259b95f3", "author":"Nikunj C. Oza and Stuart J. Russell", "title":"Online Bagging and Boosting", "venue":"Computer Science Division University of California", "year":"", "window":"AdaBoost performed significantly worse and online boosting performed marginally worse. On the Car Evaluation and <b>Chess</b> datasets, AdaBoost and online boosting performed significantly better than Naive Bayes. On the Nursery dataset, AdaBoost performed significantly better and online boosting performed marginally better. 5", "mykey":1056},
 {"datasetID":76, "supportID":"6CEA5EF59096948B99F761531EFC55FEA9819322", "rexaID":"3e9ebff12a232c9f091156827e92c55d259b95f3", "author":"Nikunj C. Oza and Stuart J. Russell", "title":"Online Bagging and Boosting", "venue":"Computer Science Division University of California", "year":"", "window":"AdaBoost and online boosting performed significantly better than Naive Bayes. On the <b>Nursery</b> dataset, AdaBoost performed significantly better and online boosting performed marginally better. 5 Conclusions The paper has described online versions of the popular bagging and boosting algorithms and has", "mykey":1057},
 {"datasetID":90, "supportID":"6CEA5EF59096948B99F761531EFC55FEA9819322", "rexaID":"3e9ebff12a232c9f091156827e92c55d259b95f3", "author":"Nikunj C. Oza and Stuart J. Russell", "title":"Online Bagging and Boosting", "venue":"Computer Science Division University of California", "year":"", "window":"and their performances relative to a single Naive Bayes classifier consistently improved as the sizes of the datasets grew. On the Balance and <b>Soybean</b> datasets, the boosting algorithms performed signi#cantly worse than Naive Bayes. On the Breast Cancer dataset, AdaBoost performed significantly worse and online", "mykey":1058},
 {"datasetID":91, "supportID":"6CEA5EF59096948B99F761531EFC55FEA9819322", "rexaID":"3e9ebff12a232c9f091156827e92c55d259b95f3", "author":"Nikunj C. Oza and Stuart J. Russell", "title":"Online Bagging and Boosting", "venue":"Computer Science Division University of California", "year":"", "window":"and their performances relative to a single Naive Bayes classifier consistently improved as the sizes of the datasets grew. On the Balance and <b>Soybean</b> datasets, the boosting algorithms performed signi#cantly worse than Naive Bayes. On the Breast Cancer dataset, AdaBoost performed significantly worse and online", "mykey":1059},
 {"datasetID":90, "supportID":"6D5CA37282ECE273713460A3E981131BE44CDE55", "rexaID":"55c411b7643dc75154d16ea7dfa732b70f750ed2", "author":"Zhi-Hua Zhou and Yang Yu", "title":"Ensembling Local Learners Through Multimodal Perturbation", "venue":"", "year":"", "window":"Attribute Data set Size Categorical Continuous Class COEF <b>soybean</b> 562 0 35 19 0.85 autos 159 15 10 7 0.91 sonar 208 60 0 2 1.73 lymph 148 3 15 4 2.06 glass 214 9 0 7 3.40 anneal 898 6 32 6 3.94 heart-c 296 6 7 5 4.55", "mykey":1060},
 {"datasetID":91, "supportID":"6D5CA37282ECE273713460A3E981131BE44CDE55", "rexaID":"55c411b7643dc75154d16ea7dfa732b70f750ed2", "author":"Zhi-Hua Zhou and Yang Yu", "title":"Ensembling Local Learners Through Multimodal Perturbation", "venue":"", "year":"", "window":"Attribute Data set Size Categorical Continuous Class COEF <b>soybean</b> 562 0 35 19 0.85 autos 159 15 10 7 0.91 sonar 208 60 0 2 1.73 lymph 148 3 15 4 2.06 glass 214 9 0 7 3.40 anneal 898 6 32 6 3.94 heart-c 296 6 7 5 4.55", "mykey":1061},
 {"datasetID":34, "supportID":"6DA89FF970749A5C92453FB296AE39D3379A77D0", "rexaID":"bf8052c51e6338fa2ab5479c08d657a06e4dbd4a", "author":"Eibe Frank and Mark Hall", "title":"Visualizing Class Probability Estimators", "venue":"PKDD", "year":"2003", "window":"(although they are not explicitly represented in the classifier). To provide a more realistic example Figure 8 shows four visualizations for pairs of attributes from the pima-indians <b>diabetes</b> dataset (Blake & Merz, 1998). This dataset has eight attributes and 768 instances (500 belonging to class tested_negative and 268 to class tested_positive). The decision tree for this problem is shown in", "mykey":1062},
 {"datasetID":53, "supportID":"6DA89FF970749A5C92453FB296AE39D3379A77D0", "rexaID":"bf8052c51e6338fa2ab5479c08d657a06e4dbd4a", "author":"Eibe Frank and Mark Hall", "title":"Visualizing Class Probability Estimators", "venue":"PKDD", "year":"2003", "window":"1.7 <b>iris</b> virginica (46.0/1.0) } 1.7 Iris-versicolor (48.0/1.0) <= 4.9 petalwidth } 4.9 Iris-virginica (3.0) <= 1.5 Iris-versicolor (3.0/1.0) } 1.5 Figure 5: The decision tree for the two-class iris dataset. (a) (b) (c) Figure 6: Visualizing the decision tree for the two-class iris data using (a) petallength and petalwidth, (b) petallength and sepallength, and (c) sepallength and sepalwidth (with the", "mykey":1063},
 {"datasetID":79, "supportID":"6DA89FF970749A5C92453FB296AE39D3379A77D0", "rexaID":"bf8052c51e6338fa2ab5479c08d657a06e4dbd4a", "author":"Eibe Frank and Mark Hall", "title":"Visualizing Class Probability Estimators", "venue":"PKDD", "year":"2003", "window":"(although they are not explicitly represented in the classifier). To provide a more realistic example Figure 8 shows four visualizations for pairs of attributes from the <b>pima</b> <b>indians</b> <b>diabetes</b> dataset (Blake & Merz, 1998). This dataset has eight attributes and 768 instances (500 belonging to class tested_negative and 268 to class tested_positive). The decision tree for this problem is shown in", "mykey":1064},
 {"datasetID":23, "supportID":"6DFAB362BC777E086E02A08CFE014188B26C3335", "rexaID":"5976a7013ed27ef8dc3309ee0427e9e110b5323d", "author":"Boonserm Kijsirikul and Sukree Sinthupinyo and Kongsak Chongkasemwongse", "title":"Approximate Match of Rules Using Backpropagation Neural Networks", "venue":"Machine Learning, 44", "year":"2001", "window":"& Feng, 1990) for learning rules. Normally we used rules produced byPROGOL as the input to BANNAR. However in our experiments on the finite element mesh design and the King-Rook-King <b>chess</b> endgame datasets described below, PROGOL failed to produce a rule set within a reasonable time. In those experiments, we employed GOLEM developed by the same research group of PROGOL. We then compared the results", "mykey":1065},
 {"datasetID":21, "supportID":"6DFAB362BC777E086E02A08CFE014188B26C3335", "rexaID":"5976a7013ed27ef8dc3309ee0427e9e110b5323d", "author":"Boonserm Kijsirikul and Sukree Sinthupinyo and Kongsak Chongkasemwongse", "title":"Approximate Match of Rules Using Backpropagation Neural Networks", "venue":"Machine Learning, 44", "year":"2001", "window":"& Feng, 1990) for learning rules. Normally we used rules produced byPROGOL as the input to BANNAR. However in our experiments on the finite element mesh design and the King-Rook-King <b>chess</b> endgame datasets described below, PROGOL failed to produce a rule set within a reasonable time. In those experiments, we employed GOLEM developed by the same research group of PROGOL. We then compared the results", "mykey":1066},
 {"datasetID":22, "supportID":"6DFAB362BC777E086E02A08CFE014188B26C3335", "rexaID":"5976a7013ed27ef8dc3309ee0427e9e110b5323d", "author":"Boonserm Kijsirikul and Sukree Sinthupinyo and Kongsak Chongkasemwongse", "title":"Approximate Match of Rules Using Backpropagation Neural Networks", "venue":"Machine Learning, 44", "year":"2001", "window":"& Feng, 1990) for learning rules. Normally we used rules produced byPROGOL as the input to BANNAR. However in our experiments on the finite element mesh design and the King-Rook-King <b>chess</b> endgame datasets described below, PROGOL failed to produce a rule set within a reasonable time. In those experiments, we employed GOLEM developed by the same research group of PROGOL. We then compared the results", "mykey":1067},
 {"datasetID":34, "supportID":"6EBA814C6891A7CD993542A7DE74E363E2603D86", "rexaID":"08bad2c42799dc0f04d6729f069239fba413cb8f", "author":"Jan C. Bioch and D. Meer and Rob Potharst", "title":"Bivariate Decision Trees", "venue":"PKDD", "year":"1997", "window":"with the standard error. From these table we can conclude 10 name cases attr classes glass 214 9 6 <b>diabetes</b> pima) 768 8 2 breast cancer 699 9 2 heart 270 13 2 wave 300 21 3 Table 1: Summary of the Datasets method glass diabetes cancer heart wave BIT1 65.3#1:1 74.3#0:7 95.4#0:3 78.5#0:3 76.1#1:3 6.2#2:1 5.2#2:5 2.8#0:2 4.1#0:5 5.0#1:6 BIT2 64.8#0:9 74.7#0:7 95.4#0:3 78.5#0:3 76.2#1:0 6.8#2:5 5.8#2:7", "mykey":1068},
 {"datasetID":42, "supportID":"6EBA814C6891A7CD993542A7DE74E363E2603D86", "rexaID":"08bad2c42799dc0f04d6729f069239fba413cb8f", "author":"Jan C. Bioch and D. Meer and Rob Potharst", "title":"Bivariate Decision Trees", "venue":"PKDD", "year":"1997", "window":"with the standard error. From these table we can conclude 10 name cases attr classes <b>glass</b> 214 9 6 diabetes(pima) 768 8 2 breast cancer 699 9 2 heart 270 13 2 wave 300 21 3 Table 1: Summary of the Datasets method glass diabetes cancer heart wave BIT1 65.3#1:1 74.3#0:7 95.4#0:3 78.5#0:3 76.1#1:3 6.2#2:1 5.2#2:5 2.8#0:2 4.1#0:5 5.0#1:6 BIT2 64.8#0:9 74.7#0:7 95.4#0:3 78.5#0:3 76.2#1:0 6.8#2:5 5.8#2:7", "mykey":1069},
 {"datasetID":45, "supportID":"6EBA814C6891A7CD993542A7DE74E363E2603D86", "rexaID":"08bad2c42799dc0f04d6729f069239fba413cb8f", "author":"Jan C. Bioch and D. Meer and Rob Potharst", "title":"Bivariate Decision Trees", "venue":"PKDD", "year":"1997", "window":"with the standard error. From these table we can conclude 10 name cases attr classes glass 214 9 6 diabetes(pima) 768 8 2 breast cancer 699 9 2 <b>heart</b> 270 13 2 wave 300 21 3 Table 1: Summary of the Datasets method glass diabetes cancer heart wave BIT1 65.3#1:1 74.3#0:7 95.4#0:3 78.5#0:3 76.1#1:3 6.2#2:1 5.2#2:5 2.8#0:2 4.1#0:5 5.0#1:6 BIT2 64.8#0:9 74.7#0:7 95.4#0:3 78.5#0:3 76.2#1:0 6.8#2:5 5.8#2:7", "mykey":1070},
 {"datasetID":70, "supportID":"6EBA814C6891A7CD993542A7DE74E363E2603D86", "rexaID":"08bad2c42799dc0f04d6729f069239fba413cb8f", "author":"Jan C. Bioch and D. Meer and Rob Potharst", "title":"Bivariate Decision Trees", "venue":"PKDD", "year":"1997", "window":"in which interactions between twovariables occur. We will test our method on two artificial data sets. The first is the <b>monk</b> 1 data set [Thr91]. This data set contains 6 attributes and two classes. The rule that generates the data is: if (x 1 = x 2 or x 5 = 1) then yes else no. Note that x 5 takes", "mykey":1071},
 {"datasetID":53, "supportID":"6F08A4BA8ED79ECF33329185BEBC1FC8397CB46B", "rexaID":"e7f14adabe196dbd08024790f0df92a43728d643", "author":"Asa Ben-Hur and David Horn and Hava T. Siegelmann and Vladimir Vapnik", "title":"A Support Vector Method for Clustering", "venue":"NIPS", "year":"2000", "window":"the core regions by an SV method with a global optimal solution. We have found examples where a local maximum is hard to identify by Roberts' method. 3.2 The <b>iris</b> data We ran SVC on the iris data set [9], which is a standard benchmark in the pattern recognition literature. It can be obtained from the UCI repository [10]. The data set contains 150 instances, each containing four measurements of", "mykey":1072},
 {"datasetID":53, "supportID":"6F29A335BB79999C91E6FB53D261EF88F5308326", "rexaID":"5d851945e80949d177fa69dc5d4799f6a0f9d9f7", "author":"Tapio Elomaa and Juho Rousu", "title":"Finding Optimal Multi-Splits for Numerical Attributes in Decision Tree Learning", "venue":"ESPRIT Working Group in Neural and Computational Learning", "year":"1996", "window":"used. Data set Examples Attributes Classes Num. Total <b>Iris</b> plant classification 150 4 4 3 Glass type identification 214 9 9 6 Australian credit card assessment 690 6 14 2 Wisconsin breast cancer data 699 9 9 2", "mykey":1073},
 {"datasetID":107, "supportID":"6F29A335BB79999C91E6FB53D261EF88F5308326", "rexaID":"5d851945e80949d177fa69dc5d4799f6a0f9d9f7", "author":"Tapio Elomaa and Juho Rousu", "title":"Finding Optimal Multi-Splits for Numerical Attributes in Decision Tree Learning", "venue":"ESPRIT Working Group in Neural and Computational Learning", "year":"1996", "window":"preprocessing time dominates the total running time, with the exception of the <b>Waveform</b> data set, which consists of truly continuos-valued attributes: Each attribute has over 500 different values in the data and almost as many boundary points. In comparison, the Shuttle domain has on average", "mykey":1074},
 {"datasetID":108, "supportID":"6F29A335BB79999C91E6FB53D261EF88F5308326", "rexaID":"5d851945e80949d177fa69dc5d4799f6a0f9d9f7", "author":"Tapio Elomaa and Juho Rousu", "title":"Finding Optimal Multi-Splits for Numerical Attributes in Decision Tree Learning", "venue":"ESPRIT Working Group in Neural and Computational Learning", "year":"1996", "window":"preprocessing time dominates the total running time, with the exception of the <b>Waveform</b> data set, which consists of truly continuos-valued attributes: Each attribute has over 500 different values in the data and almost as many boundary points. In comparison, the Shuttle domain has on average", "mykey":1075},
 {"datasetID":14, "supportID":"6F376C47D091989CDECCD7B8CB7971A38783AAD9", "rexaID":"084aa2a0b1ffd67537222e2c439d2fadce6090ca", "author":"Petri Kontkanen and Petri Myllym and Tomi Silander and Henry Tirri and Peter Gr", "title":"On predictive distributions and Bayesian networks", "venue":"Department of Computer Science, Stanford University", "year":"2000", "window":"3 we plot the performance of the methods, averaged over 100 independent test runs performed as described above, as a function of the number of the data vectors used for training in the <b>Breast</b> <b>cancer</b> dataset case. From this picture we see that in the logscore sense, the evidence-based EVU and EVJ approaches perform surprisingly well even in 15 -9 -8 -7 -6 -5 -4 -3 -2 -1 0 0 50 100 150 200 250 300", "mykey":1076},
 {"datasetID":42, "supportID":"6F376C47D091989CDECCD7B8CB7971A38783AAD9", "rexaID":"084aa2a0b1ffd67537222e2c439d2fadce6090ca", "author":"Petri Kontkanen and Petri Myllym and Tomi Silander and Henry Tirri and Peter Gr", "title":"On predictive distributions and Bayesian networks", "venue":"Department of Computer Science, Stanford University", "year":"2000", "window":"used show very similar behavior. As an illustrative example, in Figure 4 the average log-scores obtained are plotted in the Hepatitis and <b>Glass</b> dataset cases. Again, the EVU and EVJ approaches are quite robust in the sense that they predict quite well even with small training sets. This shows that the data sets used here are quite redundant, and", "mykey":1077},
 {"datasetID":45, "supportID":"6F376C47D091989CDECCD7B8CB7971A38783AAD9", "rexaID":"084aa2a0b1ffd67537222e2c439d2fadce6090ca", "author":"Petri Kontkanen and Petri Myllym and Tomi Silander and Henry Tirri and Peter Gr", "title":"On predictive distributions and Bayesian networks", "venue":"Department of Computer Science, Stanford University", "year":"2000", "window":"Dataset Data vectors Attributes Classes CV folds <b>Heart</b> Disease (HD) 270 14 2 9 Iris (IR) 150 5 3 5 Lymphography (LY) 148 19 4 5 Australian (AU) 690 15 2 10 Breast Cancer (BC) 286 10 2 11 Diabetes (DB) 768 9", "mykey":1078},
 {"datasetID":46, "supportID":"6F376C47D091989CDECCD7B8CB7971A38783AAD9", "rexaID":"084aa2a0b1ffd67537222e2c439d2fadce6090ca", "author":"Petri Kontkanen and Petri Myllym and Tomi Silander and Henry Tirri and Peter Gr", "title":"On predictive distributions and Bayesian networks", "venue":"Department of Computer Science, Stanford University", "year":"2000", "window":"2 9 Iris (IR) 150 5 3 5 Lymphography (LY) 148 19 4 5 Australian (AU) 690 15 2 10 Breast Cancer (BC) 286 10 2 11 Diabetes (DB) 768 9 2 12 Glass (GL) 214 10 6 7 <b>Hepatitis</b> (HE) 150 20 2 5 Table 1: The datasets used in the experiments For comparing the predictive accuracy of different predictive distributions, we used two different utility functions: the log-score and the 0/1-score. The log-score of a", "mykey":1079},
 {"datasetID":34, "supportID":"6F8D34C77A80879FB02CC2339B3018DA88A03882", "rexaID":"0b088e37f14f4bd30ad6c550b543306c6363eaa7", "author":"Marina Skurichina and Robert P W Duin", "title":"Boosting in Linear Discriminant Analysis", "venue":"Multiple Classifier Systems", "year":"2000", "window":"(Data II) with 225 and 126 objects belonging to the first and the second data class, respectively. The second is the 8-dimensional <b>diabetes</b> data set (Data III) consisting of 500 and 268 objects from the first and the second data class, respectively. These two data sets were also used in [8], when studying bagging and boosting for decision trees.", "mykey":1080},
 {"datasetID":52, "supportID":"6F8D34C77A80879FB02CC2339B3018DA88A03882", "rexaID":"0b088e37f14f4bd30ad6c550b543306c6363eaa7", "author":"Marina Skurichina and Robert P W Duin", "title":"Boosting in Linear Discriminant Analysis", "venue":"Multiple Classifier Systems", "year":"2000", "window":"are taken from the UCI Repository [14]. The first is the 34dimensional <b>ionosphere</b> data set (Data II) with 225 and 126 objects belonging to the first and the second data class, respectively. The second is the 8-dimensional diabetes data set (Data III) consisting of 500 and 268 objects from", "mykey":1081},
 {"datasetID":73, "supportID":"6FAA63F0D802B80633874DB98C5545682A31B37B", "rexaID":"74d13cdcdfa5836ae8056a6c6ee47d129b6512f3", "author":"Guszti Bartfai", "title":"VICTORIA UNIVERSITY OF WELLINGTON Te Whare Wananga o te Upoko o te Ika a Maui", "venue":"Department of Computer Science PO Box 600", "year":"1996", "window":"A brief survey of related neural network models is also provided. Keywords: Modular networks, Adaptive Resonance Theory, Hierarchical ART, Self-organization, Hierarchical clustering, <b>Mushroom</b> dataset. Publishing Information This paper will appear in a Special Issue of Connection Science on ``Combining Artificial Neural Networks''. 0 Author Information Guszti Bartfai is a lecturer at the", "mykey":1082},
 {"datasetID":90, "supportID":"6FAA63F0D802B80633874DB98C5545682A31B37B", "rexaID":"74d13cdcdfa5836ae8056a6c6ee47d129b6512f3", "author":"Guszti Bartfai", "title":"VICTORIA UNIVERSITY OF WELLINGTON Te Whare Wananga o te Upoko o te Ika a Maui", "venue":"Department of Computer Science PO Box 600", "year":"1996", "window":"from the UCI Repository (Merz and Murphy, 1996) were used: ffl <b>Soybean</b>  this data set contains 307 instances 5 described with 35 nominal attributes (and class information, which was ignored here). The attributes were encoded in generalised complement coding (see above) --- with", "mykey":1083},
 {"datasetID":91, "supportID":"6FAA63F0D802B80633874DB98C5545682A31B37B", "rexaID":"74d13cdcdfa5836ae8056a6c6ee47d129b6512f3", "author":"Guszti Bartfai", "title":"VICTORIA UNIVERSITY OF WELLINGTON Te Whare Wananga o te Upoko o te Ika a Maui", "venue":"Department of Computer Science PO Box 600", "year":"1996", "window":"from the UCI Repository (Merz and Murphy, 1996) were used: ffl <b>Soybean</b>  this data set contains 307 instances 5 described with 35 nominal attributes (and class information, which was ignored here). The attributes were encoded in generalised complement coding (see above) --- with", "mykey":1084},
 {"datasetID":111, "supportID":"6FAA63F0D802B80633874DB98C5545682A31B37B", "rexaID":"74d13cdcdfa5836ae8056a6c6ee47d129b6512f3", "author":"Guszti Bartfai", "title":"VICTORIA UNIVERSITY OF WELLINGTON Te Whare Wananga o te Upoko o te Ika a Maui", "venue":"Department of Computer Science PO Box 600", "year":"1996", "window":"capabilities of hart networks In order to demonstrate that the hart networks are capable of developing class hierarchies, we trained and tested the networks on the <b>Zoo</b> machine learning benchmark data set (Merz and Murphy, 1996). This data set contains 101 instances of animals described with 18 attributes such as ``hair'', ``aquatic'', ``domestic'' and so on. It is relatively small, but is adequate", "mykey":1085},
 {"datasetID":1, "supportID":"6FD9759745E0CD054310B2F8BEF395FEFB72E409", "rexaID":"b06bc893938966189c9aefe9a340bb51f7026522", "author":"Matthew Mullin and Rahul Sukthankar", "title":"Complete Cross-Validation for Nearest Neighbor Classifiers", "venue":"ICML", "year":"2000", "window":"from the UCI repository (Blake & Merz, 1998). Synthetic consists of 4 Gaussians with equal variance and significant overlap, and Bayes Error # 0.504. 6 <b>Abalone</b> 6 The Bayes error for the Synthetic data set was estimated using a Monte-Carlo simulation with 60000 samples. is a 29 class problem, however many of the classes have only very few instances. Abalone-2 and Abalone-3 are twoand three-class", "mykey":1086},
 {"datasetID":14, "supportID":"6FD9759745E0CD054310B2F8BEF395FEFB72E409", "rexaID":"b06bc893938966189c9aefe9a340bb51f7026522", "author":"Matthew Mullin and Rahul Sukthankar", "title":"Complete Cross-Validation for Nearest Neighbor Classifiers", "venue":"ICML", "year":"2000", "window":"and Abalone-3 are twoand three-class versions of the problem, where the adjacent classes were grouped so that data was divided evenly. Abalone-3 was introduced in (Waugh, 1995). In the <b>Breast</b> <b>Cancer</b> dataset, the ID field was omitted, as was a field containing missing values. 7 Since the aim of these experiments was not to improve classification accuracy but rather to compare estimation variance and", "mykey":1087},
 {"datasetID":42, "supportID":"702086AF19E8011C946FB678A7EE218CB41268D2", "rexaID":"cf334aad055b27faaeece97ee1630e146388cd10", "author":"H. Altay G uvenir and Aynur Akkus", "title":"WEIGHTED K NEAREST NEIGHBOR CLASSIFICATION ON FEATURE PROJECTIONS", "venue":"Department of Computer Engineering and Information Science Bilkent University", "year":"", "window":"row of each k value presents the accuracy of the WkNNFP algorithm with equal feature weigths, while the second row shows the accuracy obtained by WkNNFP using Table 1: Comparison on some real-world datasets. Data Set: cleveland <b>glass</b> horse hungarian iris liver sonar wine No. of Instances 303 214 368 294 150 345 208 178 No. of Features 13 9 22 13 4 6 60 13 No. of Classes 2 6 2 2 3 2 2 3 No. of Missing", "mykey":1088},
 {"datasetID":47, "supportID":"702086AF19E8011C946FB678A7EE218CB41268D2", "rexaID":"cf334aad055b27faaeece97ee1630e146388cd10", "author":"H. Altay G uvenir and Aynur Akkus", "title":"WEIGHTED K NEAREST NEIGHBOR CLASSIFICATION ON FEATURE PROJECTIONS", "venue":"Department of Computer Engineering and Information Science Bilkent University", "year":"", "window":"row of each k value presents the accuracy of the WkNNFP algorithm with equal feature weigths, while the second row shows the accuracy obtained by WkNNFP using Table 1: Comparison on some real-world datasets. Data Set: cleveland glass <b>horse</b> hungarian iris liver sonar wine No. of Instances 303 214 368 294 150 345 208 178 No. of Features 13 9 22 13 4 6 60 13 No. of Classes 2 6 2 2 3 2 2 3 No. of Missing", "mykey":1089},
 {"datasetID":53, "supportID":"702086AF19E8011C946FB678A7EE218CB41268D2", "rexaID":"cf334aad055b27faaeece97ee1630e146388cd10", "author":"H. Altay G uvenir and Aynur Akkus", "title":"WEIGHTED K NEAREST NEIGHBOR CLASSIFICATION ON FEATURE PROJECTIONS", "venue":"Department of Computer Engineering and Information Science Bilkent University", "year":"", "window":"row of each k value presents the accuracy of the WkNNFP algorithm with equal feature weigths, while the second row shows the accuracy obtained by WkNNFP using Table 1: Comparison on some real-world datasets. Data Set: cleveland glass horse hungarian <b>iris</b> liver sonar wine No. of Instances 303 214 368 294 150 345 208 178 No. of Features 13 9 22 13 4 6 60 13 No. of Classes 2 6 2 2 3 2 2 3 No. of Missing", "mykey":1090},
 {"datasetID":60, "supportID":"702086AF19E8011C946FB678A7EE218CB41268D2", "rexaID":"cf334aad055b27faaeece97ee1630e146388cd10", "author":"H. Altay G uvenir and Aynur Akkus", "title":"WEIGHTED K NEAREST NEIGHBOR CLASSIFICATION ON FEATURE PROJECTIONS", "venue":"Department of Computer Engineering and Information Science Bilkent University", "year":"", "window":"Data Set: cleveland glass horse hungarian iris <b>liver</b> sonar wine No. of Instances 303 214 368 294 150 345 208 178 No. of Features 13 9 22 13 4 6 60 13 No. of Classes 2 6 2 2 3 2 2 3 No. of Missing values 6 0", "mykey":1091},
 {"datasetID":12, "supportID":"70B9E2342E20FF3E8C2CC22B827068AD29688A51", "rexaID":"d328ae33fb50756832a1c6cd703f7176c361923f", "author":"Peter Sykacek and Stephen J. Roberts", "title":"Adaptive Classification by Variational Kalman Filtering", "venue":"NIPS", "year":"2002", "window":"with sequential variational inference. The probability of the null hypothesis, P null , that both classifiers are equal suggests that only the differences for the <b>Balance</b> <b>scale</b> and the Pima Indian data sets are significant, with either method being better in one case. Since the generalization accuracies of both methods are almost identical, we conclude that if applied to stationary problems, we may", "mykey":1092},
 {"datasetID":34, "supportID":"70B9E2342E20FF3E8C2CC22B827068AD29688A51", "rexaID":"d328ae33fb50756832a1c6cd703f7176c361923f", "author":"Peter Sykacek and Stephen J. Roberts", "title":"Adaptive Classification by Variational Kalman Filtering", "venue":"NIPS", "year":"2002", "window":"Table 2: Generalization accuracies obtained for classification of single trial EEG show that the variational Kalman filter significantly improves the results in three out of four cases. <b>diabetes</b> data set from [17] 3 . Table 1 compares the generalization accuracies (in fractions) obtained with the variational Kalman filter with generalization accuracies obtained with sequential variational inference.", "mykey":1093},
 {"datasetID":121, "supportID":"70B9E2342E20FF3E8C2CC22B827068AD29688A51", "rexaID":"d328ae33fb50756832a1c6cd703f7176c361923f", "author":"Peter Sykacek and Stephen J. Roberts", "title":"Adaptive Classification by Variational Kalman Filtering", "venue":"NIPS", "year":"2002", "window":"size N = 10, because this is a good compromise between fast tracking and high stationary accuracy. We are now ready to compare the algorithm with an equivalent static classifier using several public data sets and classification of single trial <b>EEG</b> which, due to learning effects in humans, is known to be non-stationary. In order to avoid that the model has an influence on the results, we compare the", "mykey":1094},
 {"datasetID":146, "supportID":"70B9E2342E20FF3E8C2CC22B827068AD29688A51", "rexaID":"d328ae33fb50756832a1c6cd703f7176c361923f", "author":"Peter Sykacek and Stephen J. Roberts", "title":"Adaptive Classification by Variational Kalman Filtering", "venue":"NIPS", "year":"2002", "window":"ionosphere data, balance scale weight and distance data and the wine recognition database, all taken from the StatLog database which is available at the UCI repository ([4]). The <b>satellite</b> image data set is used as is provided with 4435 samples in the training and 2000 samples in the test set. Vehicle data are merged such that we have 500 samples in the training and 252 in the test set. The other", "mykey":1095},
 {"datasetID":151, "supportID":"70BF676E462623CDDFD0DA5D6B1E682BAE21738E", "rexaID":"cf33e50e2855982af7c583c8e756a28a5e8f0aeb", "author":"Kagan Tumer and Joydeep Ghosh", "title":"Robust Combining of Disparate Classifiers through Order Statistics", "venue":"CoRR, csLG/9905013", "year":"1999", "window":"trim and spread, and derive the amount of error reduction associated with each. In Section 5 we present the performance of order statistic combiners on a real world <b>sonar</b> problem [15], and several data sets from the Proben1/UCI benchmarks [4, 25]. Section 6 discusses the implications of using linear combinations of order statistics as a strategy for pooling the outputs of individual classifiers. 2", "mykey":1096},
 {"datasetID":27, "supportID":"70BF676E462623CDDFD0DA5D6B1E682BAE21738E", "rexaID":"cf33e50e2855982af7c583c8e756a28a5e8f0aeb", "author":"Kagan Tumer and Joydeep Ghosh", "title":"Robust Combining of Disparate Classifiers through Order Statistics", "venue":"CoRR, csLG/9905013", "year":"1999", "window":"and the corresponding size of the MLP used, are 5 : ffl Card: a 51-dimensional, 2-class data set based on <b>credit approval</b> decision with 690 patterns; an MLP with 10 hidden units; ffl Gene: a 120-dimensional data set with two classes, based on the detection of splice junctions in DNA sequences,", "mykey":1097},
 {"datasetID":146, "supportID":"70BF676E462623CDDFD0DA5D6B1E682BAE21738E", "rexaID":"cf33e50e2855982af7c583c8e756a28a5e8f0aeb", "author":"Kagan Tumer and Joydeep Ghosh", "title":"Robust Combining of Disparate Classifiers through Order Statistics", "venue":"CoRR, csLG/9905013", "year":"1999", "window":"were trained half as along as they would have been, had they been stand-alone classifiers. 5 The number of hidden units was determined experimentally. 15 ffl <b>Satellite</b>  a 36-dimensional, 6-class data set with 6435 examples of feature vectors extracted from satellite imagery; an MLP with 20 hidden units. These three sets were chosen as they have relatively large number of features, somewhat large", "mykey":1098},
 {"datasetID":20, "supportID":"70C855441A8DA0CDC8ED6F4C70B2EFC41CCB1794", "rexaID":"f9c68e626a4939f0fad3605c9022219b7932e464", "author":"Masahiro Terabe and Takashi Washio and Hiroshi Motoda", "title":"The Effect of Subsampling Rate on S 3 Bagging Performance", "venue":"Mitsubishi Research Institute", "year":"", "window":"each member classifier induction. A personal computer having the specification of OS: Linux OS, CPU: PentiumIII 700 MHz, and main memory: 256 M bytes is used in this experiment. For the large size data sets, <b>census</b> income (abbreviated here as census), led(10%) and waveform are selected. Census is selected from UCI KDD Table 2. The specification of data sets for experiment 2. Data set # of Attribute", "mykey":1099},
 {"datasetID":117, "supportID":"70C855441A8DA0CDC8ED6F4C70B2EFC41CCB1794", "rexaID":"f9c68e626a4939f0fad3605c9022219b7932e464", "author":"Masahiro Terabe and Takashi Washio and Hiroshi Motoda", "title":"The Effect of Subsampling Rate on S 3 Bagging Performance", "venue":"Mitsubishi Research Institute", "year":"", "window":"each member classifier induction. A personal computer having the specification of OS: Linux OS, CPU: PentiumIII 700 MHz, and main memory: 256 M bytes is used in this experiment. For the large size data sets, <b>census</b> <b>income</b> (abbreviated here as census), led(10%) and waveform are selected. Census is selected from UCI KDD Table 2. The specification of data sets for experiment 2. Data set # of Attribute", "mykey":1100},
 {"datasetID":101, "supportID":"70C855441A8DA0CDC8ED6F4C70B2EFC41CCB1794", "rexaID":"f9c68e626a4939f0fad3605c9022219b7932e464", "author":"Masahiro Terabe and Takashi Washio and Hiroshi Motoda", "title":"The Effect of Subsampling Rate on S 3 Bagging Performance", "venue":"Mitsubishi Research Institute", "year":"", "window":"training data even when the subsampling rate is set up to 30%. In the case of <b>Tic-tac-toe</b>  the prediction accuracy becomes better when the subsampling rate set large. Tic-tac-toe is an artifical data set which does not include any redundant instances[7]. By this feature of data set, the experimental result of tictac-toe is characteristic. The effect of preventing the deterioration of prediction", "mykey":1101},
 {"datasetID":20, "supportID":"720855E1FEEDD53E2DFAC93970A2A8C55B0F2FB0", "rexaID":"6c5ed6cbf453e8eaad5400424c931537c71a5e3e", "author":"Ke Wang and Shiyu Zhou and Ada Wai-Chee Fu and Jeffrey Xu Yu", "title":"Mining Changes of Classification by Correspondence Tracing", "venue":"SDM", "year":"2003", "window":"German Credit Data from the UCI Repository of Machine Learning Databases [14], and IPUMS <b>Census</b> Data from [1]. These data sets were chosen because no special knowledge is required to understand the addressed applications. To verify if the proposed method finds the changes that are supposed to be found, we need to know such", "mykey":1102},
 {"datasetID":127, "supportID":"720855E1FEEDD53E2DFAC93970A2A8C55B0F2FB0", "rexaID":"6c5ed6cbf453e8eaad5400424c931537c71a5e3e", "author":"Ke Wang and Shiyu Zhou and Ada Wai-Chee Fu and Jeffrey Xu Yu", "title":"Mining Changes of Classification by Correspondence Tracing", "venue":"SDM", "year":"2003", "window":"German Credit Data from the UCI Repository of Machine Learning Databases [14], and <b>IPUMS</b> Census Data from [1]. These data sets were chosen because no special knowledge is required to understand the addressed applications. To verify if the proposed method finds the changes that are supposed to be found, we need to know such", "mykey":1103},
 {"datasetID":58, "supportID":"720855E1FEEDD53E2DFAC93970A2A8C55B0F2FB0", "rexaID":"6c5ed6cbf453e8eaad5400424c931537c71a5e3e", "author":"Ke Wang and Shiyu Zhou and Ada Wai-Chee Fu and Jeffrey Xu Yu", "title":"Mining Changes of Classification by Correspondence Tracing", "venue":"SDM", "year":"2003", "window":"from old ones. Our discussion focuses on forward change mining, but it is equally applicable to backward change mining with the roles of old and new rules exchanged. Example 3.1. We use the  <b>Lenses</b>  data set from the UCI repository [14] to illustrate our approach. There are four attributes, three classes, and 18 examples: Attributes: A 1 : Age: 1, 2, 3 A 2 : Spectacle Prescription: 1, 2 A 3 :", "mykey":1104},
 {"datasetID":144, "supportID":"720855E1FEEDD53E2DFAC93970A2A8C55B0F2FB0", "rexaID":"6c5ed6cbf453e8eaad5400424c931537c71a5e3e", "author":"Ke Wang and Shiyu Zhou and Ada Wai-Chee Fu and Jeffrey Xu Yu", "title":"Mining Changes of Classification by Correspondence Tracing", "venue":"SDM", "year":"2003", "window":"for each small interval to have a separate classification characteristics, either having a different class or having higher accuracy. 4 Experiments We evaluated the proposed method on two real-life data sets, <b>German Credit</b> Data from the UCI Repository of Machine Learning Databases [14], and IPUMS Census Data from [1]. These data sets were chosen because no special knowledge is required to understand", "mykey":1105},
 {"datasetID":151, "supportID":"723A5D87388B1CCAF21D082569AF2DA4C64825C0", "rexaID":"36d1c58be0e3c0c1d57492d050d8a71a04a72858", "author":"Ayhan Demiriz and Kristin P. Bennett", "title":"Chapter 1 OPTIMIZATIONAPPROACHESTOSEMI-SUPERVISED LEARNING", "venue":"Department of Decision Sciences and Engineering Systems & Department of Mathematical Sciences, Rensselaer Polytechnic Institute", "year":"", "window":"as in the previous section. Due to the long computational times for S 3 VM-IQP and transductive SVM-Light, we limit our experiments to only the Heart, Housing, Ionosphere, and <b>Sonar</b> datasets. Linear kernel functions are used for all methods used in this section. The results given in Table 1.3 show that using unlabeled data in the case of datasets Heart and Ionosphere affects", "mykey":1106},
 {"datasetID":45, "supportID":"723A5D87388B1CCAF21D082569AF2DA4C64825C0", "rexaID":"36d1c58be0e3c0c1d57492d050d8a71a04a72858", "author":"Ayhan Demiriz and Kristin P. Bennett", "title":"Chapter 1 OPTIMIZATIONAPPROACHESTOSEMI-SUPERVISED LEARNING", "venue":"Department of Decision Sciences and Engineering Systems & Department of Mathematical Sciences, Rensselaer Polytechnic Institute", "year":"", "window":"an action reduces the overall error. Like S 3 VM-IQP, SVM-Light alternates 16 APPLICATIONS AND ALGORITHMS OF COMPLEMENTARITY Table 1.3 Average Error Results for Transductive and Inductive Methods Data Set SVM-QP SVM-Light S 3 VM-IQP <b>Heart</b> 0.16 0.163 0.1966 Housing 0.1804 0.1608 0.1647 Ionosphere 0.0857 0.1572 0.0943 Sonar 0.1762 0.2524 0.1572 the labels to avoid local minima. The primary difference", "mykey":1107},
 {"datasetID":48, "supportID":"723A5D87388B1CCAF21D082569AF2DA4C64825C0", "rexaID":"36d1c58be0e3c0c1d57492d050d8a71a04a72858", "author":"Ayhan Demiriz and Kristin P. Bennett", "title":"Chapter 1 OPTIMIZATIONAPPROACHESTOSEMI-SUPERVISED LEARNING", "venue":"Department of Decision Sciences and Engineering Systems & Department of Mathematical Sciences, Rensselaer Polytechnic Institute", "year":"", "window":"of the working set is set to 50 points and rest of the data are used as the training set. We use the following formula to pick the penalty parameter: 1 The continuous response variable in <b>Housing</b> dataset was categorized at 21.5 12 APPLICATIONS AND ALGORITHMS OF COMPLEMENTARITY Table 1.2 Average Error Results for Inductive and Transductive SVM Methods Data Set SVM-RLP S 3 V M Local SVM Local S 3 V M", "mykey":1108},
 {"datasetID":52, "supportID":"723A5D87388B1CCAF21D082569AF2DA4C64825C0", "rexaID":"36d1c58be0e3c0c1d57492d050d8a71a04a72858", "author":"Ayhan Demiriz and Kristin P. Bennett", "title":"Chapter 1 OPTIMIZATIONAPPROACHESTOSEMI-SUPERVISED LEARNING", "venue":"Department of Decision Sciences and Engineering Systems & Department of Mathematical Sciences, Rensselaer Polytechnic Institute", "year":"", "window":"as in the previous section. Due to the long computational times for S 3 VM-IQP and transductive SVM-Light, we limit our experiments to only the Heart, Housing, <b>Ionosphere</b>  and Sonar datasets. Linear kernel functions are used for all methods used in this section. The results given in Table 1.3 show that using unlabeled data in the case of datasets Heart and Ionosphere affects", "mykey":1109},
 {"datasetID":151, "supportID":"7262B0206BEF7D4C553F93F1CD17BD33410BD9D3", "rexaID":"b859b5cc14ac44c92ce02385f20204fdf9e32714", "author":"Christos Emmanouilidis and A. Hunter and Dr J. MacIntyre", "title":"A Multiobjective Evolutionary Setting for Feature Selection and a Commonality-Based Crossover Operator", "venue":"Centre for Adaptive Systems, School of Computing, Engineering and Technology University of Sunderland", "year":"", "window":"the individual features distribution during evolution (Figure 5, lower row). Once feature selection is completed, final MLP models are built, based on the training and validation data. The <b>sonar</b> data set and, to a lesser extent the ionoshpere, is so sparse that employing a large number of hidden units seems to lead to overfit. Both MLP and PNN models are tested on the independent evaluation data. 0", "mykey":1110},
 {"datasetID":52, "supportID":"7262B0206BEF7D4C553F93F1CD17BD33410BD9D3", "rexaID":"b859b5cc14ac44c92ce02385f20204fdf9e32714", "author":"Christos Emmanouilidis and A. Hunter and Dr J. MacIntyre", "title":"A Multiobjective Evolutionary Setting for Feature Selection and a Commonality-Based Crossover Operator", "venue":"Centre for Adaptive Systems, School of Computing, Engineering and Technology University of Sunderland", "year":"", "window":"is that it reduces the effect of the noise in fitness evaluation. 6 Experimental Results We demonstrate how our multiobjective evolutionary algorithm feature selection works on two benchmarking data sets of considerable dimensionality. <b>Ionosphere</b>  This data set [21] consists of 351 patterns, with 34 attributes and one output with two classes, good or bad, with good implying evidence of some type of", "mykey":1111},
 {"datasetID":53, "supportID":"73131D2CE26B741D3C4228F6C0D446131FDA6442", "rexaID":"b8a6771d62cfaaffe0071e498102432d8d800573", "author":"Geoffrey Holmes and Bernhard Pfahringer and Richard Kirkby and Eibe Frank and Mark A. Hall", "title":"Multiclass Alternating Decision Trees", "venue":"ECML", "year":"2002", "window":"90.49 89.72 labor 84.67 87.5 + promoters 86.8 87.3 sick-euthyroid 97.71 97.85 + sonar 76.65 74.12 vote 96.5 96.18 +, statistically significant difference Table 3. Wrapping two-class ADTree results dataset 1vs1 1vsRest Random Exhaustive <b>iris</b> 95.13 95.33 95.33 95.33 balance-scale 83.94 85.06 + 85.06 + 85.06 + hypothyroid 99.61 99.63 99.64 99.64 anneal 99.01 98.96 99.05 99.19 + zoo 90.38 93.45 + 95.05 +", "mykey":1112},
 {"datasetID":90, "supportID":"73131D2CE26B741D3C4228F6C0D446131FDA6442", "rexaID":"b8a6771d62cfaaffe0071e498102432d8d800573", "author":"Geoffrey Holmes and Bernhard Pfahringer and Richard Kirkby and Eibe Frank and Mark A. Hall", "title":"Multiclass Alternating Decision Trees", "venue":"ECML", "year":"2002", "window":"(class sizes 3-13) but struggles against two of the later datasets. For <b>soybean</b> 1-against-1 uses a tree of size 1710, and for primary-tumor it uses a tree of size 2310. Perhaps the most remarkable result is for half-letter where 1against-1 using 780 tests has an", "mykey":1113},
 {"datasetID":91, "supportID":"73131D2CE26B741D3C4228F6C0D446131FDA6442", "rexaID":"b8a6771d62cfaaffe0071e498102432d8d800573", "author":"Geoffrey Holmes and Bernhard Pfahringer and Richard Kirkby and Eibe Frank and Mark A. Hall", "title":"Multiclass Alternating Decision Trees", "venue":"ECML", "year":"2002", "window":"(class sizes 3-13) but struggles against two of the later datasets. For <b>soybean</b> 1-against-1 uses a tree of size 1710, and for primary-tumor it uses a tree of size 2310. Perhaps the most remarkable result is for half-letter where 1against-1 using 780 tests has an", "mykey":1114},
 {"datasetID":14, "supportID":"739EA27FE24EC5E11A27EFE980FBC887003F7B6B", "rexaID":"63ebbe51c9c4dea76320f7b6a40f2a59f10cc7c0", "author":"Kristin P. Bennett and Erin J. Bredensteiner", "title":"A Parametric Optimization Method for Machine Learning", "venue":"INFORMS Journal on Computing, 9", "year":"1997", "window":"of the Federal Reserve Bank of Dallas [BS90], has 9 numeric features which range from 0 to 1. The data represent 4311 successful banks and 441 failed banks. Wisconsin <b>Breast</b> <b>Cancer</b> Database This dataset is used to classify a set of 682 patients with breast cancer [WM90]. Each patient is represented by nine integral attributes ranging in value from 1 to 10. The two classes represented are benign and", "mykey":1115},
 {"datasetID":17, "supportID":"739EA27FE24EC5E11A27EFE980FBC887003F7B6B", "rexaID":"63ebbe51c9c4dea76320f7b6a40f2a59f10cc7c0", "author":"Kristin P. Bennett and Erin J. Bredensteiner", "title":"A Parametric Optimization Method for Machine Learning", "venue":"INFORMS Journal on Computing, 9", "year":"1997", "window":"of the Federal Reserve Bank of Dallas [BS90], has 9 numeric features which range from 0 to 1. The data represent 4311 successful banks and 441 failed banks. <b>Wisconsin</b> <b>Breast</b> <b>Cancer</b> Database This dataset is used to classify a set of 682 patients with breast cancer [WM90]. Each patient is represented by nine integral attributes ranging in value from 1 to 10. The two classes represented are benign and", "mykey":1116},
 {"datasetID":15, "supportID":"739EA27FE24EC5E11A27EFE980FBC887003F7B6B", "rexaID":"63ebbe51c9c4dea76320f7b6a40f2a59f10cc7c0", "author":"Kristin P. Bennett and Erin J. Bredensteiner", "title":"A Parametric Optimization Method for Machine Learning", "venue":"INFORMS Journal on Computing, 9", "year":"1997", "window":"of the Federal Reserve Bank of Dallas [BS90], has 9 numeric features which range from 0 to 1. The data represent 4311 successful banks and 441 failed banks. <b>Wisconsin</b> <b>Breast</b> <b>Cancer</b> Database This dataset is used to classify a set of 682 patients with breast cancer [WM90]. Each patient is represented by nine integral attributes ranging in value from 1 to 10. The two classes represented are benign and", "mykey":1117},
 {"datasetID":16, "supportID":"739EA27FE24EC5E11A27EFE980FBC887003F7B6B", "rexaID":"63ebbe51c9c4dea76320f7b6a40f2a59f10cc7c0", "author":"Kristin P. Bennett and Erin J. Bredensteiner", "title":"A Parametric Optimization Method for Machine Learning", "venue":"INFORMS Journal on Computing, 9", "year":"1997", "window":"of the Federal Reserve Bank of Dallas [BS90], has 9 numeric features which range from 0 to 1. The data represent 4311 successful banks and 441 failed banks. <b>Wisconsin</b> <b>Breast</b> <b>Cancer</b> Database This dataset is used to classify a set of 682 patients with breast cancer [WM90]. Each patient is represented by nine integral attributes ranging in value from 1 to 10. The two classes represented are benign and", "mykey":1118},
 {"datasetID":34, "supportID":"739EA27FE24EC5E11A27EFE980FBC887003F7B6B", "rexaID":"63ebbe51c9c4dea76320f7b6a40f2a59f10cc7c0", "author":"Kristin P. Bennett and Erin J. Bredensteiner", "title":"A Parametric Optimization Method for Machine Learning", "venue":"INFORMS Journal on Computing, 9", "year":"1997", "window":"is available via anonymous ftp from the UCI Repository Of Machine Learning Databases [MA92]. Pima Indians <b>Diabetes</b> Database The Pima Diabetes dataset consists of 768 female patients who are at least 21 years of age and are of Pima Indian heritage. The 8 numeric attributes describe physical features of each patient. This dataset is also available", "mykey":1119},
 {"datasetID":52, "supportID":"739EA27FE24EC5E11A27EFE980FBC887003F7B6B", "rexaID":"63ebbe51c9c4dea76320f7b6a40f2a59f10cc7c0", "author":"Kristin P. Bennett and Erin J. Bredensteiner", "title":"A Parametric Optimization Method for Machine Learning", "venue":"INFORMS Journal on Computing, 9", "year":"1997", "window":"were used in the computational experiments. Johns Hopkins University <b>Ionosphere</b> Database The Ionosphere dataset is used to distinguish between good and bad radar returns. A good return is one indicating evidence of some type of structure in the ionosphere. A bad return simply passes through the ionosphere.", "mykey":1120},
 {"datasetID":60, "supportID":"739EA27FE24EC5E11A27EFE980FBC887003F7B6B", "rexaID":"63ebbe51c9c4dea76320f7b6a40f2a59f10cc7c0", "author":"Kristin P. Bennett and Erin J. Bredensteiner", "title":"A Parametric Optimization Method for Machine Learning", "venue":"INFORMS Journal on Computing, 9", "year":"1997", "window":"is also available via anonymous ftp from the UCI Repository Of Machine Learning Databases [MA92]. BUPA <b>liver</b> disorders The BUPA dataset contains 345 single male patients with 6 numeric attributes. Five of these attributes are blood tests which are thought to be relevant to liver disorders. The sixth attribute corresponds to the", "mykey":1121},
 {"datasetID":79, "supportID":"739EA27FE24EC5E11A27EFE980FBC887003F7B6B", "rexaID":"63ebbe51c9c4dea76320f7b6a40f2a59f10cc7c0", "author":"Kristin P. Bennett and Erin J. Bredensteiner", "title":"A Parametric Optimization Method for Machine Learning", "venue":"INFORMS Journal on Computing, 9", "year":"1997", "window":"is available via anonymous ftp from the UCI Repository Of Machine Learning Databases [MA92]. <b>Pima</b> <b>Indians</b> <b>Diabetes</b> Database The Pima Diabetes dataset consists of 768 female patients who are at least 21 years of age and are of Pima Indian heritage. The 8 numeric attributes describe physical features of each patient. This dataset is also available", "mykey":1122},
 {"datasetID":14, "supportID":"73AA4BA03F087EBAE0F88AC0AF397F54CA5C6120", "rexaID":"c2525357aa81ca73fb410a5013d47e3c6931342b", "author":"Rudy Setiono and Huan Liu", "title":"NeuroLinear: From neural networks to oblique decision rules", "venue":"Neurocomputing, 17", "year":"1997", "window":"A. Detailed analysis 1: The University of Wisconsin <b>Breast</b> <b>Cancer</b> Dataset. This data set has been used as the test data for several studies on pattern classification methods using linear programming techniques [1, 13] and statistical techniques [23]. Each pattern is", "mykey":1123},
 {"datasetID":17, "supportID":"73AA4BA03F087EBAE0F88AC0AF397F54CA5C6120", "rexaID":"c2525357aa81ca73fb410a5013d47e3c6931342b", "author":"Rudy Setiono and Huan Liu", "title":"NeuroLinear: From neural networks to oblique decision rules", "venue":"Neurocomputing, 17", "year":"1997", "window":"A. Detailed analysis 1: The University of <b>Wisconsin</b> <b>Breast</b> <b>Cancer</b> Dataset. This data set has been used as the test data for several studies on pattern classification methods using linear programming techniques [1, 13] and statistical techniques [23]. Each pattern is", "mykey":1124},
 {"datasetID":15, "supportID":"73AA4BA03F087EBAE0F88AC0AF397F54CA5C6120", "rexaID":"c2525357aa81ca73fb410a5013d47e3c6931342b", "author":"Rudy Setiono and Huan Liu", "title":"NeuroLinear: From neural networks to oblique decision rules", "venue":"Neurocomputing, 17", "year":"1997", "window":"A. Detailed analysis 1: The University of <b>Wisconsin</b> <b>Breast</b> <b>Cancer</b> Dataset. This data set has been used as the test data for several studies on pattern classification methods using linear programming techniques [1, 13] and statistical techniques [23]. Each pattern is", "mykey":1125},
 {"datasetID":16, "supportID":"73AA4BA03F087EBAE0F88AC0AF397F54CA5C6120", "rexaID":"c2525357aa81ca73fb410a5013d47e3c6931342b", "author":"Rudy Setiono and Huan Liu", "title":"NeuroLinear: From neural networks to oblique decision rules", "venue":"Neurocomputing, 17", "year":"1997", "window":"A. Detailed analysis 1: The University of <b>Wisconsin</b> <b>Breast</b> <b>Cancer</b> Dataset. This data set has been used as the test data for several studies on pattern classification methods using linear programming techniques [1, 13] and statistical techniques [23]. Each pattern is", "mykey":1126},
 {"datasetID":45, "supportID":"73AA4BA03F087EBAE0F88AC0AF397F54CA5C6120", "rexaID":"c2525357aa81ca73fb410a5013d47e3c6931342b", "author":"Rudy Setiono and Huan Liu", "title":"NeuroLinear: From neural networks to oblique decision rules", "venue":"Neurocomputing, 17", "year":"1997", "window":"only 2 attributes and achieve higher accuracy rate on the testing data. The separator generated from the pruned network is depicted in Fig. 5. B. Detailed analysis 2: Cleveland <b>Heart</b> Disease Dataset. The dataset consists of 303 patterns. We discarded patterns with missing attribute values and used only the remaining 297 patterns. The patterns were divided randomly into training and testing set.", "mykey":1127},
 {"datasetID":143, "supportID":"73AA4BA03F087EBAE0F88AC0AF397F54CA5C6120", "rexaID":"c2525357aa81ca73fb410a5013d47e3c6931342b", "author":"Rudy Setiono and Huan Liu", "title":"NeuroLinear: From neural networks to oblique decision rules", "venue":"Neurocomputing, 17", "year":"1997", "window":"Each neural network was given a set of initial weights randomly generated in the interval [Gamma 1; 1]. For all networks, the following 17 Table 6 Accuracy rates (%) of C4.5rules and Neurolinear. Dataset C4.5rules NeuroLinear P-value <b>Australian Credit</b> Approval 84.22 (2.93) 83.64 (5.74) 0.60 Boston Housing Data 83.81 (5.90) 80.60 (9.12) 0.28 Cleveland Heart Disease 75.45 (7.17) 78.15 (6.86) 0.24", "mykey":1128},
 {"datasetID":89, "supportID":"73B3C26D28FDF5E38DE5BE59E07A9BF12BFDD4C1", "rexaID":"377a3c79c5b7a108aa16ed38407c81d035a0740d", "author":"Nir Friedman and Daphne Koller", "title":"Being Bayesian about Network Structure", "venue":"UAI", "year":"2000", "window":"Edges Figure 2: Comparison of posterior probabilities using true posterior over orderings ( # -axis) versus ordering-MCMC ( -axis). The figures show Markov features and Edge features in the <b>Flare</b> dataset with 100 samples. ordering obtained by flipping # ! and # - . Now, consider the terms in Eq. (6); those terms corresponding to nodes ### in the ordering # that precede # ! or succeed # - do not", "mykey":1129},
 {"datasetID":73, "supportID":"744E899403D1C695F542B25AAEAC399142C05A44", "rexaID":"62c14fc762e0504f5b978f5eb120399e06c64cbe", "author":"Nicholas Howe and Claire Cardie", "title":"Examining Locally Varying Weights for Nearest Neighbor Algorithms", "venue":"ICCBR", "year":"1997", "window":"has 307 designated training instances, and 376 designated test instances. from the benchmark with continuous features were discarded. Also, NetTalk was not used because of its similarity to the NLP datasets described below, and <b>Mushroom</b> was found to be too easy. We also include an artificial task constructed specifically to exhibit feature importance that varies locally at the class level, and several", "mykey":1130},
 {"datasetID":2, "supportID":"7471D780EE25D4010170C5BC233C38D5AFF3AF95", "rexaID":"55a7f45a6b42ceb9097c7260032694bcbd8a2fd2", "author":"Gary M. Weiss and Haym Hirsh", "title":"A Quantitative Study of Small Disjuncts: Experiments and Results", "venue":"Department of Computer Science Rutgers University", "year":"2000", "window":"were compared as the training set size was varied. Because disjuncts of a specific size for most concepts cover very few examples, statistically valid comparison were possible for only 4 of the 30 datasets (Coding, Move, <b>Adult</b>  and Market2); with the other datasets the number of examples covered by disjuncts of a given size is too small. The results for the Coding dataset are shown in Figure 8.", "mykey":1131},
 {"datasetID":105, "supportID":"7471D780EE25D4010170C5BC233C38D5AFF3AF95", "rexaID":"55a7f45a6b42ceb9097c7260032694bcbd8a2fd2", "author":"Gary M. Weiss and Haym Hirsh", "title":"A Quantitative Study of Small Disjuncts: Experiments and Results", "venue":"Department of Computer Science Rutgers University", "year":"2000", "window":"validation and the results are therefore based on averages of the test set calculated over 10 runs. Unless specified otherwise, all results are based on C4.5 without pruning. 3 AN EXAMPLE: THE <b>VOTE DATASET</b> In order to illustrate the problem with small disjuncts and introduce a way of measuring this problem, we examine the concept learned by C4.5 from the Vote dataset. Figure 1 shows how the correctly", "mykey":1132},
 {"datasetID":46, "supportID":"7471D780EE25D4010170C5BC233C38D5AFF3AF95", "rexaID":"55a7f45a6b42ceb9097c7260032694bcbd8a2fd2", "author":"Gary M. Weiss and Haym Hirsh", "title":"A Quantitative Study of Small Disjuncts: Experiments and Results", "venue":"Department of Computer Science Rutgers University", "year":"2000", "window":"in the high-ER/medium-EC group, which starts with the <b>Hepatitis</b> dataset, show more improvement, but have more room for improvement due to their higher error rate. The datasets in the high-ER/low-EC group, which start with the Coding dataset, show a net increase in error", "mykey":1133},
 {"datasetID":56, "supportID":"7471D780EE25D4010170C5BC233C38D5AFF3AF95", "rexaID":"55a7f45a6b42ceb9097c7260032694bcbd8a2fd2", "author":"Gary M. Weiss and Haym Hirsh", "title":"A Quantitative Study of Small Disjuncts: Experiments and Results", "venue":"Department of Computer Science Rutgers University", "year":"2000", "window":"noise. What is much more apparent, however, is that many concepts with low EC values are extremely tolerant of noise, whereas none of the concepts with high EC's are. For example, two of the low-EC datasets, blackjack and <b>labor</b>  are so tolerant of noise that when 50% random class noise is added to the training set (i.e., the class value is replaced with a randomly selected valid value 50% of the", "mykey":1134},
 {"datasetID":30, "supportID":"748D27C9695676C8E9CD3907D31B65BB2F94BD1E", "rexaID":"2a4fe2f7469eef9d4cc985d6caaa0afe249facf3", "author":"Soumya Ray and David Page", "title":"Generalized Skewing for Functions with Continuous and Nominal Attributes", "venue":"Department of Computer Sciences and Department of Biostatistics and Medical Informatics, University of Wis", "year":"", "window":"1500 1000 500 Accuracy (%) Training Sample Size Gain/Generalized Skewing Gain Figure 11. Hard Nominal Targets, 100-v examples that a subfunction in the target (or the target itself) may be hard. The datasets we use are: <b>Contraceptive</b> Method Choice (CMC), German Credit (German), Cleveland Heart Disease (Heart), voting-records (Voting), pima-indians-diabetes (Diabetes), breast-cancerwisconsin (BCW),", "mykey":1135},
 {"datasetID":124, "supportID":"74974B74ACFD99ADF616A51144E3BD92BD032F66", "rexaID":"9182a5f903de880381f975a70365a46fe603bdf3", "author":"Xiaofeng He and Partha Niyogi", "title":"Locality Preserving Projections", "venue":"NIPS", "year":"2003", "window":"techniques such as Laplacian Eigenmap. However, LPP is computationally much more tractable. 4.3. Manifold of <b>face</b> Images In this subsection, we applied the LPP to images of faces. The <b>face image</b> data set used here is the same as that used in [5]. This dataset contains 1965 face images taken from sequential frames of a small video. The size of each image is 20 \u00d7 28, with 256 gray levels Figure 3: A", "mykey":1136},
 {"datasetID":72, "supportID":"74974B74ACFD99ADF616A51144E3BD92BD032F66", "rexaID":"9182a5f903de880381f975a70365a46fe603bdf3", "author":"Xiaofeng He and Partha Niyogi", "title":"Locality Preserving Projections", "venue":"NIPS", "year":"2003", "window":"obtained by PCA, while they are well separated in the principal direction obtained by LPP. 4.2. 2-D Data Visulization An experiment was conducted with the <b>Multiple Features</b> Database [3]. This dataset consists of features of handwritten numbers (`0'-`9') extracted from a collection of Dutch utility maps. 200 patterns per class (for a total of 2,000 patterns) have been digitized in binary images.", "mykey":1137},
 {"datasetID":12, "supportID":"74B340C88F0DB5AF29EB2EE70B523CCC72D11692", "rexaID":"d328ae33fb50756832a1c6cd703f7176c361923f", "author":"Peter Sykacek and Stephen J. Roberts", "title":"Adaptive Classification by Variational Kalman Filtering", "venue":"NIPS", "year":"2002", "window":"with sequential variational inference. The probability of the null hypothesis, P null , that both classifiers are equal suggests that only the differences for the <b>Balance</b> <b>scale</b> and the Pima Indian data sets are significant, with either method being better in one case. Since the generalization accuracies of both methods are almost identical, we conclude that if applied to 2 Vehicle data was donated to", "mykey":1138},
 {"datasetID":34, "supportID":"74B340C88F0DB5AF29EB2EE70B523CCC72D11692", "rexaID":"d328ae33fb50756832a1c6cd703f7176c361923f", "author":"Peter Sykacek and Stephen J. Roberts", "title":"Adaptive Classification by Variational Kalman Filtering", "venue":"NIPS", "year":"2002", "window":"which were both used as training and independent test sets respectively. We also use the pima <b>diabetes</b> data set from [16] 3 . Table 1 compares the generalization accuracies (in fractions) obtained with the variational Kalman filter with generalization accuracies obtained with sequential variational inference.", "mykey":1139},
 {"datasetID":121, "supportID":"74B340C88F0DB5AF29EB2EE70B523CCC72D11692", "rexaID":"d328ae33fb50756832a1c6cd703f7176c361923f", "author":"Peter Sykacek and Stephen J. Roberts", "title":"Adaptive Classification by Variational Kalman Filtering", "venue":"NIPS", "year":"2002", "window":"size N = 10, because this is a good compromise between fast tracking and high stationary accuracy. We are now ready to compare the algorithm with an equivalent static classifier using several public data sets and classification of single trial <b>EEG</b> which, due to learning effects in humans, is known to be non-stationary. In order to avoid that the model has an influence on 1 This data set can be obtained", "mykey":1140},
 {"datasetID":79, "supportID":"74B340C88F0DB5AF29EB2EE70B523CCC72D11692", "rexaID":"d328ae33fb50756832a1c6cd703f7176c361923f", "author":"Peter Sykacek and Stephen J. Roberts", "title":"Adaptive Classification by Variational Kalman Filtering", "venue":"NIPS", "year":"2002", "window":"which were both used as training and independent test sets respectively. We also use the <b>pima</b> <b>diabetes</b> data set from [16] 3 . Table 1 compares the generalization accuracies (in fractions) obtained with the variational Kalman filter with generalization accuracies obtained with sequential variational inference.", "mykey":1141},
 {"datasetID":146, "supportID":"74B340C88F0DB5AF29EB2EE70B523CCC72D11692", "rexaID":"d328ae33fb50756832a1c6cd703f7176c361923f", "author":"Peter Sykacek and Stephen J. Roberts", "title":"Adaptive Classification by Variational Kalman Filtering", "venue":"NIPS", "year":"2002", "window":"ionosphere data, balance scale weight and distance data and the wine recognition database, all taken from the StatLog database which is available at the UCI repository ([4]). The <b>satellite</b> image data set is used as is provided with 4435 samples in the training and 2000 samples in the test set. Vehicle data are merged such that we have 500 samples in the training and 252 in the test set. The other", "mykey":1142},
 {"datasetID":52, "supportID":"74C01DA43B215794D83BBB05569B744721F382EA", "rexaID":"66e4adde5454a4c13960687a350b7bd2b12859db", "author":"Hyunsoo Kim and Se Hyun Park", "title":"Data Reduction in Support Vector Machines by a Kernelized Ionic Interaction Model", "venue":"SDM", "year":"2004", "window":"The number of 50% data points are approximately similar to the number of support vectors that was found after training with the full number of data points, except the <b>Ionosphere</b> and Mushroom data sets. For the Mushroom data set, the percentage of the points predicted as support vectors is mush less than 50% though the desired selection percentage was 50% in the IoI algorithm. By selecting", "mykey":1143},
 {"datasetID":73, "supportID":"74C01DA43B215794D83BBB05569B744721F382EA", "rexaID":"66e4adde5454a4c13960687a350b7bd2b12859db", "author":"Hyunsoo Kim and Se Hyun Park", "title":"Data Reduction in Support Vector Machines by a Kernelized Ionic Interaction Model", "venue":"SDM", "year":"2004", "window":"time to obtain ten-fold cross validation accuracy. (c) The number of support vectors. All results were drawn against the percentage of selected data points on the UC Irvine 8124 \u00d7 22 <b>Mushroom</b> data set. The solid, dashed, and dashdot lines were the results of IoI for the selection threshold values \u00a9 t of 0.1, 0.01, and 0.0, respectively. The circle points were the results of KIB2. The star,", "mykey":1144},
 {"datasetID":53, "supportID":"75401E5B7D456A3ABD6751DC88FF8E37F72C1F82", "rexaID":"9acaa3dfebccac9d79abb08f7eced65482be1631", "author":"Inderjit S. Dhillon and Dharmendra S. Modha and W. Scott Spangler", "title":"Class visualization of high-dimensional data with applications", "venue":"Department of Computer Sciences, University of Texas", "year":"2002", "window":"sketch the outline of the paper. Section 2 introduces class-preserving projections and class-eigenvector plots, and contains several illustrations of the <b>Iris</b> plant and ISOLET speech recognition data sets (Blake et al., 1998). Class-similarity graphs and class tours are discussed in Sections 3 and 4. We illustrate the value of the above visualization tools in Section 5, where we present a detailed", "mykey":1145},
 {"datasetID":54, "supportID":"75401E5B7D456A3ABD6751DC88FF8E37F72C1F82", "rexaID":"9acaa3dfebccac9d79abb08f7eced65482be1631", "author":"Inderjit S. Dhillon and Dharmendra S. Modha and W. Scott Spangler", "title":"Class visualization of high-dimensional data with applications", "venue":"Department of Computer Sciences, University of Texas", "year":"2002", "window":"we will use later in this paper. In the isolated letter speech recognition  <b>ISOLET</b>  speech recognition data set, k = 26, n = 7797 and d = 617, while in the PENDIGITS example k = 10, n = 10992 and d = 16. The non-linear SOM and MDS projections are even more expensive to compute than PCA. Our superior eNciency", "mykey":1146},
 {"datasetID":31, "supportID":"7540A934F393C20AA2002A1BCD3990EF6BC18CCD", "rexaID":"097c7a7a0199cda61be7fac0050755303cec3b46", "author":"Arto Klami and Samuel Kaski and Ty n ohjaaja and Janne Sinkkonen", "title":"HELSINKI UNIVERSITY OF TECHNOLOGY Department of Engineering Physics and Mathematics Arto Klami Regularized Discriminative Clustering", "venue":"Regularized Discriminative Clustering", "year":"", "window":"Dimensions Classes Samples TIMIT phoneme data 12 41 99983 Letter Recognition Data 16 26 20000 Forest <b>CoverType</b> data 10 7 100000 6.2.1 Data sets The data sets used for testing were chosen to be suitable for the task of discriminative clustering. The feature vectors are continuous and real-valued, and a categorical variable to be used as", "mykey":1147},
 {"datasetID":59, "supportID":"7540A934F393C20AA2002A1BCD3990EF6BC18CCD", "rexaID":"097c7a7a0199cda61be7fac0050755303cec3b46", "author":"Arto Klami and Samuel Kaski and Ty n ohjaaja and Janne Sinkkonen", "title":"HELSINKI UNIVERSITY OF TECHNOLOGY Department of Engineering Physics and Mathematics Arto Klami Regularized Discriminative Clustering", "venue":"Regularized Discriminative Clustering", "year":"", "window":"runs were selected. The model was then re-trained with the whole training data to produce the final clustering. The value of M was five in <b>Letter recognition</b>  and three for the other two data sets. I had to keep the number of candidate values, from which the best value was selected, quite small because of computational reasons. Preliminary runs were used to select a set of reasonable values", "mykey":1148},
 {"datasetID":20, "supportID":"756CC4560801E921C597466D70A05F4A43F0F258", "rexaID":"ca1c2b19089d0ca0d4069b04c35d6f6c4312db79", "author":"Jinyan Li and Guozhu Dong and Kotagiri Ramamohanarao and Limsoon Wong", "title":"DeEPs: A New Instance-based Discovery and Classification System", "venue":"Proceedings of the Fourth European Conference on Principles and Practice of Knowledge Discovery in Databases", "year":"2001", "window":"of DeEPs over the number of training instances. Decision speed. DeEPs is a fast classifier. Indeed, decision time per instance is typically a small fraction of a second. For only 5 of the 40 data sets  <b>census</b> inc, letter, satimage, pendigits, and waveform) decision time per instance exceeds 1 second. All these five data sets have a very large volume of training instances, high dimensions, or", "mykey":1149},
 {"datasetID":23, "supportID":"756CC4560801E921C597466D70A05F4A43F0F258", "rexaID":"ca1c2b19089d0ca0d4069b04c35d6f6c4312db79", "author":"Jinyan Li and Guozhu Dong and Kotagiri Ramamohanarao and Limsoon Wong", "title":"DeEPs: A New Instance-based Discovery and Classification System", "venue":"Proceedings of the Fourth European Conference on Principles and Practice of Knowledge Discovery in Databases", "year":"2001", "window":"(This will be explained in Section 8.3). Note that for data sets such as <b>chess</b>  flare, splice, mushroom, voting, soybean-l, t-t-t, and zoo which do not contain any continuous attributes, DeEPs does not require an ff. The accuracies of k-nearest neighbor and C5.0", "mykey":1150},
 {"datasetID":21, "supportID":"756CC4560801E921C597466D70A05F4A43F0F258", "rexaID":"ca1c2b19089d0ca0d4069b04c35d6f6c4312db79", "author":"Jinyan Li and Guozhu Dong and Kotagiri Ramamohanarao and Limsoon Wong", "title":"DeEPs: A New Instance-based Discovery and Classification System", "venue":"Proceedings of the Fourth European Conference on Principles and Practice of Knowledge Discovery in Databases", "year":"2001", "window":"(This will be explained in Section 8.3). Note that for data sets such as <b>chess</b>  flare, splice, mushroom, voting, soybean-l, t-t-t, and zoo which do not contain any continuous attributes, DeEPs does not require an ff. The accuracies of k-nearest neighbor and C5.0", "mykey":1151},
 {"datasetID":22, "supportID":"756CC4560801E921C597466D70A05F4A43F0F258", "rexaID":"ca1c2b19089d0ca0d4069b04c35d6f6c4312db79", "author":"Jinyan Li and Guozhu Dong and Kotagiri Ramamohanarao and Limsoon Wong", "title":"DeEPs: A New Instance-based Discovery and Classification System", "venue":"Proceedings of the Fourth European Conference on Principles and Practice of Knowledge Discovery in Databases", "year":"2001", "window":"(This will be explained in Section 8.3). Note that for data sets such as <b>chess</b>  flare, splice, mushroom, voting, soybean-l, t-t-t, and zoo which do not contain any continuous attributes, DeEPs does not require an ff. The accuracies of k-nearest neighbor and C5.0", "mykey":1152},
 {"datasetID":53, "supportID":"756CC4560801E921C597466D70A05F4A43F0F258", "rexaID":"ca1c2b19089d0ca0d4069b04c35d6f6c4312db79", "author":"Jinyan Li and Guozhu Dong and Kotagiri Ramamohanarao and Limsoon Wong", "title":"DeEPs: A New Instance-based Discovery and Classification System", "venue":"Proceedings of the Fourth European Conference on Principles and Practice of Knowledge Discovery in Databases", "year":"2001", "window":"we highlight some interesting points. 1. DeEPs versus k-NN. ffl Both DeEPs and k-NN perform equally accurately on soybean-small (100%) and on <b>iris</b> (96%). ffl DeEPs wins on 26 data sets; k-NN wins on 11. It can be seen that the accuracy of DeEPs is generally better than that of k-NN. ffl The speed of DeEPs is about 1.5 times slower than that of k-NN. The main reason is that DeEPs", "mykey":1153},
 {"datasetID":69, "supportID":"756CC4560801E921C597466D70A05F4A43F0F258", "rexaID":"ca1c2b19089d0ca0d4069b04c35d6f6c4312db79", "author":"Jinyan Li and Guozhu Dong and Kotagiri Ramamohanarao and Limsoon Wong", "title":"DeEPs: A New Instance-based Discovery and Classification System", "venue":"Proceedings of the Fourth European Conference on Principles and Practice of Knowledge Discovery in Databases", "year":"2001", "window":"(This will be explained in Section 8.3). Note that for data sets such as chess, flare, <b>splice</b>  mushroom, voting, soybean-l, t-t-t, and zoo which do not contain any continuous attributes, DeEPs does not require an ff. The accuracies of k-nearest neighbor and C5.0", "mykey":1154},
 {"datasetID":73, "supportID":"756CC4560801E921C597466D70A05F4A43F0F258", "rexaID":"ca1c2b19089d0ca0d4069b04c35d6f6c4312db79", "author":"Jinyan Li and Guozhu Dong and Kotagiri Ramamohanarao and Limsoon Wong", "title":"DeEPs: A New Instance-based Discovery and Classification System", "venue":"Proceedings of the Fourth European Conference on Principles and Practice of Knowledge Discovery in Databases", "year":"2001", "window":"This concise pattern representation technique greatly saves the cost of DeEPs. ffl Boundary EPs (typically small in number, e.g., 81 in the <b>mushroom</b> data set) are considered in DeEPs' classification. The selected EPs are ``good'' representatives of all EPs occurring in the considered instance. This selection also significantly reduces the number of EPs", "mykey":1155},
 {"datasetID":89, "supportID":"756CC4560801E921C597466D70A05F4A43F0F258", "rexaID":"ca1c2b19089d0ca0d4069b04c35d6f6c4312db79", "author":"Jinyan Li and Guozhu Dong and Kotagiri Ramamohanarao and Limsoon Wong", "title":"DeEPs: A New Instance-based Discovery and Classification System", "venue":"Proceedings of the Fourth European Conference on Principles and Practice of Knowledge Discovery in Databases", "year":"2001", "window":"(This will be explained in Section 8.3). Note that for data sets such as chess, <b>flare</b>  splice, mushroom, voting, soybean-l, t-t-t, and zoo which do not contain any continuous attributes, DeEPs does not require an ff. The accuracies of k-nearest neighbor and C5.0", "mykey":1156},
 {"datasetID":148, "supportID":"7578CF4B107440ED75C3EC0A81FC0A10DD4E05C1", "rexaID":"af8d7cc40c6d9dd6d1b261e68d612c371055b313", "author":"Jeffrey P. Bradford and Clayton Kunz and Ron Kohavi and Clifford Brunk and Carla Brodley", "title":"Appears in ECML-98 as a research note Pruning Decision Trees with Misclassification Costs", "venue":"School of Electrical Engineering", "year":"", "window":"classification based on census bureau data), breast cancer diagnosis, chess, crx (credit), german (credit), pima diabetes, road (dirt), satellite images, <b>shuttle</b>  and vehicle. In choosing the datasets, we decided on the following desiderata: 1. Datasets should be two-class to make the evaluation easier. This desideratum was hard to satisfy and we resorted to converting several multi-class", "mykey":1157},
 {"datasetID":149, "supportID":"7578CF4B107440ED75C3EC0A81FC0A10DD4E05C1", "rexaID":"af8d7cc40c6d9dd6d1b261e68d612c371055b313", "author":"Jeffrey P. Bradford and Clayton Kunz and Ron Kohavi and Clifford Brunk and Carla Brodley", "title":"Appears in ECML-98 as a research note Pruning Decision Trees with Misclassification Costs", "venue":"School of Electrical Engineering", "year":"", "window":"classification based on census bureau data), breast cancer diagnosis, chess, crx (credit), german (credit), pima diabetes, road (dirt), satellite images, shuttle, and <b>vehicle</b>  In choosing the datasets, we decided on the following desiderata: 1. Datasets should be two-class to make the evaluation easier. This desideratum was hard to satisfy and we resorted to converting several multi-class", "mykey":1158},
 {"datasetID":52, "supportID":"759CF1725B8CEFF0F3E33D8548D21690FFEEDCCB", "rexaID":"a3fca86cb1f854c7fd7975e66aae2a5ba3710f8a", "author":"Mikhail Bilenko and Sugato Basu and Raymond J. Mooney", "title":"Integrating constraints and metric learning in semi-supervised clustering", "venue":"ICML", "year":"2004", "window":"from the UCI repository: Iris, Wine, and <b>Ionosphere</b> (Blake & Merz, 1998); the Protein dataset used by Xing et al. (2003) and Bar-Hillel et al. (2003), and randomly sampled subsets from the Digits and Letters handwritten character recognition datasets, also from the UCI repository. For Digits", "mykey":1159},
 {"datasetID":53, "supportID":"759CF1725B8CEFF0F3E33D8548D21690FFEEDCCB", "rexaID":"a3fca86cb1f854c7fd7975e66aae2a5ba3710f8a", "author":"Mikhail Bilenko and Sugato Basu and Raymond J. Mooney", "title":"Integrating constraints and metric learning in semi-supervised clustering", "venue":"ICML", "year":"2004", "window":"Experiments were conducted on three datasets from the UCI repository: <b>Iris</b>  Wine, and Ionosphere (Blake & Merz, 1998); the Protein dataset used by Xing et al. (2003) and Bar-Hillel et al. (2003), and randomly sampled subsets from the Digits", "mykey":1160},
 {"datasetID":81, "supportID":"759CF1725B8CEFF0F3E33D8548D21690FFEEDCCB", "rexaID":"a3fca86cb1f854c7fd7975e66aae2a5ba3710f8a", "author":"Mikhail Bilenko and Sugato Basu and Raymond J. Mooney", "title":"Integrating constraints and metric learning in semi-supervised clustering", "venue":"ICML", "year":"2004", "window":"used by Xing et al. (2003) and Bar-Hillel et al. (2003), and randomly sampled subsets from the <b>Digits</b> and Letters <b>handwritten</b> character <b>recognition</b> datasets, also from the UCI repository. For Digits and Letters, we chose two sets of three classes: {I, J, L} from Letters and {3, 8, 9} from Digits, sampling 10% of the data points from the original", "mykey":1161},
 {"datasetID":154, "supportID":"759CF1725B8CEFF0F3E33D8548D21690FFEEDCCB", "rexaID":"a3fca86cb1f854c7fd7975e66aae2a5ba3710f8a", "author":"Mikhail Bilenko and Sugato Basu and Raymond J. Mooney", "title":"Integrating constraints and metric learning in semi-supervised clustering", "venue":"ICML", "year":"2004", "window":"from the UCI repository: Iris, Wine, and Ionosphere (Blake & Merz, 1998); the <b>Protein</b> dataset used by Xing et al. (2003) and Bar-Hillel et al. (2003), and randomly sampled subsets from the Digits and Letters handwritten character recognition datasets, also from the UCI repository. For Digits", "mykey":1162},
 {"datasetID":109, "supportID":"759CF1725B8CEFF0F3E33D8548D21690FFEEDCCB", "rexaID":"a3fca86cb1f854c7fd7975e66aae2a5ba3710f8a", "author":"Mikhail Bilenko and Sugato Basu and Raymond J. Mooney", "title":"Integrating constraints and metric learning in semi-supervised clustering", "venue":"ICML", "year":"2004", "window":"Experiments were conducted on three datasets from the UCI repository: Iris, <b>Wine</b>  and Ionosphere (Blake & Merz, 1998); the Protein dataset used by Xing et al. (2003) and Bar-Hillel et al. (2003), and randomly sampled subsets from the Digits", "mykey":1163},
 {"datasetID":53, "supportID":"75AA3FB892CE86F4986C7C2877D66AE4FFB20D6B", "rexaID":"9acaa3dfebccac9d79abb08f7eced65482be1631", "author":"Inderjit S. Dhillon and Dharmendra S. Modha and W. Scott Spangler", "title":"Class visualization of high-dimensional data with applications", "venue":"Department of Computer Sciences, University of Texas", "year":"2002", "window":"to 2 preserve the inter-class structure present in the original multidimensional space R d . The underlying theory, supported by several illustrations on the <b>Iris</b> plant and ISOLET speech recognition data sets [9], is presented in Section 2. Class-similarity graphs to enhance each individual two-dimensional projection of the data. Such graphs provide a skeleton of the data and serve as guides through the", "mykey":1164},
 {"datasetID":54, "supportID":"75AA3FB892CE86F4986C7C2877D66AE4FFB20D6B", "rexaID":"9acaa3dfebccac9d79abb08f7eced65482be1631", "author":"Inderjit S. Dhillon and Dharmendra S. Modha and W. Scott Spangler", "title":"Class visualization of high-dimensional data with applications", "venue":"Department of Computer Sciences, University of Texas", "year":"2002", "window":"to 2 preserve the inter-class structure present in the original multidimensional space R d . The underlying theory, supported by several illustrations on the Iris plant and <b>ISOLET</b> speech recognition data sets [9], is presented in Section 2. Class-similarity graphs to enhance each individual two-dimensional projection of the data. Such graphs provide a skeleton of the data and serve as guides through the", "mykey":1165},
 {"datasetID":14, "supportID":"75B4E8C56241B2FBBFC10B775DCF933A24ABB878", "rexaID":"3ecc983417b61977b8f998e8a843948dad8fa21c", "author":"Andr\u00e1s Antos and Bal\u00e1zs K\u00e9gl and Tam\u00e1s Linder and G\u00e1bor Lugosi", "title":"Data-dependent margin-based generalization bounds for classification", "venue":"Journal of Machine Learning Research, 3", "year":"2002", "window":"attributes were binary coded in a 1-out-of-n fashion. Data points with missing attributes were removed. Each attribute was normalized to have zero mean and 1= p d standard deviation. The four data sets were the Wisconsin <b>breast</b> <b>cancer</b> (n = 683, d = 9), the ionosphere (n = 351, d = 34), the Japanese credit screening (n = 653, d = 42), and the tic-tac-toe endgame (n = 958, d = 27) database. 84", "mykey":1166},
 {"datasetID":17, "supportID":"75B4E8C56241B2FBBFC10B775DCF933A24ABB878", "rexaID":"3ecc983417b61977b8f998e8a843948dad8fa21c", "author":"Andr\u00e1s Antos and Bal\u00e1zs K\u00e9gl and Tam\u00e1s Linder and G\u00e1bor Lugosi", "title":"Data-dependent margin-based generalization bounds for classification", "venue":"Journal of Machine Learning Research, 3", "year":"2002", "window":"attributes were binary coded in a 1-out-of-n fashion. Data points with missing attributes were removed. Each attribute was normalized to have zero mean and 1= p d standard deviation. The four data sets were the <b>Wisconsin</b> <b>breast</b> <b>cancer</b> (n = 683, d = 9), the ionosphere (n = 351, d = 34), the Japanese credit screening (n = 653, d = 42), and the tic-tac-toe endgame (n = 958, d = 27) database. 84", "mykey":1167},
 {"datasetID":15, "supportID":"75B4E8C56241B2FBBFC10B775DCF933A24ABB878", "rexaID":"3ecc983417b61977b8f998e8a843948dad8fa21c", "author":"Andr\u00e1s Antos and Bal\u00e1zs K\u00e9gl and Tam\u00e1s Linder and G\u00e1bor Lugosi", "title":"Data-dependent margin-based generalization bounds for classification", "venue":"Journal of Machine Learning Research, 3", "year":"2002", "window":"attributes were binary coded in a 1-out-of-n fashion. Data points with missing attributes were removed. Each attribute was normalized to have zero mean and 1= p d standard deviation. The four data sets were the <b>Wisconsin</b> <b>breast</b> <b>cancer</b> (n = 683, d = 9), the ionosphere (n = 351, d = 34), the Japanese credit screening (n = 653, d = 42), and the tic-tac-toe endgame (n = 958, d = 27) database. 84", "mykey":1168},
 {"datasetID":16, "supportID":"75B4E8C56241B2FBBFC10B775DCF933A24ABB878", "rexaID":"3ecc983417b61977b8f998e8a843948dad8fa21c", "author":"Andr\u00e1s Antos and Bal\u00e1zs K\u00e9gl and Tam\u00e1s Linder and G\u00e1bor Lugosi", "title":"Data-dependent margin-based generalization bounds for classification", "venue":"Journal of Machine Learning Research, 3", "year":"2002", "window":"attributes were binary coded in a 1-out-of-n fashion. Data points with missing attributes were removed. Each attribute was normalized to have zero mean and 1= p d standard deviation. The four data sets were the <b>Wisconsin</b> <b>breast</b> <b>cancer</b> (n = 683, d = 9), the ionosphere (n = 351, d = 34), the Japanese credit screening (n = 653, d = 42), and the tic-tac-toe endgame (n = 958, d = 27) database. 84", "mykey":1169},
 {"datasetID":14, "supportID":"75EBD298A57E2BBB3DE34EA2E6D6997E61E6944F", "rexaID":"349644828ad3c8da8e1a15eee5aec0d704db34a1", "author":"Kamal Ali and Michael J. Pazzani", "title":"Error Reduction through Learning Multiple Descriptions", "venue":"Machine Learning, 24", "year":"1996", "window":"search using Likelihood Combination is able to statistically significantly (95% confidence) reduce or maintain error on all domains except the (Ljubljana) <b>breast</b> <b>cancer</b> domain. On that breast cancer data set few learning methods have been able to get an accuracy significantly higher than that obtained by guessing the most frequent class suggesting it lacks the attributes relevant for discriminating the", "mykey":1170},
 {"datasetID":45, "supportID":"75EBD298A57E2BBB3DE34EA2E6D6997E61E6944F", "rexaID":"349644828ad3c8da8e1a15eee5aec0d704db34a1", "author":"Kamal Ali and Michael J. Pazzani", "title":"Error Reduction through Learning Multiple Descriptions", "venue":"Machine Learning, 24", "year":"1996", "window":"to test learned models on noise-free examples (including noisy variants of the KRK and LED domains) but for the natural domains we tested on possibly noisy examples. The large variant of the Soybean data set was used and the 5-class variant of the <b>Heart</b> data set was used. 5.1. Does using multiple rule sets lead to lower error? In this section we present results of an experiment designed to answer the", "mykey":1171},
 {"datasetID":57, "supportID":"75EBD298A57E2BBB3DE34EA2E6D6997E61E6944F", "rexaID":"349644828ad3c8da8e1a15eee5aec0d704db34a1", "author":"Kamal Ali and Michael J. Pazzani", "title":"Error Reduction through Learning Multiple Descriptions", "venue":"Machine Learning, 24", "year":"1996", "window":"also experienced significant reduction with the error being halved (for DNA this represented an increase in accuracy from 67.9% to 86.8%!). The error reduction is least for the noisy KRK and <b>LED data</b> sets and for the presumably noisy medical diagnosis data sets. Eighty percent of the data sets which scored unimpressive error ratios (above 0.8) were noisy data sets. This finding is further explored", "mykey":1172},
 {"datasetID":67, "supportID":"75EBD298A57E2BBB3DE34EA2E6D6997E61E6944F", "rexaID":"349644828ad3c8da8e1a15eee5aec0d704db34a1", "author":"Kamal Ali and Michael J. Pazzani", "title":"Error Reduction through Learning Multiple Descriptions", "venue":"Machine Learning, 24", "year":"1996", "window":"(representing an increase in accuracy from 93.3% to 98.9%!) and by large (around 3 or 4) factors for LED and Tic-tactoe. The <b>molecular biology</b> data sets also experienced significant reduction with the error being halved (for DNA this represented an increase in accuracy from 67.9% to 86.8%!). The error reduction is least for the noisy KRK and LED", "mykey":1173},
 {"datasetID":68, "supportID":"75EBD298A57E2BBB3DE34EA2E6D6997E61E6944F", "rexaID":"349644828ad3c8da8e1a15eee5aec0d704db34a1", "author":"Kamal Ali and Michael J. Pazzani", "title":"Error Reduction through Learning Multiple Descriptions", "venue":"Machine Learning, 24", "year":"1996", "window":"(representing an increase in accuracy from 93.3% to 98.9%!) and by large (around 3 or 4) factors for LED and Tic-tactoe. The <b>molecular biology</b> data sets also experienced significant reduction with the error being halved (for DNA this represented an increase in accuracy from 67.9% to 86.8%!). The error reduction is least for the noisy KRK and LED", "mykey":1174},
 {"datasetID":69, "supportID":"75EBD298A57E2BBB3DE34EA2E6D6997E61E6944F", "rexaID":"349644828ad3c8da8e1a15eee5aec0d704db34a1", "author":"Kamal Ali and Michael J. Pazzani", "title":"Error Reduction through Learning Multiple Descriptions", "venue":"Machine Learning, 24", "year":"1996", "window":"(representing an increase in accuracy from 93.3% to 98.9%!) and by large (around 3 or 4) factors for LED and Tic-tactoe. The <b>molecular biology</b> data sets also experienced significant reduction with the error being halved (for DNA this represented an increase in accuracy from 67.9% to 86.8%!). The error reduction is least for the noisy KRK and LED", "mykey":1175},
 {"datasetID":73, "supportID":"75EBD298A57E2BBB3DE34EA2E6D6997E61E6944F", "rexaID":"349644828ad3c8da8e1a15eee5aec0d704db34a1", "author":"Kamal Ali and Michael J. Pazzani", "title":"Error Reduction through Learning Multiple Descriptions", "venue":"Machine Learning, 24", "year":"1996", "window":"class labels. 6. The r 2 's between Er and OE e without the significant error reduction restriction are: 50.7% (Uniform), 33.7% (Bayes), 6.8% (Distribution) and 31.6% (Likelihood). The <b>Mushroom</b> data set causes a problem for the Distribution combination strategy because both the ensemble error and multiple models error are close to 0 so the ratio cannot be reliably estimated. The r 2 for", "mykey":1176},
 {"datasetID":83, "supportID":"75EBD298A57E2BBB3DE34EA2E6D6997E61E6944F", "rexaID":"349644828ad3c8da8e1a15eee5aec0d704db34a1", "author":"Kamal Ali and Michael J. Pazzani", "title":"Error Reduction through Learning Multiple Descriptions", "venue":"Machine Learning, 24", "year":"1996", "window":"uniformly weighted voting between eleven, stochastically-generated descriptions is only one seventh that of the error obtained by using a single description. On the other hand, on the <b>primary</b> <b>tumor</b> data set, the error obtained by the identical multiple models procedure is the same as that obtained by using a single description. Much of the work on learning multiple models is motivated by Bayesian", "mykey":1177},
 {"datasetID":90, "supportID":"75EBD298A57E2BBB3DE34EA2E6D6997E61E6944F", "rexaID":"349644828ad3c8da8e1a15eee5aec0d704db34a1", "author":"Kamal Ali and Michael J. Pazzani", "title":"Error Reduction through Learning Multiple Descriptions", "venue":"Machine Learning, 24", "year":"1996", "window":"to test learned models on noise-free examples (including noisy variants of the KRK and LED domains) but for the natural domains we tested on possibly noisy examples. The large variant of the <b>Soybean</b> data set was used and the 5-class variant of the Heart data set was used. 5.1. Does using multiple rule sets lead to lower error? In this section we present results of an experiment designed to answer the", "mykey":1178},
 {"datasetID":91, "supportID":"75EBD298A57E2BBB3DE34EA2E6D6997E61E6944F", "rexaID":"349644828ad3c8da8e1a15eee5aec0d704db34a1", "author":"Kamal Ali and Michael J. Pazzani", "title":"Error Reduction through Learning Multiple Descriptions", "venue":"Machine Learning, 24", "year":"1996", "window":"to test learned models on noise-free examples (including noisy variants of the KRK and LED domains) but for the natural domains we tested on possibly noisy examples. The large variant of the <b>Soybean</b> data set was used and the 5-class variant of the Heart data set was used. 5.1. Does using multiple rule sets lead to lower error? In this section we present results of an experiment designed to answer the", "mykey":1179},
 {"datasetID":109, "supportID":"75EBD298A57E2BBB3DE34EA2E6D6997E61E6944F", "rexaID":"349644828ad3c8da8e1a15eee5aec0d704db34a1", "author":"Kamal Ali and Michael J. Pazzani", "title":"Error Reduction through Learning Multiple Descriptions", "venue":"Machine Learning, 24", "year":"1996", "window":"the efficacy of using multiple models. It is important to analyze these experimental data because the amount of error reduction obtained by using multiple models varies a great deal. On the <b>wine</b> data set, for example, the error obtained by uniformly weighted voting between eleven, stochastically-generated descriptions is only one seventh that of the error obtained by using a single description. On", "mykey":1180},
 {"datasetID":54, "supportID":"75FD06D392304EF1E1448A068315BB993DE16052", "rexaID":"80b881ba423bc64d836bde049f1e5def83c1d5f8", "author":"Erin L. Allwein and Robert E. Schapire and Yoram Singer", "title":"Reducing Multiclass to Binary: A Unifying Approach for Margin Classifiers", "venue":"ICML", "year":"2000", "window":"according to the number of classes): Dermatology, Satimage, Glass, Segmentation, E-coli, Pendigits, Yeast, Vowel, Soybean-large, Thyroid, Audiology, <b>Isolet</b>  Letterrecognition. The properties of the datasets are summarized in Table 1. In the SVM experiments, we skipped Audiology, Isolet, Letter-recognition, Segmentation, and Thyroid, because these datasets were either too big to be handled by our", "mykey":1181},
 {"datasetID":102, "supportID":"75FD06D392304EF1E1448A068315BB993DE16052", "rexaID":"80b881ba423bc64d836bde049f1e5def83c1d5f8", "author":"Erin L. Allwein and Robert E. Schapire and Yoram Singer", "title":"Reducing Multiclass to Binary: A Unifying Approach for Margin Classifiers", "venue":"ICML", "year":"2000", "window":"are summarized in Table 1. In the SVM experiments, we skipped Audiology, Isolet, Letter-recognition, Segmentation, and <b>Thyroid</b>  because these datasets were either too big to be handled by our current implementation of SVM or contained many nominal features with missing values which are problematic for SVM. All datasets have at least six classes.", "mykey":1182},
 {"datasetID":110, "supportID":"75FD06D392304EF1E1448A068315BB993DE16052", "rexaID":"80b881ba423bc64d836bde049f1e5def83c1d5f8", "author":"Erin L. Allwein and Robert E. Schapire and Yoram Singer", "title":"Reducing Multiclass to Binary: A Unifying Approach for Margin Classifiers", "venue":"ICML", "year":"2000", "window":"the bars in the top row of Figure 6 correspond to large positive values.) One-against-all often results in error rates that are much higher than the error rates of other codes. For instance, for the dataset <b>Yeast</b>  the one-against-all code has an error rate of 72% while the error rate of all the other codes is no more than 47:1% (random sparse) and can be as low as 39:6% 137 ALLWEIN, SCHAPIRE & SINGER", "mykey":1183},
 {"datasetID":53, "supportID":"76ABCB836282ECA0ABDC3CDDF1144D553FD85F79", "rexaID":"3ce34105eca9577e44de969f9ef94ca3cdcad3d9", "author":"Foster J. Provost and Tom Fawcett and Ron Kohavi", "title":"The Case against Accuracy Estimation for Comparing Induction Algorithms", "venue":"ICML", "year":"1998", "window":"we often do not know whether the existing distribution is the natural distribution, or whether it has been stratified. The <b>iris</b> data set has exactly 50 instances of each class. The splice junction data set (DNA) has 50% donor sites, 25% acceptor sites and 25% nonboundary sites, even though the natural class distribution is very", "mykey":1184},
 {"datasetID":67, "supportID":"76ABCB836282ECA0ABDC3CDDF1144D553FD85F79", "rexaID":"3ce34105eca9577e44de969f9ef94ca3cdcad3d9", "author":"Foster J. Provost and Tom Fawcett and Ron Kohavi", "title":"The Case against Accuracy Estimation for Comparing Induction Algorithms", "venue":"ICML", "year":"1998", "window":"has exactly 50 instances of each class. The splice junction data set  <b>DNA</b>  has 50% donor sites, 25% acceptor sites and 25% nonboundary sites, even though the natural class distribution is very skewed: no more than 6% of DNA actually codes for human genes (Saitta and", "mykey":1185},
 {"datasetID":69, "supportID":"76ABCB836282ECA0ABDC3CDDF1144D553FD85F79", "rexaID":"3ce34105eca9577e44de969f9ef94ca3cdcad3d9", "author":"Foster J. Provost and Tom Fawcett and Ron Kohavi", "title":"The Case against Accuracy Estimation for Comparing Induction Algorithms", "venue":"ICML", "year":"1998", "window":"has exactly 50 instances of each class. The <b>splice</b> junction data set (DNA) has 50% donor sites, 25% acceptor sites and 25% nonboundary sites, even though the natural class distribution is very skewed: no more than 6% of DNA actually codes for human genes (Saitta and", "mykey":1186},
 {"datasetID":150, "supportID":"775C98323B7E30BAD65E90A22A39385E851E3CC8", "rexaID":"21b1e6cdcd730f2bf7438109509c9abdf01767d7", "author":"Rayid Ghani", "title":"KDD Project Report Using Error-Correcting Codes for Efficient Text Classification with a Large Number of Categories", "venue":"Center for Automated Learning and Discovery, School of Computer Science, Carnegie Mellon University", "year":"", "window":"After removing tokens that occur only once, the <b>corpus</b> contains 1.2 million words with a vocabulary size of 29964. 4.1.2 HOOVERS DATASET This corpus of company web pages was assembled using the Hoovers Online Web resource (www.hoovers.com) by obtaining a list of the names and home-page URLs for 4285 companies on the web and using a", "mykey":1187},
 {"datasetID":53, "supportID":"77703A2783F64DFCEB638AA9EEBD9C9C501BB835", "rexaID":"3ce34105eca9577e44de969f9ef94ca3cdcad3d9", "author":"Foster J. Provost and Tom Fawcett and Ron Kohavi", "title":"The Case against Accuracy Estimation for Comparing Induction Algorithms", "venue":"ICML", "year":"1998", "window":"we often do not know whether the existing distribution is the natural distribution, or whether it has been stratified. The <b>iris</b> data set has exactly 50 instances of each class. The splice junction data set (DNA) has 50% donor sites, 25% acceptor sites and 25% nonboundary sites, even though the natural class distribution is very", "mykey":1188},
 {"datasetID":67, "supportID":"77703A2783F64DFCEB638AA9EEBD9C9C501BB835", "rexaID":"3ce34105eca9577e44de969f9ef94ca3cdcad3d9", "author":"Foster J. Provost and Tom Fawcett and Ron Kohavi", "title":"The Case against Accuracy Estimation for Comparing Induction Algorithms", "venue":"ICML", "year":"1998", "window":"has exactly 50 instances of each class. The splice junction data set  <b>DNA</b>  has 50% donor sites, 25% acceptor sites and 25% nonboundary sites, even though the natural class distribution is very skewed: no more than 6% of DNA actually codes for human genes (Saitta and", "mykey":1189},
 {"datasetID":69, "supportID":"77703A2783F64DFCEB638AA9EEBD9C9C501BB835", "rexaID":"3ce34105eca9577e44de969f9ef94ca3cdcad3d9", "author":"Foster J. Provost and Tom Fawcett and Ron Kohavi", "title":"The Case against Accuracy Estimation for Comparing Induction Algorithms", "venue":"ICML", "year":"1998", "window":"has exactly 50 instances of each class. The <b>splice</b> junction data set (DNA) has 50% donor sites, 25% acceptor sites and 25% nonboundary sites, even though the natural class distribution is very skewed: no more than 6% of DNA actually codes for human genes (Saitta and", "mykey":1190},
 {"datasetID":34, "supportID":"77F4EEBD7DDDDCD93BFE35DB2F803C992B9D48E1", "rexaID":"9a0d2c026da6cf5175639b0c375bf7ab56106e1e", "author":"Wojciech Kwedlo and Marek Kretowski", "title":"Discovery of Decision Rules from Databases: An Evolutionary Approach", "venue":"PKDD", "year":"1998", "window":"experimental results merely entitle us to conclude, that the classification accuracy of the current version of EDRL is comparable to that of C4.5. A real improvement was observed only for <b>diabetes</b> dataset. However we believe that the performance of our system can be further improved. Several directions of future research exist. Currently all the continuous-valued features are discretized globally [4]", "mykey":1191},
 {"datasetID":53, "supportID":"77F4EEBD7DDDDCD93BFE35DB2F803C992B9D48E1", "rexaID":"9a0d2c026da6cf5175639b0c375bf7ab56106e1e", "author":"Wojciech Kwedlo and Marek Kretowski", "title":"Discovery of Decision Rules from Databases: An Evolutionary Approach", "venue":"PKDD", "year":"1998", "window":"Features Examples Classes australian 15 (9 nominal) 690 2 diabetes 8 768 2 german 20 (13 nominal) 1000 2 glass 9 214 7 hepatitis 19 (13 nominal) 155 2 <b>iris</b> 4 150 3 Table 1. Description of the datasets used in the experiments. Dataset Majority C4.5 EDRL fi australian 55.5 85:3 Sigma 0:2 86:1 Sigma 0:4 0.05 diabetes 65.1 74:6 Sigma 0:3 77:9 Sigma 0:3 0.2 german 70.0 71:6 Sigma 0:3 70:1 Sigma", "mykey":1192},
 {"datasetID":1, "supportID":"78701A94F998B1CD737B20E3B53571611B48FF7F", "rexaID":"5b9d84cbe5327bcd60f720ca1424f815b6b08753", "author":"Ilhan Uysal and H. Altay Guvenir", "title":"Instance-Based Regression by Partitioning Feature Projections", "venue":"Applied", "year":"2004", "window":"are shown in Table 2. In order to save space, they are coded with two letters (e.g., AB for <b>Abalone</b> . 72 Uysal and G\u00a8 uvenir Table 2. Datasets. Features Missing Target Dataset Code Instances (C + N)values feature Abalone [6] AB 4177 8 (7 + 1) None Rings Airport [21] AI 135 4 (4 + 0) None Tons of mail Auto [6] AU 398 7 (6 + 1) 6 Gas", "mykey":1193},
 {"datasetID":1, "supportID":"78E71A6A649DD6778BB1C0923F626D6573CC2B06", "rexaID":"5cf41a736e03821c90e0d2ef143e7abf5abe02c9", "author":"Bernhard Pfahringer and Hilan Bensusan", "title":"Tell me who can learn you and I can tell you who you are: Landmarking Various Learning Algorithms", "venue":"Austrian Research Institute for Artificial Intelligence", "year":"", "window":"had from 5 to 12 attributes and were classified by simple parity, dnf and cnf rules as well as at random. The 18 UCI datasets were: mushrooms, <b>abalone</b>  crx, sat, acetylation, titanic, waveform, yeast, car, chess(kingrook-vs-king), led7, led24, tic-tac-toe, monk1, monk2, monk3, satimage, quisclas. The performance of every", "mykey":1194},
 {"datasetID":14, "supportID":"792877D83346DEB890A6455133B724F8F8790D50", "rexaID":"f820b10d6723504d343c9741f7333fbc3393fd05", "author":"Lorne Mason and Jonathan Baxter and Peter L. Bartlett and Marcus Frean", "title":"Boosting Algorithms as Gradient Descent", "venue":"NIPS", "year":"1999", "window":"0% noise - AdaBoost 0% noise - DOOM II 15% noise - AdaBoost 15% noise - DOOM II Figure 2: Margin distributions for AdaBoost and DOOM II with 0% and 15% label noise for the <b>breast</b> <b>cancer</b> and splice data sets. Given that AdaBoost does suffer from overfitting and is guaranteed to minimize an exponential cost function of the margins, this cost function certainly does not relate to test error. How does the", "mykey":1195},
 {"datasetID":151, "supportID":"792877D83346DEB890A6455133B724F8F8790D50", "rexaID":"f820b10d6723504d343c9741f7333fbc3393fd05", "author":"Lorne Mason and Jonathan Baxter and Peter L. Bartlett and Marcus Frean", "title":"Boosting Algorithms as Gradient Descent", "venue":"NIPS", "year":"1999", "window":"These results show that DOOM II generally outperforms AdaBoost and that the improvement is more pronounced in the presence of label noise. -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 2.5 3 3.5 Error advantage (%) Data set <b>sonar</b> cleve ionosphere vote1 credit breast-cancer pima-indians hypo1 splice 0% noise 5% noise 15% noise Figure 1: Summary of test error advantage (with standard error bars) of DOOM II over AdaBoost", "mykey":1196},
 {"datasetID":52, "supportID":"792877D83346DEB890A6455133B724F8F8790D50", "rexaID":"f820b10d6723504d343c9741f7333fbc3393fd05", "author":"Lorne Mason and Jonathan Baxter and Peter L. Bartlett and Marcus Frean", "title":"Boosting Algorithms as Gradient Descent", "venue":"NIPS", "year":"1999", "window":"These results show that DOOM II generally outperforms AdaBoost and that the improvement is more pronounced in the presence of label noise. -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 2.5 3 3.5 Error advantage (%) Data set sonar cleve <b>ionosphere</b> vote1 credit breast-cancer pima-indians hypo1 splice 0% noise 5% noise 15% noise Figure 1: Summary of test error advantage (with standard error bars) of DOOM II over AdaBoost", "mykey":1197},
 {"datasetID":56, "supportID":"792877D83346DEB890A6455133B724F8F8790D50", "rexaID":"f820b10d6723504d343c9741f7333fbc3393fd05", "author":"Lorne Mason and Jonathan Baxter and Peter L. Bartlett and Marcus Frean", "title":"Boosting Algorithms as Gradient Descent", "venue":"NIPS", "year":"1999", "window":"the minimum of AdaBoost's test error and the minimum of the normalized sigmoid cost very nearly coincide. In the <b>labor</b> data set AdaBoost's test error converges and overfitting does not occur. For this data set both the normalized sigmoid cost and the exponential cost converge. In the vote1 data set AdaBoost initially", "mykey":1198},
 {"datasetID":69, "supportID":"792877D83346DEB890A6455133B724F8F8790D50", "rexaID":"f820b10d6723504d343c9741f7333fbc3393fd05", "author":"Lorne Mason and Jonathan Baxter and Peter L. Bartlett and Marcus Frean", "title":"Boosting Algorithms as Gradient Descent", "venue":"NIPS", "year":"1999", "window":"0% noise - AdaBoost 0% noise - DOOM II 15% noise - AdaBoost 15% noise - DOOM II Figure 2: Margin distributions for AdaBoost and DOOM II with 0% and 15% label noise for the breast-cancer and <b>splice</b> data sets. Given that AdaBoost does suffer from overfitting and is guaranteed to minimize an exponential cost function of the margins, this cost function certainly does not relate to test error. How does the", "mykey":1199},
 {"datasetID":1, "supportID":"7989EBED2CD1267D5CD9F9025ECA48984E305D8C", "rexaID":"72d5bd2d46a0dc05c20340f71ac9c9e0eb6e0abf", "author":"Nir Friedman and Iftach Nachman", "title":"Gaussian Process Networks", "venue":"UAI", "year":"2000", "window":"contains 506 samples with 14 attributes. 300 samples were used as a test set. # <b>Abalone</b> data set - a data set of physical measurements of abalones. The data set contains 4177 samples with 9 attributes. 300 samples were used as a test set. # Glass identification data set - a data describing the", "mykey":1200},
 {"datasetID":42, "supportID":"7989EBED2CD1267D5CD9F9025ECA48984E305D8C", "rexaID":"72d5bd2d46a0dc05c20340f71ac9c9e0eb6e0abf", "author":"Nir Friedman and Iftach Nachman", "title":"Gaussian Process Networks", "venue":"UAI", "year":"2000", "window":"contains 4177 samples with 9 attributes. 300 samples were used as a test set. # <b>Glass</b> identification data set - a data describing the material concentrations in glasses, with a class attribute denoting the type of the glass. The data set contains 214 samples with 10 attributes. 64 samples were used as a", "mykey":1201},
 {"datasetID":48, "supportID":"7989EBED2CD1267D5CD9F9025ECA48984E305D8C", "rexaID":"72d5bd2d46a0dc05c20340f71ac9c9e0eb6e0abf", "author":"Nir Friedman and Iftach Nachman", "title":"Gaussian Process Networks", "venue":"UAI", "year":"2000", "window":"from the UCI machine learning repository [2]. These data sets are: # Boston <b>housing</b> data set - a data set describing different aspects of neighborhoods in the Boston area, and the median price of houses in those neighborhoods. The data set contains 506", "mykey":1202},
 {"datasetID":42, "supportID":"79ECC51BADF9847CEF4854DA6C71881A5D9BE0DB", "rexaID":"abe4b819b1c9858440502f14d418706d2446c387", "author":"Giorgio Valentini and Francesco Masulli", "title":"NEURObjects: an object-oriented library for neural network development", "venue":"Neurocomputing, 48", "year":"2002", "window":"validation [35]. The folds can be prepared using the program dofold in a simple way: dofold <b>glass</b> data -nf 10 -na 9 -name glass This command build the folds for a ten fold cross validation test. The data set is glass from the UCI Machine Learning repository [29]. Ten folds from the data file glass.data (named glass.fi.train and glass.fi.test varying i from 1 to 10) are extracted (the option -na", "mykey":1203},
 {"datasetID":69, "supportID":"7A30125A4EFBF73DE5CAFB65326D7B8AB83EF20A", "rexaID":"f877b9d13dc80d6292789ea42108706658a378ab", "author":"Susanne Hoche and Stefan Wrobel", "title":"Scaling Boosting by Margin-Based Inclusionof Features and Relations", "venue":"ECML", "year":"2002", "window":"in four domains, and without a significant deterioration of predictive accuracy in the one domain where only few features are present. C 2 RIB shows a poor performance on the <b <b>splice</b> junction</b> dataset, most likely due to the great number of features. However, C 2 RIB D clearly outperforms C 2 RIB both in accuracy and learning time. Table 3. Accuracy, standard deviation and learning time in", "mykey":1204},
 {"datasetID":90, "supportID":"7A38063526938C1D4C443BF08AE1A64A4284BE41", "rexaID":"9533c70dee11979650e1d68d349556d5624b7ba7", "author":"I\u00f1aki Inza and Pedro Larraaga and Basilio Sierra", "title":"Bayesian networks for feature subset selection", "venue":"Department of Computer Sciences and Artificial Intelligence", "year":"", "window":"on their stop-generation numbers to extract differences between the behaviour of GA-o and GA-u. Thus, observing Table 6, the one-point crossover is better suited for Horsecolic and <b>Soybean</b> large datasets than the uniform crossover; otherwise, we see the opposite behaviour in Ionosphere and Anneal. By the use of FSS-EBNA, we also avoid this tunning among different crossover operators for each", "mykey":1205},
 {"datasetID":91, "supportID":"7A38063526938C1D4C443BF08AE1A64A4284BE41", "rexaID":"9533c70dee11979650e1d68d349556d5624b7ba7", "author":"I\u00f1aki Inza and Pedro Larraaga and Basilio Sierra", "title":"Bayesian networks for feature subset selection", "venue":"Department of Computer Sciences and Artificial Intelligence", "year":"", "window":"on their stop-generation numbers to extract differences between the behaviour of GA-o and GA-u. Thus, observing Table 6, the one-point crossover is better suited for Horsecolic and <b>Soybean</b> large datasets than the uniform crossover; otherwise, we see the opposite behaviour in Ionosphere and Anneal. By the use of FSS-EBNA, we also avoid this tunning among different crossover operators for each", "mykey":1206},
 {"datasetID":2, "supportID":"7A61FFC825663B262D914786DD062735BDE0B1DC", "rexaID":"679efdb662f70091c573c74a40aadab38fc0d868", "author":"S. Sathiya Keerthi and Chih-Jen Lin", "title":"Asymptotic Behaviors of Support Vector Machines with Gaussian Kernel", "venue":"Neural Computation, 15", "year":"2003", "window":"into (training set, test set) partitions. We consider only the first of those realizations. In addition, the problem <b>adult</b>  from the UCI adult\" data set (Blake and Merz 1998) and the problem web, both as compiled by Platt (1998), are also included. For each of these two datasets also, there are several realizations. For our study here, we only", "mykey":1207},
 {"datasetID":34, "supportID":"7AAF395A9DF639ABEC5B6F65FDF2A9F2FBB9E774", "rexaID":"ded3146242d6b322f31afbf57a8afde22e4ec8e1", "author":"Ilya Blayvas and Ron Kimmel", "title":"Multiresolution Approximation for Classification", "venue":"CS Dept. Technion", "year":"2002", "window":"\u00b7 D). 3 Experimental Results The proposed method was implemented in VC++ 6.0 and run on `IBM PC 300 PL' with 600MHZ Pentium III processor and 256MB RAM. It was tested on the Pima Indians <b>Diabetes</b> dataset [10], and a large artificial dataset generated with the DatGen program [11]. The results were compared to the Smooth SVM [12] and Sparse Grids [3]. 3.1 Pima Indians The Pima Indians Diabetes", "mykey":1208},
 {"datasetID":79, "supportID":"7AAF395A9DF639ABEC5B6F65FDF2A9F2FBB9E774", "rexaID":"ded3146242d6b322f31afbf57a8afde22e4ec8e1", "author":"Ilya Blayvas and Ron Kimmel", "title":"Multiresolution Approximation for Classification", "venue":"CS Dept. Technion", "year":"2002", "window":"\u00b7 D). 3 Experimental Results The proposed method was implemented in VC++ 6.0 and run on `IBM PC 300 PL' with 600MHZ Pentium III processor and 256MB RAM. It was tested on the <b>Pima</b> <b>Indians</b> <b>Diabetes</b> dataset [10], and a large artificial dataset generated with the DatGen program [11]. The results were compared to the Smooth SVM [12] and Sparse Grids [3]. 3.1 Pima Indians The Pima Indians Diabetes", "mykey":1209},
 {"datasetID":53, "supportID":"7AE7ADE5939A6BA59141B09FB62C959DE2CBC600", "rexaID":"9a9186e8fcb4a3d9a2f01bb4033499e4a59833e9", "author":"Neil Davey and Rod Adams and Mary J. George", "title":"The Architecture and Performance of a Stochastic Competitive Evolutionary Neural Tree Network", "venue":"Appl. Intell, 12", "year":"2000", "window":"5 and 6 are illustrated in Figures 2 and 3. The <b>IRIS</b> data set is included to provide a benchmark performance. Set 1 2-D single source Gaussian cluster, zero mean and unit variance. Simple cluster, base line test. Set 2 20-D single source Gaussian cluster, zero", "mykey":1210},
 {"datasetID":111, "supportID":"7AE7ADE5939A6BA59141B09FB62C959DE2CBC600", "rexaID":"9a9186e8fcb4a3d9a2f01bb4033499e4a59833e9", "author":"Neil Davey and Rod Adams and Mary J. George", "title":"The Architecture and Performance of a Stochastic Competitive Evolutionary Neural Tree Network", "venue":"Appl. Intell, 12", "year":"2000", "window":"0 7 Aquatic 1 8 Predator 1 9 Toothed 0 10 Backbone 1 11 Breathes 1 12 Venomous 0 13 Fins 0 14 Legs 4 15 Tail 1 16 Domestic 0 17 Catsize 1 Table 3. The input vector for the platypus instance in <b>Zoo</b> data set. 4.2.2 Representational Results 4.2.2.1 Picture Data Results Figure 8 shows a typical result formed by using SCENT on the 153 vector data set. Each final and non final node is represented in the", "mykey":1211},
 {"datasetID":107, "supportID":"7B11AB79F580C58076646136467AAFCBC998E454", "rexaID":"ce55aa21eaf8e5fc2b67e9ef0b031b5c32af6b41", "author":"Eibe Frank and Mark Hall and Bernhard Pfahringer", "title":"Locally Weighted Naive Bayes", "venue":"UAI", "year":"2003", "window":"3772 6.0 23 6 2 sonar 208 0.0 60 0 2 soybean 683 9.8 0 35 19 splice 3190 0.0 0 61 3 vehicle 846 0.0 18 0 4 vote 435 5.6 0 16 2 vowel 990 0.0 10 3 11 <b>waveform</b> 5000 0.0 40 0 3 zoo 101 0.0 1 15 7 19 datasets for k = 5 and k = 10 respectively. When distance weighting is used with k-nearest neighbours, our method is significantly more accurate on 13 and 17 datasets for k = 5 and k = 10 respectively.", "mykey":1212},
 {"datasetID":108, "supportID":"7B11AB79F580C58076646136467AAFCBC998E454", "rexaID":"ce55aa21eaf8e5fc2b67e9ef0b031b5c32af6b41", "author":"Eibe Frank and Mark Hall and Bernhard Pfahringer", "title":"Locally Weighted Naive Bayes", "venue":"UAI", "year":"2003", "window":"3772 6.0 23 6 2 sonar 208 0.0 60 0 2 soybean 683 9.8 0 35 19 splice 3190 0.0 0 61 3 vehicle 846 0.0 18 0 4 vote 435 5.6 0 16 2 vowel 990 0.0 10 3 11 <b>waveform</b> 5000 0.0 40 0 3 zoo 101 0.0 1 15 7 19 datasets for k = 5 and k = 10 respectively. When distance weighting is used with k-nearest neighbours, our method is significantly more accurate on 13 and 17 datasets for k = 5 and k = 10 respectively.", "mykey":1213},
 {"datasetID":111, "supportID":"7B11AB79F580C58076646136467AAFCBC998E454", "rexaID":"ce55aa21eaf8e5fc2b67e9ef0b031b5c32af6b41", "author":"Eibe Frank and Mark Hall and Bernhard Pfahringer", "title":"Locally Weighted Naive Bayes", "venue":"UAI", "year":"2003", "window":"3772 6.0 23 6 2 sonar 208 0.0 60 0 2 soybean 683 9.8 0 35 19 splice 3190 0.0 0 61 3 vehicle 846 0.0 18 0 4 vote 435 5.6 0 16 2 vowel 990 0.0 10 3 11 waveform 5000 0.0 40 0 3 <b>zoo</b> 101 0.0 1 15 7 19 datasets for k = 5 and k = 10 respectively. When distance weighting is used with k-nearest neighbours, our method is significantly more accurate on 13 and 17 datasets for k = 5 and k = 10 respectively.", "mykey":1214},
 {"datasetID":25, "supportID":"7B4103AA3422194A8AF1D91A8A7690A2EF04FE2A", "rexaID":"44c3304f0bae0a9da4798db6e28bda955a80d576", "author":"Matthew Brand", "title":"Pattern discovery via entropy minimization", "venue":"MERL -- A MITSUBISHI ELECTRIC RESEARCH LABORATORY", "year":"1998", "window":"occupant. An HMM conventionally estimated from the same initial conditions is fully connected and thus too bushy to profitably illustrate or interpret. Prediction in <b>Bach</b> <b>chorales</b>  We obtained a dataset of melodic lines from 100 of J.S. Bach's 371 surviving chorales from the UCI repository [Merz and Murphy, 1998], and transposed all into the key of C. We compared entropically and conventionally", "mykey":1215},
 {"datasetID":152, "supportID":"7B4103AA3422194A8AF1D91A8A7690A2EF04FE2A", "rexaID":"44c3304f0bae0a9da4798db6e28bda955a80d576", "author":"Matthew Brand", "title":"Pattern discovery via entropy minimization", "venue":"MERL -- A MITSUBISHI ELECTRIC RESEARCH LABORATORY", "year":"1998", "window":"DA circumvents this local optimum and finds a model which generalizes even better. 9.2 Radial basis function networks (RBFNs) Vowel classification: We obtained the British English <b>vowel recognition</b> dataset from the CMU Neural-Bench Archive. Each example of a vowel is characterized by LPC coefficients from two time-frames. This dataset has been treated several times using perceptrons, neural networks,", "mykey":1216},
 {"datasetID":1, "supportID":"7C07B97F0B402562AF4B25F928517FF50EBDF344", "rexaID":"f7fdf9dbb5f98a218956025550c1f603b3cb24f2", "author":"Alexander G. Gray and Bernd Fischer and Johann Schumann and Wray L. Buntine", "title":"Automatic Derivation of Statistical Algorithms: The EM Family and Beyond", "venue":"NIPS", "year":"2002", "window":"extension of our running example, integrating several features, yields a Gaussian Bayes classifier model # # . # # has been successfully tested on various standard benchmarks [1], e.g., the <b>Abalone</b> dataset. Currently, the number of expected classes has to be given in advance. Mixture models and EM. A wide range of # -Gaussian mixture models can be handled by AUTOBAYES, ranging from the simple 1D ( # #", "mykey":1217},
 {"datasetID":14, "supportID":"7C389E774D4C158FFD6B8D41DB4E905DE7703B79", "rexaID":"b6e169d69cd67763b95698e8961696fec9ca93bf", "author":"Charles Campbell and Nello Cristianini", "title":"Simple Learning Algorithms for Training Support Vector Machines", "venue":"Dept. of Engineering Mathematics", "year":"", "window":"include a sonar classification problem [14], the Wisconsin <b>breast</b> <b>cancer</b> dataset [35] and a database of handwritten digits collected by the US Postal Service [17]. As examples of the improvements with generalisation ability which can be achieved with a soft margin we will also", "mykey":1218},
 {"datasetID":17, "supportID":"7C389E774D4C158FFD6B8D41DB4E905DE7703B79", "rexaID":"b6e169d69cd67763b95698e8961696fec9ca93bf", "author":"Charles Campbell and Nello Cristianini", "title":"Simple Learning Algorithms for Training Support Vector Machines", "venue":"Dept. of Engineering Mathematics", "year":"", "window":"include a sonar classification problem [14], the <b>Wisconsin</b> <b>breast</b> <b>cancer</b> dataset [35] and a database of handwritten digits collected by the US Postal Service [17]. As examples of the improvements with generalisation ability which can be achieved with a soft margin we will also", "mykey":1219},
 {"datasetID":15, "supportID":"7C389E774D4C158FFD6B8D41DB4E905DE7703B79", "rexaID":"b6e169d69cd67763b95698e8961696fec9ca93bf", "author":"Charles Campbell and Nello Cristianini", "title":"Simple Learning Algorithms for Training Support Vector Machines", "venue":"Dept. of Engineering Mathematics", "year":"", "window":"include a sonar classification problem [14], the <b>Wisconsin</b> <b>breast</b> <b>cancer</b> dataset [35] and a database of handwritten digits collected by the US Postal Service [17]. As examples of the improvements with generalisation ability which can be achieved with a soft margin we will also", "mykey":1220},
 {"datasetID":16, "supportID":"7C389E774D4C158FFD6B8D41DB4E905DE7703B79", "rexaID":"b6e169d69cd67763b95698e8961696fec9ca93bf", "author":"Charles Campbell and Nello Cristianini", "title":"Simple Learning Algorithms for Training Support Vector Machines", "venue":"Dept. of Engineering Mathematics", "year":"", "window":"include a sonar classification problem [14], the <b>Wisconsin</b> <b>breast</b> <b>cancer</b> dataset [35] and a database of handwritten digits collected by the US Postal Service [17]. As examples of the improvements with generalisation ability which can be achieved with a soft margin we will also", "mykey":1221},
 {"datasetID":151, "supportID":"7C389E774D4C158FFD6B8D41DB4E905DE7703B79", "rexaID":"b6e169d69cd67763b95698e8961696fec9ca93bf", "author":"Charles Campbell and Nello Cristianini", "title":"Simple Learning Algorithms for Training Support Vector Machines", "venue":"Dept. of Engineering Mathematics", "year":"", "window":"include mirror symmetry [19], n-parity [19] and the two-spirals problem [11]. The real world datasets include a <b>sonar</b> classification problem [14], the Wisconsin breast cancer dataset [35] and a database of handwritten digits collected by the US Postal Service [17]. As examples of the improvements", "mykey":1222},
 {"datasetID":32, "supportID":"7C389E774D4C158FFD6B8D41DB4E905DE7703B79", "rexaID":"b6e169d69cd67763b95698e8961696fec9ca93bf", "author":"Charles Campbell and Nello Cristianini", "title":"Simple Learning Algorithms for Training Support Vector Machines", "venue":"Dept. of Engineering Mathematics", "year":"", "window":"problem of Gorman and Sejnowski [14] consists of 208 instances each with 60 attributes (excluding the labels) representing returns from a roughly cylindrical rock or a metal <b>cylinder</b>  This dataset is equally divided into training and test sets. For the aspect-angle dependent dataset these authors trained a standard backpropagation neural network with 60 inputs and 2 output nodes. Experiments", "mykey":1223},
 {"datasetID":34, "supportID":"7C389E774D4C158FFD6B8D41DB4E905DE7703B79", "rexaID":"b6e169d69cd67763b95698e8961696fec9ca93bf", "author":"Charles Campbell and Nello Cristianini", "title":"Simple Learning Algorithms for Training Support Vector Machines", "venue":"Dept. of Engineering Mathematics", "year":"", "window":"Service [17]. As examples of the improvements with generalisation ability which can be achieved with a soft margin we will also describe experiments with the ionosphere and Pima Indians <b>diabetes</b> datasets from the UCI Repository [4]. Though we have successfully used other kernels with KA we will only describe experiments using Gaussian kernels in this section. We will predominantly use the KA", "mykey":1224},
 {"datasetID":52, "supportID":"7C389E774D4C158FFD6B8D41DB4E905DE7703B79", "rexaID":"b6e169d69cd67763b95698e8961696fec9ca93bf", "author":"Charles Campbell and Nello Cristianini", "title":"Simple Learning Algorithms for Training Support Vector Machines", "venue":"Dept. of Engineering Mathematics", "year":"", "window":"Service [17]. As examples of the improvements with generalisation ability which can be achieved with a soft margin we will also describe experiments with the <b>ionosphere</b> and Pima Indians diabetes datasets from the UCI Repository [4]. Though we have successfully used other kernels with KA we will only describe experiments using Gaussian kernels in this section. We will predominantly use the KA", "mykey":1225},
 {"datasetID":81, "supportID":"7C389E774D4C158FFD6B8D41DB4E905DE7703B79", "rexaID":"b6e169d69cd67763b95698e8961696fec9ca93bf", "author":"Charles Campbell and Nello Cristianini", "title":"Simple Learning Algorithms for Training Support Vector Machines", "venue":"Dept. of Engineering Mathematics", "year":"", "window":"include a sonar classification problem [14], the Wisconsin breast cancer dataset [35] and a database of <b>handwritten</b> <b>digits</b> collected by the US Postal Service [17]. As examples of the improvements with generalisation ability which can be achieved with a soft margin we will also", "mykey":1226},
 {"datasetID":79, "supportID":"7C389E774D4C158FFD6B8D41DB4E905DE7703B79", "rexaID":"b6e169d69cd67763b95698e8961696fec9ca93bf", "author":"Charles Campbell and Nello Cristianini", "title":"Simple Learning Algorithms for Training Support Vector Machines", "venue":"Dept. of Engineering Mathematics", "year":"", "window":"Service [17]. As examples of the improvements with generalisation ability which can be achieved with a soft margin we will also describe experiments with the ionosphere and <b>Pima</b> <b>Indians</b> <b>diabetes</b> datasets from the UCI Repository [4]. Though we have successfully used other kernels with KA we will only describe experiments using Gaussian kernels in this section. We will predominantly use the KA", "mykey":1227},
 {"datasetID":2, "supportID":"7C8FA56A0051F59942C1FEB65E084A6D1EF4BCF6", "rexaID":"6a6b4ca13137132e00185012fd7c4667ff964769", "author":"S. Sathiya Keerthi and Kaibo Duan and Shirish Krishnaj Shevade and Aun Neow Poo", "title":"A Fast Dual Algorithm for Kernel Logistic Regression", "venue":"ICML", "year":"2002", "window":"much faster than the BFGS algorithm. The difference is much higher for large values of C. To see how the cost of the SMO algorithm scales with data size, an experiment was done on the UCI  <b>Adult</b>  dataset (Merz and Murphy, 1998) by gradually increasing the training set size from 1605 to 22696 in eight steps and observing the training time. A line was then fitted to the plot of the log of the training", "mykey":1228},
 {"datasetID":69, "supportID":"7C8FA56A0051F59942C1FEB65E084A6D1EF4BCF6", "rexaID":"6a6b4ca13137132e00185012fd7c4667ff964769", "author":"S. Sathiya Keerthi and Kaibo Duan and Shirish Krishnaj Shevade and Aun Neow Poo", "title":"A Fast Dual Algorithm for Kernel Logistic Regression", "venue":"ICML", "year":"2002", "window":"software at the site http://www.ece.nwu.edu/~nocedal/lbfgs.html was used. The Gaussian kernel K(x; # x) = exp( kx # xk 2 2# 2 ) was used. In all the experiments, # was set to 10 6 . Five benchmark datasets were used: Banana, Image, <b>Splice</b>  Waveform and Tree. The Tree dataset was originally used in (Bailey, Pettit, Borocho#, Manry, and Jiang, 1993). Detailed information about the remaining datasets", "mykey":1229},
 {"datasetID":107, "supportID":"7C8FA56A0051F59942C1FEB65E084A6D1EF4BCF6", "rexaID":"6a6b4ca13137132e00185012fd7c4667ff964769", "author":"S. Sathiya Keerthi and Kaibo Duan and Shirish Krishnaj Shevade and Aun Neow Poo", "title":"A Fast Dual Algorithm for Kernel Logistic Regression", "venue":"ICML", "year":"2002", "window":"software at the site http://www.ece.nwu.edu/~nocedal/lbfgs.html was used. The Gaussian kernel K(x; # x) = exp( kx # xk 2 2# 2 ) was used. In all the experiments, # was set to 10 6 . Five benchmark datasets were used: Banana, Image, Splice, <b>Waveform</b> and Tree. The Tree dataset was originally used in (Bailey, Pettit, Borocho#, Manry, and Jiang, 1993). Detailed information about the remaining datasets", "mykey":1230},
 {"datasetID":108, "supportID":"7C8FA56A0051F59942C1FEB65E084A6D1EF4BCF6", "rexaID":"6a6b4ca13137132e00185012fd7c4667ff964769", "author":"S. Sathiya Keerthi and Kaibo Duan and Shirish Krishnaj Shevade and Aun Neow Poo", "title":"A Fast Dual Algorithm for Kernel Logistic Regression", "venue":"ICML", "year":"2002", "window":"software at the site http://www.ece.nwu.edu/~nocedal/lbfgs.html was used. The Gaussian kernel K(x; # x) = exp( kx # xk 2 2# 2 ) was used. In all the experiments, # was set to 10 6 . Five benchmark datasets were used: Banana, Image, Splice, <b>Waveform</b> and Tree. The Tree dataset was originally used in (Bailey, Pettit, Borocho#, Manry, and Jiang, 1993). Detailed information about the remaining datasets", "mykey":1231},
 {"datasetID":72, "supportID":"7CC4C4E05068BB81F63B8D5C51953964B1E347E6", "rexaID":"c86d4f3c8bd74c27a77d236259751fb1d2d50b1c", "author":"Simon Perkins and James Theiler", "title":"Online Feature Selection using Grafting", "venue":"ICML", "year":"2003", "window":"were generated and the results shown are mean results. Dataset C is the  <b>Multiple Features</b>  database from the UCI repository. This is a handwritten digit recognition task, where digitized images of digits have been represented using 649 features of various", "mykey":1232},
 {"datasetID":151, "supportID":"7D019669E542567169D476AA1F55E7A9E15EA2EB", "rexaID":"7372c1c34417795a4d0752a07c77b3595dba16c4", "author":"Wl/odzisl/aw Duch and Karol Grudzinski", "title":"Ensembles of Similarity-based Models", "venue":"Intelligent Information Systems", "year":"2001", "window":"(except for the data described here we have tried <b>sonar</b> and hepatitis datasets from UCI [12]) the improvements have been insignificant. This shows that an ensemble of models of similar types may sometimes fail to improve the results. One reason for this may come from", "mykey":1233},
 {"datasetID":46, "supportID":"7D019669E542567169D476AA1F55E7A9E15EA2EB", "rexaID":"7372c1c34417795a4d0752a07c77b3595dba16c4", "author":"Wl/odzisl/aw Duch and Karol Grudzinski", "title":"Ensembles of Similarity-based Models", "venue":"Intelligent Information Systems", "year":"2001", "window":"(except for the data described here we have tried sonar and <b>hepatitis</b> datasets from UCI [12]) the improvements have been insignificant. This shows that an ensemble of models of similar types may sometimes fail to improve the results. One reason for this may come from", "mykey":1234},
 {"datasetID":70, "supportID":"7D019669E542567169D476AA1F55E7A9E15EA2EB", "rexaID":"7372c1c34417795a4d0752a07c77b3595dba16c4", "author":"Wl/odzisl/aw Duch and Karol Grudzinski", "title":"Ensembles of Similarity-based Models", "venue":"Intelligent Information Systems", "year":"2001", "window":"range. A single weight corresponding to a highly-ranked feature is fixed at 1 to establish an absolute scale for distances. First the ensemble selection method has been used with two artificial datasets, <b>Monk</b> 1 and Monk-3 [13]. These problems are designed for rule-based symbolic machine learning algorithms and the nearest neighbor algorithms usually do not work well in such cases. 6 symbolic", "mykey":1235},
 {"datasetID":146, "supportID":"7D057A83E835B97E36E076FE0509F7B0AB62C5AD", "rexaID":"a5aba686ed10e6524f1a20a041eebfbf4ca08436", "author":"Igor V. Tetko", "title":"Associative Neural Network", "venue":"Neural Processing Letters, 16", "year":"2002", "window":"of such data without a need to retrain the neural network ensemble. Applications of ASNN for prediction of lipophilicity of chemical compounds and classification of UCI letter and <b>satellite</b> data set are presented. The developed algorithm is available on-line at http://www.virtuallaboratory.org/lab/asnn. Key words. associative memory, bias correction, classification, function approximation,", "mykey":1236},
 {"datasetID":45, "supportID":"7D170D48B2D534F3CA2FBF2ABFC41B74271261EE", "rexaID":"a11c91c5cf02794b7d1d4bdc0222b3c11dce9cac", "author":"David Page and Soumya Ray", "title":"Skewing: An Efficient Alternative to Lookahead for Decision Tree Induction", "venue":"IJCAI", "year":"2003", "window":"Skewing ID3, No Skewing Figure 8: Five-Variable Hard Targets 50 60 70 80 90 100 200 400 600 800 1000 Accuracy (%) Sample Size ID3 with Skewing ID3, No Skewing Figure 9: Six-Variable Hard Targets Data Set Standard ID3 ID3 with Skewing <b>Heart</b> 71.9 74.5 Voting 94.0 94.2 Voting-2 87.4 88.6 Contra 60.4 61.5 Monks-1 92.6 100.0 Monks-2 86.5 89.3 Monks-3 89.8 91.7 Table 4: Accuracies of ID3 and ID3 with", "mykey":1237},
 {"datasetID":150, "supportID":"7D1B1A425384445710A31B94BB888BF27722F4BE", "rexaID":"49c65a0dfe258129414e27f6a3b7c42b7fbe426e", "author":"Kai Ming Ting and Boon Toh Low", "title":"Theory Combination: an alternative to Data Combination", "venue":"University of Waikato", "year":"", "window":"on or off, plus seventeen irrelevant binary attributes. Each attribute value is inverted with a probability of 0.1. The task is to classify the input as one of the ten digits. The four real-world datasets are the euthyroid, <b>nettalk</b> stress), splice junction and protein coding. The selection criteria are that the datasets must have large number of instances and each class must be supported by large", "mykey":1238},
 {"datasetID":69, "supportID":"7D1B1A425384445710A31B94BB888BF27722F4BE", "rexaID":"49c65a0dfe258129414e27f6a3b7c42b7fbe426e", "author":"Kai Ming Ting and Boon Toh Low", "title":"Theory Combination: an alternative to Data Combination", "venue":"University of Waikato", "year":"", "window":"on or off, plus seventeen irrelevant binary attributes. Each attribute value is inverted with a probability of 0.1. The task is to classify the input as one of the ten digits. The four real-world datasets are the euthyroid, nettalk(stress), <b>splice</b> junction and protein coding. The selection criteria are that the datasets must have large number of instances and each class must be supported by large", "mykey":1239},
 {"datasetID":154, "supportID":"7D1B1A425384445710A31B94BB888BF27722F4BE", "rexaID":"49c65a0dfe258129414e27f6a3b7c42b7fbe426e", "author":"Kai Ming Ting and Boon Toh Low", "title":"Theory Combination: an alternative to Data Combination", "venue":"University of Waikato", "year":"", "window":"3190 sequences where a small number of them contains some combination values (i.e., values combined from four base values). These sequences are eliminated in our experiments. 6 The <b>protein</b> coding dataset, introduced by Craven and Shavlik (1993), contains DNA nucleotide sequences and its classification task is to differentiate the coding sequences from the non-coding ones. Each sequence has fifteen", "mykey":1240},
 {"datasetID":102, "supportID":"7D1B1A425384445710A31B94BB888BF27722F4BE", "rexaID":"49c65a0dfe258129414e27f6a3b7c42b7fbe426e", "author":"Kai Ming Ting and Boon Toh Low", "title":"Theory Combination: an alternative to Data Combination", "venue":"University of Waikato", "year":"", "window":"is as follows. The euthyroid dataset is one of the sets of <b>Thyroid</b> examples from the Garvan Institute of Medical Research in Sydney described in Quinlan, Compton, Horn and Lazarus (1987). It consists of 3163 case data and diagnoses for", "mykey":1241},
 {"datasetID":107, "supportID":"7D1B1A425384445710A31B94BB888BF27722F4BE", "rexaID":"49c65a0dfe258129414e27f6a3b7c42b7fbe426e", "author":"Kai Ming Ting and Boon Toh Low", "title":"Theory Combination: an alternative to Data Combination", "venue":"University of Waikato", "year":"", "window":"why theory combination can not outperform data combination in this region. Note that the behaviour of the oracle is different from the usual learning curve in the <b>waveform</b> and protein coding datasets when NB* is employed. This indicates that some learning algorithm can specialise in different regions of the description space when the sizes of the data batches are relatively small. This can", "mykey":1242},
 {"datasetID":108, "supportID":"7D1B1A425384445710A31B94BB888BF27722F4BE", "rexaID":"49c65a0dfe258129414e27f6a3b7c42b7fbe426e", "author":"Kai Ming Ting and Boon Toh Low", "title":"Theory Combination: an alternative to Data Combination", "venue":"University of Waikato", "year":"", "window":"why theory combination can not outperform data combination in this region. Note that the behaviour of the oracle is different from the usual learning curve in the <b>waveform</b> and protein coding datasets when NB* is employed. This indicates that some learning algorithm can specialise in different regions of the description space when the sizes of the data batches are relatively small. This can", "mykey":1243},
 {"datasetID":53, "supportID":"7D2E76C681C0D058C87E699E774A1A3C11185A77", "rexaID":"a8a26052ead9d54b217e18b4e159137da340946f", "author":"Jun Wang and Bin Yu and Les Gasser", "title":"Concept Tree Based Clustering Visualization with Shaded Similarity Matrices", "venue":"ICDM", "year":"2002", "window":"we will briefly show how shaded similarity matrices are constructed and how one looks through an example. The data used in the example is part of the <b>Iris</b> data from the UCI repository[9]. The Iris data set contains 150 instances, evenly distributed in 3 classes. We fetch 5 instances from each class, and thus obtain 15 instances (Table 1). The similarity matrix was computed based on Euclidean distance", "mykey":1244},
 {"datasetID":148, "supportID":"7D2E76C681C0D058C87E699E774A1A3C11185A77", "rexaID":"a8a26052ead9d54b217e18b4e159137da340946f", "author":"Jun Wang and Bin Yu and Les Gasser", "title":"Concept Tree Based Clustering Visualization with Shaded Similarity Matrices", "venue":"ICDM", "year":"2002", "window":"similarity. has a scalability limitation. One solution is to use sampling and ensemble approaches. Using small sample sizes such as 100 or 200, we have tested the sampling approach on some Statlog datasets, including the <b>Shuttle</b> dataset which contains 43, 500 instances[6]. The results are promising. 6. Summary This paper proposes a new approach for getting better interpretations for clustering", "mykey":1245},
 {"datasetID":98, "supportID":"7D2E76C681C0D058C87E699E774A1A3C11185A77", "rexaID":"a8a26052ead9d54b217e18b4e159137da340946f", "author":"Jun Wang and Bin Yu and Les Gasser", "title":"Concept Tree Based Clustering Visualization with Shaded Similarity Matrices", "venue":"ICDM", "year":"2002", "window":"similarity. has a scalability limitation. One solution is to use sampling and ensemble approaches. Using small sample sizes such as 100 or 200, we have tested the sampling approach on some <b>Statlog</b> datasets, including the Shuttle dataset which contains 43, 500 instances[6]. The results are promising. 6. Summary This paper proposes a new approach for getting better interpretations for clustering", "mykey":1246},
 {"datasetID":2, "supportID":"7D39C07DA8BE80455C439B740F8D69019BCBC9A3", "rexaID":"03a71aaf988c71c8022be08734da8e376f7fe037", "author":"Omid Madani and David M. Pennock and Gary William Flake", "title":"Co-Validation: Using Model Disagreement to Validate Classification Algorithms", "venue":"Yahoo! Research Labs", "year":"", "window":"unlabeled data does not tend to wildly underestimate error, even though it's theoretically possible. 3 Experiments We conducted experiments on the 20 Newsgroups and Reuters-21578 test categorization datasets 1 , and the Votes, Chess, <b>Adult</b>  and Optics datasets from the UCI collection [BKM98]. We chose 1 Available from http://www.ics.uci.edu/ and http://www.daviddlewis.com/resources/testcollections/ two", "mykey":1247},
 {"datasetID":23, "supportID":"7D39C07DA8BE80455C439B740F8D69019BCBC9A3", "rexaID":"03a71aaf988c71c8022be08734da8e376f7fe037", "author":"Omid Madani and David M. Pennock and Gary William Flake", "title":"Co-Validation: Using Model Disagreement to Validate Classification Algorithms", "venue":"Yahoo! Research Labs", "year":"", "window":"unlabeled data does not tend to wildly underestimate error, even though it's theoretically possible. 3 Experiments We conducted experiments on the 20 Newsgroups and Reuters-21578 test categorization datasets 1 , and the Votes, <b>Chess</b>  Adult, and Optics datasets from the UCI collection [BKM98]. We chose 1 Available from http://www.ics.uci.edu/ and http://www.daviddlewis.com/resources/testcollections/ two", "mykey":1248},
 {"datasetID":21, "supportID":"7D39C07DA8BE80455C439B740F8D69019BCBC9A3", "rexaID":"03a71aaf988c71c8022be08734da8e376f7fe037", "author":"Omid Madani and David M. Pennock and Gary William Flake", "title":"Co-Validation: Using Model Disagreement to Validate Classification Algorithms", "venue":"Yahoo! Research Labs", "year":"", "window":"unlabeled data does not tend to wildly underestimate error, even though it's theoretically possible. 3 Experiments We conducted experiments on the 20 Newsgroups and Reuters-21578 test categorization datasets 1 , and the Votes, <b>Chess</b>  Adult, and Optics datasets from the UCI collection [BKM98]. We chose 1 Available from http://www.ics.uci.edu/ and http://www.daviddlewis.com/resources/testcollections/ two", "mykey":1249},
 {"datasetID":22, "supportID":"7D39C07DA8BE80455C439B740F8D69019BCBC9A3", "rexaID":"03a71aaf988c71c8022be08734da8e376f7fe037", "author":"Omid Madani and David M. Pennock and Gary William Flake", "title":"Co-Validation: Using Model Disagreement to Validate Classification Algorithms", "venue":"Yahoo! Research Labs", "year":"", "window":"unlabeled data does not tend to wildly underestimate error, even though it's theoretically possible. 3 Experiments We conducted experiments on the 20 Newsgroups and Reuters-21578 test categorization datasets 1 , and the Votes, <b>Chess</b>  Adult, and Optics datasets from the UCI collection [BKM98]. We chose 1 Available from http://www.ics.uci.edu/ and http://www.daviddlewis.com/resources/testcollections/ two", "mykey":1250},
 {"datasetID":137, "supportID":"7D39C07DA8BE80455C439B740F8D69019BCBC9A3", "rexaID":"03a71aaf988c71c8022be08734da8e376f7fe037", "author":"Omid Madani and David M. Pennock and Gary William Flake", "title":"Co-Validation: Using Model Disagreement to Validate Classification Algorithms", "venue":"Yahoo! Research Labs", "year":"", "window":"unlabeled data does not tend to wildly underestimate error, even though it's theoretically possible. 3 Experiments We conducted experiments on the 20 Newsgroups and <b>Reuters</b> 21578 test categorization datasets 1 , and the Votes, Chess, Adult, and Optics datasets from the UCI collection [BKM98]. We chose 1 Available from http://www.ics.uci.edu/ and http://www.daviddlewis.com/resources/testcollections/ two", "mykey":1251},
 {"datasetID":148, "supportID":"7D5F0DD976FCF6D041FBCE07FC667B6FCA4C4C58", "rexaID":"51207e20a41e3bd01fe89979f51688bdb782e4ef", "author":"Richard Nock", "title":"Inducing Interpretable Voting Classifiers without Trading Accuracy for Simplicity: Theoretical Results, Approximation Algorithms, and Experiments", "venue":"J. Artif. Intell. Res. (JAIR, 17", "year":"2002", "window":"on which we ran C4.5, WIDC(p) finds smaller formulas, and still beats C4.5's accuracy on 9 of them. A quantitative comparison of l DC against the number of nodes of the DTs shows that on 4 datasets out of the 13 (Pole, <b>Shuttle</b>  TicTacToe, Australian), the DCs are more than 6 times smaller, while they only incur a loss in accuracy for 2 of them, and limited to 1.8%. For this latter problem", "mykey":1252},
 {"datasetID":14, "supportID":"7E0D93E77081654DA2B299AD219591639C52809F", "rexaID":"899bdb470e48c308144216cc22048c88816ee035", "author":"Rafael S. Parpinelli and Heitor S. Lopes and Alex Alves Freitas", "title":"An Ant Colony Based System for Data Mining: Applications to Medical Data", "venue":"CEFET-PR, CPGEI Av. Sete de Setembro, 3165", "year":"", "window":"node). Each of these generated intervals is considered a discrete value for the attribute being discretized. (See the above reference for details.) The AntMiner system was tested using the following datasets: . Ljubljana <b>breast</b> <b>cancer</b>  this database has 282 cases, two classes and nine predicting attributes (all categorical); . Wisconsin breast cancer: This database has 683 cases, two classes and nine", "mykey":1253},
 {"datasetID":17, "supportID":"7E0D93E77081654DA2B299AD219591639C52809F", "rexaID":"899bdb470e48c308144216cc22048c88816ee035", "author":"Rafael S. Parpinelli and Heitor S. Lopes and Alex Alves Freitas", "title":"An Ant Colony Based System for Data Mining: Applications to Medical Data", "venue":"CEFET-PR, CPGEI Av. Sete de Setembro, 3165", "year":"", "window":"AntClass 75.13% \u00b1 6.00 5.20 \u00b1 0.87 8.80 \u00b1 1.89 C4.5 73.34% \u00b1 3.21 6.2 \u00b1 4.2 12.8 \u00b1 9.83 <b>Wisconsin</b> <b>Breast</b> <b>Cancer</b> Data Set AntClass 95.47% \u00b1 1.62 5.60 \u00b1 0.80 12.50 \u00b1 2.84 C4.5 95.02% \u00b1 0.31 11.1 \u00b1 1.45 44.1 \u00b1 7.48 Hepatitis Data Set AntClass 88.75% \u00b1 6.73 2.70 \u00b1 0.46 7.50 \u00b1 2.01 C4.5 85.96% \u00b1 1.07 4.4 \u00b1 0.93 8.5 \u00b1 3.04", "mykey":1254},
 {"datasetID":15, "supportID":"7E0D93E77081654DA2B299AD219591639C52809F", "rexaID":"899bdb470e48c308144216cc22048c88816ee035", "author":"Rafael S. Parpinelli and Heitor S. Lopes and Alex Alves Freitas", "title":"An Ant Colony Based System for Data Mining: Applications to Medical Data", "venue":"CEFET-PR, CPGEI Av. Sete de Setembro, 3165", "year":"", "window":"AntClass 75.13% \u00b1 6.00 5.20 \u00b1 0.87 8.80 \u00b1 1.89 C4.5 73.34% \u00b1 3.21 6.2 \u00b1 4.2 12.8 \u00b1 9.83 <b>Wisconsin</b> <b>Breast</b> <b>Cancer</b> Data Set AntClass 95.47% \u00b1 1.62 5.60 \u00b1 0.80 12.50 \u00b1 2.84 C4.5 95.02% \u00b1 0.31 11.1 \u00b1 1.45 44.1 \u00b1 7.48 Hepatitis Data Set AntClass 88.75% \u00b1 6.73 2.70 \u00b1 0.46 7.50 \u00b1 2.01 C4.5 85.96% \u00b1 1.07 4.4 \u00b1 0.93 8.5 \u00b1 3.04", "mykey":1255},
 {"datasetID":16, "supportID":"7E0D93E77081654DA2B299AD219591639C52809F", "rexaID":"899bdb470e48c308144216cc22048c88816ee035", "author":"Rafael S. Parpinelli and Heitor S. Lopes and Alex Alves Freitas", "title":"An Ant Colony Based System for Data Mining: Applications to Medical Data", "venue":"CEFET-PR, CPGEI Av. Sete de Setembro, 3165", "year":"", "window":"AntClass 75.13% \u00b1 6.00 5.20 \u00b1 0.87 8.80 \u00b1 1.89 C4.5 73.34% \u00b1 3.21 6.2 \u00b1 4.2 12.8 \u00b1 9.83 <b>Wisconsin</b> <b>Breast</b> <b>Cancer</b> Data Set AntClass 95.47% \u00b1 1.62 5.60 \u00b1 0.80 12.50 \u00b1 2.84 C4.5 95.02% \u00b1 0.31 11.1 \u00b1 1.45 44.1 \u00b1 7.48 Hepatitis Data Set AntClass 88.75% \u00b1 6.73 2.70 \u00b1 0.46 7.50 \u00b1 2.01 C4.5 85.96% \u00b1 1.07 4.4 \u00b1 0.93 8.5 \u00b1 3.04", "mykey":1256},
 {"datasetID":33, "supportID":"7E0D93E77081654DA2B299AD219591639C52809F", "rexaID":"899bdb470e48c308144216cc22048c88816ee035", "author":"Rafael S. Parpinelli and Heitor S. Lopes and Alex Alves Freitas", "title":"An Ant Colony Based System for Data Mining: Applications to Medical Data", "venue":"CEFET-PR, CPGEI Av. Sete de Setembro, 3165", "year":"", "window":"AntClass 88.75% \u00b1 6.73 2.70 \u00b1 0.46 7.50 \u00b1 2.01 C4.5 85.96% \u00b1 1.07 4.4 \u00b1 0.93 8.5 \u00b1 3.04 <b>Dermatology</b> Data Set AntClass 84.21% \u00b1 6.34 6.00 \u00b1 0.00 79.00 \u00b1 3.46 C4.5 89.05% \u00b1 0.62 23.2 \u00b1 1.99 91.7 \u00b1 10.64 Furthermore, Table 3 compares AntMiner with an evolutionary algorithm for rule discovery called ESIA -", "mykey":1257},
 {"datasetID":46, "supportID":"7E0D93E77081654DA2B299AD219591639C52809F", "rexaID":"899bdb470e48c308144216cc22048c88816ee035", "author":"Rafael S. Parpinelli and Heitor S. Lopes and Alex Alves Freitas", "title":"An Ant Colony Based System for Data Mining: Applications to Medical Data", "venue":"CEFET-PR, CPGEI Av. Sete de Setembro, 3165", "year":"", "window":"AntClass 95.47% \u00b1 1.62 5.60 \u00b1 0.80 12.50 \u00b1 2.84 C4.5 95.02% \u00b1 0.31 11.1 \u00b1 1.45 44.1 \u00b1 7.48 <b>Hepatitis</b> Data Set AntClass 88.75% \u00b1 6.73 2.70 \u00b1 0.46 7.50 \u00b1 2.01 C4.5 85.96% \u00b1 1.07 4.4 \u00b1 0.93 8.5 \u00b1 3.04 Dermatology Data Set AntClass 84.21% \u00b1 6.34 6.00 \u00b1 0.00 79.00 \u00b1 3.46 C4.5 89.05% \u00b1 0.62 23.2 \u00b1 1.99 91.7 \u00b1", "mykey":1258},
 {"datasetID":67, "supportID":"7E450DE0A684E429F33CB2CA1E6F47F0B46BC962", "rexaID":"6f9e5db8e117763c59d6ffabcb511900b9f62159", "author":"Cesar Guerra-Salcedo and L. Darrell Whitley", "title":"Genetic Approach to Feature Selection for Ensemble Creation", "venue":"GECCO", "year":"1999", "window":"belonging to a particular class depend on the number of elements for that class and e. This modification allows us to avoid elements from different classes being mixed in one cluster. Table 1: Dataset employed for the experiments. In the <b>DNA</b> dataset the attributes values are 0 or 1. In the Segment dataset the attributes values are floats. In the LandSat dataset the attribute values are integers.", "mykey":1259},
 {"datasetID":146, "supportID":"7E450DE0A684E429F33CB2CA1E6F47F0B46BC962", "rexaID":"6f9e5db8e117763c59d6ffabcb511900b9f62159", "author":"Cesar Guerra-Salcedo and L. Darrell Whitley", "title":"Genetic Approach to Feature Selection for Ensemble Creation", "venue":"GECCO", "year":"1999", "window":"the attributes values are 0 or 1. In the Segment dataset the attributes values are floats. In the <b>LandSat</b> dataset the attribute values are integers. Dataset Features Classes Train Size Test Size LandSat 36 6 4435 2000 DNA 180 39 2000 1186 Segment 19 7 210", "mykey":1260},
 {"datasetID":98, "supportID":"7E450DE0A684E429F33CB2CA1E6F47F0B46BC962", "rexaID":"6f9e5db8e117763c59d6ffabcb511900b9f62159", "author":"Cesar Guerra-Salcedo and L. Darrell Whitley", "title":"Genetic Approach to Feature Selection for Ensemble Creation", "venue":"GECCO", "year":"1999", "window":"Features Classes Train Size Test Size LandSat 36 6 4435 2000 DNA 180 39 2000 1186 Segment 19 7 210 2100 3 EXPERIMENTAL SETUP A series of experiments were carried out using publicly available datasets provided by the Project <b>Statlog</b> 1 and by UCI machine learning repository [C. Blake and Merz, 1998]. Table 1 shows the datasets employed for this research. 3.1 ENSEMBLE RELATED SETUPS Our main", "mykey":1261},
 {"datasetID":151, "supportID":"7E4FD48EB811B940D3BEDB07504205AD23FE6850", "rexaID":"8432b52f06e00a68cb8a7a49e3938190338231de", "author":"Yin Zhang and W. Nick Street", "title":"Bagging with Adaptive Costs", "venue":"Management Sciences Department University of Iowa Iowa City", "year":"", "window":"[14]: Autompg, Bupa, Glass, Haberman, Housing, Cleveland-heart-disease, Hepatitis, Ion, Pima, <b>Sonar</b>  Vehicle, WDBC, Wine and WPBC. These data sets were picked mainly because they are relatively small in size so that the SVM problem can be solved in reasonable time. Some of the data sets do not originally depict two-class problems so we did", "mykey":1262},
 {"datasetID":42, "supportID":"7E4FD48EB811B940D3BEDB07504205AD23FE6850", "rexaID":"8432b52f06e00a68cb8a7a49e3938190338231de", "author":"Yin Zhang and W. Nick Street", "title":"Bagging with Adaptive Costs", "venue":"Management Sciences Department University of Iowa Iowa City", "year":"", "window":"and the out-of-bag margin estimation will result in better generalization as it does in stacking. III. COMPUTATIONAL EXPERIMENTS Bacing was implemented using MATLAB and tested on 14 UCI repository data sets [14]: Autompg, Bupa, <b>Glass</b>  Haberman, Housing, Cleveland-heart-disease, Hepatitis, Ion, Pima, Sonar, Vehicle, WDBC, Wine and WPBC. These data sets were picked mainly because they are relatively", "mykey":1263},
 {"datasetID":43, "supportID":"7E4FD48EB811B940D3BEDB07504205AD23FE6850", "rexaID":"8432b52f06e00a68cb8a7a49e3938190338231de", "author":"Yin Zhang and W. Nick Street", "title":"Bagging with Adaptive Costs", "venue":"Management Sciences Department University of Iowa Iowa City", "year":"", "window":"and the out-of-bag margin estimation will result in better generalization as it does in stacking. III. COMPUTATIONAL EXPERIMENTS Bacing was implemented using MATLAB and tested on 14 UCI repository data sets [14]: Autompg, Bupa, Glass, <b>Haberman</b>  Housing, Cleveland-heart-disease, Hepatitis, Ion, Pima, Sonar, Vehicle, WDBC, Wine and WPBC. These data sets were picked mainly because they are relatively", "mykey":1264},
 {"datasetID":48, "supportID":"7E4FD48EB811B940D3BEDB07504205AD23FE6850", "rexaID":"8432b52f06e00a68cb8a7a49e3938190338231de", "author":"Yin Zhang and W. Nick Street", "title":"Bagging with Adaptive Costs", "venue":"Management Sciences Department University of Iowa Iowa City", "year":"", "window":"and the out-of-bag margin estimation will result in better generalization as it does in stacking. III. COMPUTATIONAL EXPERIMENTS Bacing was implemented using MATLAB and tested on 14 UCI repository data sets [14]: Autompg, Bupa, Glass, Haberman, <b>Housing</b>  Cleveland-heart-disease, Hepatitis, Ion, Pima, Sonar, Vehicle, WDBC, Wine and WPBC. These data sets were picked mainly because they are relatively", "mykey":1265},
 {"datasetID":149, "supportID":"7E4FD48EB811B940D3BEDB07504205AD23FE6850", "rexaID":"8432b52f06e00a68cb8a7a49e3938190338231de", "author":"Yin Zhang and W. Nick Street", "title":"Bagging with Adaptive Costs", "venue":"Management Sciences Department University of Iowa Iowa City", "year":"", "window":"[14]: Autompg, Bupa, Glass, Haberman, Housing, Cleveland-heart-disease, Hepatitis, Ion, Pima, Sonar, <b>Vehicle</b>  WDBC, Wine and WPBC. These data sets were picked mainly because they are relatively small in size so that the SVM problem can be solved in reasonable time. Some of the data sets do not originally depict two-class problems so we did", "mykey":1266},
 {"datasetID":109, "supportID":"7E4FD48EB811B940D3BEDB07504205AD23FE6850", "rexaID":"8432b52f06e00a68cb8a7a49e3938190338231de", "author":"Yin Zhang and W. Nick Street", "title":"Bagging with Adaptive Costs", "venue":"Management Sciences Department University of Iowa Iowa City", "year":"", "window":"[14]: Autompg, Bupa, Glass, Haberman, Housing, Cleveland-heart-disease, Hepatitis, Ion, Pima, Sonar, Vehicle, WDBC, <b>Wine</b> and WPBC. These data sets were picked mainly because they are relatively small in size so that the SVM problem can be solved in reasonable time. Some of the data sets do not originally depict two-class problems so we did", "mykey":1267},
 {"datasetID":96, "supportID":"7E83ECA95D54F25380983E5ABAE1E57D8B1A1439", "rexaID":"208ff3b44c2d57f4e00f263ddfbee69f4ad21972", "author":"Carlotta Domeniconi and Bojun Yan", "title":"Nearest Neighbor Ensemble", "venue":"ICPR (1)", "year":"2004", "window":"10 times, and the average error rate was computed. The results show that our Weight-driven approach offers significant accuracy improvements (over both ADAMENN and the Random approach) for the three data sets with a larger number of dimensions  <b>spectf</b> test, lung, sonar). For liver and ionosphere the Random and Weight approaches give similar performances. This result provides evidence that bootstrapping", "mykey":1268},
 {"datasetID":7, "supportID":"7EB67AAE1F9029214030F896B40B57361D9CE71E", "rexaID":"7125dc87ceddf952f1840074b40ac0526dcc03dc", "author":"Marcus Hutter and Marco Zaffalon", "title":"Distribution of Mutual Information from Complete and Incomplete Data", "venue":"CoRR, csLG/0403025", "year":"2004", "window":"0 100 200 300 400 500 600 700 800 900 1000 1100 Instance number Aver. number of excluded features (Spam) F FF BF Figure 4: Average number of attributes excluded by the di\u00aeerent filters on the Spam data set. Name #feat. #inst. #m.d. mode freq. <b>Audiology</b> 69 226 317 0.212 Crx 15 690 67 0.555 Horse-colic 18 368 1281 0.630 Hypothyroidloss 23 3163 1980 0.952 Soybean-large 35 683 2337 0.135 Table 3:", "mykey":1269},
 {"datasetID":8, "supportID":"7EB67AAE1F9029214030F896B40B57361D9CE71E", "rexaID":"7125dc87ceddf952f1840074b40ac0526dcc03dc", "author":"Marcus Hutter and Marco Zaffalon", "title":"Distribution of Mutual Information from Complete and Incomplete Data", "venue":"CoRR, csLG/0403025", "year":"2004", "window":"0 100 200 300 400 500 600 700 800 900 1000 1100 Instance number Aver. number of excluded features (Spam) F FF BF Figure 4: Average number of attributes excluded by the di\u00aeerent filters on the Spam data set. Name #feat. #inst. #m.d. mode freq. <b>Audiology</b> 69 226 317 0.212 Crx 15 690 67 0.555 Horse-colic 18 368 1281 0.630 Hypothyroidloss 23 3163 1980 0.952 Soybean-large 35 683 2337 0.135 Table 3:", "mykey":1270},
 {"datasetID":23, "supportID":"7EB67AAE1F9029214030F896B40B57361D9CE71E", "rexaID":"7125dc87ceddf952f1840074b40ac0526dcc03dc", "author":"Marcus Hutter and Marco Zaffalon", "title":"Distribution of Mutual Information from Complete and Incomplete Data", "venue":"CoRR, csLG/0403025", "year":"2004", "window":"The remaining cases are described by means of the following figures. Figure 2 shows that FF allowed the naive Bayes to significantly do better predictions than F for the greatest part of the <b>Chess</b> data set. The maximum di\u00aeerence in prediction accuracy is obtained at instance 422, where the accuracies are 0.889 and 0.832 for the cases FF and F, respectively. Figure 2 does not report the BF case,", "mykey":1271},
 {"datasetID":21, "supportID":"7EB67AAE1F9029214030F896B40B57361D9CE71E", "rexaID":"7125dc87ceddf952f1840074b40ac0526dcc03dc", "author":"Marcus Hutter and Marco Zaffalon", "title":"Distribution of Mutual Information from Complete and Incomplete Data", "venue":"CoRR, csLG/0403025", "year":"2004", "window":"The remaining cases are described by means of the following figures. Figure 2 shows that FF allowed the naive Bayes to significantly do better predictions than F for the greatest part of the <b>Chess</b> data set. The maximum di\u00aeerence in prediction accuracy is obtained at instance 422, where the accuracies are 0.889 and 0.832 for the cases FF and F, respectively. Figure 2 does not report the BF case,", "mykey":1272},
 {"datasetID":22, "supportID":"7EB67AAE1F9029214030F896B40B57361D9CE71E", "rexaID":"7125dc87ceddf952f1840074b40ac0526dcc03dc", "author":"Marcus Hutter and Marco Zaffalon", "title":"Distribution of Mutual Information from Complete and Incomplete Data", "venue":"CoRR, csLG/0403025", "year":"2004", "window":"The remaining cases are described by means of the following figures. Figure 2 shows that FF allowed the naive Bayes to significantly do better predictions than F for the greatest part of the <b>Chess</b> data set. The maximum di\u00aeerence in prediction accuracy is obtained at instance 422, where the accuracies are 0.889 and 0.832 for the cases FF and F, respectively. Figure 2 does not report the BF case,", "mykey":1273},
 {"datasetID":63, "supportID":"7EB67AAE1F9029214030F896B40B57361D9CE71E", "rexaID":"7125dc87ceddf952f1840074b40ac0526dcc03dc", "author":"Marcus Hutter and Marco Zaffalon", "title":"Distribution of Mutual Information from Complete and Incomplete Data", "venue":"CoRR, csLG/0403025", "year":"2004", "window":"on a number of di\u00aeerent domains. For example, Shuttle-small reports Distribution of Mutual Information 19 data on diagnosing failures of the space shuttle; <b>Lymphography</b> and Hypothyroid are medical data sets; Spam is a body of e-mails that can be spam or non-spam; etc. Name #feat. #inst. mode freq. Australian 36 690 0.555 Chess 36 3196 0.520 Crx 15 653 0.547 German-org 17 1000 0.700 Hypothyroid 23 2238", "mykey":1274},
 {"datasetID":102, "supportID":"7ED4072F684D093EB37B81289D1ABAE9A88DF733", "rexaID":"3c13fe240b68a2a06ebdefdd6e51e0d5e34f66e6", "author":"Wl/odzisl/aw Duch and Rafal Adamczak and Krzysztof Grabczewski", "title":"Extraction of crisp logical rules from medical datasets", "venue":"Department of Computer Methods, Nicholas Copernicus University", "year":"", "window":"generated 24 rules, but these rules used more features and were not so accurate (98% on the test set) as MLP2LN rules. TABLE I Classification results for various classifiers applied to the <b>thyroid</b> dataset. Method Training set accuracy % Test set accuracy % BP+conjugate gradient 94.6 93.8 Best Backpropagation 99.1 97.6 RPROP 99.6 98.0 Quickprop 99.6 98.3 BP+ genetic optimization 99.4 98.4 Local", "mykey":1275},
 {"datasetID":52, "supportID":"7ED56AFC7EB57F7D0647981EED32026D82CD536D", "rexaID":"a81018cca22b93c0c71ff6e9983d38669daeb54e", "author":"Markus Breitenbach and Rodney Nielsen and Gregory Z. Grudic", "title":"Probabilistic Random Forests: Predicting Data Point Specific Misclassification Probabilities", "venue":"Department of Computer Science University of Colorado", "year":"", "window":"2, and asterisks demonstrate the bottom line efficacy of the algorithm, which adjusts the first-order predictions according to performance on the out-of-bag data within the same prediction interval. Dataset PRF MPMCL MPMCG SVML SVMG RF <b>Ionosphere</b> 80.8 \u00b1 .7 % 85.4% 93.0% 87.8% 91.5% 92.9% Sonar 81.0 \u00b1 .9 % 75.1% 89.8% 75.9% 86.7% 84.1% Breast Cancer 95.9 \u00b1 .3 % 97.2% 97.3% 92.6% 98.5% 97.1% Pima", "mykey":1276},
 {"datasetID":109, "supportID":"7F6A990C61B7F551156B105DC675F32D17BE2D41", "rexaID":"18f3e26b60920b9b7aa5ca210057c15d0c0cc75a", "author":"Stefan Mutter and Mark Hall and Eibe Frank", "title":"Using Classification to Evaluate the Output of Confidence-Based Association Rule Mining", "venue":"Australian Conference on Artificial Intelligence", "year":"2004", "window":"294 6 3 4 2 20.4 iris 150 4 0 0 3 0.0 labor 57 8 3 5 2 35.7 led7 1000 0 7 0 10 0.0 lenses 24 0 0 4 3 0.0 pima 768 8 0 0 2 0.0 tic-tac-toe 958 0 0 9 2 0.0 <b>wine</b> 178 13 0 0 3 0.0 Table 1. The UCI datasets used for the experiments and their properties. In the led7 dataset 10% of the instances are noisy. In every experiment the support threshold s min of Apriori was set to 1% of all instances and the", "mykey":1277},
 {"datasetID":50, "supportID":"7FF8F0363289161F9CA8B223E5B317357C29CC1A", "rexaID":"c495658301b0128f01553cf69d64ab348a217d67", "author":"James Tin and Yau Kwok", "title":"Moderating the Outputs of Support Vector Machine Classifiers", "venue":"Department of Computer Science Hong Kong Baptist University Hong Kong", "year":"", "window":"NETWORKS, VOL. XX, NO. Y, MONTH 1999 5 TABLE I Values of G from various SVM outputs. clipped MAP unmoderated regularized, unmoderated moderated 1 1 3801 1417 B. <b>image segmentation</b> Problem The second data set is the image segmentation data from the UCI machine learning repository[37]. Each pattern has 19 continuous attributes and corresponds to a 3 # 3 region of an outdoor image. The problem is to", "mykey":1278},
 {"datasetID":147, "supportID":"7FF8F0363289161F9CA8B223E5B317357C29CC1A", "rexaID":"c495658301b0128f01553cf69d64ab348a217d67", "author":"James Tin and Yau Kwok", "title":"Moderating the Outputs of Support Vector Machine Classifiers", "venue":"Department of Computer Science Hong Kong Baptist University Hong Kong", "year":"", "window":"NETWORKS, VOL. XX, NO. Y, MONTH 1999 5 TABLE I Values of G from various SVM outputs. clipped MAP unmoderated regularized, unmoderated moderated 1 1 3801 1417 B. <b>image segmentation</b> Problem The second data set is the image segmentation data from the UCI machine learning repository[37]. Each pattern has 19 continuous attributes and corresponds to a 3 # 3 region of an outdoor image. The problem is to", "mykey":1279},
 {"datasetID":151, "supportID":"810216C84636BACDCFEC286974FD0E5FFC809FE8", "rexaID":"36d1c58be0e3c0c1d57492d050d8a71a04a72858", "author":"Ayhan Demiriz and Kristin P. Bennett", "title":"Chapter 1 OPTIMIZATIONAPPROACHESTOSEMI-SUPERVISED LEARNING", "venue":"Department of Decision Sciences and Engineering Systems & Department of Mathematical Sciences, Rensselaer Polytechnic Institute", "year":"", "window":"as in the previous section. Due to the long computational times for S 3 VM-IQP and transductive SVM-Light, we limit our experiments to only the Heart, Housing, Ionosphere, and <b>Sonar</b> datasets. Linear kernel functions are used for all methods used in this section. The results given in Table 1.3 show that using unlabeled data in the case of datasets Heart and Ionosphere affects", "mykey":1280},
 {"datasetID":45, "supportID":"810216C84636BACDCFEC286974FD0E5FFC809FE8", "rexaID":"36d1c58be0e3c0c1d57492d050d8a71a04a72858", "author":"Ayhan Demiriz and Kristin P. Bennett", "title":"Chapter 1 OPTIMIZATIONAPPROACHESTOSEMI-SUPERVISED LEARNING", "venue":"Department of Decision Sciences and Engineering Systems & Department of Mathematical Sciences, Rensselaer Polytechnic Institute", "year":"", "window":"an action reduces the overall error. Like S 3 VM-IQP, SVM-Light alternates 16 APPLICATIONS AND ALGORITHMS OF COMPLEMENTARITY Table 1.3 Average Error Results for Transductive and Inductive Methods Data Set SVM-QP SVM-Light S 3 VM-IQP <b>Heart</b> 0.16 0.163 0.1966 Housing 0.1804 0.1608 0.1647 Ionosphere 0.0857 0.1572 0.0943 Sonar 0.1762 0.2524 0.1572 the labels to avoid local minima. The primary difference", "mykey":1281},
 {"datasetID":48, "supportID":"810216C84636BACDCFEC286974FD0E5FFC809FE8", "rexaID":"36d1c58be0e3c0c1d57492d050d8a71a04a72858", "author":"Ayhan Demiriz and Kristin P. Bennett", "title":"Chapter 1 OPTIMIZATIONAPPROACHESTOSEMI-SUPERVISED LEARNING", "venue":"Department of Decision Sciences and Engineering Systems & Department of Mathematical Sciences, Rensselaer Polytechnic Institute", "year":"", "window":"of the working set is set to 50 points and rest of the data are used as the training set. We use the following formula to pick the penalty parameter: 1 The continuous response variable in <b>Housing</b> dataset was categorized at 21.5 12 APPLICATIONS AND ALGORITHMS OF COMPLEMENTARITY Table 1.2 Average Error Results for Inductive and Transductive SVM Methods Data Set SVM-RLP S 3 V M Local SVM Local S 3 V M", "mykey":1282},
 {"datasetID":52, "supportID":"810216C84636BACDCFEC286974FD0E5FFC809FE8", "rexaID":"36d1c58be0e3c0c1d57492d050d8a71a04a72858", "author":"Ayhan Demiriz and Kristin P. Bennett", "title":"Chapter 1 OPTIMIZATIONAPPROACHESTOSEMI-SUPERVISED LEARNING", "venue":"Department of Decision Sciences and Engineering Systems & Department of Mathematical Sciences, Rensselaer Polytechnic Institute", "year":"", "window":"as in the previous section. Due to the long computational times for S 3 VM-IQP and transductive SVM-Light, we limit our experiments to only the Heart, Housing, <b>Ionosphere</b>  and Sonar datasets. Linear kernel functions are used for all methods used in this section. The results given in Table 1.3 show that using unlabeled data in the case of datasets Heart and Ionosphere affects", "mykey":1283},
 {"datasetID":2, "supportID":"8128AC09A26E5C9890926D1A9E7915312225825C", "rexaID":"686ccd571de3e92d89394f207348bd5a669558d2", "author":"Rakesh Agrawal and Ramakrishnan ikant and Dilys Thomas", "title":"Privacy Preserving OLAP", "venue":"SIGMOD Conference", "year":"2005", "window":"present among the original set of queried columns. 7. EXPERIMENTS We next present an empirical evaluation of our algorithms on real as well as synthetic data. For real data, we used the <b>Adult</b> dataset, from the UCI Machine Learning Repository [5], which has census information. The Adult dataset contains about 32,000 rows with 4 numerical columns. The columns and their ranges are: age[17 - 90],", "mykey":1284},
 {"datasetID":20, "supportID":"8128AC09A26E5C9890926D1A9E7915312225825C", "rexaID":"686ccd571de3e92d89394f207348bd5a669558d2", "author":"Rakesh Agrawal and Ramakrishnan ikant and Dilys Thomas", "title":"Privacy Preserving OLAP", "venue":"SIGMOD Conference", "year":"2005", "window":"from the UCI Machine Learning Repository [5], which has <b>census</b> information. The Adult dataset contains about 32,000 rows with 4 numerical columns. The columns and their ranges are: age[17 - 90], fnlwgt[10000 - 1500000], hrsweek[1 - 100] and edunum[1 - 16]. For synthetic data, we used", "mykey":1285},
 {"datasetID":46, "supportID":"813DB67CE83A6E5C9BCC9B54717999E2E2E66267", "rexaID":"177158d5e743e0f71b640c20a162ee8af5c79dcf", "author":"Takao Mohri and Hidehiko Tanaka", "title":"An Optimal Weighting Criterion of Case Indexing for Both Numeric and Symbolic Attributes", "venue":"Information Engineering Course, Faculty of Engineering The University of Tokyo", "year":"", "window":"(vote, soybean, crx, hypo) were in the distribution floppy disk of Quinlan's C4.5 book (Quinlan 1993). The remaining four data sets (iris, <b>hepatitis</b>  led, led-noise) were obtained from the Irvine Machine Learning Database (Murphy & Aha 1994). Including our 3 methods,VDM, PCF, CCF, IB4, and C4.5 are compared. Quinlan's C4.5 is a", "mykey":1286},
 {"datasetID":53, "supportID":"813DB67CE83A6E5C9BCC9B54717999E2E2E66267", "rexaID":"177158d5e743e0f71b640c20a162ee8af5c79dcf", "author":"Takao Mohri and Hidehiko Tanaka", "title":"An Optimal Weighting Criterion of Case Indexing for Both Numeric and Symbolic Attributes", "venue":"Information Engineering Course, Faculty of Engineering The University of Tokyo", "year":"", "window":"(vote, soybean, crx, hypo) were in the distribution floppy disk of Quinlan's C4.5 book (Quinlan 1993). The remaining four data sets  <b>iris</b>  hepatitis, led, led-noise) were obtained from the Irvine Machine Learning Database (Murphy & Aha 1994). Including our 3 methods,VDM, PCF, CCF, IB4, and C4.5 are compared. Quinlan's C4.5 is a", "mykey":1287},
 {"datasetID":90, "supportID":"813DB67CE83A6E5C9BCC9B54717999E2E2E66267", "rexaID":"177158d5e743e0f71b640c20a162ee8af5c79dcf", "author":"Takao Mohri and Hidehiko Tanaka", "title":"An Optimal Weighting Criterion of Case Indexing for Both Numeric and Symbolic Attributes", "venue":"Information Engineering Course, Faculty of Engineering The University of Tokyo", "year":"", "window":"that they maximize the variance ratio j 2 . Therefore, they have a theoretical basis and clear meaning. Experiments The experimental results for several benchmark data are shown in Table 3. Four data sets (vote, <b>soybean</b>  crx, hypo) were in the distribution floppy disk of Quinlan's C4.5 book (Quinlan 1993). The remaining four data sets (iris, hepatitis, led, led-noise) were obtained from the Irvine", "mykey":1288},
 {"datasetID":91, "supportID":"813DB67CE83A6E5C9BCC9B54717999E2E2E66267", "rexaID":"177158d5e743e0f71b640c20a162ee8af5c79dcf", "author":"Takao Mohri and Hidehiko Tanaka", "title":"An Optimal Weighting Criterion of Case Indexing for Both Numeric and Symbolic Attributes", "venue":"Information Engineering Course, Faculty of Engineering The University of Tokyo", "year":"", "window":"that they maximize the variance ratio j 2 . Therefore, they have a theoretical basis and clear meaning. Experiments The experimental results for several benchmark data are shown in Table 3. Four data sets (vote, <b>soybean</b>  crx, hypo) were in the distribution floppy disk of Quinlan's C4.5 book (Quinlan 1993). The remaining four data sets (iris, hepatitis, led, led-noise) were obtained from the Irvine", "mykey":1289},
 {"datasetID":34, "supportID":"8171D46317AFB0C794451F405AEB2ACC26D8948C", "rexaID":"5f8eb537fc397ad5e506b2eae4f6676b48990ba6", "author":"Thomas G. Dietterich", "title":"Approximate Statistical Test For Comparing Supervised Classification Learning Algorithms", "venue":"Neural Computation, 10", "year":"1998", "window":"measured on the 10,000 calibration examples) matched the average performance of C4.5 to within 0.1%. For the Pima Indians <b>Diabetes</b> data set, we drew 1000 data sets of size 300 from the 768 available examples. For each of these data sets, the remaining 468 examples were retained for calibration. Each of the 1000 data sets of size 300 was", "mykey":1290},
 {"datasetID":59, "supportID":"8171D46317AFB0C794451F405AEB2ACC26D8948C", "rexaID":"5f8eb537fc397ad5e506b2eae4f6676b48990ba6", "author":"Thomas G. Dietterich", "title":"Approximate Statistical Test For Comparing Supervised Classification Learning Algorithms", "venue":"Neural Computation, 10", "year":"1998", "window":"1 (Quinlan, 1993) and the first nearest-neighbor (NN) algorithm (Dasarathy, 1991). We then selected three difficult problems: the EXP6 problem developed by Kong (1995), the <b>Letter Recognition</b> data set (Frey & Slate, 1991), and the Pima Indians Diabetes Task (Merz & Murphy, 1996). Of course, C4.5 and NN do not have the same performance on these data sets. In EXP6 and Letter Recognition, NN", "mykey":1291},
 {"datasetID":79, "supportID":"8171D46317AFB0C794451F405AEB2ACC26D8948C", "rexaID":"5f8eb537fc397ad5e506b2eae4f6676b48990ba6", "author":"Thomas G. Dietterich", "title":"Approximate Statistical Test For Comparing Supervised Classification Learning Algorithms", "venue":"Neural Computation, 10", "year":"1998", "window":"measured on the 10,000 calibration examples) matched the average performance of C4.5 to within 0.1%. For the <b>Pima</b> <b>Indians</b> <b>Diabetes</b> data set, we drew 1000 data sets of size 300 from the 768 available examples. For each of these data sets, the remaining 468 examples were retained for calibration. Each of the 1000 data sets of size 300 was", "mykey":1292},
 {"datasetID":42, "supportID":"81BCEE95A323170579DE2013114C54475378AF52", "rexaID":"f624e93bd6b670bc3dc31925c1c885b538131534", "author":"Chih-Wei Hsu and Cheng-Ru Lin", "title":"A Comparison of Methods for Multi-class Support Vector Machines", "venue":"Department of Computer Science and Information Engineering National Taiwan University", "year":"", "window":"section we present experimental results on several problems from the Statlog collection [20] and the UCI Repository of machine learning databases [1]. From UCI Repository we choose the following datasets: iris, wine, <b>glass</b>  and vowel. Those problems had already been tested in [27]. From Statlog collection we choose all multi-class datasets: vehicle, segment, dna, satimage, letter, and shuttle. Note", "mykey":1293},
 {"datasetID":53, "supportID":"81BCEE95A323170579DE2013114C54475378AF52", "rexaID":"f624e93bd6b670bc3dc31925c1c885b538131534", "author":"Chih-Wei Hsu and Cheng-Ru Lin", "title":"A Comparison of Methods for Multi-class Support Vector Machines", "venue":"Department of Computer Science and Information Engineering National Taiwan University", "year":"", "window":"section we present experimental results on several problems from the Statlog collection [20] and the UCI Repository of machine learning databases [1]. From UCI Repository we choose the following datasets: <b>iris</b>  wine, glass, and vowel. Those problems had already been tested in [27]. From Statlog collection we choose all multi-class datasets: vehicle, segment, dna, satimage, letter, and shuttle. Note", "mykey":1294},
 {"datasetID":67, "supportID":"81BCEE95A323170579DE2013114C54475378AF52", "rexaID":"f624e93bd6b670bc3dc31925c1c885b538131534", "author":"Chih-Wei Hsu and Cheng-Ru Lin", "title":"A Comparison of Methods for Multi-class Support Vector Machines", "venue":"Department of Computer Science and Information Engineering National Taiwan University", "year":"", "window":"iris, wine, glass, and vowel. Those problems had already been tested in [27]. From Statlog collection we choose all multi-class datasets: vehicle, segment, <b>dna</b>  satimage, letter, and shuttle. Note that except problem dna we scale all training data to be in [-1, 1]. Then test data are adjusted using the same linear transformation.", "mykey":1295},
 {"datasetID":148, "supportID":"81BCEE95A323170579DE2013114C54475378AF52", "rexaID":"f624e93bd6b670bc3dc31925c1c885b538131534", "author":"Chih-Wei Hsu and Cheng-Ru Lin", "title":"A Comparison of Methods for Multi-class Support Vector Machines", "venue":"Department of Computer Science and Information Engineering National Taiwan University", "year":"", "window":"; 2 3 ; 2 2 ; : : : ; 2 10 ] and C = [2 12 ; 2 11 ; 2 10 ; : : : ; 2 2 ]. Therefore, for each problem we try 15 # 15 = 225 combinations. We use two criteria to estimate the generalized accuracy. For datasets dna, satimage, letter, and <b>shuttle</b> where both training and testing sets are available, for each pair of (C; #), the validation performance is measured by training 70% of the training set and", "mykey":1296},
 {"datasetID":149, "supportID":"81BCEE95A323170579DE2013114C54475378AF52", "rexaID":"f624e93bd6b670bc3dc31925c1c885b538131534", "author":"Chih-Wei Hsu and Cheng-Ru Lin", "title":"A Comparison of Methods for Multi-class Support Vector Machines", "venue":"Department of Computer Science and Information Engineering National Taiwan University", "year":"", "window":"iris, wine, glass, and vowel. Those problems had already been tested in [27]. From Statlog collection we choose all multi-class datasets: <b>vehicle</b>  segment, dna, satimage, letter, and shuttle. Note that except problem dna we scale all training data to be in [-1, 1]. Then test data are adjusted using the same linear transformation.", "mykey":1297},
 {"datasetID":98, "supportID":"81BCEE95A323170579DE2013114C54475378AF52", "rexaID":"f624e93bd6b670bc3dc31925c1c885b538131534", "author":"Chih-Wei Hsu and Cheng-Ru Lin", "title":"A Comparison of Methods for Multi-class Support Vector Machines", "venue":"Department of Computer Science and Information Engineering National Taiwan University", "year":"", "window":"iris, wine, glass, and vowel. Those problems had already been tested in [27]. From <b>Statlog</b> collection we choose all multi-class datasets: vehicle, segment, dna, satimage, letter, and shuttle. Note that except problem dna we scale all training data to be in [-1, 1]. Then test data are adjusted using the same linear transformation.", "mykey":1298},
 {"datasetID":109, "supportID":"81BCEE95A323170579DE2013114C54475378AF52", "rexaID":"f624e93bd6b670bc3dc31925c1c885b538131534", "author":"Chih-Wei Hsu and Cheng-Ru Lin", "title":"A Comparison of Methods for Multi-class Support Vector Machines", "venue":"Department of Computer Science and Information Engineering National Taiwan University", "year":"", "window":"section we present experimental results on several problems from the Statlog collection [20] and the UCI Repository of machine learning databases [1]. From UCI Repository we choose the following datasets: iris, <b>wine</b>  glass, and vowel. Those problems had already been tested in [27]. From Statlog collection we choose all multi-class datasets: vehicle, segment, dna, satimage, letter, and shuttle. Note", "mykey":1299},
 {"datasetID":9, "supportID":"81CFEC9792E3C838407EDA64D716D13C610284B8", "rexaID":"ae50055cb7507aa6cad70a2adf7d0f55a25268c7", "author":"Zhi-Hua Zhou and Shifu Chen and Zhaoqian Chen", "title":"A Statistics Based Approach for Extracting Priority Rules from Trained Neural Networks", "venue":"IJCNN (3)", "year":"2000", "window":"80 2 19 19 0 UCI-Iris Plant 150 3 4 0 4 UCI-Lung Cancer 27 3 56 56 0 IS Fault Diagnosis 352 10 22 8 14 Table 3. Comparison of STARE and Crave & Shavlik's approach Rule number Test set fidelity Data set CS94 STARE CS94 STARE UCI-1985 <b>Auto</b> Imports 27 36 93.1% 100% UCI-Credit Screening 32 38 96.6% 98.3% UCI-Hepatitis 11 12 92.5% 100% UCI-Iris Plant 10 13 92.0% 97.3% UCI-Lung Cancer 7 8 92.6% 100% IS", "mykey":1300},
 {"datasetID":151, "supportID":"820E5427D5D8D1CA509AA08D4387187DABE4175E", "rexaID":"dc4f2babc5ca4534b435280aec32f5816ddb53b0", "author":"Stavros J. Perantonis and Vassilis Virvilis", "title":"Input Feature Extraction for Multilayered Perceptrons Using Supervised Principal Component Analysis", "venue":"Neural Processing Letters, 10", "year":"1999", "window":"[15]. It comprises 768 patterns taken from patients who may show signs of diabetes. Each sample is described by 8 attributes. 4. The  <b>sonar</b> Targets\" dataset [16]. The task is to distinguish between sonar returns from a metal cylinder and sonar returns from a cylindrically shaped rock. The set comprises 208 patterns with 60 features for each pattern. For", "mykey":1301},
 {"datasetID":34, "supportID":"820E5427D5D8D1CA509AA08D4387187DABE4175E", "rexaID":"dc4f2babc5ca4534b435280aec32f5816ddb53b0", "author":"Stavros J. Perantonis and Vassilis Virvilis", "title":"Input Feature Extraction for Multilayered Perceptrons Using Supervised Principal Component Analysis", "venue":"Neural Processing Letters, 10", "year":"1999", "window":"the basis of 6 attributes originating from blood test results and daily alcohol consumption figures. The set comprises 345 patterns with 6 features for each pattern. 3. the \"Pima Indians <b>Diabetes</b>  data set [15]. It comprises 768 patterns taken from patients who may show signs of diabetes. Each sample is described by 8 attributes. 4. The \"Sonar Targets\" dataset [16]. The task is to distinguish between", "mykey":1302},
 {"datasetID":52, "supportID":"820E5427D5D8D1CA509AA08D4387187DABE4175E", "rexaID":"dc4f2babc5ca4534b435280aec32f5816ddb53b0", "author":"Stavros J. Perantonis and Vassilis Virvilis", "title":"Input Feature Extraction for Multilayered Perceptrons Using Supervised Principal Component Analysis", "venue":"Neural Processing Letters, 10", "year":"1999", "window":"Real World Examples: We give results concerning four supervised learning examples from the University of California-Irvine machine learning repository [13], namely 8 1. the  <b>Ionosphere</b>  data set [14]. Here the task is to distinguish between two sets of radar returns from the ionosphere. This set comprises 351 patterns with 33 features for each pattern. 2. the \"BUPA Liver Disorders\" set. The", "mykey":1303},
 {"datasetID":79, "supportID":"820E5427D5D8D1CA509AA08D4387187DABE4175E", "rexaID":"dc4f2babc5ca4534b435280aec32f5816ddb53b0", "author":"Stavros J. Perantonis and Vassilis Virvilis", "title":"Input Feature Extraction for Multilayered Perceptrons Using Supervised Principal Component Analysis", "venue":"Neural Processing Letters, 10", "year":"1999", "window":"the basis of 6 attributes originating from blood test results and daily alcohol consumption figures. The set comprises 345 patterns with 6 features for each pattern. 3. the  <b>Pima</b> <b>Indians</b> <b>Diabetes</b>  data set [15]. It comprises 768 patterns taken from patients who may show signs of diabetes. Each sample is described by 8 attributes. 4. The \"Sonar Targets\" dataset [16]. The task is to distinguish between", "mykey":1304},
 {"datasetID":34, "supportID":"821D0F140D9DA49EE1D765307DBD744395E175C2", "rexaID":"5a08d3d508c193c4964f1f3f66bdcb78b98a905b", "author":"Ilya Blayvas and Ron Kimmel", "title":"INVITED PAPER Special Issue on Multiresolution Analysis Machine Learning via Multiresolution Approximation", "venue":"", "year":"", "window":"problems with huge training sets seems to be important family of problems, it was hard to find such training sets in public databases. Our method was tested on the Pima Indians <b>Diabetes</b> dataset [4], a large artificial dataset generated with the DatGen program [14] and the Forest Cover Type data set. The results were compared to [3], [8], [9], [11], [12]. 3.1 Pima Indians Dataset This is an", "mykey":1305},
 {"datasetID":79, "supportID":"821D0F140D9DA49EE1D765307DBD744395E175C2", "rexaID":"5a08d3d508c193c4964f1f3f66bdcb78b98a905b", "author":"Ilya Blayvas and Ron Kimmel", "title":"INVITED PAPER Special Issue on Multiresolution Analysis Machine Learning via Multiresolution Approximation", "venue":"", "year":"", "window":"problems with huge training sets seems to be important family of problems, it was hard to find such training sets in public databases. Our method was tested on the <b>Pima</b> <b>Indians</b> <b>Diabetes</b> dataset [4], a large artificial dataset generated with the DatGen program [14] and the Forest Cover Type data set. The results were compared to [3], [8], [9], [11], [12]. 3.1 Pima Indians Dataset This is an", "mykey":1306},
 {"datasetID":34, "supportID":"8298C9DB47FF31E1D28063938B15CA0453C6AB1D", "rexaID":"19d5e6db33eb1a6c31be626538ea656f79e901b1", "author":"Matthias Scherf and W. Brauer", "title":"Feature Selection by Means of a Feature Weighting Approach", "venue":"GSF - National Research Center for Environment and Health", "year":"", "window":"of feature f2g. The application of the RBF-DDA lead to an approximately equal result, i.e. 97% classification accuracy with 152 RBF nodes. Pima Indians <b>Diabetes</b> Data Base The Pima Indians Diabetes data set contains 768 instances with 8 real valued features. The underlying task is to decide, whether an at least 21 year old female of Pima Indian heritage shows signs of diabetes according to World Health", "mykey":1307},
 {"datasetID":79, "supportID":"8298C9DB47FF31E1D28063938B15CA0453C6AB1D", "rexaID":"19d5e6db33eb1a6c31be626538ea656f79e901b1", "author":"Matthias Scherf and W. Brauer", "title":"Feature Selection by Means of a Feature Weighting Approach", "venue":"GSF - National Research Center for Environment and Health", "year":"", "window":"of feature f2g. The application of the RBF-DDA lead to an approximately equal result, i.e. 97% classification accuracy with 152 RBF nodes. <b>Pima</b> <b>Indians</b> <b>Diabetes</b> Data Base The Pima Indians Diabetes data set contains 768 instances with 8 real valued features. The underlying task is to decide, whether an at least 21 year old female of Pima Indian heritage shows signs of diabetes according to World Health", "mykey":1308},
 {"datasetID":107, "supportID":"8298C9DB47FF31E1D28063938B15CA0453C6AB1D", "rexaID":"19d5e6db33eb1a6c31be626538ea656f79e901b1", "author":"Matthias Scherf and W. Brauer", "title":"Feature Selection by Means of a Feature Weighting Approach", "venue":"GSF - National Research Center for Environment and Health", "year":"", "window":"and compare the results of EUBAFES and RELIEF-F. <b>Waveform</b> 40 The Waveform-40 data set was introduced in [3] and applied in [25] to examine how well a feature selection algorithm works in the presence of a high number of irrelevant features. The data set enfolds 300 instances with", "mykey":1309},
 {"datasetID":108, "supportID":"8298C9DB47FF31E1D28063938B15CA0453C6AB1D", "rexaID":"19d5e6db33eb1a6c31be626538ea656f79e901b1", "author":"Matthias Scherf and W. Brauer", "title":"Feature Selection by Means of a Feature Weighting Approach", "venue":"GSF - National Research Center for Environment and Health", "year":"", "window":"and compare the results of EUBAFES and RELIEF-F. <b>Waveform</b> 40 The Waveform-40 data set was introduced in [3] and applied in [25] to examine how well a feature selection algorithm works in the presence of a high number of irrelevant features. The data set enfolds 300 instances with", "mykey":1310},
 {"datasetID":109, "supportID":"8298C9DB47FF31E1D28063938B15CA0453C6AB1D", "rexaID":"19d5e6db33eb1a6c31be626538ea656f79e901b1", "author":"Matthias Scherf and W. Brauer", "title":"Feature Selection by Means of a Feature Weighting Approach", "venue":"GSF - National Research Center for Environment and Health", "year":"", "window":"0 Monks1 124 0 6 Monks2 169 0 6 Monks3 122 0 6 Parity5+5 100 0 10 Vowel 528+462 10 0 Wisconsin Breast Cancer 569 30 0 Pima-Diabetes 768 8 0 Liver Disorders 745 6 0 <b>Wine</b> 178 13 0 Table 1. Summary of data sets used reason for the choice of this special RBF network is its ability to automatically determine the number of base functions which is a measure of classifier complexity. 6.1 Critical Parameters", "mykey":1311},
 {"datasetID":1, "supportID":"82E59B02699154AF3FA45D86133372F7737D69A1", "rexaID":"44326d77510f1b8976c3d73c224e9a1cb80ecc3c", "author":"Khaled A. Alsabti and Sanjay Ranka and Vineet Singh", "title":"CLOUDS: A Decision Tree Classifier for Large Datasets", "venue":"KDD", "year":"1998", "window":"are taken from the STATLOG project, which has been a widely used benchmark in classification. 3 The  <b>Abalone</b> \" Waveform,\" and Isolet\" datasets can be found in [13]. The Synth1\" and Synth2\" datasets have been used in [15, 17] for evaluating SLIQ and SPRINT; they have been referred to as the Function2\" dataset. The main parameter of our", "mykey":1312},
 {"datasetID":54, "supportID":"82E59B02699154AF3FA45D86133372F7737D69A1", "rexaID":"44326d77510f1b8976c3d73c224e9a1cb80ecc3c", "author":"Khaled A. Alsabti and Sanjay Ranka and Vineet Singh", "title":"CLOUDS: A Decision Tree Classifier for Large Datasets", "venue":"KDD", "year":"1998", "window":"are taken from the STATLOG project, which has been a widely used benchmark in classification. 3 The Abalone,\" Waveform,\" and  <b>Isolet</b>  datasets can be found in [13]. The Synth1\" and Synth2\" datasets have been used in [15, 17] for evaluating SLIQ and SPRINT; they have been referred to as the Function2\" dataset. The main parameter of our", "mykey":1313},
 {"datasetID":148, "supportID":"82E59B02699154AF3FA45D86133372F7737D69A1", "rexaID":"44326d77510f1b8976c3d73c224e9a1cb80ecc3c", "author":"Khaled A. Alsabti and Sanjay Ranka and Vineet Singh", "title":"CLOUDS: A Decision Tree Classifier for Large Datasets", "venue":"KDD", "year":"1998", "window":"lower than most of the other points along the splitting attribute as well as other attributes. Figure 1 gives the value of the gini index along each of the nine numeric attributes of the <b>Shuttle</b> dataset. We show that the above properties can be used to develop an I/O and computationally e\u00c6cient method for estimating the split at every internal node. Experimental results on real and synthetic", "mykey":1314},
 {"datasetID":98, "supportID":"82E59B02699154AF3FA45D86133372F7737D69A1", "rexaID":"44326d77510f1b8976c3d73c224e9a1cb80ecc3c", "author":"Khaled A. Alsabti and Sanjay Ranka and Vineet Singh", "title":"CLOUDS: A Decision Tree Classifier for Large Datasets", "venue":"KDD", "year":"1998", "window":"The first four datasets are taken from the <b>STATLOG</b> project, which has been a widely used benchmark in classification. 3 The Abalone,\" Waveform,\" and Isolet\" datasets can be found in [13]. The Synth1\" and Synth2\"", "mykey":1315},
 {"datasetID":107, "supportID":"82E59B02699154AF3FA45D86133372F7737D69A1", "rexaID":"44326d77510f1b8976c3d73c224e9a1cb80ecc3c", "author":"Khaled A. Alsabti and Sanjay Ranka and Vineet Singh", "title":"CLOUDS: A Decision Tree Classifier for Large Datasets", "venue":"KDD", "year":"1998", "window":"are taken from the STATLOG project, which has been a widely used benchmark in classification. 3 The Abalone,\"  <b>Waveform</b> \" and Isolet\" datasets can be found in [13]. The Synth1\" and Synth2\" datasets have been used in [15, 17] for evaluating SLIQ and SPRINT; they have been referred to as the Function2\" dataset. The main parameter of our", "mykey":1316},
 {"datasetID":108, "supportID":"82E59B02699154AF3FA45D86133372F7737D69A1", "rexaID":"44326d77510f1b8976c3d73c224e9a1cb80ecc3c", "author":"Khaled A. Alsabti and Sanjay Ranka and Vineet Singh", "title":"CLOUDS: A Decision Tree Classifier for Large Datasets", "venue":"KDD", "year":"1998", "window":"are taken from the STATLOG project, which has been a widely used benchmark in classification. 3 The Abalone,\"  <b>Waveform</b> \" and Isolet\" datasets can be found in [13]. The Synth1\" and Synth2\" datasets have been used in [15, 17] for evaluating SLIQ and SPRINT; they have been referred to as the Function2\" dataset. The main parameter of our", "mykey":1317},
 {"datasetID":23, "supportID":"8363958134F892B60447671346B400BD4C5E35D7", "rexaID":"9b1522e8a25d84453b24d3891ac99e165d3eebaa", "author":"Ira Cohen and Fabio Gagliardi Cozman and Nicu Sebe and Marcelo Cesar Cirelo and Thomas S. Huang", "title":"Semisupervised Learning of Classifiers: Theory, Algorithms, and Their Application to Human-Computer Interaction", "venue":"IEEE Trans. Pattern Anal. Mach. Intell, 26", "year":"2004", "window":"EM-TAN can sometimes improve performance over TAN with just labeled data (Shuttle). With the <b>Chess</b> dataset, discarding the unlabeled data and using only TAN seems the best approach. We have compared two likelihood based structure learning methods (K2 and MCMC) on the same datasets as well [20], showing", "mykey":1318},
 {"datasetID":21, "supportID":"8363958134F892B60447671346B400BD4C5E35D7", "rexaID":"9b1522e8a25d84453b24d3891ac99e165d3eebaa", "author":"Ira Cohen and Fabio Gagliardi Cozman and Nicu Sebe and Marcelo Cesar Cirelo and Thomas S. Huang", "title":"Semisupervised Learning of Classifiers: Theory, Algorithms, and Their Application to Human-Computer Interaction", "venue":"IEEE Trans. Pattern Anal. Mach. Intell, 26", "year":"2004", "window":"EM-TAN can sometimes improve performance over TAN with just labeled data (Shuttle). With the <b>Chess</b> dataset, discarding the unlabeled data and using only TAN seems the best approach. We have compared two likelihood based structure learning methods (K2 and MCMC) on the same datasets as well [20], showing", "mykey":1319},
 {"datasetID":22, "supportID":"8363958134F892B60447671346B400BD4C5E35D7", "rexaID":"9b1522e8a25d84453b24d3891ac99e165d3eebaa", "author":"Ira Cohen and Fabio Gagliardi Cozman and Nicu Sebe and Marcelo Cesar Cirelo and Thomas S. Huang", "title":"Semisupervised Learning of Classifiers: Theory, Algorithms, and Their Application to Human-Computer Interaction", "venue":"IEEE Trans. Pattern Anal. Mach. Intell, 26", "year":"2004", "window":"EM-TAN can sometimes improve performance over TAN with just labeled data (Shuttle). With the <b>Chess</b> dataset, discarding the unlabeled data and using only TAN seems the best approach. We have compared two likelihood based structure learning methods (K2 and MCMC) on the same datasets as well [20], showing", "mykey":1320},
 {"datasetID":148, "supportID":"8363958134F892B60447671346B400BD4C5E35D7", "rexaID":"9b1522e8a25d84453b24d3891ac99e165d3eebaa", "author":"Ira Cohen and Fabio Gagliardi Cozman and Nicu Sebe and Marcelo Cesar Cirelo and Thomas S. Huang", "title":"Semisupervised Learning of Classifiers: Theory, Algorithms, and Their Application to Human-Computer Interaction", "venue":"IEEE Trans. Pattern Anal. Mach. Intell, 26", "year":"2004", "window":"EM-TAN can sometimes improve performance over TAN with just labeled data  <b>Shuttle</b> . With the Chess dataset, discarding the unlabeled data and using only TAN seems the best approach. We have compared two likelihood based structure learning methods (K2 and MCMC) on the same datasets as well [20], showing", "mykey":1321},
 {"datasetID":151, "supportID":"836E900DDF25DB5E6CC27AEEABBE00A76346C02B", "rexaID":"9918eecfef7c8bd9df06505cbdf9c6bdd860dab9", "author":"ESEARCH R and D. R. Ort and Perry Moerland and E. Fiesler and I. Ubarretxena-Belandia", "title":"Multilayer Perceptrons for Optical Implementation", "venue":"Optical Engineering, ol", "year":"", "window":"implementation. eXclusive OR (R OR) The training set consists of the boolean exclusive OR function. It is the classical example of a simple problem that is not linearly separable [12]. <b>Sonar</b> This data set was originally used by R. Gorman and T. Sejnowski in their study of the classification of sonar signals using a neural network. The task is to discriminate between sonar signals bounced off a metal", "mykey":1322},
 {"datasetID":2, "supportID":"83D49E5BDC76A6E8B712564E95ECED8BCC92A55B", "rexaID":"cefc3da70b90693b8046b574392c86d2db1f196a", "author":"S. V. N Vishwanathan and Alexander J. Smola and M. Narasimha Murty", "title":"considerably faster than competing methods such as Sequential Minimal Optimization or the Nearest Point Algorithm", "venue":"Machine Learning Program, National ICT for Australia", "year":"", "window":"was proposed by Alexis Wieland of MITRE Corporation and it is available from the CMU Artificial Intelligence repository. Both WSPBC and the <b>Adult</b> datasets are available from the UCI Machine Learning repository (Blake & Merz, 1998). We used the same values of # 2 as in Keerthi 3 It is a common misperception that different SV optimization algorithms", "mykey":1323},
 {"datasetID":90, "supportID":"8437B2826F8E86B9EA3F5C77003859DD0484549D", "rexaID":"aac8eec290538d2a1731b2b4f92c6bf401a3df8a", "author":"Perry Moerland", "title":"A Comparison of Mixture Models for Density Estimation", "venue":"IDIAP", "year":"", "window":"The raw data has been pre-processed in various ways. First of all, the ordinal inputs have been normalized to have zero mean and unit standard deviation on the training data. For the  <b>soybean</b>  data set part of the inputs are categorical and these are mapped to a 1-of-c coding, thus increasing the number of attributes (see the fifth column of Table 1). Finally, for the data sets indicated with a #,", "mykey":1324},
 {"datasetID":91, "supportID":"8437B2826F8E86B9EA3F5C77003859DD0484549D", "rexaID":"aac8eec290538d2a1731b2b4f92c6bf401a3df8a", "author":"Perry Moerland", "title":"A Comparison of Mixture Models for Density Estimation", "venue":"IDIAP", "year":"", "window":"The raw data has been pre-processed in various ways. First of all, the ordinal inputs have been normalized to have zero mean and unit standard deviation on the training data. For the  <b>soybean</b>  data set part of the inputs are categorical and these are mapped to a 1-of-c coding, thus increasing the number of attributes (see the fifth column of Table 1). Finally, for the data sets indicated with a #,", "mykey":1325},
 {"datasetID":81, "supportID":"846F2FB0EFC4381DD06EE50FA903C9938B2FA0DD", "rexaID":"f357844bd41649705c6dd963cf4b64ada94e001b", "author":"Georg Thimm and Emile Fiesler", "title":"IDIAP Technical report High Order and Multilayer Perceptron Initialization", "venue":"IEEE Transactions", "year":"1994", "window":"in the same row or the same column in the image. This configuration should allow the extraction of sufficient features to learn the <b>digits</b>  Training sessions on the in section 4.1 described digits data set gave an acceptable <b>recognition</b> of untrained digits, despite the small training 7 Non10E-5 0.001 0.01 0.1 10E-6 10 10E-4 1 Convergence Time log( Initial Weight Variance ) B gence A ConverFigure 2:", "mykey":1326},
 {"datasetID":87, "supportID":"846F2FB0EFC4381DD06EE50FA903C9938B2FA0DD", "rexaID":"f357844bd41649705c6dd963cf4b64ada94e001b", "author":"Georg Thimm and Emile Fiesler", "title":"IDIAP Technical report High Order and Multilayer Perceptron Initialization", "venue":"IEEE Transactions", "year":"1994", "window":"itself has a large influence on the optimal initial weight variance: for the solar, wine, and <b>servo</b> data sets, the networks have about the same size for the same order, but the optimal value for the weight variance differs a lot for the network with the logistic 11 0.01 0.1 1 10E-4 0.001 10E-5 10E-6", "mykey":1327},
 {"datasetID":109, "supportID":"846F2FB0EFC4381DD06EE50FA903C9938B2FA0DD", "rexaID":"f357844bd41649705c6dd963cf4b64ada94e001b", "author":"Georg Thimm and Emile Fiesler", "title":"IDIAP Technical report High Order and Multilayer Perceptron Initialization", "venue":"IEEE Transactions", "year":"1994", "window":"itself has a large influence on the optimal initial weight variance: for the solar, <b>wine</b>  and servo data sets, the networks have about the same size for the same order, but the optimal value for the weight variance differs a lot for the network with the logistic 11 0.01 0.1 1 10E-4 0.001 10E-5 10E-6", "mykey":1328},
 {"datasetID":148, "supportID":"84A9EB9AA681B0EEF24C996849ABD592187A42A6", "rexaID":"4dac55f2daa66b00a555405664b6f275370782ac", "author":"Haixun Wang and Carlo Zaniolo", "title":"CMP: A Fast Decision Tree Classifier Using Multivariate Predictions", "venue":"ICDE", "year":"2000", "window":"might fall. Figure 2 shows a hypothetical gini curve, and three alive intervals (shaded areas in the figure). Our experiments of the estimation method are summarized in Table 1. The first four small datasets (Letter, Satimage, Segment and <b>Shuttle</b>  in the table are from the STATLOG project[6], and the two large datasets (Function 2 and Function 7) are synthetic datasets described in [5]. In these test", "mykey":1329},
 {"datasetID":98, "supportID":"84A9EB9AA681B0EEF24C996849ABD592187A42A6", "rexaID":"4dac55f2daa66b00a555405664b6f275370782ac", "author":"Haixun Wang and Carlo Zaniolo", "title":"CMP: A Fast Decision Tree Classifier Using Multivariate Predictions", "venue":"ICDE", "year":"2000", "window":"(Letter, Satimage, Segment and Shuttle) in the table are from the <b>STATLOG</b> project[6], and the two large datasets (Function 2 and Function 7) are synthetic datasets described in [5]. In these test cases there were at most N = 2 alive intervals: i) the one whose left boundary (or right boundary, depending on", "mykey":1330},
 {"datasetID":155, "supportID":"84D58B0385515EA36E879662E4EBB69AAA6C9927", "rexaID":"4bcdbeaa8f46590fca87d041c66b18a6622b82b3", "author":"Kristiaan Pelckmans and Jos De Brabanter and J. A. K Suykens and Bart De Moor and K. U. Leuven - ESAT", "title":"The Differogram: Non-parametric Noise Variance Estimation and its Use for Model Selection", "venue":"SCDSISTA", "year":"2004", "window":"the tuning parameters of SVMs, see e.g. (Vapnik, 1998; Cherkassky and Ma, 2004). 6 Experiments Figure 1 and 2 show artificially generated data and the differogram <b>cloud</b> of a linear and nonlinear toy dataset respectively. The latter example was taken from (Wahba, 1990) using the function f(x) = 4.26(e -x -4e -2x +3e -3x ) such that y i = f(x i )+e i 21 for all i = 1, . . . , N . The random white noise", "mykey":1331},
 {"datasetID":48, "supportID":"84D58B0385515EA36E879662E4EBB69AAA6C9927", "rexaID":"4bcdbeaa8f46590fca87d041c66b18a6622b82b3", "author":"Kristiaan Pelckmans and Jos De Brabanter and J. A. K Suykens and Bart De Moor and K. U. Leuven - ESAT", "title":"The Differogram: Non-parametric Noise Variance Estimation and its Use for Model Selection", "venue":"SCDSISTA", "year":"2004", "window":"can therefor be used for picking good startingvalues for a local search based on a more powerful and computationally intensive way to achieve a good generalization performance. The Boston <b>housing</b> dataset (Blake and Merz, 1998) concerning the housing values in suburbs of Boston was used to benchmark the proposed method on a real world dataset. This set contains 506 instances of 12 continuous and 1", "mykey":1332},
 {"datasetID":14, "supportID":"84E683FAF2CD08B20401EF2471DB2B85B44F2BB7", "rexaID":"edc7361f3edbcbb2421477d257bf1305834c5c61", "author":"Sherrie L. W and Zijian Zheng", "title":"A BENCHMARK FOR CLASSIFIER LEARNING", "venue":"Basser Department of Computer Science The University of Sydney", "year":"", "window":"in the first quartile of all the candidate values. Large and high mean in the last quartile of all the candidate values. Medium means between the small (or low) and the large (or high). Table 1. Datasets in the benchmark Name Description <b>Breast</b> <b>Cancer</b> (W) Medical diagnosis applied to breast cytology (Wisconsin) Diabetes Pima Indians diabetes database for diagnosing diabetes Hepatitis Predicting", "mykey":1333},
 {"datasetID":150, "supportID":"84E683FAF2CD08B20401EF2471DB2B85B44F2BB7", "rexaID":"edc7361f3edbcbb2421477d257bf1305834c5c61", "author":"Sherrie L. W and Zijian Zheng", "title":"A BENCHMARK FOR CLASSIFIER LEARNING", "venue":"Basser Department of Computer Science The University of Sydney", "year":"", "window":"e.g. Promoter (106) and Lymphography (148) ffl Medium (between 210 and 3170), e.g. Diabetes (768) and Thyroid (3163) ffl Large (more than 3170), e.g. <b>NetTalk</b> (Phoneme) (5438) and Mushroom (8124) 8. Dataset density (3 values): Usually a classifier learning algorithm can learn a more accurate theory from a larger number of training examples than from fewer examples. However, because different domains", "mykey":1334},
 {"datasetID":73, "supportID":"84E683FAF2CD08B20401EF2471DB2B85B44F2BB7", "rexaID":"edc7361f3edbcbb2421477d257bf1305834c5c61", "author":"Sherrie L. W and Zijian Zheng", "title":"A BENCHMARK FOR CLASSIFIER LEARNING", "venue":"Basser Department of Computer Science The University of Sydney", "year":"", "window":"e.g. Promoter (106) and Lymphography (148) ffl Medium (between 210 and 3170), e.g. Diabetes (768) and Thyroid (3163) ffl Large (more than 3170), e.g. NetTalk (Phoneme) (5438) and <b>Mushroom</b> (8124) 8. Dataset density (3 values): Usually a classifier learning algorithm can learn a more accurate theory from a larger number of training examples than from fewer examples. However, because different domains", "mykey":1335},
 {"datasetID":90, "supportID":"84E683FAF2CD08B20401EF2471DB2B85B44F2BB7", "rexaID":"edc7361f3edbcbb2421477d257bf1305834c5c61", "author":"Sherrie L. W and Zijian Zheng", "title":"A BENCHMARK FOR CLASSIFIER LEARNING", "venue":"Basser Department of Computer Science The University of Sydney", "year":"", "window":"None, e.g. Lymphography and NetTalk (Phoneme) ffl Few (between 0 and 5.6%), e.g. Mushroom (1.39%) and Breast Cancer (W) (0.25%) ffl Many (more than 5.6%), e.g. <b>Soybean</b> (9.78%) and Thyroid (6.74%) 7. Dataset size (3 values): ffl Small (less than 210), e.g. Promoter (106) and Lymphography (148) ffl Medium (between 210 and 3170), e.g. Diabetes (768) and Thyroid (3163) ffl Large (more than 3170), e.g.", "mykey":1336},
 {"datasetID":91, "supportID":"84E683FAF2CD08B20401EF2471DB2B85B44F2BB7", "rexaID":"edc7361f3edbcbb2421477d257bf1305834c5c61", "author":"Sherrie L. W and Zijian Zheng", "title":"A BENCHMARK FOR CLASSIFIER LEARNING", "venue":"Basser Department of Computer Science The University of Sydney", "year":"", "window":"None, e.g. Lymphography and NetTalk (Phoneme) ffl Few (between 0 and 5.6%), e.g. Mushroom (1.39%) and Breast Cancer (W) (0.25%) ffl Many (more than 5.6%), e.g. <b>Soybean</b> (9.78%) and Thyroid (6.74%) 7. Dataset size (3 values): ffl Small (less than 210), e.g. Promoter (106) and Lymphography (148) ffl Medium (between 210 and 3170), e.g. Diabetes (768) and Thyroid (3163) ffl Large (more than 3170), e.g.", "mykey":1337},
 {"datasetID":102, "supportID":"84E683FAF2CD08B20401EF2471DB2B85B44F2BB7", "rexaID":"edc7361f3edbcbb2421477d257bf1305834c5c61", "author":"Sherrie L. W and Zijian Zheng", "title":"A BENCHMARK FOR CLASSIFIER LEARNING", "venue":"Basser Department of Computer Science The University of Sydney", "year":"", "window":"None, e.g. Lymphography and NetTalk (Phoneme) ffl Few (between 0 and 5.6%), e.g. Mushroom (1.39%) and Breast Cancer (W) (0.25%) ffl Many (more than 5.6%), e.g. Soybean (9.78%) and <b>Thyroid</b> (6.74%) 7. Dataset size (3 values): ffl Small (less than 210), e.g. Promoter (106) and Lymphography (148) ffl Medium (between 210 and 3170), e.g. Diabetes (768) and Thyroid (3163) ffl Large (more than 3170), e.g.", "mykey":1338},
 {"datasetID":42, "supportID":"866FFDF4A75BB5A8C0DFE89C86FE1954D9E74C3E", "rexaID":"945d8ba4c7eacfaed696aaf0bd72fd576efa78d5", "author":"Aynur Akku and H. Altay Guvenir", "title":"Weighting Features in k Nearest Neighbor Classification on Feature Projections", "venue":"Department of Computer Engineering and Information Science Bilkent University", "year":"", "window":"weight to features by this method on realworld taken from the UCI repository (Murphy, 1995). The results of this experiments will be presented in Section 4. 5 Table 1: Comparison on some real-world datasets. Data Set: bcancerw cleveland <b>glass</b> hungarian ionosphere iris liver wine No. of Instances 273 303 214 294 351 150 345 178 No. of Features 9 13 9 13 34 4 6 13 No. of Classes 2 2 6 2 2 3 2 3 No. of", "mykey":1339},
 {"datasetID":52, "supportID":"866FFDF4A75BB5A8C0DFE89C86FE1954D9E74C3E", "rexaID":"945d8ba4c7eacfaed696aaf0bd72fd576efa78d5", "author":"Aynur Akku and H. Altay Guvenir", "title":"Weighting Features in k Nearest Neighbor Classification on Feature Projections", "venue":"Department of Computer Engineering and Information Science Bilkent University", "year":"", "window":"Data Set: bcancerw cleveland glass hungarian <b>ionosphere</b> iris liver wine No. of Instances 273 303 214 294 351 150 345 178 No. of Features 9 13 9 13 34 4 6 13 No. of Classes 2 2 6 2 2 3 2 3 No. of Missing", "mykey":1340},
 {"datasetID":53, "supportID":"866FFDF4A75BB5A8C0DFE89C86FE1954D9E74C3E", "rexaID":"945d8ba4c7eacfaed696aaf0bd72fd576efa78d5", "author":"Aynur Akku and H. Altay Guvenir", "title":"Weighting Features in k Nearest Neighbor Classification on Feature Projections", "venue":"Department of Computer Engineering and Information Science Bilkent University", "year":"", "window":"significantly. This should be because all the features are equally relevant. On the cleveland, liver, <b>iris</b> and glass (except k = 1) datasets, the weights learned by the individual accuracies always performed significantly better than the others. The weight learning method based on the homogeneity performed better than the other on the", "mykey":1341},
 {"datasetID":60, "supportID":"866FFDF4A75BB5A8C0DFE89C86FE1954D9E74C3E", "rexaID":"945d8ba4c7eacfaed696aaf0bd72fd576efa78d5", "author":"Aynur Akku and H. Altay Guvenir", "title":"Weighting Features in k Nearest Neighbor Classification on Feature Projections", "venue":"Department of Computer Engineering and Information Science Bilkent University", "year":"", "window":"significantly. This should be because all the features are equally relevant. On the cleveland, <b>liver</b>  iris and glass (except k = 1) datasets, the weights learned by the individual accuracies always performed significantly better than the others. The weight learning method based on the homogeneity performed better than the other on the", "mykey":1342},
 {"datasetID":109, "supportID":"866FFDF4A75BB5A8C0DFE89C86FE1954D9E74C3E", "rexaID":"945d8ba4c7eacfaed696aaf0bd72fd576efa78d5", "author":"Aynur Akku and H. Altay Guvenir", "title":"Weighting Features in k Nearest Neighbor Classification on Feature Projections", "venue":"Department of Computer Engineering and Information Science Bilkent University", "year":"", "window":"except k = 1. There were no significant difference between the two weight learning algorithms on the <b>wine</b> dataset. 7 5 Conclusions A version of the famous k-NN algorithm, that stores the classification knowledge as the projections of the training instances on the features, called k-NNFP algorithm, had been", "mykey":1343},
 {"datasetID":14, "supportID":"86BF781F5FEE0F2F73524D39904D4BBCABA5243C", "rexaID":"eef68e57188bfabe4dff279cf32efe545bd7fe3c", "author":"Fei Sha and Lawrence K. Saul and Daniel D. Lee", "title":"Multiplicative Updates for Nonnegative Quadratic Programming in Support Vector Machines", "venue":"NIPS", "year":"2002", "window":"Kernel Polynomial Radial Data k=4 k=6 #=0.3 #=1.0 #=3.0 Sonar 9.6% 9.6% 7.6% 6.7% 10.6% <b>breast</b> <b>cancer</b> 5.1% 3.6% 4.4% 4.4% 4.4% Table 1: Misclassification error rates on the sonar and breast cancer data sets after 512 iterations of the multiplicative updates. 3.1 Multiplicative updates The loss function in eq. (6) is a special case of eq. (1) with A ij = y i y j K(x i , x j ) and b i =- 1. Thus, the", "mykey":1344},
 {"datasetID":151, "supportID":"86BF781F5FEE0F2F73524D39904D4BBCABA5243C", "rexaID":"eef68e57188bfabe4dff279cf32efe545bd7fe3c", "author":"Fei Sha and Lawrence K. Saul and Daniel D. Lee", "title":"Multiplicative Updates for Nonnegative Quadratic Programming in Support Vector Machines", "venue":"NIPS", "year":"2002", "window":"Kernel Polynomial Radial Data k=4 k=6 #=0.3 #=1.0 #=3.0 <b>sonar</b> 9.6% 9.6% 7.6% 6.7% 10.6% Breast cancer 5.1% 3.6% 4.4% 4.4% 4.4% Table 1: Misclassification error rates on the sonar and breast cancer data sets after 512 iterations of the multiplicative updates. 3.1 Multiplicative updates The loss function in eq. (6) is a special case of eq. (1) with A ij = y i y j K(x i , x j ) and b i =- 1. Thus, the", "mykey":1345},
 {"datasetID":30, "supportID":"86C6D7774769E70C1A1CA4E2A1AA18A71F03DC1A", "rexaID":"2a4fe2f7469eef9d4cc985d6caaa0afe249facf3", "author":"Soumya Ray and David Page", "title":"Generalized Skewing for Functions with Continuous and Nominal Attributes", "venue":"Department of Computer Sciences and Department of Biostatistics and Medical Informatics, University of Wis", "year":"", "window":"1500 1000 500 Accuracy (%) Training Sample Size Gain/Generalized Skewing Gain Figure 11. Hard Nominal Targets, 100-v examples that a subfunction in the target (or the target itself) may be hard. The datasets we use are: <b>Contraceptive</b> Method Choice (CMC), German Credit (German), Cleveland Heart Disease (Heart), voting-records (Voting), pima-indians-diabetes (Diabetes), breast-cancerwisconsin (BCW),", "mykey":1346},
 {"datasetID":1, "supportID":"876648730F2CE199B9368ED5DF5B6A6838C48DA0", "rexaID":"da329267bf8880c2becb15eae121a5b002347349", "author":"Rong-En Fan and P. -H Chen and C. -J Lin", "title":"Working Set Selection Using the Second Order Information for Training SVM", "venue":"Department of Computer Science and Information Engineering National Taiwan University", "year":"", "window":"incorporates the shrinking technique. 20 0 0.2 0.4 0.6 0.8 1 1.2 1.4 image splice tree a1a australian breast-cancer diabetes fourclass german.numer w1a <b>abalone</b> cadata cpusmall space_ga mg Ratio Data sets time (40M cache) time (100K cache) total #iter 0 0.2 0.4 0.6 0.8 1 1.2 1.4 image splice tree a1a australian breast-cancer diabetes fourclass german.numer w1a abalone cadata cpusmall space_ga mg", "mykey":1347},
 {"datasetID":2, "supportID":"876648730F2CE199B9368ED5DF5B6A6838C48DA0", "rexaID":"da329267bf8880c2becb15eae121a5b002347349", "author":"Rong-En Fan and P. -H Chen and C. -J Lin", "title":"Working Set Selection Using the Second Order Information for Training SVM", "venue":"Department of Computer Science and Information Engineering National Taiwan University", "year":"", "window":"image, diabetes, covtype, breast-cancer, and abalone are from the UCI machine learning repository (Blake and Merz, 1998). Problems a1a and a9a are compiled in (Platt, 1998) from the UCI  <b>adult</b>  data set. Problems w1a and w8a are also from (Platt, 1998). The tree data set was originally used in (Bailey et al., 1993). The problem mg is a Mackey-Glass time series. The data sets cpusmall and splice are", "mykey":1348},
 {"datasetID":14, "supportID":"876648730F2CE199B9368ED5DF5B6A6838C48DA0", "rexaID":"da329267bf8880c2becb15eae121a5b002347349", "author":"Rong-En Fan and P. -H Chen and C. -J Lin", "title":"Working Set Selection Using the Second Order Information for Training SVM", "venue":"Department of Computer Science and Information Engineering National Taiwan University", "year":"", "window":"The data sets image, diabetes, covtype, <b>breast</b> <b>cancer</b>  and abalone are from the UCI machine learning repository (Blake and Merz, 1998). Problems a1a and a9a are compiled in (Platt, 1998) from the UCI \"adult\"", "mykey":1349},
 {"datasetID":34, "supportID":"876648730F2CE199B9368ED5DF5B6A6838C48DA0", "rexaID":"da329267bf8880c2becb15eae121a5b002347349", "author":"Rong-En Fan and P. -H Chen and C. -J Lin", "title":"Working Set Selection Using the Second Order Information for Training SVM", "venue":"Department of Computer Science and Information Engineering National Taiwan University", "year":"", "window":"Data statistics are in Tables 1 and 3. Problems german.numer and australian are from the Statlog collection (Michie et al., 1994). We select space ga and cadata from StatLib (http://lib.stat.cmu.edu/datasets). The data sets image, <b>diabetes</b>  covtype, breast-cancer, and abalone are from the UCI machine learning repository (Blake and Merz, 1998). Problems a1a and a9a are compiled in (Platt, 1998) from the", "mykey":1350},
 {"datasetID":42, "supportID":"876648730F2CE199B9368ED5DF5B6A6838C48DA0", "rexaID":"da329267bf8880c2becb15eae121a5b002347349", "author":"Rong-En Fan and P. -H Chen and C. -J Lin", "title":"Working Set Selection Using the Second Order Information for Training SVM", "venue":"Department of Computer Science and Information Engineering National Taiwan University", "year":"", "window":"was originally used in (Bailey et al., 1993). The problem mg is a Mackey <b>Glass</b> time series. The data sets cpusmall and splice are from the Delve archive (http://www.cs.toronto.edu/~delve). Problem fourclass is from (Ho and Kleinberg, 1996) and we further transform it to a two-class set. The problem", "mykey":1351},
 {"datasetID":69, "supportID":"876648730F2CE199B9368ED5DF5B6A6838C48DA0", "rexaID":"da329267bf8880c2becb15eae121a5b002347349", "author":"Rong-En Fan and P. -H Chen and C. -J Lin", "title":"Working Set Selection Using the Second Order Information for Training SVM", "venue":"Department of Computer Science and Information Engineering National Taiwan University", "year":"", "window":"was originally used in (Bailey et al., 1993). The problem mg is a Mackey-Glass time series. The data sets cpusmall and <b>splice</b> are from the Delve archive (http://www.cs.toronto.edu/~delve). Problem fourclass is from (Ho and Kleinberg, 1996) and we further transform it to a two-class set. The problem", "mykey":1352},
 {"datasetID":98, "supportID":"876648730F2CE199B9368ED5DF5B6A6838C48DA0", "rexaID":"da329267bf8880c2becb15eae121a5b002347349", "author":"Rong-En Fan and P. -H Chen and C. -J Lin", "title":"Working Set Selection Using the Second Order Information for Training SVM", "venue":"Department of Computer Science and Information Engineering National Taiwan University", "year":"", "window":"D. J. Spiegelhalter, and C. C. Taylor. Machine Learning, Neural and Statistical Classification. Prentice Hall, Englewood Cliffs, N.J., 1994. Data available at http://www.ncc.up.pt/liacc/ML <b>statlog</b> datasets.html. E. Osuna, R. Freund, and F. Girosi. Training support vector machines: An application to face detection. In Proceedings of CVPR'97, pages 130--136, New York, NY, 1997. IEEE. Laura Palagi and", "mykey":1353},
 {"datasetID":2, "supportID":"8780573935919B75F6A4A96C6D7672C4383A4935", "rexaID":"ef65a20ee05551cc411c8acd0c5af4796dc1a39e", "author":"Ahmed Hussain Khan and Intensive Care", "title":"Multiplier-Free Feedforward Networks", "venue":"174", "year":"", "window":"network configuration, and many hundreds for promising ones. Test data results reported here represent the best performance of the optimal configurations. A. Forecasting the Onset of Diabetes This data set 5 is related to a group of <b>adult</b> women belonging to the Pima Indian tribe and was collected by the US National Institute of Diabetes and Digestive and Kidney Diseases [1]. The learning task is to", "mykey":1354},
 {"datasetID":34, "supportID":"8780573935919B75F6A4A96C6D7672C4383A4935", "rexaID":"ef65a20ee05551cc411c8acd0c5af4796dc1a39e", "author":"Ahmed Hussain Khan and Intensive Care", "title":"Multiplier-Free Feedforward Networks", "venue":"174", "year":"", "window":"forward-pass capability. It differs from the conventional model in restricting its synapses to the set{- 1, 0, 1} while allowing unrestricted offsets. Simulation results on the `onset of <b>diabetes</b>  data set and a handwritten numeral recognition database indicate that the new network, despite having strong constraints on its synapses, has a generalization performance similar to that of its conventional", "mykey":1355},
 {"datasetID":81, "supportID":"8780573935919B75F6A4A96C6D7672C4383A4935", "rexaID":"ef65a20ee05551cc411c8acd0c5af4796dc1a39e", "author":"Ahmed Hussain Khan and Intensive Care", "title":"Multiplier-Free Feedforward Networks", "venue":"174", "year":"", "window":"forward-pass capability. It differs from the conventional model in restricting its synapses to the set{- 1, 0, 1} while allowing unrestricted offsets. Simulation results on the `onset of diabetes' data set and a <b>handwritten</b> numeral <b>recognition</b> database indicate that the new network, despite having strong constraints on its synapses, has a generalization performance similar to that of its conventional", "mykey":1356},
 {"datasetID":10, "supportID":"8835F7B012847327754EDB4F22548EE5EAA47335", "rexaID":"e746c17201da2dd72583f7b9b0c2a6ba310412f4", "author":"Geraldine E. Rosario and Elke A. Rundensteiner and David C. Brown and Matthew O. Ward", "title":"Mapping Nominal Values to Numbers for Effective Visualization", "venue":"INFOVIS", "year":"2003", "window":"strength of association between two nominal variables)? In general, which quantification do you feel is better (easier to understand, more believable ordering and spacing)? 7.2.1 <b>Automobile</b> Data Set Case Study We chose the Automobile Data Set because it is easy to interpret. Figures 12, 13 and 14 display the quantified versions of selected variables in a Parallel Coordinates display. In", "mykey":1357},
 {"datasetID":42, "supportID":"889A8004320A03C92562EDE922E80B7E6A70C63E", "rexaID":"a9190b213f00d51e490ba40125b46f31d04d77ca", "author":"Pramod Viswanath and M. Narasimha Murty and Shalabh Bhatnagar", "title":"Partition Based Pattern Synthesis Technique with Efficient Algorithms for Nearest Neighbor Classification", "venue":"Department of Computer Science and Automation, Indian Institute of Science", "year":"", "window":"We performed experiments with five different datasets, viz., OCR, WINE, VOWEL, THYROID, <b>GLASS</b> and PENDIGITS, respectively. Except the OCR dataset, all others are from the UCI Repository [19]. OCR dataset is also used in [20,18]. The properties of the", "mykey":1358},
 {"datasetID":102, "supportID":"889A8004320A03C92562EDE922E80B7E6A70C63E", "rexaID":"a9190b213f00d51e490ba40125b46f31d04d77ca", "author":"Pramod Viswanath and M. Narasimha Murty and Shalabh Bhatnagar", "title":"Partition Based Pattern Synthesis Technique with Efficient Algorithms for Nearest Neighbor Classification", "venue":"Department of Computer Science and Automation, Indian Institute of Science", "year":"", "window":"We performed experiments with five different datasets, viz., OCR, WINE, VOWEL, <b>THYROID</b>  GLASS and PENDIGITS, respectively. Except the OCR dataset, all others are from the UCI Repository [19]. OCR dataset is also used in [20,18]. The properties of the", "mykey":1359},
 {"datasetID":109, "supportID":"889A8004320A03C92562EDE922E80B7E6A70C63E", "rexaID":"a9190b213f00d51e490ba40125b46f31d04d77ca", "author":"Pramod Viswanath and M. Narasimha Murty and Shalabh Bhatnagar", "title":"Partition Based Pattern Synthesis Technique with Efficient Algorithms for Nearest Neighbor Classification", "venue":"Department of Computer Science and Automation, Indian Institute of Science", "year":"", "window":"We performed experiments with five different datasets, viz., OCR, <b>WINE</b>  VOWEL, THYROID, GLASS and PENDIGITS, respectively. Except the OCR dataset, all others are from the UCI Repository [19]. OCR dataset is also used in [20,18]. The properties of the", "mykey":1360},
 {"datasetID":14, "supportID":"88BC70B95532CAF1AC6E0CE99D0E3DFAB30135FF", "rexaID":"50b9babac023b426f1531a4265076bea534d121e", "author":"Adil M. Bagirov and Alex Rubinov and A. N. Soukhojak and John Yearwood", "title":"Unsupervised and supervised data classification via nonsmooth and global optimization", "venue":"School of Information Technology and Mathematical Sciences, The University of Ballarat", "year":"", "window":"The Australian credit dataset, the Wisconsin <b>breast</b> <b>cancer</b> dataset, the diabetes dataset, the heart disease dataset and the liver-disorder dataset have been used in numerical experiments. The description of these datasets can be", "mykey":1361},
 {"datasetID":17, "supportID":"88BC70B95532CAF1AC6E0CE99D0E3DFAB30135FF", "rexaID":"50b9babac023b426f1531a4265076bea534d121e", "author":"Adil M. Bagirov and Alex Rubinov and A. N. Soukhojak and John Yearwood", "title":"Unsupervised and supervised data classification via nonsmooth and global optimization", "venue":"School of Information Technology and Mathematical Sciences, The University of Ballarat", "year":"", "window":"The Australian credit dataset, the <b>Wisconsin</b> <b>breast</b> <b>cancer</b> dataset, the diabetes dataset, the heart disease dataset and the liver-disorder dataset have been used in numerical experiments. The description of these datasets can be", "mykey":1362},
 {"datasetID":15, "supportID":"88BC70B95532CAF1AC6E0CE99D0E3DFAB30135FF", "rexaID":"50b9babac023b426f1531a4265076bea534d121e", "author":"Adil M. Bagirov and Alex Rubinov and A. N. Soukhojak and John Yearwood", "title":"Unsupervised and supervised data classification via nonsmooth and global optimization", "venue":"School of Information Technology and Mathematical Sciences, The University of Ballarat", "year":"", "window":"The Australian credit dataset, the <b>Wisconsin</b> <b>breast</b> <b>cancer</b> dataset, the diabetes dataset, the heart disease dataset and the liver-disorder dataset have been used in numerical experiments. The description of these datasets can be", "mykey":1363},
 {"datasetID":16, "supportID":"88BC70B95532CAF1AC6E0CE99D0E3DFAB30135FF", "rexaID":"50b9babac023b426f1531a4265076bea534d121e", "author":"Adil M. Bagirov and Alex Rubinov and A. N. Soukhojak and John Yearwood", "title":"Unsupervised and supervised data classification via nonsmooth and global optimization", "venue":"School of Information Technology and Mathematical Sciences, The University of Ballarat", "year":"", "window":"The Australian credit dataset, the <b>Wisconsin</b> <b>breast</b> <b>cancer</b> dataset, the diabetes dataset, the heart disease dataset and the liver-disorder dataset have been used in numerical experiments. The description of these datasets can be", "mykey":1364},
 {"datasetID":34, "supportID":"88BC70B95532CAF1AC6E0CE99D0E3DFAB30135FF", "rexaID":"50b9babac023b426f1531a4265076bea534d121e", "author":"Adil M. Bagirov and Alex Rubinov and A. N. Soukhojak and John Yearwood", "title":"Unsupervised and supervised data classification via nonsmooth and global optimization", "venue":"School of Information Technology and Mathematical Sciences, The University of Ballarat", "year":"", "window":"local optimization (Discrete gradient method, see Section 4). For testing the efficiency of the combination of k-means and Discrete Gradient method, we use four well-known medium-size test datasets: Australian credit dataset, <b>Diabetes</b> dataset, Liver disorder dataset and Vehicle dataset. The description of these datasets can be found in Appendix. We studied these datasets, using different", "mykey":1365},
 {"datasetID":45, "supportID":"88BC70B95532CAF1AC6E0CE99D0E3DFAB30135FF", "rexaID":"50b9babac023b426f1531a4265076bea534d121e", "author":"Adil M. Bagirov and Alex Rubinov and A. N. Soukhojak and John Yearwood", "title":"Unsupervised and supervised data classification via nonsmooth and global optimization", "venue":"School of Information Technology and Mathematical Sciences, The University of Ballarat", "year":"", "window":"1.0 7 152/297 26.73 1.5 6 122/297 14.43 2.0 5 107/297 8.25 4.0 5 65/297 5.05 6.0 5 41/297 3.34 8.0 5 28/297 3.22 The results presented in Table 9 show that appropriate values for the <b>heart</b> disease dataset are c 2 [0, 1.5], because further decrease in c leads to changes in the cluster structure of the dataset. We can again see that these values of c allow significant reduction in the number of", "mykey":1366},
 {"datasetID":50, "supportID":"88BC70B95532CAF1AC6E0CE99D0E3DFAB30135FF", "rexaID":"50b9babac023b426f1531a4265076bea534d121e", "author":"Adil M. Bagirov and Alex Rubinov and A. N. Soukhojak and John Yearwood", "title":"Unsupervised and supervised data classification via nonsmooth and global optimization", "venue":"School of Information Technology and Mathematical Sciences, The University of Ballarat", "year":"", "window":"some databases with known classes. We used the diabetes, liver disorder, heart disease, breast cancer, vehicles, synthetic, penbased recognition of handwritten digits (PBRHD) and <b>image segmentation</b> datasets in numerical experiments. Descriptions of these datasets can be found in Appendix. First, we normalized all features. This was done by a nonsingular matrix so that mean values of all features were", "mykey":1367},
 {"datasetID":60, "supportID":"88BC70B95532CAF1AC6E0CE99D0E3DFAB30135FF", "rexaID":"50b9babac023b426f1531a4265076bea534d121e", "author":"Adil M. Bagirov and Alex Rubinov and A. N. Soukhojak and John Yearwood", "title":"Unsupervised and supervised data classification via nonsmooth and global optimization", "venue":"School of Information Technology and Mathematical Sciences, The University of Ballarat", "year":"", "window":"Australian credit dataset, Diabetes dataset, <b>Liver</b> disorder dataset and Vehicle dataset. The description of these datasets can be found in Appendix. We studied these datasets, using different subsets of features and", "mykey":1368},
 {"datasetID":81, "supportID":"88BC70B95532CAF1AC6E0CE99D0E3DFAB30135FF", "rexaID":"50b9babac023b426f1531a4265076bea534d121e", "author":"Adil M. Bagirov and Alex Rubinov and A. N. Soukhojak and John Yearwood", "title":"Unsupervised and supervised data classification via nonsmooth and global optimization", "venue":"School of Information Technology and Mathematical Sciences, The University of Ballarat", "year":"", "window":"was donated by Richard S. Forsyth BUPA Medical research Ltd. It contains 2 classes, 345 observations and 6 attributes. Pen-based <b>recognition</b> of <b>handwritten</b> <b>digits</b> (Pendig) This dataset was introduced by E. Alpaydin and Fevzi Alimoglu. It contains 10 classes, 10992 observations, 16 attributes. All input attributes are integers 1. . . 100. Satellite image (SatIm, image segmentation)", "mykey":1369},
 {"datasetID":143, "supportID":"88BC70B95532CAF1AC6E0CE99D0E3DFAB30135FF", "rexaID":"50b9babac023b426f1531a4265076bea534d121e", "author":"Adil M. Bagirov and Alex Rubinov and A. N. Soukhojak and John Yearwood", "title":"Unsupervised and supervised data classification via nonsmooth and global optimization", "venue":"School of Information Technology and Mathematical Sciences, The University of Ballarat", "year":"", "window":"local optimization (Discrete gradient method, see Section 4). For testing the efficiency of the combination of k-means and Discrete Gradient method, we use four well-known medium-size test datasets: <b>Australian credit</b> dataset, Diabetes dataset, Liver disorder dataset and Vehicle dataset. The description of these datasets can be found in Appendix. We studied these datasets, using different", "mykey":1370},
 {"datasetID":147, "supportID":"88BC70B95532CAF1AC6E0CE99D0E3DFAB30135FF", "rexaID":"50b9babac023b426f1531a4265076bea534d121e", "author":"Adil M. Bagirov and Alex Rubinov and A. N. Soukhojak and John Yearwood", "title":"Unsupervised and supervised data classification via nonsmooth and global optimization", "venue":"School of Information Technology and Mathematical Sciences, The University of Ballarat", "year":"", "window":"some databases with known classes. We used the diabetes, liver disorder, heart disease, breast cancer, vehicles, synthetic, penbased recognition of handwritten digits (PBRHD) and <b>image segmentation</b> datasets in numerical experiments. Descriptions of these datasets can be found in Appendix. First, we normalized all features. This was done by a nonsingular matrix so that mean values of all features were", "mykey":1371},
 {"datasetID":149, "supportID":"88BC70B95532CAF1AC6E0CE99D0E3DFAB30135FF", "rexaID":"50b9babac023b426f1531a4265076bea534d121e", "author":"Adil M. Bagirov and Alex Rubinov and A. N. Soukhojak and John Yearwood", "title":"Unsupervised and supervised data classification via nonsmooth and global optimization", "venue":"School of Information Technology and Mathematical Sciences, The University of Ballarat", "year":"", "window":"Diabetes dataset, Liver disorder dataset and <b>Vehicle</b> dataset. The description of these datasets can be found in Appendix. We studied these datasets, using different subsets of features and different numbers of", "mykey":1372},
 {"datasetID":23, "supportID":"8914A2145499CD47886E07AA76F0F7EAED40D3C6", "rexaID":"c334718c7e09d2671197a8c9526018de1d816903", "author":"Adam J. Grove and Dale Schuurmans", "title":"Boosting in the Limit: Maximizing the Margin of Learned Ensembles", "venue":"AAAI/IAAI", "year":"1998", "window":"that this depends crucially on the base learner always being able to find a sufficiently good hypothesis if one exists; see Section 5 for further discussion of this issue. 8 However, for some large data sets, <b>chess</b> and splice, we inverted the train/test proportions. FindAttrTest Adaboost LP-Adaboost DualLPboost Data set error% win% error% margin error% win% margin error% win% margin Audiology 52.30", "mykey":1373},
 {"datasetID":21, "supportID":"8914A2145499CD47886E07AA76F0F7EAED40D3C6", "rexaID":"c334718c7e09d2671197a8c9526018de1d816903", "author":"Adam J. Grove and Dale Schuurmans", "title":"Boosting in the Limit: Maximizing the Margin of Learned Ensembles", "venue":"AAAI/IAAI", "year":"1998", "window":"that this depends crucially on the base learner always being able to find a sufficiently good hypothesis if one exists; see Section 5 for further discussion of this issue. 8 However, for some large data sets, <b>chess</b> and splice, we inverted the train/test proportions. FindAttrTest Adaboost LP-Adaboost DualLPboost Data set error% win% error% margin error% win% margin error% win% margin Audiology 52.30", "mykey":1374},
 {"datasetID":22, "supportID":"8914A2145499CD47886E07AA76F0F7EAED40D3C6", "rexaID":"c334718c7e09d2671197a8c9526018de1d816903", "author":"Adam J. Grove and Dale Schuurmans", "title":"Boosting in the Limit: Maximizing the Margin of Learned Ensembles", "venue":"AAAI/IAAI", "year":"1998", "window":"that this depends crucially on the base learner always being able to find a sufficiently good hypothesis if one exists; see Section 5 for further discussion of this issue. 8 However, for some large data sets, <b>chess</b> and splice, we inverted the train/test proportions. FindAttrTest Adaboost LP-Adaboost DualLPboost Data set error% win% error% margin error% win% margin error% win% margin Audiology 52.30", "mykey":1375},
 {"datasetID":69, "supportID":"8914A2145499CD47886E07AA76F0F7EAED40D3C6", "rexaID":"c334718c7e09d2671197a8c9526018de1d816903", "author":"Adam J. Grove and Dale Schuurmans", "title":"Boosting in the Limit: Maximizing the Margin of Learned Ensembles", "venue":"AAAI/IAAI", "year":"1998", "window":"that this depends crucially on the base learner always being able to find a sufficiently good hypothesis if one exists; see Section 5 for further discussion of this issue. 8 However, for some large data sets, chess and <b>splice</b>  we inverted the train/test proportions. FindAttrTest Adaboost LP-Adaboost DualLPboost Data set error% win% error% margin error% win% margin error% win% margin Audiology 52.30", "mykey":1376},
 {"datasetID":105, "supportID":"892794895B20068969F03AD3D5FD62F2E710353D", "rexaID":"dad1e8c3efe3862d65266fb0d04ff7a8ef116f0f", "author":"Eui-Hong Han and George Karypis and Vipin Kumar and Bamshad Mobasher", "title":"Clustering Based On Association Rule Hypergraphs", "venue":"DMKD", "year":"1997", "window":"the cluster which has the highest score with respect to that transaction. We performed clustering of transactions on 1984 United States Congressional <b>Voting Records</b> Database provided by [MM96]. The data set includes 435 transactions each corresponding to one Congressman's votes on 16 key issues. We removed class values from each transaction, and we followed the steps specified in Section 2.1 to", "mykey":1377},
 {"datasetID":57, "supportID":"897B1AA01208B2103F4CD858628A0BFBD8221927", "rexaID":"bad113cebe2984b00706b843e7540f280cfb59e1", "author":"Ramon Sangesa and Ulises Cortes", "title":"Possibilistic Conditional Dependency, Similarity and Information Measures: an application to causal network recovery", "venue":"Departament de Llenguatges i Sistemes Informtics Departament de Llenguatges i Sistemes Informtics Technical University of Catalonia Technical University of Catalonia", "year":"", "window":"from the UCI Machine Learning Database Repository [16]: ALARM [2], and LED [11]. It has also been applied to data coming from a real sensor measurements of a Wastewater Treatment Plant [22]. The <b>LED dataset</b> represents a faulty LED device where a certain button (a variable with eight values indicating which display to set on) should set a led device ON. Some of the devices react incorrectly to this", "mykey":1378},
 {"datasetID":148, "supportID":"89C017078A7B9C9D4A4455BD88F8F9D429BE25B4", "rexaID":"af8d7cc40c6d9dd6d1b261e68d612c371055b313", "author":"Jeffrey P. Bradford and Clayton Kunz and Ron Kohavi and Clifford Brunk and Carla Brodley", "title":"Appears in ECML-98 as a research note Pruning Decision Trees with Misclassification Costs", "venue":"School of Electrical Engineering", "year":"", "window":"classification based on census bureau data), breast cancer diagnosis, chess, crx (credit), german (credit), pima diabetes, road (dirt), satellite images, <b>shuttle</b>  and vehicle. In choosing the datasets, we decided on the following desiderata: 1. Datasets should be two-class to make the evaluation easier. This desideratum was hard to satisfy and we resorted to converting several multi-class", "mykey":1379},
 {"datasetID":149, "supportID":"89C017078A7B9C9D4A4455BD88F8F9D429BE25B4", "rexaID":"af8d7cc40c6d9dd6d1b261e68d612c371055b313", "author":"Jeffrey P. Bradford and Clayton Kunz and Ron Kohavi and Clifford Brunk and Carla Brodley", "title":"Appears in ECML-98 as a research note Pruning Decision Trees with Misclassification Costs", "venue":"School of Electrical Engineering", "year":"", "window":"classification based on census bureau data), breast cancer diagnosis, chess, crx (credit), german (credit), pima diabetes, road (dirt), satellite images, shuttle, and <b>vehicle</b>  In choosing the datasets, we decided on the following desiderata: 1. Datasets should be two-class to make the evaluation easier. This desideratum was hard to satisfy and we resorted to converting several multi-class", "mykey":1380},
 {"datasetID":154, "supportID":"89EB57295BF213AE44C2AB4E88C69F8F3958D6E1", "rexaID":"e949e51c587c01cc64b362cb4fedfaa7dcf5dd68", "author":"Michihiro Kuramochi and George Karypis", "title":"Finding Frequent Patterns in a Large Sparse Graph", "venue":"SDM", "year":"2004", "window":"\"5\". Finally, because some of the vertices in the resulting graph had a very high degree (i.e., authorities and hubs), we kept only the vertices whose degree was less or equal to 15. The Contact Map dataset is made of 170 proteins from the <b>Protein</b> Data Bank [5] with pairwise sequence identity lower than 25%. The vertices in these graphs correspond to the different amino acids and the edges connect two", "mykey":1381},
 {"datasetID":155, "supportID":"8A070118793051ED47394A317202EB7C6844CD81", "rexaID":"379f34d22ceaf54a97c9a95f0cb0e6eb2a44c1dc", "author":"Cesar Guerra-Salcedo and Stephen Chen and Darrell Whitley and Sarah Smith", "title":"Fast and Accurate Feature Selection Using Hybrid Genetic Strategies", "venue":"Department of Computer Science Colorado State University", "year":"", "window":"(LandSat), a DNA classification dataset and a <b>Cloud</b> classification dataset. On the other hand, the artificially generated classification problem rely on a LED identification problem. LED cases are artificially generated using a test case", "mykey":1382},
 {"datasetID":67, "supportID":"8A070118793051ED47394A317202EB7C6844CD81", "rexaID":"379f34d22ceaf54a97c9a95f0cb0e6eb2a44c1dc", "author":"Cesar Guerra-Salcedo and Stephen Chen and Darrell Whitley and Sarah Smith", "title":"Fast and Accurate Feature Selection Using Hybrid Genetic Strategies", "venue":"Department of Computer Science Colorado State University", "year":"", "window":"were employed and one artificially generated classification problem. The real-world classification problems are: satellite classification dataset (LandSat), a <b>DNA</b> classification dataset and a Cloud classification dataset. On the other hand, the artificially generated classification problem rely on a LED identification problem. LED cases are", "mykey":1383},
 {"datasetID":69, "supportID":"8A070118793051ED47394A317202EB7C6844CD81", "rexaID":"379f34d22ceaf54a97c9a95f0cb0e6eb2a44c1dc", "author":"Cesar Guerra-Salcedo and Stephen Chen and Darrell Whitley and Sarah Smith", "title":"Fast and Accurate Feature Selection Using Hybrid Genetic Strategies", "venue":"Department of Computer Science Colorado State University", "year":"", "window":"is a DNA dataset. The dataset represents Primate <b <b>splice</b> junction</b> gene sequences (DNA). There are 2000 training cases, 1186 test cases, and 180 binary features for each case. Three different classes exist in this", "mykey":1384},
 {"datasetID":146, "supportID":"8A070118793051ED47394A317202EB7C6844CD81", "rexaID":"379f34d22ceaf54a97c9a95f0cb0e6eb2a44c1dc", "author":"Cesar Guerra-Salcedo and Stephen Chen and Darrell Whitley and Sarah Smith", "title":"Fast and Accurate Feature Selection Using Hybrid Genetic Strategies", "venue":"Department of Computer Science Colorado State University", "year":"", "window":"were employed and one artificially generated classification problem. The real-world classification problems are: <b>satellite</b> classification dataset  <b>LandSat</b> , a DNA classification dataset and a Cloud classification dataset. On the other hand, the artificially generated classification problem rely on a LED identification problem. LED cases are", "mykey":1385},
 {"datasetID":53, "supportID":"8A178D2A7AAFC4E87CA76F486406ABE4CE41A8ED", "rexaID":"674344e880b3ca3ce47332c3d53ed9554b661f8b", "author":"Wl odzisl/aw Duch and Karol Grudzinski", "title":"Prototype based rules - a new way to understand the data", "venue":"Department of Computer Methods, Nicholas Copernicus University", "year":"", "window":"were extracted recently [2]. For comparison we have analyzed some of these dataset here. <b>Iris</b> flowers data, taken from the UCI repository [14], has been used in many previous studies. It contains 3 classes (Iris Setosa, Virginica and Versicolor flowers), 4 attributes (sepal and", "mykey":1386},
 {"datasetID":2, "supportID":"8AAFF7E62C4CE8724298A276A56AC519C804BB74", "rexaID":"46a760d333f90e12eb326d2b17c90aa70c01d1df", "author":"John C. Platt", "title":"Using Analytic QP and Sparseness to Speed Training of Support Vector Machines", "venue":"NIPS", "year":"1998", "window":"can be found in [8, 7]. The first test set is the UCI <b>Adult</b> data set [5]. The SVM is given 14 attributes of a census form of a household and asked to predict whether that household has an income greater than $50,000. Out of the 14 attributes, eight are categorical", "mykey":1387},
 {"datasetID":20, "supportID":"8AAFF7E62C4CE8724298A276A56AC519C804BB74", "rexaID":"46a760d333f90e12eb326d2b17c90aa70c01d1df", "author":"John C. Platt", "title":"Using Analytic QP and Sparseness to Speed Training of Support Vector Machines", "venue":"NIPS", "year":"1998", "window":"can be found in [8, 7]. The first test set is the UCI Adult data set [5]. The SVM is given 14 attributes of a <b>census</b> form of a household and asked to predict whether that household has an income greater than $50,000. Out of the 14 attributes, eight are categorical", "mykey":1388},
 {"datasetID":2, "supportID":"8BADC381362646E0849AACBD9449B5D1124E858B", "rexaID":"0767f0171b0f74f96978a41f3e947ea91cc9dbd3", "author":"Alexander J. Smola and Vishy Vishwanathan and Eleazar Eskin", "title":"Laplace Propagation", "venue":"NIPS", "year":"2003", "window":"X i h# i , #i, (17) with the joint minimizer being the average of the individual solutions. 5 Experiments To test our ideas we performed a set of experiments with the widely available Web and <b>Adult</b> datasets from the UCI repository [1]. All experiments were performed on a 2.4 MHz Intel Xeon machine with 1 GB RAM using MATLAB R13. We used a RBF kernel with # 2 = 10 [8], to obtain comparable results. We", "mykey":1389},
 {"datasetID":42, "supportID":"8BADD7F945EF2413390D2FEBB4F9737992137E86", "rexaID":"63460ee5afd1e104ba91e654e9aee9b977433c4c", "author":"S. Augustine Su and Jennifer G. Dy", "title":"Automated hierarchical mixtures of probabilistic principal component analyzers", "venue":"ICML", "year":"2004", "window":"5 wine -3-8 .478 .627 8 2 .623 .722 4 Interestingly, the mixture of PPCA approach is not always better than PCA+EM. Mixtures of PPCA performed better than PCA+EM in terms of NMI and FM on most small datasets (toy, oil, and <b>glass</b> , which are well modeled by mixtures of Gaussians. PPCA with fewer clusters has a comparable performance with EM + PCA on the large data sets (optical digits, satellite image,", "mykey":1390},
 {"datasetID":146, "supportID":"8BADD7F945EF2413390D2FEBB4F9737992137E86", "rexaID":"63460ee5afd1e104ba91e654e9aee9b977433c4c", "author":"S. Augustine Su and Jennifer G. Dy", "title":"Automated hierarchical mixtures of probabilistic principal component analyzers", "venue":"ICML", "year":"2004", "window":"(toy, oil, and glass), which are well modeled by mixtures of Gaussians. PPCA with fewer clusters has a comparable performance with EM + PCA on the large data sets (optical digits, <b>satellite</b> image, segment), except for the letter data, where PPCA performed better. Finally, mixtures of PPCA are worse than PCA+EM on the chart and wine data. Upon closer", "mykey":1391},
 {"datasetID":1, "supportID":"8C3750B994826A11F8D7E8D602D298D8E0F2DCCE", "rexaID":"7637452ffb3440dd348caafed1512ce844300dac", "author":"Shai Fine and Katya Scheinberg", "title":"Incremental Learning and Selective Sampling via Parametric Optimization Framework for SVM", "venue":"NIPS", "year":"2001", "window":"bias is most likely far away from the global solution, and as such, the results presented here should be regarded as a lower bound. We examined performances on a moderate size problem, the <b>Abalone</b> data set from the UCI Repository [2]. We fed the training algorithm with increasing subsets up to the whole set (of size 4177). The gender encoding (male/female/infant) was mapped into", "mykey":1392},
 {"datasetID":19, "supportID":"8D64C051CF022841541BEA3ED92C41B2098BD9EA", "rexaID":"7e787ada7263180d2a9bad6a3c490e7f8b0d4cd0", "author":"Daniel J. Lizotte and Omid Madani and Russell Greiner", "title":"Budgeted Learning, Part II: The Na#ve-Bayes Case", "venue":"Department of Computing Science University of Alberta", "year":"", "window":"SFL at 50 queries is better than its performance at 50 when the budget is set at 300. Other policies do not take the budget into account. We have observed the same overall patterns on several other datasets that we have tested the policies so far  <b>CAR</b>  DIABETES, CHESS, BREAST): the performance of SFL is superior or comparable to the performance of other policies, and BIASED ROBIN is the best algorithm", "mykey":1393},
 {"datasetID":105, "supportID":"8D64C051CF022841541BEA3ED92C41B2098BD9EA", "rexaID":"7e787ada7263180d2a9bad6a3c490e7f8b0d4cd0", "author":"Daniel J. Lizotte and Omid Madani and Russell Greiner", "title":"Budgeted Learning, Part II: The Na#ve-Bayes Case", "venue":"Department of Computing Science University of Alberta", "year":"", "window":"problem with nine features that can take on between two and five values. The relative performances of the policies are closer to each other, but their behaviour is similar to Figure 4(a). The <b>votes dataset</b> (Figure 4(c)) is a binary class problem (whether or not republican), with 16 binary features, 435 instances, and a positive class probability of 0:61. In the votes dataset, there is a high", "mykey":1394},
 {"datasetID":73, "supportID":"8D64C051CF022841541BEA3ED92C41B2098BD9EA", "rexaID":"7e787ada7263180d2a9bad6a3c490e7f8b0d4cd0", "author":"Daniel J. Lizotte and Omid Madani and Russell Greiner", "title":"Budgeted Learning, Part II: The Na#ve-Bayes Case", "venue":"Department of Computing Science University of Alberta", "year":"", "window":"from the UCI Machine Learning Repository [BM98]. These plots show cross validation error (20% of the dataset) on the <b>mushroom</b> and votes datasets of the different policies. Each point is an average of 50 trials where in each trial a random balanced partition of classes were made for training and validation.", "mykey":1395},
 {"datasetID":76, "supportID":"8D64C051CF022841541BEA3ED92C41B2098BD9EA", "rexaID":"7e787ada7263180d2a9bad6a3c490e7f8b0d4cd0", "author":"Daniel J. Lizotte and Omid Madani and Russell Greiner", "title":"Budgeted Learning, Part II: The Na#ve-Bayes Case", "venue":"Department of Computing Science University of Alberta", "year":"", "window":"75 times, while non-discriminative features such a feature 18 is queried an average of 2 times only. For some budgets, the 0/1 error of SFL is nearly half that generated by round robin. The <b>nursery</b> dataset (Figure 4(b)) is a five class problem with nine features that can take on between two and five values. The relative performances of the policies are closer to each other, but their behaviour is", "mykey":1396},
 {"datasetID":51, "supportID":"8D937890A8A59B7C87B1F9096F3F16EB27EA6873", "rexaID":"f38cf28d327cab22e1f3c138ae648f6c6776a2d7", "author":"Sergio A. Alvarez and Takeshi Kawato and Carolina Ruiz", "title":"Mining over loosely coupled data sources using neural experts", "venue":"Computer Science Dept. Boston College", "year":"", "window":"of mining over multiple data sources by applying a mixture of attribute experts ANN to the problem of detecting advertisments in images embedded in web documents, using the <b>Internet</b> <b>Advertisements</b> dataset from the UCI Machine Learning Repository [4]. We conclude with a discussion of our results and suggestions for future work. 2. ARTIFICIAL NEURAL NETWORKS Artificial neural networks (ANN) are models", "mykey":1397},
 {"datasetID":53, "supportID":"8E24EBCCC27CAFD99635AF43C47257F02DAE690E", "rexaID":"7d87efd12b445262eff46191ede868722a2be569", "author":"Geoffrey Holmes and Leonard E. Trigg", "title":"A Diagnostic Tool for Tree Based Supervised Classification Learning Algorithms", "venue":"Department of Computer Science University of Waikato Hamilton New Zealand", "year":"", "window":"difference by the range of the tested attribute, giving the formula: cost = | v 1 - v 2 | max a 1 -min a 1 Figure 2 illustrates the problem for case 4 with an example taken from the familiar <b>iris</b> dataset. The minimum cost edit sequence to transform the tree on the left involves deleting the non-root Petal width nodes and their rightmost leaf nodes (giving a cost of 4). We are left with two trees", "mykey":1398},
 {"datasetID":14, "supportID":"8F3C952DA5B26AF4D092034012E9826624E1C2BC", "rexaID":"d342517262ff52ffd3566bd8f520b36723486aa3", "author":"M. A. Galway and Michael G. Madden", "title":"DEPARTMENT OF INFORMATION TECHNOLOGY technical report NUIG-IT-011002 Evaluation of the Performance of the Markov Blanket Bayesian Classifier Algorithm", "venue":"Department of Information Technology National University of Ireland, Galway", "year":"", "window":"and all four are equally good on the <b>Breast</b> <b>Cancer</b> dataset. Na\u00efve TAN K2 MBBC Chess 87.63\u00b1 1.61 91.68\u00b1 1.09 94.03\u00b1 0.87 97.03\u00b1 0.54 WBCD 97.81\u00b1 0.51 97.47\u00b1 0.68 97.17\u00b1 1.05 97.30\u00b1 1.01 LED-24 73.28\u00b1 0.70 73.18\u00b1 0.63 73.14\u00b1 0.73 73.14\u00b1 0.73 DNA 94.80\u00b1 0.44", "mykey":1399},
 {"datasetID":23, "supportID":"8F3C952DA5B26AF4D092034012E9826624E1C2BC", "rexaID":"d342517262ff52ffd3566bd8f520b36723486aa3", "author":"M. A. Galway and Michael G. Madden", "title":"DEPARTMENT OF INFORMATION TECHNOLOGY technical report NUIG-IT-011002 Evaluation of the Performance of the Markov Blanket Bayesian Classifier Algorithm", "venue":"Department of Information Technology National University of Ireland, Galway", "year":"", "window":"were selected that had these characteristics and that were not very small. The datasets are listed in Table 1. Dataset #I #A <b>Chess</b> (King & Rook vs King & Pawn) 3196 32 Wisconsin Breast Cancer Diagnosis 699 9 LED-24 (17 irrelevant attributes) 3200 24 DNA: Splice Junction Gene Sequences", "mykey":1400},
 {"datasetID":21, "supportID":"8F3C952DA5B26AF4D092034012E9826624E1C2BC", "rexaID":"d342517262ff52ffd3566bd8f520b36723486aa3", "author":"M. A. Galway and Michael G. Madden", "title":"DEPARTMENT OF INFORMATION TECHNOLOGY technical report NUIG-IT-011002 Evaluation of the Performance of the Markov Blanket Bayesian Classifier Algorithm", "venue":"Department of Information Technology National University of Ireland, Galway", "year":"", "window":"were selected that had these characteristics and that were not very small. The datasets are listed in Table 1. Dataset #I #A <b>Chess</b> (King & Rook vs King & Pawn) 3196 32 Wisconsin Breast Cancer Diagnosis 699 9 LED-24 (17 irrelevant attributes) 3200 24 DNA: Splice Junction Gene Sequences", "mykey":1401},
 {"datasetID":22, "supportID":"8F3C952DA5B26AF4D092034012E9826624E1C2BC", "rexaID":"d342517262ff52ffd3566bd8f520b36723486aa3", "author":"M. A. Galway and Michael G. Madden", "title":"DEPARTMENT OF INFORMATION TECHNOLOGY technical report NUIG-IT-011002 Evaluation of the Performance of the Markov Blanket Bayesian Classifier Algorithm", "venue":"Department of Information Technology National University of Ireland, Galway", "year":"", "window":"were selected that had these characteristics and that were not very small. The datasets are listed in Table 1. Dataset #I #A <b>Chess</b> (King & Rook vs King & Pawn) 3196 32 Wisconsin Breast Cancer Diagnosis 699 9 LED-24 (17 irrelevant attributes) 3200 24 DNA: Splice Junction Gene Sequences", "mykey":1402},
 {"datasetID":63, "supportID":"8F3C952DA5B26AF4D092034012E9826624E1C2BC", "rexaID":"d342517262ff52ffd3566bd8f520b36723486aa3", "author":"M. A. Galway and Michael G. Madden", "title":"DEPARTMENT OF INFORMATION TECHNOLOGY technical report NUIG-IT-011002 Evaluation of the Performance of the Markov Blanket Bayesian Classifier Algorithm", "venue":"Department of Information Technology National University of Ireland, Galway", "year":"", "window":"with the fewest instances, this procedure was repeated 10 times. For the SPECT and <b>Lymphography</b> datasets, the procedure was repeated 50 times to reduce variability. Prediction accuracy results and standard deviations are reported in Table 2. Following usual conventions, for each dataset the algorithm", "mykey":1403},
 {"datasetID":67, "supportID":"8F3C952DA5B26AF4D092034012E9826624E1C2BC", "rexaID":"d342517262ff52ffd3566bd8f520b36723486aa3", "author":"M. A. Galway and Michael G. Madden", "title":"DEPARTMENT OF INFORMATION TECHNOLOGY technical report NUIG-IT-011002 Evaluation of the Performance of the Markov Blanket Bayesian Classifier Algorithm", "venue":"Department of Information Technology National University of Ireland, Galway", "year":"", "window":"(based on a paired T-test at the 99% confidence level) and they outperform the other algorithms, they are both highlighted in bold. For example, K2 and MBBC are both best on the <b>DNA</b> Splice dataset and all four are equally good on the Breast Cancer dataset. Na\u00efve TAN K2 MBBC Chess 87.63\u00b1 1.61 91.68\u00b1 1.09 94.03\u00b1 0.87 97.03\u00b1 0.54 WBCD 97.81\u00b1 0.51 97.47\u00b1 0.68 97.17\u00b1 1.05 97.30\u00b1 1.01 LED-24 73.28\u00b1", "mykey":1404},
 {"datasetID":69, "supportID":"8F3C952DA5B26AF4D092034012E9826624E1C2BC", "rexaID":"d342517262ff52ffd3566bd8f520b36723486aa3", "author":"M. A. Galway and Michael G. Madden", "title":"DEPARTMENT OF INFORMATION TECHNOLOGY technical report NUIG-IT-011002 Evaluation of the Performance of the Markov Blanket Bayesian Classifier Algorithm", "venue":"Department of Information Technology National University of Ireland, Galway", "year":"", "window":"(based on a paired T-test at the 99% confidence level) and they outperform the other algorithms, they are both highlighted in bold. For example, K2 and MBBC are both best on the DNA <b>Splice</b> dataset and all four are equally good on the Breast Cancer dataset. Na\u00efve TAN K2 MBBC Chess 87.63\u00b1 1.61 91.68\u00b1 1.09 94.03\u00b1 0.87 97.03\u00b1 0.54 WBCD 97.81\u00b1 0.51 97.47\u00b1 0.68 97.17\u00b1 1.05 97.30\u00b1 1.01 LED-24 73.28\u00b1", "mykey":1405},
 {"datasetID":76, "supportID":"8F3C952DA5B26AF4D092034012E9826624E1C2BC", "rexaID":"d342517262ff52ffd3566bd8f520b36723486aa3", "author":"M. A. Galway and Michael G. Madden", "title":"DEPARTMENT OF INFORMATION TECHNOLOGY technical report NUIG-IT-011002 Evaluation of the Performance of the Markov Blanket Bayesian Classifier Algorithm", "venue":"Department of Information Technology National University of Ireland, Galway", "year":"", "window":"one for each of the analyses described above in Section 4.2. ROC graphs are best suited to two-class problems, which all but one of the datasets are. For the <b>Nursery</b> dataset, the ROC curve is for the prediction of the `Priority' class. On a ROC graph, the point (0 0) represents the strategy of never returning a positive classification, no", "mykey":1406},
 {"datasetID":95, "supportID":"8F3C952DA5B26AF4D092034012E9826624E1C2BC", "rexaID":"d342517262ff52ffd3566bd8f520b36723486aa3", "author":"M. A. Galway and Michael G. Madden", "title":"DEPARTMENT OF INFORMATION TECHNOLOGY technical report NUIG-IT-011002 Evaluation of the Performance of the Markov Blanket Bayesian Classifier Algorithm", "venue":"Department of Information Technology National University of Ireland, Galway", "year":"", "window":"with the fewest instances, this procedure was repeated 10 times. For the <b>SPECT</b> and Lymphography datasets, the procedure was repeated 50 times to reduce variability. Prediction accuracy results and standard deviations are reported in Table 2. Following usual conventions, for each dataset the algorithm", "mykey":1407},
 {"datasetID":48, "supportID":"8F985F0F72A5C8984DAF5DD4A6B700AA883CF632", "rexaID":"45391c9343dc03f6033eba55364366eddbb693db", "author":"Predrag Radivojac and Zoran Obradovic and A. Keith Dunker and Slobodan Vucetic", "title":"Feature Selection Filters Based on the Permutation Test", "venue":"ECML", "year":"2004", "window":"summarized in Table 1. The first nine were downloaded from the UCI repository [38], with dataset <b>HOUSING</b> converted into a binary classification problem according to the mean value of the target. Datasets MAMMOGRAPHY and OIL were constructed in [39] and [40], respectively, and provided to us by", "mykey":1408},
 {"datasetID":52, "supportID":"8F985F0F72A5C8984DAF5DD4A6B700AA883CF632", "rexaID":"45391c9343dc03f6033eba55364366eddbb693db", "author":"Predrag Radivojac and Zoran Obradovic and A. Keith Dunker and Slobodan Vucetic", "title":"Feature Selection Filters Based on the Permutation Test", "venue":"ECML", "year":"2004", "window":"basic characteristics. NF and CF indicate the number of numerical and categorical features, respectively. Dataset Size Size of class 1 NF CF <b>IONOSPHERE</b> 351 225 34 0 VOTES 435 267 0 48 GLASS 214 163 9 0 HEART 303 139 6 7 LABOR 57 37 8 21 HOUSING 506 250 13 0 CREDIT 690 307 6 41 PIMA 768 268 9 0 ZOO 78 41 1 15", "mykey":1409},
 {"datasetID":9, "supportID":"8FB40528206D2A347D66E6064BD8E287FEED39C3", "rexaID":"5d88347a087b01681bec3121a13b7b29465cdc71", "author":"Mauro Birattari and Gianluca Bontempi and Hugues Bersini", "title":"Lazy Learning Meets the Recursive Least Squares Algorithm", "venue":"NIPS", "year":"1998", "window":"classical mean square error criterion: ^ y q = x 0 q ^ fi( ^ k); with ^ k = arg min k MSE(k) = arg min k P k i=1 ! i (e cv i (k)) 2 P k i=1 ! i ; (9) Table 1: A summary of the characteristics of the datasets considered. Dataset Housing Cpu Prices <b>Mpg</b> Servo Ozone Number of examples 506 209 159 392 167 330 Number of regressors 13 6 16 7 8 8 where ! i are weights than can be conveniently used to discount", "mykey":1410},
 {"datasetID":48, "supportID":"8FB40528206D2A347D66E6064BD8E287FEED39C3", "rexaID":"5d88347a087b01681bec3121a13b7b29465cdc71", "author":"Mauro Birattari and Gianluca Bontempi and Hugues Bersini", "title":"Lazy Learning Meets the Recursive Least Squares Algorithm", "venue":"NIPS", "year":"1998", "window":"classical mean square error criterion: ^ y q = x 0 q ^ fi( ^ k); with ^ k = arg min k MSE(k) = arg min k P k i=1 ! i (e cv i (k)) 2 P k i=1 ! i ; (9) Table 1: A summary of the characteristics of the datasets considered. Dataset <b>Housing</b> Cpu Prices Mpg Servo Ozone Number of examples 506 209 159 392 167 330 Number of regressors 13 6 16 7 8 8 where ! i are weights than can be conveniently used to discount", "mykey":1411},
 {"datasetID":87, "supportID":"8FB40528206D2A347D66E6064BD8E287FEED39C3", "rexaID":"5d88347a087b01681bec3121a13b7b29465cdc71", "author":"Mauro Birattari and Gianluca Bontempi and Hugues Bersini", "title":"Lazy Learning Meets the Recursive Least Squares Algorithm", "venue":"NIPS", "year":"1998", "window":"considered. Dataset Housing Cpu Prices Mpg <b>Servo</b> Ozone Number of examples 506 209 159 392 167 330 Number of regressors 13 6 16 7 8 8 where ! i are weights than can be conveniently used to discount each error according", "mykey":1412},
 {"datasetID":1, "supportID":"8FC40C2531C8D5D088AE1A3FB26965D7B6945FD7", "rexaID":"ae82a44ada49c66439b67eae7ff10392ff209df9", "author":"Christopher J. Merz", "title":"Using Correspondence Analysis to Combine Classifiers", "venue":"Machine Learning, 36", "year":"1999", "window":"number of examples; number of attributes; number of numeric attributes; number of classes; and whether missing values exist. Data Set Exs. Atts. Num. Class Missing <b>abalone</b> 4177 8 7 3 no balance 625 4 4 3 no breast 286 9 4 2 yes credit 690 15 6 2 yes dementia 118 26 26 3 no glass 214 10 10 7 no heart 303 13 6 2 yes ionosphere 351", "mykey":1413},
 {"datasetID":42, "supportID":"8FC40C2531C8D5D088AE1A3FB26965D7B6945FD7", "rexaID":"ae82a44ada49c66439b67eae7ff10392ff209df9", "author":"Christopher J. Merz", "title":"Using Correspondence Analysis to Combine Classifiers", "venue":"Machine Learning, 36", "year":"1999", "window":"The resulting weighting schemes reversed this effect by counting a vote for one of the confused classes as a vote for the other, and vice versa. PV performs well on the <b>glass</b>  lymph and wave data sets where the errors of the learned models are measured (using the statistic) to be fairly uncorrelated. Here, SCANN performs similarly to PV, but S-BP and S-Bayes (except for wave) appear to be", "mykey":1414},
 {"datasetID":14, "supportID":"8FEA143FF02CA3A980D9BEB475E94C762B6E28AB", "rexaID":"349644828ad3c8da8e1a15eee5aec0d704db34a1", "author":"Kamal Ali and Michael J. Pazzani", "title":"Error Reduction through Learning Multiple Descriptions", "venue":"Machine Learning, 24", "year":"1996", "window":"search using Likelihood Combination is able to statistically significantly (95% confidence) reduce or maintain error on all domains except the (Ljubljana) <b>breast</b> <b>cancer</b> domain. On that breast cancer data set few learning methods have been able to get an accuracy significantly higher than that obtained by guessing the most frequent class suggesting it lacks the attributes relevant for discriminating the", "mykey":1415},
 {"datasetID":45, "supportID":"8FEA143FF02CA3A980D9BEB475E94C762B6E28AB", "rexaID":"349644828ad3c8da8e1a15eee5aec0d704db34a1", "author":"Kamal Ali and Michael J. Pazzani", "title":"Error Reduction through Learning Multiple Descriptions", "venue":"Machine Learning, 24", "year":"1996", "window":"to test learned models on noise-free examples (including noisy variants of the KRK and LED domains) but for the natural domains we tested on possibly noisy examples. The large variant of the Soybean data set was used and the 5-class variant of the <b>Heart</b> data set was used. 5.1. Does using multiple rule sets lead to lower error? In this section we present results of an experiment designed to answer the", "mykey":1416},
 {"datasetID":57, "supportID":"8FEA143FF02CA3A980D9BEB475E94C762B6E28AB", "rexaID":"349644828ad3c8da8e1a15eee5aec0d704db34a1", "author":"Kamal Ali and Michael J. Pazzani", "title":"Error Reduction through Learning Multiple Descriptions", "venue":"Machine Learning, 24", "year":"1996", "window":"also experienced significant reduction with the error being halved (for DNA this represented an increase in accuracy from 67.9% to 86.8%!). The error reduction is least for the noisy KRK and <b>LED data</b> sets and for the presumably noisy medical diagnosis data sets. Eighty percent of the data sets which scored unimpressive error ratios (above 0.8) were noisy data sets. This finding is further explored", "mykey":1417},
 {"datasetID":67, "supportID":"8FEA143FF02CA3A980D9BEB475E94C762B6E28AB", "rexaID":"349644828ad3c8da8e1a15eee5aec0d704db34a1", "author":"Kamal Ali and Michael J. Pazzani", "title":"Error Reduction through Learning Multiple Descriptions", "venue":"Machine Learning, 24", "year":"1996", "window":"(representing an increase in accuracy from 93.3% to 98.9%!) and by large (around 3 or 4) factors for LED and Tic-tactoe. The <b>molecular biology</b> data sets also experienced significant reduction with the error being halved (for DNA this represented an increase in accuracy from 67.9% to 86.8%!). The error reduction is least for the noisy KRK and LED", "mykey":1418},
 {"datasetID":68, "supportID":"8FEA143FF02CA3A980D9BEB475E94C762B6E28AB", "rexaID":"349644828ad3c8da8e1a15eee5aec0d704db34a1", "author":"Kamal Ali and Michael J. Pazzani", "title":"Error Reduction through Learning Multiple Descriptions", "venue":"Machine Learning, 24", "year":"1996", "window":"(representing an increase in accuracy from 93.3% to 98.9%!) and by large (around 3 or 4) factors for LED and Tic-tactoe. The <b>molecular biology</b> data sets also experienced significant reduction with the error being halved (for DNA this represented an increase in accuracy from 67.9% to 86.8%!). The error reduction is least for the noisy KRK and LED", "mykey":1419},
 {"datasetID":69, "supportID":"8FEA143FF02CA3A980D9BEB475E94C762B6E28AB", "rexaID":"349644828ad3c8da8e1a15eee5aec0d704db34a1", "author":"Kamal Ali and Michael J. Pazzani", "title":"Error Reduction through Learning Multiple Descriptions", "venue":"Machine Learning, 24", "year":"1996", "window":"(representing an increase in accuracy from 93.3% to 98.9%!) and by large (around 3 or 4) factors for LED and Tic-tactoe. The <b>molecular biology</b> data sets also experienced significant reduction with the error being halved (for DNA this represented an increase in accuracy from 67.9% to 86.8%!). The error reduction is least for the noisy KRK and LED", "mykey":1420},
 {"datasetID":73, "supportID":"8FEA143FF02CA3A980D9BEB475E94C762B6E28AB", "rexaID":"349644828ad3c8da8e1a15eee5aec0d704db34a1", "author":"Kamal Ali and Michael J. Pazzani", "title":"Error Reduction through Learning Multiple Descriptions", "venue":"Machine Learning, 24", "year":"1996", "window":"class labels. 6. The r 2 's between Er and OE e without the significant error reduction restriction are: 50.7% (Uniform), 33.7% (Bayes), 6.8% (Distribution) and 31.6% (Likelihood). The <b>Mushroom</b> data set causes a problem for the Distribution combination strategy because both the ensemble error and multiple models error are close to 0 so the ratio cannot be reliably estimated. The r 2 for", "mykey":1421},
 {"datasetID":83, "supportID":"8FEA143FF02CA3A980D9BEB475E94C762B6E28AB", "rexaID":"349644828ad3c8da8e1a15eee5aec0d704db34a1", "author":"Kamal Ali and Michael J. Pazzani", "title":"Error Reduction through Learning Multiple Descriptions", "venue":"Machine Learning, 24", "year":"1996", "window":"uniformly weighted voting between eleven, stochastically-generated descriptions is only one seventh that of the error obtained by using a single description. On the other hand, on the <b>primary</b> <b>tumor</b> data set, the error obtained by the identical multiple models procedure is the same as that obtained by using a single description. Much of the work on learning multiple models is motivated by Bayesian", "mykey":1422},
 {"datasetID":90, "supportID":"8FEA143FF02CA3A980D9BEB475E94C762B6E28AB", "rexaID":"349644828ad3c8da8e1a15eee5aec0d704db34a1", "author":"Kamal Ali and Michael J. Pazzani", "title":"Error Reduction through Learning Multiple Descriptions", "venue":"Machine Learning, 24", "year":"1996", "window":"to test learned models on noise-free examples (including noisy variants of the KRK and LED domains) but for the natural domains we tested on possibly noisy examples. The large variant of the <b>Soybean</b> data set was used and the 5-class variant of the Heart data set was used. 5.1. Does using multiple rule sets lead to lower error? In this section we present results of an experiment designed to answer the", "mykey":1423},
 {"datasetID":91, "supportID":"8FEA143FF02CA3A980D9BEB475E94C762B6E28AB", "rexaID":"349644828ad3c8da8e1a15eee5aec0d704db34a1", "author":"Kamal Ali and Michael J. Pazzani", "title":"Error Reduction through Learning Multiple Descriptions", "venue":"Machine Learning, 24", "year":"1996", "window":"to test learned models on noise-free examples (including noisy variants of the KRK and LED domains) but for the natural domains we tested on possibly noisy examples. The large variant of the <b>Soybean</b> data set was used and the 5-class variant of the Heart data set was used. 5.1. Does using multiple rule sets lead to lower error? In this section we present results of an experiment designed to answer the", "mykey":1424},
 {"datasetID":109, "supportID":"8FEA143FF02CA3A980D9BEB475E94C762B6E28AB", "rexaID":"349644828ad3c8da8e1a15eee5aec0d704db34a1", "author":"Kamal Ali and Michael J. Pazzani", "title":"Error Reduction through Learning Multiple Descriptions", "venue":"Machine Learning, 24", "year":"1996", "window":"the efficacy of using multiple models. It is important to analyze these experimental data because the amount of error reduction obtained by using multiple models varies a great deal. On the <b>wine</b> data set, for example, the error obtained by uniformly weighted voting between eleven, stochastically-generated descriptions is only one seventh that of the error obtained by using a single description. On", "mykey":1425},
 {"datasetID":48, "supportID":"90325B3F970152E022925C857371C7BF3C247134", "rexaID":"81fe50fa69f7adda42abf462e865a3b68089f333", "author":"Sreerama K. Murthy and Simon Kasif and Steven Salzberg", "title":"A System for Induction of Oblique Decision Trees", "venue":"Department of Computer Science Johns Hopkins University", "year":"1994", "window":"of three different types of iris flower. Weiss and Kapouleas (1989) obtained accuracies of 96.7% and 96.0% on this data with back propagation and 1-NN, respectively. <b>Housing</b> Costs in Boston. This data set, also available as a part of the UCI ML repository, describes housing values in the suburbs of Boston as a function of 12 continuous attributes and 1 binary attribute (Harrison & Rubinfeld, 1978).", "mykey":1426},
 {"datasetID":154, "supportID":"9037E44618C06F94B3B4B52EDEE2A9DCC7117FF6", "rexaID":"e949e51c587c01cc64b362cb4fedfaa7dcf5dd68", "author":"Michihiro Kuramochi and George Karypis", "title":"Finding Frequent Patterns in a Large Sparse Graph", "venue":"SDM", "year":"2004", "window":"\"5\". Finally, because some of the vertices in the resulting graph had a very high degree (i.e., authorities and hubs), we kept only the vertices whose degree was less or equal to 15. The Contact Map dataset is made of 170 proteins from the <b>Protein</b> Data Bank [5] with pairwise sequence identity lower than 25%. The vertices in these graphs correspond to the different amino acids and the edges connect two", "mykey":1427},
 {"datasetID":72, "supportID":"90A89D0CF28DF1B1D0B6B0ED1912F074257D9AFC", "rexaID":"d594d4b87f95f33ad4a476985e45fef49afac5ee", "author":"Pavel Paclik and Robert P W Duin and Geert M. P. van Kempen and Reinhard Kohlus", "title":"On Feature Selection with Measurement Cost and Grouped Features", "venue":"Pattern Recognition Group, Delft University of Technology", "year":"", "window":"algorithms per orm sequential orward selection with criterion (1). 3 Experiments 3.1 Handwritten Digit Recognition In the first experiment, we use the proposed methods on the handwritten digit <b>mfeat</b> dataset rom [1]. The dataset contains 10 digit classes with 200 samples per class and six di.erent eature sets (649 eatures). In order to lower the computational requirements in this illustrative example,", "mykey":1428},
 {"datasetID":45, "supportID":"90C23F9C87A8E20BD4530E8A49A8B326991E429F", "rexaID":"56eb4e64499baa9ef620d2aeb5383e1739a2e6e3", "author":"Remco R. Bouckaert and Eibe Frank", "title":"Evaluating the Replicability of Significance Tests for Comparing Learning Algorithms", "venue":"PAKDD", "year":"2004", "window":"perform differently in 19 out of 27 cases. For some rows, the test consistently indicates no difference between any two of the three schemes, in particular for the iris and Hungarian <b>heart</b> disease datasets. However, most rows contain at least one cell where the outcomes of the test are not consistent. The row labeled \"consistent\" at the bottom of the table lists the number of datasets for which all", "mykey":1429},
 {"datasetID":53, "supportID":"90C23F9C87A8E20BD4530E8A49A8B326991E429F", "rexaID":"56eb4e64499baa9ef620d2aeb5383e1739a2e6e3", "author":"Remco R. Bouckaert and Eibe Frank", "title":"Evaluating the Replicability of Significance Tests for Comparing Learning Algorithms", "venue":"PAKDD", "year":"2004", "window":"perform differently in 19 out of 27 cases. For some rows, the test consistently indicates no difference between any two of the three schemes, in particular for the <b>iris</b> and Hungarian heart disease datasets. However, most rows contain at least one cell where the outcomes of the test are not consistent. The row labeled \"consistent\" at the bottom of the table lists the number of datasets for which all", "mykey":1430},
 {"datasetID":149, "supportID":"90C23F9C87A8E20BD4530E8A49A8B326991E429F", "rexaID":"56eb4e64499baa9ef620d2aeb5383e1739a2e6e3", "author":"Remco R. Bouckaert and Eibe Frank", "title":"Evaluating the Replicability of Significance Tests for Comparing Learning Algorithms", "venue":"PAKDD", "year":"2004", "window":"very sensitive to the particular partitioning of the anneal data. Looking at the column for naive Bayes vs. C4.5, this test could be used to justify the claim that the two perform the same for all datasets except the <b>vehicle</b> dataset just by choosing appropriate random number seeds. However, it could just as well be used to support the claim that the two algorithms perform differently in 19 out of 27", "mykey":1431},
 {"datasetID":2, "supportID":"90F4A3A0381D72F99871748F18362A4758FD4BED", "rexaID":"0527f760f533e2a7a827710a924d6987bfc9a06e", "author":"Stephen D. Bay", "title":"Multivariate Discretization for Set Mining", "venue":"Knowl. Inf. Syst, 3", "year":"2001", "window":"Data Set #Features # Continuous # Examples <b>Adult</b> 14 5 48812 Census-Income 5 41 7 199523 SatImage 37 36 6435 Shuttle 10 9 48480 UCI Admissions 19 8 123028 Table 3. Discretization Time in CPU seconds Data Set", "mykey":1432},
 {"datasetID":20, "supportID":"90F4A3A0381D72F99871748F18362A4758FD4BED", "rexaID":"0527f760f533e2a7a827710a924d6987bfc9a06e", "author":"Stephen D. Bay", "title":"Multivariate Discretization for Set Mining", "venue":"Knowl. Inf. Syst, 3", "year":"2001", "window":"for <b>Census</b> Income. We required differences between adjacent cells to be at least as large as 1% of N . ME-MDL requires a class variable and for the Adult, Census-Income, SatImage, and Shuttle datasets we used the class variable that had been used in previous analyses. For UCI Admissions we used Admit = fyes, nog (i.e. was the student admitted to UCI) as the class variable. 6.1. Execution Time", "mykey":1433},
 {"datasetID":117, "supportID":"90F4A3A0381D72F99871748F18362A4758FD4BED", "rexaID":"0527f760f533e2a7a827710a924d6987bfc9a06e", "author":"Stephen D. Bay", "title":"Multivariate Discretization for Set Mining", "venue":"Knowl. Inf. Syst, 3", "year":"2001", "window":"for <b>Census</b> <b>Income</b>  We required differences between adjacent cells to be at least as large as 1% of N . ME-MDL requires a class variable and for the Adult, Census-Income, SatImage, and Shuttle datasets we used the class variable that had been used in previous analyses. For UCI Admissions we used Admit = fyes, nog (i.e. was the student admitted to UCI) as the class variable. 6.1. Execution Time", "mykey":1434},
 {"datasetID":92, "supportID":"90F4A3A0381D72F99871748F18362A4758FD4BED", "rexaID":"0527f760f533e2a7a827710a924d6987bfc9a06e", "author":"Stephen D. Bay", "title":"Multivariate Discretization for Set Mining", "venue":"Knowl. Inf. Syst, 3", "year":"2001", "window":"that deals with the positioning of radiators in the <b>Space</b> <b>Shuttle</b>  { UCI Admissions Data. This dataset represents all undergraduate student applications to UCI for the years 1993-1999. There are about 18000 applicants per year and the data contains variables such as ethnicity, UCI School (e.g. Arts,", "mykey":1435},
 {"datasetID":146, "supportID":"90F4A3A0381D72F99871748F18362A4758FD4BED", "rexaID":"0527f760f533e2a7a827710a924d6987bfc9a06e", "author":"Stephen D. Bay", "title":"Multivariate Discretization for Set Mining", "venue":"Knowl. Inf. Syst, 3", "year":"2001", "window":"of variables and number of records) and more detailed (i.e. standard census variables such as industry-code or occupation are recorded at a more detailed level in this database). { SatImage. This data set was generated from <b>Landsat</b> Multi-Spectral Scanner image data (i.e. it is a satellite image). It contains multi-spectral values for 3#3 pixel neighborhood and the soil type (e.g. red soil, cotton", "mykey":1436},
 {"datasetID":148, "supportID":"90F4A3A0381D72F99871748F18362A4758FD4BED", "rexaID":"0527f760f533e2a7a827710a924d6987bfc9a06e", "author":"Stephen D. Bay", "title":"Multivariate Discretization for Set Mining", "venue":"Knowl. Inf. Syst, 3", "year":"2001", "window":"data (i.e. it is a satellite image). It contains multi-spectral values for 3#3 pixel neighborhood and the soil type (e.g. red soil, cotton crop, grey soil, etc.). { <b>Shuttle</b>  This is a classification dataset that deals with the positioning of radiators in the Space Shuttle. { UCI Admissions Data. This dataset represents all undergraduate student applications to UCI for the years 1993-1999. There are", "mykey":1437},
 {"datasetID":67, "supportID":"9149F7825548668B0D549735ECBF9C476AC620D0", "rexaID":"144fcc2dcfad39ba4047467d4ce38139bc284f01", "author":"Daphne Koller and Mehran Sahami", "title":"Toward Optimal Feature Selection", "venue":"ICML", "year":"1996", "window":"include: the Corral data which was artificially constructed by John et al (1994) specifically for research in feature selection; the LED24, Vote, and <b>DNA</b> datasets from the UCI repository (Murphy & Aha 1995); and two datasets which are a subset of the Reuters document collection (Reuters 1995). These datasets are detailed in Table 1. We selected these", "mykey":1438},
 {"datasetID":137, "supportID":"9149F7825548668B0D549735ECBF9C476AC620D0", "rexaID":"144fcc2dcfad39ba4047467d4ce38139bc284f01", "author":"Daphne Koller and Mehran Sahami", "title":"Toward Optimal Feature Selection", "venue":"ICML", "year":"1996", "window":"from the UCI repository (Murphy & Aha 1995); and two datasets which are a subset of the <b>Reuters</b> document collection (Reuters 1995). These datasets are detailed in Table 1. We selected these datasets as they are either well understood in terms of feature", "mykey":1439},
 {"datasetID":42, "supportID":"9151390625191451E4FA6A7B67B40DC58C5C2065", "rexaID":"10cb2e0135796333fda81f2d65da6be1e1ed9e06", "author":"Aynur Akkus and H. Altay G\u00fcvenir", "title":"K Nearest Neighbor Classification on Feature Projections", "venue":"ICML", "year":"1996", "window":"Many Irrelevant Features, Proceedings of the Ninth National Conference on Artificial Intelligence, 547-552. Dasarathy, B. V., (1990). Nearest Neighbor (NN) Table 2: Comparison on some real-world datasets. Data Set: bcancerw cleveland <b>glass</b> hungarian ionosphere iris musk wine No. of Instances 273 303 214 294 351 150 476 178 No. of Features 9 13 9 13 34 4 166 13 No. of Classes 2 2 6 2 2 3 2 3 No. of", "mykey":1440},
 {"datasetID":52, "supportID":"9151390625191451E4FA6A7B67B40DC58C5C2065", "rexaID":"10cb2e0135796333fda81f2d65da6be1e1ed9e06", "author":"Aynur Akkus and H. Altay G\u00fcvenir", "title":"K Nearest Neighbor Classification on Feature Projections", "venue":"ICML", "year":"1996", "window":"Data Set: bcancerw cleveland glass hungarian <b>ionosphere</b> iris musk wine No. of Instances 273 303 214 294 351 150 476 178 No. of Features 9 13 9 13 34 4 166 13 No. of Classes 2 2 6 2 2 3 2 3 No. of Missing", "mykey":1441},
 {"datasetID":53, "supportID":"91AB6CADA27F00A15F45CE175E67C5434E1A337C", "rexaID":"5736888202fbd07e1945b8a64a3e0f3ddeb2840f", "author":"Judith E. Devaney and Steven G. Satterfield and John G. Hagedorn and John T. Kelso and Adele P. Peskin and William George and Terence J. Griffin and Howard K. Hung and Ronald D. Kriz", "title":"Science at the Speed of Thought", "venue":"Ambient Intelligence for Scientific Discovery", "year":"2004", "window":"GPP [40]. We also have our own equation discovery software [41]. This provides us with many avenues for displaying, interacting, and gaining insight into our results. Our visualization of the <b>Iris</b> data set [42] contains multiple representations. Figure 10-a shows part of our visualization. On the near side of the left wall is a parallel coordinate plot [36] of the cluster identified with the", "mykey":1442},
 {"datasetID":39, "supportID":"9232D71F0659DB894334B3372C4DE3F090821EE4", "rexaID":"95c80719727ccfba365ee616549b088436692fa8", "author":"Huajie Zhang and Charles X. Ling", "title":"An Improved Learning Algorithm for Augmented Naive Bayes", "venue":"PAKDD", "year":"2001", "window":"we used in our experiment. Dataset Attributes Class Instances <b>Ecoli</b> 7 8 336 Vote 16 2 435 Pima 8 2 768 Australia 14 2 690 Breast 10 2 683 Segment 19 7 1540 Vehicle 18 4 846 Bank 20 2 1162 Table 1. Descriptions of domains used in our", "mykey":1443},
 {"datasetID":14, "supportID":"9268F85BF598D6853B8769785180FFFE864FF806", "rexaID":"646cb388234937c3b01d36219e119b0904112be2", "author":"Bernhard Pfahringer and Geoffrey Holmes and Gabi Schmidberger", "title":"Wrapping Boosters against Noise", "venue":"Australian Joint Conference on Artificial Intelligence", "year":"2001", "window":"induction: 7 Table 4. Predictive error, no noise. The best entry in each line is set in boldface, a prefix star marks values that are significantly different from the value in the first column. Dataset ADTree Bagging Wrapping Wensemble <b>BREAST</b> <b>CANCER</b> 31.59 * 28.57 * 26.02 * 25.32 BREAST-W 3.83 * 3.49 * 4.32 3.63 CLEVE 21.78 * 17.15 * 17.28 * 16.03 CREDIT-A 15.10 * 13.22 15.68 15.07 CREDIT-G 25.50 *", "mykey":1444},
 {"datasetID":7, "supportID":"93187059BF6EAB8DBAE1762B80ED7071F9A435BF", "rexaID":"e2b2b723df700c90e69a31a4403b740c2d2a7b2f", "author":"Alexander K. Seewald", "title":"Dissertation Towards Understanding Stacking Studies of a General Ensemble Learning Scheme ausgefuhrt zum Zwecke der Erlangung des akademischen Grades eines Doktors der technischen Naturwissenschaften", "venue":"", "year":"", "window":"with number of classes and examples, discrete and continuous attributes, baseline accuracy (%) and entropy in bits per example (Kononenko & Bratko, 1991). Dataset cl Inst disc cont bL E <b>audiology</b> 24 226 69 0 25.22 3.51 autos 7 205 10 16 32.68 2.29 balance-scale 3 625 0 4 45.76 1.32 breast-cancer 2 286 10 0 70.28 0.88 breast-w 2 699 0 9 65.52 0.93 colic 2 368", "mykey":1445},
 {"datasetID":8, "supportID":"93187059BF6EAB8DBAE1762B80ED7071F9A435BF", "rexaID":"e2b2b723df700c90e69a31a4403b740c2d2a7b2f", "author":"Alexander K. Seewald", "title":"Dissertation Towards Understanding Stacking Studies of a General Ensemble Learning Scheme ausgefuhrt zum Zwecke der Erlangung des akademischen Grades eines Doktors der technischen Naturwissenschaften", "venue":"", "year":"", "window":"with number of classes and examples, discrete and continuous attributes, baseline accuracy (%) and entropy in bits per example (Kononenko & Bratko, 1991). Dataset cl Inst disc cont bL E <b>audiology</b> 24 226 69 0 25.22 3.51 autos 7 205 10 16 32.68 2.29 balance-scale 3 625 0 4 45.76 1.32 breast-cancer 2 286 10 0 70.28 0.88 breast-w 2 699 0 9 65.52 0.93 colic 2 368", "mykey":1446},
 {"datasetID":12, "supportID":"93187059BF6EAB8DBAE1762B80ED7071F9A435BF", "rexaID":"e2b2b723df700c90e69a31a4403b740c2d2a7b2f", "author":"Alexander K. Seewald", "title":"Dissertation Towards Understanding Stacking Studies of a General Ensemble Learning Scheme ausgefuhrt zum Zwecke der Erlangung des akademischen Grades eines Doktors der technischen Naturwissenschaften", "venue":"", "year":"", "window":"the baseline accuracy is already 66.7%. Interestingly in this case the best model is from DecisionStump which learns a single J48 node, obtaining 88.9% accuracy, corresponding to a single error on dataset <b>balance</b> <b>scale</b>  It seems J48 is prone to overfitting on this meta dataset. The training set model is based on meanAbsSkew. All but two times, the following model appears: meanAbsSkew <= 0.31 : class", "mykey":1447},
 {"datasetID":14, "supportID":"93187059BF6EAB8DBAE1762B80ED7071F9A435BF", "rexaID":"e2b2b723df700c90e69a31a4403b740c2d2a7b2f", "author":"Alexander K. Seewald", "title":"Dissertation Towards Understanding Stacking Studies of a General Ensemble Learning Scheme ausgefuhrt zum Zwecke der Erlangung des akademischen Grades eines Doktors der technischen Naturwissenschaften", "venue":"", "year":"", "window":"balance-scale Compressed glyph visualization for dataset <b>breast</b> <b>cancer</b> Compressed glyph visualization for dataset breast-w Compressed glyph visualization for dataset colic Compressed glyph visualization for dataset credit-a Compressed glyph visualization", "mykey":1448},
 {"datasetID":151, "supportID":"93187059BF6EAB8DBAE1762B80ED7071F9A435BF", "rexaID":"e2b2b723df700c90e69a31a4403b740c2d2a7b2f", "author":"Alexander K. Seewald", "title":"Dissertation Towards Understanding Stacking Studies of a General Ensemble Learning Scheme ausgefuhrt zum Zwecke der Erlangung des akademischen Grades eines Doktors der technischen Naturwissenschaften", "venue":"", "year":"", "window":"segment Compressed glyph visualization for dataset <b>sonar</b> Compressed glyph visualization for dataset soybean Compressed glyph visualization for dataset vehicle Compressed glyph visualization for dataset vote Compressed glyph visualization for dataset", "mykey":1449},
 {"datasetID":34, "supportID":"93187059BF6EAB8DBAE1762B80ED7071F9A435BF", "rexaID":"e2b2b723df700c90e69a31a4403b740c2d2a7b2f", "author":"Alexander K. Seewald", "title":"Dissertation Towards Understanding Stacking Studies of a General Ensemble Learning Scheme ausgefuhrt zum Zwecke der Erlangung des akademischen Grades eines Doktors der technischen Naturwissenschaften", "venue":"", "year":"", "window":"credit-g Compressed glyph visualization for dataset <b>diabetes</b> Compressed glyph visualization for dataset glass Compressed glyph visualization for dataset heart-c Compressed glyph visualization for dataset heart-h Compressed glyph visualization for", "mykey":1450},
 {"datasetID":42, "supportID":"93187059BF6EAB8DBAE1762B80ED7071F9A435BF", "rexaID":"e2b2b723df700c90e69a31a4403b740c2d2a7b2f", "author":"Alexander K. Seewald", "title":"Dissertation Towards Understanding Stacking Studies of a General Ensemble Learning Scheme ausgefuhrt zum Zwecke der Erlangung des akademischen Grades eines Doktors der technischen Naturwissenschaften", "venue":"", "year":"", "window":"#10 Training set (8-9=CV, 7=75%, 6=62%,.. 1=25%) Hold-out accuracy Figure 6.2: Learning curves for dataset balance-scale to <b>glass</b>  56 0 1 2 3 4 5 6 7 8 9 0.65 0.7 0.75 0.8 0.85 0.9 Learncurve for Dataset #11 Training set (8-9=CV, 7=75%, 6=62%,.. 1=25%) Hold-out accuracy 0 1 2 3 4 5 6 7 8 9 0.65 0.7 0.75", "mykey":1451},
 {"datasetID":45, "supportID":"93187059BF6EAB8DBAE1762B80ED7071F9A435BF", "rexaID":"e2b2b723df700c90e69a31a4403b740c2d2a7b2f", "author":"Alexander K. Seewald", "title":"Dissertation Towards Understanding Stacking Studies of a General Ensemble Learning Scheme ausgefuhrt zum Zwecke der Erlangung des akademischen Grades eines Doktors der technischen Naturwissenschaften", "venue":"", "year":"", "window":"#18 Training set (8-9=CV, 7=75%, 6=62%,.. 1=25%) Hold-out accuracy Figure 6.3: Learning curves for dataset <b>heart</b> c to lymph. 57 0 1 2 3 4 5 6 7 8 9 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0.55 0.6 Learncurve for Dataset #19 Training set (8-9=CV, 7=75%, 6=62%,.. 1=25%) Hold-out accuracy 0 1 2 3 4 5 6 7 8 9 0.86", "mykey":1452},
 {"datasetID":46, "supportID":"93187059BF6EAB8DBAE1762B80ED7071F9A435BF", "rexaID":"e2b2b723df700c90e69a31a4403b740c2d2a7b2f", "author":"Alexander K. Seewald", "title":"Dissertation Towards Understanding Stacking Studies of a General Ensemble Learning Scheme ausgefuhrt zum Zwecke der Erlangung des akademischen Grades eines Doktors der technischen Naturwissenschaften", "venue":"", "year":"", "window":"heart-statlog Compressed glyph visualization for dataset <b>hepatitis</b> Figure 8.6: Glyph visualization for datasets audiology to hepatitis. 79 Compressed glyph visualization for dataset ionosphere Compressed glyph visualization for dataset iris Compressed", "mykey":1453},
 {"datasetID":47, "supportID":"93187059BF6EAB8DBAE1762B80ED7071F9A435BF", "rexaID":"e2b2b723df700c90e69a31a4403b740c2d2a7b2f", "author":"Alexander K. Seewald", "title":"Dissertation Towards Understanding Stacking Studies of a General Ensemble Learning Scheme ausgefuhrt zum Zwecke der Erlangung des akademischen Grades eines Doktors der technischen Naturwissenschaften", "venue":"", "year":"", "window":"breast-w Compressed glyph visualization for dataset <b>colic</b> Compressed glyph visualization for dataset credit-a Compressed glyph visualization for dataset credit-g Compressed glyph visualization for dataset diabetes Compressed glyph visualization for", "mykey":1454},
 {"datasetID":52, "supportID":"93187059BF6EAB8DBAE1762B80ED7071F9A435BF", "rexaID":"e2b2b723df700c90e69a31a4403b740c2d2a7b2f", "author":"Alexander K. Seewald", "title":"Dissertation Towards Understanding Stacking Studies of a General Ensemble Learning Scheme ausgefuhrt zum Zwecke der Erlangung des akademischen Grades eines Doktors der technischen Naturwissenschaften", "venue":"", "year":"", "window":"audiology to hepatitis. 79 Compressed glyph visualization for dataset <b>ionosphere</b> Compressed glyph visualization for dataset iris Compressed glyph visualization for dataset labor Compressed glyph visualization for dataset lymph Compressed glyph visualization for", "mykey":1455},
 {"datasetID":53, "supportID":"93187059BF6EAB8DBAE1762B80ED7071F9A435BF", "rexaID":"e2b2b723df700c90e69a31a4403b740c2d2a7b2f", "author":"Alexander K. Seewald", "title":"Dissertation Towards Understanding Stacking Studies of a General Ensemble Learning Scheme ausgefuhrt zum Zwecke der Erlangung des akademischen Grades eines Doktors der technischen Naturwissenschaften", "venue":"", "year":"", "window":"ionosphere Compressed glyph visualization for dataset <b>iris</b> Compressed glyph visualization for dataset labor Compressed glyph visualization for dataset lymph Compressed glyph visualization for dataset primary-tumor Compressed glyph visualization for", "mykey":1456},
 {"datasetID":56, "supportID":"93187059BF6EAB8DBAE1762B80ED7071F9A435BF", "rexaID":"e2b2b723df700c90e69a31a4403b740c2d2a7b2f", "author":"Alexander K. Seewald", "title":"Dissertation Towards Understanding Stacking Studies of a General Ensemble Learning Scheme ausgefuhrt zum Zwecke der Erlangung des akademischen Grades eines Doktors der technischen Naturwissenschaften", "venue":"", "year":"", "window":"vote. When removing the base classifier dependent features, IBk is still the best classifier with an additional error on <b>labor</b>  the smallest dataset. In this case 12 All base learners plus 1R and DecisionStump 36 MLR, another high-bias and global learner, is equally good. So we may tentatively conclude that for this meta dataset, there seems to", "mykey":1457},
 {"datasetID":83, "supportID":"93187059BF6EAB8DBAE1762B80ED7071F9A435BF", "rexaID":"e2b2b723df700c90e69a31a4403b740c2d2a7b2f", "author":"Alexander K. Seewald", "title":"Dissertation Towards Understanding Stacking Studies of a General Ensemble Learning Scheme ausgefuhrt zum Zwecke der Erlangung des akademischen Grades eines Doktors der technischen Naturwissenschaften", "venue":"", "year":"", "window":"#26 Training set (8-9=CV, 7=75%, 6=62%,.. 1=25%) Hold-out accuracy Figure 6.4: Learning curves for dataset <b>primary</b> <b>tumor</b> to zoo. 58 Chapter 7 Towards a Theoretical Framework In this chapter, we show that the ensemble learning scheme Stacking is universal in the sense that most ensemble learning schemes", "mykey":1458},
 {"datasetID":90, "supportID":"93187059BF6EAB8DBAE1762B80ED7071F9A435BF", "rexaID":"e2b2b723df700c90e69a31a4403b740c2d2a7b2f", "author":"Alexander K. Seewald", "title":"Dissertation Towards Understanding Stacking Studies of a General Ensemble Learning Scheme ausgefuhrt zum Zwecke der Erlangung des akademischen Grades eines Doktors der technischen Naturwissenschaften", "venue":"", "year":"", "window":"instances. We rejected Principal Components Analysis after initial experiments because its linear projection on orthogonal axes is less general and the representation is noticeably worse for some datasets, e.g. <b>soybean</b>  We rejected self-organizing 1 We used the Sammon Mapping implementation from Vesanto, Himberg, Alhoniemi & Parhankangas (2000), which was written in MatLab script language. It worked", "mykey":1459},
 {"datasetID":91, "supportID":"93187059BF6EAB8DBAE1762B80ED7071F9A435BF", "rexaID":"e2b2b723df700c90e69a31a4403b740c2d2a7b2f", "author":"Alexander K. Seewald", "title":"Dissertation Towards Understanding Stacking Studies of a General Ensemble Learning Scheme ausgefuhrt zum Zwecke der Erlangung des akademischen Grades eines Doktors der technischen Naturwissenschaften", "venue":"", "year":"", "window":"instances. We rejected Principal Components Analysis after initial experiments because its linear projection on orthogonal axes is less general and the representation is noticeably worse for some datasets, e.g. <b>soybean</b>  We rejected self-organizing 1 We used the Sammon Mapping implementation from Vesanto, Himberg, Alhoniemi & Parhankangas (2000), which was written in MatLab script language. It worked", "mykey":1460},
 {"datasetID":145, "supportID":"93187059BF6EAB8DBAE1762B80ED7071F9A435BF", "rexaID":"e2b2b723df700c90e69a31a4403b740c2d2a7b2f", "author":"Alexander K. Seewald", "title":"Dissertation Towards Understanding Stacking Studies of a General Ensemble Learning Scheme ausgefuhrt zum Zwecke der Erlangung des akademischen Grades eines Doktors der technischen Naturwissenschaften", "venue":"", "year":"", "window":"<b>heart</b> h Compressed glyph visualization for dataset heart <b>statlog</b> Compressed glyph visualization for dataset hepatitis Figure 8.6: Glyph visualization for datasets audiology to hepatitis. 79 Compressed glyph visualization for dataset ionosphere", "mykey":1461},
 {"datasetID":149, "supportID":"93187059BF6EAB8DBAE1762B80ED7071F9A435BF", "rexaID":"e2b2b723df700c90e69a31a4403b740c2d2a7b2f", "author":"Alexander K. Seewald", "title":"Dissertation Towards Understanding Stacking Studies of a General Ensemble Learning Scheme ausgefuhrt zum Zwecke der Erlangung des akademischen Grades eines Doktors der technischen Naturwissenschaften", "venue":"", "year":"", "window":"soybean Compressed glyph visualization for dataset <b>vehicle</b> Compressed glyph visualization for dataset vote Compressed glyph visualization for dataset vowel Compressed glyph visualization for dataset zoo Figure 8.7: Glyph visualization for datasets", "mykey":1462},
 {"datasetID":98, "supportID":"93187059BF6EAB8DBAE1762B80ED7071F9A435BF", "rexaID":"e2b2b723df700c90e69a31a4403b740c2d2a7b2f", "author":"Alexander K. Seewald", "title":"Dissertation Towards Understanding Stacking Studies of a General Ensemble Learning Scheme ausgefuhrt zum Zwecke der Erlangung des akademischen Grades eines Doktors der technischen Naturwissenschaften", "venue":"", "year":"", "window":"features which uniquely characterize the dataset. These were inspired by the <b>StatLOG</b> project (Brazdil, Gama & Henry, 1994). Space restrictions prevent us from giving exact formulas for each case, but a reference implementation is available from", "mykey":1463},
 {"datasetID":111, "supportID":"93187059BF6EAB8DBAE1762B80ED7071F9A435BF", "rexaID":"e2b2b723df700c90e69a31a4403b740c2d2a7b2f", "author":"Alexander K. Seewald", "title":"Dissertation Towards Understanding Stacking Studies of a General Ensemble Learning Scheme ausgefuhrt zum Zwecke der Erlangung des akademischen Grades eines Doktors der technischen Naturwissenschaften", "venue":"", "year":"", "window":"#26 Training set (8-9=CV, 7=75%, 6=62%,.. 1=25%) Hold-out accuracy Figure 6.4: Learning curves for dataset primary-tumor to <b>zoo</b>  58 Chapter 7 Towards a Theoretical Framework In this chapter, we show that the ensemble learning scheme Stacking is universal in the sense that most ensemble learning schemes", "mykey":1464},
 {"datasetID":53, "supportID":"93199F00DDCF1A8444714E0D38F66EECE480CF18", "rexaID":"80463673ea023e8c5162497b051bd8c54346d686", "author":"Fernando Fern#andez and Pedro Isasi", "title":"Designing Nearest Neighbour Classifiers by the Evolution of a Population of Prototypes", "venue":"Universidad Carlos III de Madrid", "year":"", "window":"first version is due to the high number of centroids to eliminate. An example of the classifier found is given in #gure1(a), showing the centroids located in the mean of the distributions. 3.2 <b>Iris</b> Data Set Iris Data Set from UCI Machine Learning Repository 1 [3] is used in the second experiment. This dataset consits of 150 samples of three classes, where each class has 50 examples. The dimension of", "mykey":1465},
 {"datasetID":45, "supportID":"9329F4A1D4A0F6FD149E1D4C86C18BBCCFC3D9C8", "rexaID":"4eff6dda32898a8bdc8268d2a790a96b93f2e262", "author":"Thomas G. Dietterich", "title":"An Experimental Comparison of Three Methods for Constructing Ensembles of Decision Trees: Bagging, Boosting, and Randomization", "venue":"Machine Learning, 40", "year":"2000", "window":"attribute values rather than as missing values. In auto, the class variable was the make of the automobile. In the breast cancer domains, all features were treated as continuous. The <b>heart</b> disease data sets were recoded to use discrete values where appropriate. All attributes were treated as continuous in the kingrook-vs-king (krk) data set. In lymphography, the lymph-nodes-dimin, lymph-nodes-enlar,", "mykey":1466},
 {"datasetID":63, "supportID":"9329F4A1D4A0F6FD149E1D4C86C18BBCCFC3D9C8", "rexaID":"4eff6dda32898a8bdc8268d2a790a96b93f2e262", "author":"Thomas G. Dietterich", "title":"An Experimental Comparison of Three Methods for Constructing Ensembles of Decision Trees: Bagging, Boosting, and Randomization", "venue":"Machine Learning, 40", "year":"2000", "window":"were recoded to use discrete values where appropriate. All attributes were treated as continuous in the kingrook-vs-king (krk) data set. In <b>lymphography</b>  the lymph-nodes-dimin, lymph-nodes-enlar, and no-of-nodes-in attributes were treated as continuous. In segment, all features were rounded to four significant digits to avoid", "mykey":1467},
 {"datasetID":69, "supportID":"9329F4A1D4A0F6FD149E1D4C86C18BBCCFC3D9C8", "rexaID":"4eff6dda32898a8bdc8268d2a790a96b93f2e262", "author":"Thomas G. Dietterich", "title":"An Experimental Comparison of Three Methods for Constructing Ensembles of Decision Trees: Bagging, Boosting, and Randomization", "venue":"Machine Learning, 40", "year":"2000", "window":"the effect of classification noise, we added random class noise to nine domains (audiology, hypo, king-rook-vs-king-pawn (krkp), satimage, sick, <b>splice</b>  segment, vehicle, and waveform). These data sets were chosen because at least one pair of the ensemble methods gave statistically significantly different performance on these domains. We did not perform noise experiments with letter-recognition", "mykey":1468},
 {"datasetID":149, "supportID":"9329F4A1D4A0F6FD149E1D4C86C18BBCCFC3D9C8", "rexaID":"4eff6dda32898a8bdc8268d2a790a96b93f2e262", "author":"Thomas G. Dietterich", "title":"An Experimental Comparison of Three Methods for Constructing Ensembles of Decision Trees: Bagging, Boosting, and Randomization", "venue":"Machine Learning, 40", "year":"2000", "window":"the effect of classification noise, we added random class noise to nine domains (audiology, hypo, king-rook-vs-king-pawn (krkp), satimage, sick, splice, segment, <b>vehicle</b>  and waveform). These data sets were chosen because at least one pair of the ensemble methods gave statistically significantly different performance on these domains. We did not perform noise experiments with letter-recognition", "mykey":1469},
 {"datasetID":107, "supportID":"9329F4A1D4A0F6FD149E1D4C86C18BBCCFC3D9C8", "rexaID":"4eff6dda32898a8bdc8268d2a790a96b93f2e262", "author":"Thomas G. Dietterich", "title":"An Experimental Comparison of Three Methods for Constructing Ensembles of Decision Trees: Bagging, Boosting, and Randomization", "venue":"Machine Learning, 40", "year":"2000", "window":"the effect of classification noise, we added random class noise to nine domains (audiology, hypo, king-rook-vs-king-pawn (krkp), satimage, sick, splice, segment, vehicle, and <b>waveform</b> . These data sets were chosen because at least one pair of the ensemble methods gave statistically significantly different performance on these domains. We did not perform noise experiments with letter-recognition", "mykey":1470},
 {"datasetID":108, "supportID":"9329F4A1D4A0F6FD149E1D4C86C18BBCCFC3D9C8", "rexaID":"4eff6dda32898a8bdc8268d2a790a96b93f2e262", "author":"Thomas G. Dietterich", "title":"An Experimental Comparison of Three Methods for Constructing Ensembles of Decision Trees: Bagging, Boosting, and Randomization", "venue":"Machine Learning, 40", "year":"2000", "window":"the effect of classification noise, we added random class noise to nine domains (audiology, hypo, king-rook-vs-king-pawn (krkp), satimage, sick, splice, segment, vehicle, and <b>waveform</b> . These data sets were chosen because at least one pair of the ensemble methods gave statistically significantly different performance on these domains. We did not perform noise experiments with letter-recognition", "mykey":1471},
 {"datasetID":14, "supportID":"93B40FBD5FA495B9AE332498E102260F45FC05C8", "rexaID":"eef68e57188bfabe4dff279cf32efe545bd7fe3c", "author":"Fei Sha and Lawrence K. Saul and Daniel D. Lee", "title":"Multiplicative Updates for Nonnegative Quadratic Programming in Support Vector Machines", "venue":"NIPS", "year":"2002", "window":"Kernel Polynomial Radial Data k=4 k=6 #=0.3 #=1.0 #=3.0 Sonar 9.6% 9.6% 7.6% 6.7% 10.6% <b>breast</b> <b>cancer</b> 5.1% 3.6% 4.4% 4.4% 4.4% Table 1: Misclassification error rates on the sonar and breast cancer data sets after 512 iterations of the multiplicative updates. 3.1 Multiplicative updates The loss function in eq. (6) is a special case of eq. (1) with A ij = y i y j K(x i , x j ) and b i =- 1. Thus, the", "mykey":1472},
 {"datasetID":151, "supportID":"93B40FBD5FA495B9AE332498E102260F45FC05C8", "rexaID":"eef68e57188bfabe4dff279cf32efe545bd7fe3c", "author":"Fei Sha and Lawrence K. Saul and Daniel D. Lee", "title":"Multiplicative Updates for Nonnegative Quadratic Programming in Support Vector Machines", "venue":"NIPS", "year":"2002", "window":"Kernel Polynomial Radial Data k=4 k=6 #=0.3 #=1.0 #=3.0 <b>sonar</b> 9.6% 9.6% 7.6% 6.7% 10.6% Breast cancer 5.1% 3.6% 4.4% 4.4% 4.4% Table 1: Misclassification error rates on the sonar and breast cancer data sets after 512 iterations of the multiplicative updates. 3.1 Multiplicative updates The loss function in eq. (6) is a special case of eq. (1) with A ij = y i y j K(x i , x j ) and b i =- 1. Thus, the", "mykey":1473},
 {"datasetID":151, "supportID":"93EA823E7F542C16A90FEA8A80521A4B9E54F91E", "rexaID":"03534af8424987a10b9e264147f082059665f7b7", "author":"Dennis DeCoste", "title":"Anytime Query-Tuned Kernel Machines via Cholesky Factorization", "venue":"SDM", "year":"2003", "window":"signs against minWz n 's typically less aggressive but steady improvements. Other hybrids are likely even better and worthy of future research. 5 Examples We checked our approach on two UCI datasets [1], <b>Sonar</b> and Haberman, and the MNIST digit-recognition dataset [10]. We confirmed that L k (x) # f(x) # H k (x) always held. Table 3 summarizes some of our results. Rows labelled 1-2 summarize", "mykey":1474},
 {"datasetID":43, "supportID":"93EA823E7F542C16A90FEA8A80521A4B9E54F91E", "rexaID":"03534af8424987a10b9e264147f082059665f7b7", "author":"Dennis DeCoste", "title":"Anytime Query-Tuned Kernel Machines via Cholesky Factorization", "venue":"SDM", "year":"2003", "window":"signs against minWz n 's typically less aggressive but steady improvements. Other hybrids are likely even better and worthy of future research. 5 Examples We checked our approach on two UCI datasets [1], Sonar and <b>Haberman</b>  and the MNIST digit-recognition dataset [10]. We confirmed that L k (x) # f(x) # H k (x) always held. Table 3 summarizes some of our results. Rows labelled 1-2 summarize", "mykey":1475},
 {"datasetID":81, "supportID":"93EA823E7F542C16A90FEA8A80521A4B9E54F91E", "rexaID":"03534af8424987a10b9e264147f082059665f7b7", "author":"Dennis DeCoste", "title":"Anytime Query-Tuned Kernel Machines via Cholesky Factorization", "venue":"SDM", "year":"2003", "window":"Lanckriet, L. E. Ghaoui, C. Bhattacharyya, and M. I. Jordan. Minmax probability machine. Advances in Neural Information Processing Systems (NIPS) 14, 2002. [10] Y. LeCun. MNIST <b>handwritten</b> <b>digits</b> dataset. Available at http://www.research.att.com/ #yann/ ocr/ mnist/, 2000. [11] S. Mika, G. Ratsch, and K.-R. Muller. A mathematical programming approach to the kernel Fisher algorithm. In Advances in", "mykey":1476},
 {"datasetID":124, "supportID":"9420108AB1160C43755C76C0AE57C71F2162F004", "rexaID":"4a338bab101f6721ea5dc4d76de5456721fbf0a3", "author":"Marina Meila and Michael I. Jordan", "title":"Learning with Mixtures of Trees", "venue":"Journal of Machine Learning Research, 1", "year":"2000", "window":"For the third density estimation experiment, we used a subset of 576 images from the normalized <b <b>face</b> images</b> dataset of [43]. These images were downsampled to 48 variables (pixels) and 5 gray levels. We divided the data randomly into N train = 500 and N test = 76 examples; of the 500 training examples, 50 were", "mykey":1477},
 {"datasetID":67, "supportID":"9420108AB1160C43755C76C0AE57C71F2162F004", "rexaID":"4a338bab101f6721ea5dc4d76de5456721fbf0a3", "author":"Marina Meila and Michael I. Jordan", "title":"Learning with Mixtures of Trees", "venue":"Journal of Machine Learning Research, 1", "year":"2000", "window":"exon, or coding section, begins. Hence, the class variable can take 3 values (EI, IE or no junction) and the other variables take 4 values corresponding to the 4 possible <b>DNA</b> bases (C, A, G, T). The dataset consists of 3,175 labeled examples 2 . We ran two series of experiments comparing the MT model with competing models. In the first series of experiments, we compared to the results of [41], who used", "mykey":1478},
 {"datasetID":69, "supportID":"9420108AB1160C43755C76C0AE57C71F2162F004", "rexaID":"4a338bab101f6721ea5dc4d76de5456721fbf0a3", "author":"Marina Meila and Michael I. Jordan", "title":"Learning with Mixtures of Trees", "venue":"Journal of Machine Learning Research, 1", "year":"2000", "window":"0.05 0.06 0.07 0.08 Error rate #: 0 0 0 1 10 100 0 1 10 100 KBNN NN TANB NB | {z } Tree | {z } MT m = 3 Figure 15: Comparison of classification performance of the MT and other models on the <b>SPLICE</b> data set when N train = 2000, N test = 1175. Tree represents a mixture of trees with m = 1, MT is a mixture of trees with m = 3. KBNN is the Knowledge based neural net, NN is a neural net. 5.3.5 The SPLICE", "mykey":1479},
 {"datasetID":76, "supportID":"9420108AB1160C43755C76C0AE57C71F2162F004", "rexaID":"4a338bab101f6721ea5dc4d76de5456721fbf0a3", "author":"Marina Meila and Michael I. Jordan", "title":"Learning with Mixtures of Trees", "venue":"Journal of Machine Learning Research, 1", "year":"2000", "window":"the MT has m = 12, and the MF has m = 30. (b) On the <b>NURSERY</b> data set; the MT has m = 30, the MF has m = 70. TANB and NB are the tree augmented naive Bayes and the naive Bayes classifiers respectively. The plots show the average and standard deviation test set error", "mykey":1480},
 {"datasetID":81, "supportID":"9420108AB1160C43755C76C0AE57C71F2162F004", "rexaID":"4a338bab101f6721ea5dc4d76de5456721fbf0a3", "author":"Marina Meila and Michael I. Jordan", "title":"Learning with Mixtures of Trees", "venue":"Journal of Machine Learning Research, 1", "year":"2000", "window":"on. 5.2 Density estimation experiments 5.2.1 <b>digits</b> and digit pairs images Our first density estimation experiment involved a subset of binary vector representations of <b>handwritten</b> digits. The datasets consist of normalized and quantized 8\u00d78 binary images of handwritten digits made available by the US Postal Service Office for Advanced Technology. One dataset---which we refer to as the \"digits\"", "mykey":1481},
 {"datasetID":81, "supportID":"9469176D3705A0AAE88A4673E398CC265228A568", "rexaID":"00b6f77ab5353f8974383e14fbef4cd03a846f8a", "author":"Greg Hamerly and Charles Elkan", "title":"Learning the k in k-means", "venue":"NIPS", "year":"2003", "window":"them slow for more than 8 to 12 dimensions. All our code is written in Matlab; X-means is written in C. 3.1 Discovering true clusters in labeled data We tested these algorithms on two real-world datasets for <b>handwritten</b> digit <b>recognition</b>  the NIST dataset [12] and the Pendigits dataset [2]. The goal is to cluster the data without knowledge of the labels and measure how well the clustering captures", "mykey":1482},
 {"datasetID":74, "supportID":"94AD4DA2BC0409012B738018D00763FF936A35E1", "rexaID":"7025b49e49b58acbb1881ee289e9d719a528d505", "author":"Giorgio Valentini and Thomas G. Dietterich", "title":"Low Bias Bagged Support Vector Machines", "venue":"ICML", "year":"2003", "window":"Spam Linear 0.1356 0.1340 0.1627 0-4-1 5-0-0 5-0-0 Polyn. 0.1309 0.1338 0.1388 1-4-0 2-3-0 2-2-1 Gauss. 0.1239 0.1349 0.1407 3-2-0 3-2-0 2-3-0 Data set <b>Musk</b> Linear 0.1244 0.1247 0.1415 0-5-0 4-1-0 4-1-0 Polyn. 0.1039 0.1193 0.1192 4-1-0 4-0-1 2-2-1 Gauss. 0.0872 0.0972 0.0920 4-1-0 2-2-1 1-0-4 6.1. Experimental setup We employed two synthetic data", "mykey":1483},
 {"datasetID":75, "supportID":"94AD4DA2BC0409012B738018D00763FF936A35E1", "rexaID":"7025b49e49b58acbb1881ee289e9d719a528d505", "author":"Giorgio Valentini and Thomas G. Dietterich", "title":"Low Bias Bagged Support Vector Machines", "venue":"ICML", "year":"2003", "window":"Spam Linear 0.1356 0.1340 0.1627 0-4-1 5-0-0 5-0-0 Polyn. 0.1309 0.1338 0.1388 1-4-0 2-3-0 2-2-1 Gauss. 0.1239 0.1349 0.1407 3-2-0 3-2-0 2-3-0 Data set <b>Musk</b> Linear 0.1244 0.1247 0.1415 0-5-0 4-1-0 4-1-0 Polyn. 0.1039 0.1193 0.1192 4-1-0 4-0-1 2-2-1 Gauss. 0.0872 0.0972 0.0920 4-1-0 2-2-1 1-0-4 6.1. Experimental setup We employed two synthetic data", "mykey":1484},
 {"datasetID":146, "supportID":"94AD4DA2BC0409012B738018D00763FF936A35E1", "rexaID":"7025b49e49b58acbb1881ee289e9d719a528d505", "author":"Giorgio Valentini and Thomas G. Dietterich", "title":"Low Bias Bagged Support Vector Machines", "venue":"ICML", "year":"2003", "window":"Waveform Linear 0.0811 0.0821 0.0955 2-3-0 5-0-0 5-0-0 Polyn. 0.0625 0.0677 0.0698 2-3-0 2-3-0 3-2-0 Gauss. 0.0574 0.0653 0.0666 4-1-0 4-1-0 2-3-0 Data set Grey <b>Landsat</b> Linear 0.0508 0.0510 0.0601 0-5-0 3-2-0 3-2-0 Polyn. 0.0432 0.0493 0.0535 1-4-0 2-3-0 1-4-0 Gauss. 0.0475 0.0486 0.0483 1-3-1 1-3-1 0-5-0 Data set Letter-Two Linear 0.0832 0.0864 0.1011", "mykey":1485},
 {"datasetID":107, "supportID":"94AD4DA2BC0409012B738018D00763FF936A35E1", "rexaID":"7025b49e49b58acbb1881ee289e9d719a528d505", "author":"Giorgio Valentini and Thomas G. Dietterich", "title":"Low Bias Bagged Support Vector Machines", "venue":"ICML", "year":"2003", "window":"P2 Polyn. 0.1687 0.1863 0.1892 4-1-0 4-1-0 1-4-0 Gauss. 0.1429 0.1534 0.1605 4-1-0 5-0-0 3-2-0 Data set <b>Waveform</b> Linear 0.0811 0.0821 0.0955 2-3-0 5-0-0 5-0-0 Polyn. 0.0625 0.0677 0.0698 2-3-0 2-3-0 3-2-0 Gauss. 0.0574 0.0653 0.0666 4-1-0 4-1-0 2-3-0 Data set Grey-Landsat Linear 0.0508 0.0510 0.0601", "mykey":1486},
 {"datasetID":108, "supportID":"94AD4DA2BC0409012B738018D00763FF936A35E1", "rexaID":"7025b49e49b58acbb1881ee289e9d719a528d505", "author":"Giorgio Valentini and Thomas G. Dietterich", "title":"Low Bias Bagged Support Vector Machines", "venue":"ICML", "year":"2003", "window":"P2 Polyn. 0.1687 0.1863 0.1892 4-1-0 4-1-0 1-4-0 Gauss. 0.1429 0.1534 0.1605 4-1-0 5-0-0 3-2-0 Data set <b>Waveform</b> Linear 0.0811 0.0821 0.0955 2-3-0 5-0-0 5-0-0 Polyn. 0.0625 0.0677 0.0698 2-3-0 2-3-0 3-2-0 Gauss. 0.0574 0.0653 0.0666 4-1-0 4-1-0 2-3-0 Data set Grey-Landsat Linear 0.0508 0.0510 0.0601", "mykey":1487},
 {"datasetID":47, "supportID":"950F28AC7FCB5EC339FAB6BF3E1B61C9D055F1D5", "rexaID":"939595ca638eb3390e9bb9c4e6cc1352163cbf18", "author":"Kai Ming Ting and Ian H. Witten", "title":"Stacked Generalization: when does it work", "venue":"Department of Computer Science University of Waikato", "year":"", "window":"2 -- 0.00 0.93 0.01 0.00 0.00 0.07 Table 8: Ave. error rates of BestCV, Majority Vote and MLR (model ~ M 0 ), along with the standard error (#SE) between BestCV and the worst level-0 generalizers. Dataset #SE BestCV Majority MLR <b>Horse</b> 0.5 17.1 15.0 15.2 Splice 2.5 4.5 4.0 3.8 Abalone 3.3 40.1 39.0 37.9 Led24 8.7 32.8 31.8 32.1 Credit 8.9 17.4 16.1 16.2 Nettalk(s) 10.8 12.7 12.2 11.5 Coding 12.7 25.0", "mykey":1488},
 {"datasetID":69, "supportID":"950F28AC7FCB5EC339FAB6BF3E1B61C9D055F1D5", "rexaID":"939595ca638eb3390e9bb9c4e6cc1352163cbf18", "author":"Kai Ming Ting and Ian H. Witten", "title":"Stacked Generalization: when does it work", "venue":"Department of Computer Science University of Waikato", "year":"", "window":"other three level-1 generalizers in that its model can easily be interpreted. Examples of the combination weights it derives (for the probability-based model ~ M 0 ) appear in Table 5 for the <b>Splice</b> dataset. The weights indicate the relative importance of the level-0 generalizers for each prediction class. In this dataset, NB is the dominant generalizer for predicting class 2, NB and IB1 are both good", "mykey":1489},
 {"datasetID":107, "supportID":"950F28AC7FCB5EC339FAB6BF3E1B61C9D055F1D5", "rexaID":"939595ca638eb3390e9bb9c4e6cc1352163cbf18", "author":"Kai Ming Ting and Ian H. Witten", "title":"Stacked Generalization: when does it work", "venue":"Department of Computer Science University of Waikato", "year":"", "window":"from the UCI Repository of machine learning databases [Merz and Murphy, 1996]. Details of these are given in Table 1. For the artificial datasets---Led24 and <b>Waveform</b> -- each training dataset L is generated using a different Table 1: Details of the datasets used in the experiment. Datasets # Samples # Classes # Attr & Type Led24 200--5000 10", "mykey":1490},
 {"datasetID":108, "supportID":"950F28AC7FCB5EC339FAB6BF3E1B61C9D055F1D5", "rexaID":"939595ca638eb3390e9bb9c4e6cc1352163cbf18", "author":"Kai Ming Ting and Ian H. Witten", "title":"Stacked Generalization: when does it work", "venue":"Department of Computer Science University of Waikato", "year":"", "window":"from the UCI Repository of machine learning databases [Merz and Murphy, 1996]. Details of these are given in Table 1. For the artificial datasets---Led24 and <b>Waveform</b> -- each training dataset L is generated using a different Table 1: Details of the datasets used in the experiment. Datasets # Samples # Classes # Attr & Type Led24 200--5000 10", "mykey":1491},
 {"datasetID":45, "supportID":"957719505A60A4E2DA09BD85F2B7268DCA287DFB", "rexaID":"58642563848aaece947320829da7d338224cb8b3", "author":"Yoav Freund and Lorne Mason", "title":"The Alternating Decision Tree Learning Algorithm", "venue":"ICML", "year":"1999", "window":"representation. To demonstrate our interpretation, we consider the alternating tree presented in Figure 4. This tree is the result of running our learning algorithm for six iterations on the cleve data set from Irvine. This is a data set of <b>heart</b> disease diagnostics for which the goal is to discriminate between sick and healthy people 3 In our mapping positive classification correspond to healthy and", "mykey":1492},
 {"datasetID":69, "supportID":"957719505A60A4E2DA09BD85F2B7268DCA287DFB", "rexaID":"58642563848aaece947320829da7d338224cb8b3", "author":"Yoav Freund and Lorne Mason", "title":"The Alternating Decision Tree Learning Algorithm", "venue":"ICML", "year":"1999", "window":"1 Proportion +ve Prediction <b>splice</b> train test 0 0.2 0.4 0.6 0.8 1 -1 -0.5 0 0.5 1 Proportion +ve Prediction sick-euthyroid train test Figure 7: Calibration graphs for the splice and sick-euthyroid data sets for train and test after 100 rounds of ADTree. In addition to justifying our interpretation, the calibration graphs can potentially be used to improve our performance situation where our", "mykey":1493},
 {"datasetID":14, "supportID":"95A785BEB7AA9BF691CE8FB330A7D36776B9BC56", "rexaID":"8f5ae7219e74a85e3f722b58b3fedb30eab7a1d7", "author":"Huan Liu", "title":"A Family of Efficient Rule Generators", "venue":"Department of Information Systems and Computer Science National University of Singapore", "year":"", "window":"testing set are randomly selected. The rest are used for training. The data has 22 discrete attributes. Each attribute can have 2 to 10 values. ffl Wisconsin <b>Breast</b> <b>Cancer</b> The training and testing datasets contains 350 and 349 instances respectively. 350 instances are randomly selected for training, the other half is for testing. There are 9 discrete attributes. Each attribute has 10 values. The", "mykey":1494},
 {"datasetID":17, "supportID":"95A785BEB7AA9BF691CE8FB330A7D36776B9BC56", "rexaID":"8f5ae7219e74a85e3f722b58b3fedb30eab7a1d7", "author":"Huan Liu", "title":"A Family of Efficient Rule Generators", "venue":"Department of Information Systems and Computer Science National University of Singapore", "year":"", "window":"testing set are randomly selected. The rest are used for training. The data has 22 discrete attributes. Each attribute can have 2 to 10 values. ffl <b>Wisconsin</b> <b>Breast</b> <b>Cancer</b> The training and testing datasets contains 350 and 349 instances respectively. 350 instances are randomly selected for training, the other half is for testing. There are 9 discrete attributes. Each attribute has 10 values. The", "mykey":1495},
 {"datasetID":15, "supportID":"95A785BEB7AA9BF691CE8FB330A7D36776B9BC56", "rexaID":"8f5ae7219e74a85e3f722b58b3fedb30eab7a1d7", "author":"Huan Liu", "title":"A Family of Efficient Rule Generators", "venue":"Department of Information Systems and Computer Science National University of Singapore", "year":"", "window":"testing set are randomly selected. The rest are used for training. The data has 22 discrete attributes. Each attribute can have 2 to 10 values. ffl <b>Wisconsin</b> <b>Breast</b> <b>Cancer</b> The training and testing datasets contains 350 and 349 instances respectively. 350 instances are randomly selected for training, the other half is for testing. There are 9 discrete attributes. Each attribute has 10 values. The", "mykey":1496},
 {"datasetID":16, "supportID":"95A785BEB7AA9BF691CE8FB330A7D36776B9BC56", "rexaID":"8f5ae7219e74a85e3f722b58b3fedb30eab7a1d7", "author":"Huan Liu", "title":"A Family of Efficient Rule Generators", "venue":"Department of Information Systems and Computer Science National University of Singapore", "year":"", "window":"testing set are randomly selected. The rest are used for training. The data has 22 discrete attributes. Each attribute can have 2 to 10 values. ffl <b>Wisconsin</b> <b>Breast</b> <b>Cancer</b> The training and testing datasets contains 350 and 349 instances respectively. 350 instances are randomly selected for training, the other half is for testing. There are 9 discrete attributes. Each attribute has 10 values. The", "mykey":1497},
 {"datasetID":19, "supportID":"95A785BEB7AA9BF691CE8FB330A7D36776B9BC56", "rexaID":"8f5ae7219e74a85e3f722b58b3fedb30eab7a1d7", "author":"Huan Liu", "title":"A Family of Efficient Rule Generators", "venue":"Department of Information Systems and Computer Science National University of Singapore", "year":"", "window":"Equipped with the above two evaluation measures, we can conduct some experiments to empirically compare the four versions of rule generators, and with other known methods [13, 22]. A. Method One data set - <b>CAR</b> will be used to show the differences between the versions of our rule generator, and to compare with the results reported in [22] since they have done some comparison with other methods such", "mykey":1498},
 {"datasetID":53, "supportID":"95A785BEB7AA9BF691CE8FB330A7D36776B9BC56", "rexaID":"8f5ae7219e74a85e3f722b58b3fedb30eab7a1d7", "author":"Huan Liu", "title":"A Family of Efficient Rule Generators", "venue":"Department of Information Systems and Computer Science National University of Singapore", "year":"", "window":"and to compare with the results reported in [22] since they have done some comparison with other methods such as ID3 [14] and the one by Han et al [7]. Then, we show the results for another two data sets: Golf-Playing [13] and <b>Iris</b> [4]. The authors of [13, 22] did not provide testing data. Only the Iris data is divided evenly into two sets (75 patterns each) for training and testing. Datasets CAR", "mykey":1499},
 {"datasetID":73, "supportID":"95A785BEB7AA9BF691CE8FB330A7D36776B9BC56", "rexaID":"8f5ae7219e74a85e3f722b58b3fedb30eab7a1d7", "author":"Huan Liu", "title":"A Family of Efficient Rule Generators", "venue":"Department of Information Systems and Computer Science National University of Singapore", "year":"", "window":"are chosen for further experiments from the machine learning databases at University of California, Irvine [12]. They are: ffl <b>Mushroom</b> The training and testing datasets contains 7124 and 1000 instances respectively. The 1000 instances in the testing set are randomly selected. The rest are used for training. The data has 22 discrete attributes. Each attribute can", "mykey":1500},
 {"datasetID":20, "supportID":"95CD9917CAE64EC65C07257E171B6430CFAD5619", "rexaID":"32c4d28ceaa19d2a6906dfa39df99da7026c9cc5", "author":"Douglas Burdick and Manuel Calimlim and Jason Flannick and Johannes Gehrke and Tomi Yiu", "title":"MAFIA: A Performance Study of Mining Maximal Frequent Itemsets", "venue":"FIMI", "year":"2003", "window":"itemset patterns that peak around 10-25 items (see Figure 4). Chess and Connect4 are gathered from game state information and are available from the UCI Machine Learning Repository [5]. The Pumsb dataset is <b>census</b> data from PUMS (Public Use Microdata Sample). Pumsb-star is the same dataset as Pumsb except all items of 80% support or more have been removed, making it less dense and easier to mine.", "mykey":1501},
 {"datasetID":23, "supportID":"95CD9917CAE64EC65C07257E171B6430CFAD5619", "rexaID":"32c4d28ceaa19d2a6906dfa39df99da7026c9cc5", "author":"Douglas Burdick and Manuel Calimlim and Jason Flannick and Johannes Gehrke and Tomi Yiu", "title":"MAFIA: A Performance Study of Mining Maximal Frequent Itemsets", "venue":"FIMI", "year":"2003", "window":"0.1 0.12 Min Sup (%) Time (s) NONE ADAPTIVE Compression on BMS-WebView-2 10 100 1000 10000 0 0.01 0.02 0.03 0.04 0.05 0.06 Min Sup (%) Time (s) NONE ADAPTIVE Figure 8. Compression on more sparse datasets Compression on <b>Chess</b> 1 10 100 1000 0 5 10 15 20 25 30 35 Min Sup (%) Time (s) NONE ADAPTIVE Compression on Pumsb 10 100 1000 10000 0 10 20 30 40 50 60 70 Min Sup (%) Time (s) NONE ADAPTIVE", "mykey":1502},
 {"datasetID":21, "supportID":"95CD9917CAE64EC65C07257E171B6430CFAD5619", "rexaID":"32c4d28ceaa19d2a6906dfa39df99da7026c9cc5", "author":"Douglas Burdick and Manuel Calimlim and Jason Flannick and Johannes Gehrke and Tomi Yiu", "title":"MAFIA: A Performance Study of Mining Maximal Frequent Itemsets", "venue":"FIMI", "year":"2003", "window":"0.1 0.12 Min Sup (%) Time (s) NONE ADAPTIVE Compression on BMS-WebView-2 10 100 1000 10000 0 0.01 0.02 0.03 0.04 0.05 0.06 Min Sup (%) Time (s) NONE ADAPTIVE Figure 8. Compression on more sparse datasets Compression on <b>Chess</b> 1 10 100 1000 0 5 10 15 20 25 30 35 Min Sup (%) Time (s) NONE ADAPTIVE Compression on Pumsb 10 100 1000 10000 0 10 20 30 40 50 60 70 Min Sup (%) Time (s) NONE ADAPTIVE", "mykey":1503},
 {"datasetID":22, "supportID":"95CD9917CAE64EC65C07257E171B6430CFAD5619", "rexaID":"32c4d28ceaa19d2a6906dfa39df99da7026c9cc5", "author":"Douglas Burdick and Manuel Calimlim and Jason Flannick and Johannes Gehrke and Tomi Yiu", "title":"MAFIA: A Performance Study of Mining Maximal Frequent Itemsets", "venue":"FIMI", "year":"2003", "window":"0.1 0.12 Min Sup (%) Time (s) NONE ADAPTIVE Compression on BMS-WebView-2 10 100 1000 10000 0 0.01 0.02 0.03 0.04 0.05 0.06 Min Sup (%) Time (s) NONE ADAPTIVE Figure 8. Compression on more sparse datasets Compression on <b>Chess</b> 1 10 100 1000 0 5 10 15 20 25 30 35 Min Sup (%) Time (s) NONE ADAPTIVE Compression on Pumsb 10 100 1000 10000 0 10 20 30 40 50 60 70 Min Sup (%) Time (s) NONE ADAPTIVE", "mykey":1504},
 {"datasetID":52, "supportID":"96083689692AE21F865A1F42DFAC035F8E4F8E86", "rexaID":"1570e3547e5d0131eee5b25f40359409b8d8c396", "author":"Rajesh Parekh and Jihoon Yang and Vasant Honavar", "title":"Constructive Neural-Network Learning Algorithms for Pattern Classification", "venue":"", "year":"", "window":"Outputs represents the number of output classes, and Attributes describes the type of input attributes of the patterns. The real-world datasets <b>ionosphere</b>  pima, segmentation, and vehicle are available at the UCI Machine Learning Repository [34] while the 3-circles dataset was artificially generated. The 3-circles dataset comprises of 1800", "mykey":1505},
 {"datasetID":149, "supportID":"96083689692AE21F865A1F42DFAC035F8E4F8E86", "rexaID":"1570e3547e5d0131eee5b25f40359409b8d8c396", "author":"Rajesh Parekh and Jihoon Yang and Vasant Honavar", "title":"Constructive Neural-Network Learning Algorithms for Pattern Classification", "venue":"", "year":"", "window":"Outputs represents the number of output classes, and Attributes describes the type of input attributes of the patterns. The real-world datasets ionosphere, pima, segmentation, and <b>vehicle</b> are available at the UCI Machine Learning Repository [34] while the 3-circles dataset was artificially generated. The 3-circles dataset comprises of 1800", "mykey":1506},
 {"datasetID":14, "supportID":"96255EE83ABB19925C340F5124EA14A75D3D853F", "rexaID":"3c3eb7beca3f6ab6fcebe2863131fa3dbae6cb7f", "author":"Yuh-Jeng Lee", "title":"Smooth Support Vector Machines", "venue":"Preliminary Thesis Proposal Computer Sciences Department University of Wisconsin", "year":"2000", "window":"A medical application is also proposed here. A linear support vector machine (SVM) is used to extract 6 features from a total of 31 features in a dataset of 253 <b>breast</b> <b>cancer</b> patients. Five features are nuclear features obtained during a non-invasive diagnostic procedure while one feature, tumor size, is obtained during surgery. The linear SVM", "mykey":1507},
 {"datasetID":17, "supportID":"96255EE83ABB19925C340F5124EA14A75D3D853F", "rexaID":"3c3eb7beca3f6ab6fcebe2863131fa3dbae6cb7f", "author":"Yuh-Jeng Lee", "title":"Smooth Support Vector Machines", "venue":"Preliminary Thesis Proposal Computer Sciences Department University of Wisconsin", "year":"2000", "window":"[37]. To evaluate the efficacy of SSVM, we compared computational times of SSVM with those of RLP and SV M k\u00b7k1 . We ran all tests on six publicly available datasets: the <b>Wisconsin</b> Prognostic <b>Breast</b> <b>Cancer</b> Database [34] and five datasets from the Irvine Machine Learning Database Repository [36]. It turned out that tenfold testing correctness of the SSVM is the", "mykey":1508},
 {"datasetID":15, "supportID":"96255EE83ABB19925C340F5124EA14A75D3D853F", "rexaID":"3c3eb7beca3f6ab6fcebe2863131fa3dbae6cb7f", "author":"Yuh-Jeng Lee", "title":"Smooth Support Vector Machines", "venue":"Preliminary Thesis Proposal Computer Sciences Department University of Wisconsin", "year":"2000", "window":"[37]. To evaluate the efficacy of SSVM, we compared computational times of SSVM with those of RLP and SV M k\u00b7k1 . We ran all tests on six publicly available datasets: the <b>Wisconsin</b> Prognostic <b>Breast</b> <b>Cancer</b> Database [34] and five datasets from the Irvine Machine Learning Database Repository [36]. It turned out that tenfold testing correctness of the SSVM is the", "mykey":1509},
 {"datasetID":16, "supportID":"96255EE83ABB19925C340F5124EA14A75D3D853F", "rexaID":"3c3eb7beca3f6ab6fcebe2863131fa3dbae6cb7f", "author":"Yuh-Jeng Lee", "title":"Smooth Support Vector Machines", "venue":"Preliminary Thesis Proposal Computer Sciences Department University of Wisconsin", "year":"2000", "window":"[37]. To evaluate the efficacy of SSVM, we compared computational times of SSVM with those of RLP and SV M k\u00b7k1 . We ran all tests on six publicly available datasets: the <b>Wisconsin</b> Prognostic <b>Breast</b> <b>Cancer</b> Database [34] and five datasets from the Irvine Machine Learning Database Repository [36]. It turned out that tenfold testing correctness of the SSVM is the", "mykey":1510},
 {"datasetID":53, "supportID":"9648A3790C62CEE4253299F21368CE8028E3C8A6", "rexaID":"04c0b59b684ddc546667ac58aa34b51f6ecbf50f", "author":"Eric P. Kasten and Philip K. McKinley", "title":"MESO: Perceptual Memory to Support Online Learning in Adaptive Software", "venue":"Proceedings of the Third International Conference on Development and Learning (ICDL", "year":"", "window":"sizes and feature counts. Data Set Size Features Classes <b>Iris</b> 150 4 3 ATT Faces 360 10,304 40 Mult. Feature 2,000 649 10 Mushroom 8,124 22 2 Japanese Vowel 9,859 12 9 Letter 20,000 16 26 Cover Type 581,012 54 7 set. As such, no", "mykey":1511},
 {"datasetID":73, "supportID":"9648A3790C62CEE4253299F21368CE8028E3C8A6", "rexaID":"04c0b59b684ddc546667ac58aa34b51f6ecbf50f", "author":"Eric P. Kasten and Philip K. McKinley", "title":"MESO: Perceptual Memory to Support Online Learning in Adaptive Software", "venue":"Proceedings of the Third International Conference on Development and Learning (ICDL", "year":"", "window":"contains continuous features, measuring features such as elevation or slope, and binary values indicating whether a pattern is a particular soil type. However, the <b>Mushroom</b> data set consists entirely of nominal values encoded as alpha characters converted to their ASCII equivalent for processing by MESO. In contrast, the ATT Faces data set comprises image pixel values of human", "mykey":1512},
 {"datasetID":1, "supportID":"9689A0CB301FFED7D71BD2DD5DB76408B2CE7B4B", "rexaID":"b2d8d3d5275f9de64f7d1d58ed346fc673f94065", "author":"Edward Snelson and Carl Edward Rasmussen and Zoubin Ghahramani", "title":"Draft version; accepted for NIPS*03 Warped Gaussian Processes", "venue":"Gatsby Computational Neuroscience Unit University College London", "year":"", "window":"D t min t max N train N test creep 30 18 MPa 530 MPa 800 1266 <b>abalone</b> 8 1 yr 29 yrs 1000 3177 ailerons 4010Dataset Model Absolute error Squared errorcreep GP 16.4 654 4.46 GP + log 15.6 587 4.24 warped GP 15.0 554 4.19 abalone GP 1.53 4.79 2.19 GP + log 1.48 4.62 2.01 warped GP 1.47 4.63 1.96 ailerons GP 1.23 \u00d7", "mykey":1513},
 {"datasetID":46, "supportID":"96AD75FBED0DBED69A06F97797FAEC4C17FC06B0", "rexaID":"61172cddb9dbfd4d26caeeaa3a8a8deb58a26bb5", "author":"Amaury Habrard and Marc Bernard and Marc Sebban", "title":"IOS Press Detecting Irrelevant Subtrees to Improve Probabilistic Learning from Tree-structured Data", "venue":"Fundamenta Informaticae", "year":"2004", "window":"has 1000 examples with 42 different leaves, but has larger trees than the previous one. . We also use a sample proposed for the PKDD'02 discovery challenge 1 (dataset on <b>hepatitis</b> ; the transformation of data into trees is described in [19]. This dataset has 4000 examples with 253 leaves. Our experimental setup consists in adding noise in each dataset, as", "mykey":1514},
 {"datasetID":1, "supportID":"9759C861982F9CE41EA0A61C8E8785C3DA6C770D", "rexaID":"3fe33086239c6beec412b11fde015871a85c6011", "author":"Sally Jo Cunningham", "title":"Dataset cataloging metadata for machine learning applications and research", "venue":"Department of Computer Science University of Waikato", "year":"", "window":"At UCI, for example, the documentation records range from the highly specific (for example, the <b>abalone</b> dataset record in Figure 1a) to the nearly non-existent (the \"undocumented databases\" directory at UCI, as typified by the economic sanctions data description in Figure 1b). This variability is not", "mykey":1515},
 {"datasetID":153, "supportID":"9759C861982F9CE41EA0A61C8E8785C3DA6C770D", "rexaID":"3fe33086239c6beec412b11fde015871a85c6011", "author":"Sally Jo Cunningham", "title":"Dataset cataloging metadata for machine learning applications and research", "venue":"Department of Computer Science University of Waikato", "year":"", "window":"University of ;;;Pazzani AAAI-86 or EWSL-88 Tasmania GPO Box 252C, Hobart, Tasmania 7001, Australia (c) Date received: December 1995 (a) portion of abalone data description (b) <b>economic <b>sanctions</b> /b> dataset header Figure 1. Content independent catalog samples from the UCI ML dataset repository content-dependent: In comparison to content-independent metadata, content-dependent descriptors are directly", "mykey":1516},
 {"datasetID":14, "supportID":"979447A2F4B90A03DBDAA977E8B4F37795DB0A18", "rexaID":"f820b10d6723504d343c9741f7333fbc3393fd05", "author":"Lorne Mason and Jonathan Baxter and Peter L. Bartlett and Marcus Frean", "title":"Boosting Algorithms as Gradient Descent", "venue":"NIPS", "year":"1999", "window":"0% noise - AdaBoost 0% noise - DOOM II 15% noise - AdaBoost 15% noise - DOOM II Figure 2: Margin distributions for AdaBoost and DOOM II with 0% and 15% label noise for the <b>breast</b> <b>cancer</b> and splice data sets. Given that AdaBoost does suffer from overfitting and is guaranteed to minimize an exponential cost function of the margins, this cost function certainly does not relate to test error. How does the", "mykey":1517},
 {"datasetID":151, "supportID":"979447A2F4B90A03DBDAA977E8B4F37795DB0A18", "rexaID":"f820b10d6723504d343c9741f7333fbc3393fd05", "author":"Lorne Mason and Jonathan Baxter and Peter L. Bartlett and Marcus Frean", "title":"Boosting Algorithms as Gradient Descent", "venue":"NIPS", "year":"1999", "window":"These results show that DOOM II generally outperforms AdaBoost and that the improvement is more pronounced in the presence of label noise. -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 2.5 3 3.5 Error advantage (%) Data set <b>sonar</b> cleve ionosphere vote1 credit breast-cancer pima-indians hypo1 splice 0% noise 5% noise 15% noise Figure 1: Summary of test error advantage (with standard error bars) of DOOM II over AdaBoost", "mykey":1518},
 {"datasetID":52, "supportID":"979447A2F4B90A03DBDAA977E8B4F37795DB0A18", "rexaID":"f820b10d6723504d343c9741f7333fbc3393fd05", "author":"Lorne Mason and Jonathan Baxter and Peter L. Bartlett and Marcus Frean", "title":"Boosting Algorithms as Gradient Descent", "venue":"NIPS", "year":"1999", "window":"These results show that DOOM II generally outperforms AdaBoost and that the improvement is more pronounced in the presence of label noise. -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 2.5 3 3.5 Error advantage (%) Data set sonar cleve <b>ionosphere</b> vote1 credit breast-cancer pima-indians hypo1 splice 0% noise 5% noise 15% noise Figure 1: Summary of test error advantage (with standard error bars) of DOOM II over AdaBoost", "mykey":1519},
 {"datasetID":56, "supportID":"979447A2F4B90A03DBDAA977E8B4F37795DB0A18", "rexaID":"f820b10d6723504d343c9741f7333fbc3393fd05", "author":"Lorne Mason and Jonathan Baxter and Peter L. Bartlett and Marcus Frean", "title":"Boosting Algorithms as Gradient Descent", "venue":"NIPS", "year":"1999", "window":"the minimum of AdaBoost's test error and the minimum of the normalized sigmoid cost very nearly coincide. In the <b>labor</b> data set AdaBoost's test error converges and overfitting does not occur. For this data set both the normalized sigmoid cost and the exponential cost converge. In the vote1 data set AdaBoost initially", "mykey":1520},
 {"datasetID":69, "supportID":"979447A2F4B90A03DBDAA977E8B4F37795DB0A18", "rexaID":"f820b10d6723504d343c9741f7333fbc3393fd05", "author":"Lorne Mason and Jonathan Baxter and Peter L. Bartlett and Marcus Frean", "title":"Boosting Algorithms as Gradient Descent", "venue":"NIPS", "year":"1999", "window":"0% noise - AdaBoost 0% noise - DOOM II 15% noise - AdaBoost 15% noise - DOOM II Figure 2: Margin distributions for AdaBoost and DOOM II with 0% and 15% label noise for the breast-cancer and <b>splice</b> data sets. Given that AdaBoost does suffer from overfitting and is guaranteed to minimize an exponential cost function of the margins, this cost function certainly does not relate to test error. How does the", "mykey":1521},
 {"datasetID":2, "supportID":"97FC8CAD79557A06EBD7B79951BFEDC4855EC5B4", "rexaID":"74b68cad54c62b62b7ad64911291e2e0b0620dd8", "author":"Bart Hamers and J. A. K Suykens", "title":"Coupled Transductive Ensemble Learning of Kernel Models", "venue":"Bart De Moor", "year":"2003", "window":"0.16 5.08e-4 0.27 0.28 7.92e-4 0.21 0.21 8.37e-4 0.15 0.14 5.08e-4 0.24 0.24 8.34e-4 1 0 1 9.8e-5 1 2.7e-3 Table 2: Misclassification rates on a test set (Tic-Tac-Toe (TTT), Australian Credit Card Data Set (ACR) and the <b>ADULT</b> Data Set (ADULT)). The number of models is indicated by the second number in Table 1, example TTT11 is an ensemble model based on 11 individual models on, the TTT prediction. We", "mykey":1522},
 {"datasetID":20, "supportID":"97FC8CAD79557A06EBD7B79951BFEDC4855EC5B4", "rexaID":"74b68cad54c62b62b7ad64911291e2e0b0620dd8", "author":"Bart Hamers and J. A. K Suykens", "title":"Coupled Transductive Ensemble Learning of Kernel Models", "venue":"Bart De Moor", "year":"2003", "window":"donated by Kohavi. It involves the prediction whether income exceeds 50,000 dollars a year based on <b>census</b> data. The original data set consists out of 48,842 observations each described by six numerical and eight categorical attributes. All the observations with missing values were removed from consideration. To show the use of our", "mykey":1523},
 {"datasetID":48, "supportID":"97FC8CAD79557A06EBD7B79951BFEDC4855EC5B4", "rexaID":"74b68cad54c62b62b7ad64911291e2e0b0620dd8", "author":"Bart Hamers and J. A. K Suykens", "title":"Coupled Transductive Ensemble Learning of Kernel Models", "venue":"Bart De Moor", "year":"2003", "window":"models of both ensembles (uncoupled (left); coupled (right)). This shows the e\u00aeect and improvement obtained by coupling of the learning processes for the individual submodels. 5.1.2 Boston <b>housing</b> Data Set The Boston housing data set is a multivariate regression data set of 506 cases in 14 attributes. It has two prototasks: NOX, in which the nitrous oxide level is to be predicted, and price MEDV, in", "mykey":1524},
 {"datasetID":143, "supportID":"97FC8CAD79557A06EBD7B79951BFEDC4855EC5B4", "rexaID":"74b68cad54c62b62b7ad64911291e2e0b0620dd8", "author":"Bart Hamers and J. A. K Suykens", "title":"Coupled Transductive Ensemble Learning of Kernel Models", "venue":"Bart De Moor", "year":"2003", "window":"0.16 5.08e-4 0.27 0.28 7.92e-4 0.21 0.21 8.37e-4 0.15 0.14 5.08e-4 0.24 0.24 8.34e-4 1 0 1 9.8e-5 1 2.7e-3 Table 2: Misclassification rates on a test set (Tic-Tac-Toe (TTT), <b>Australian Credit</b> Card Data Set (ACR) and the Adult Data Set (ADULT)). The number of models is indicated by the second number in Table 1, example TTT11 is an ensemble model based on 11 individual models on, the TTT prediction. We", "mykey":1525},
 {"datasetID":98, "supportID":"97FC8CAD79557A06EBD7B79951BFEDC4855EC5B4", "rexaID":"74b68cad54c62b62b7ad64911291e2e0b0620dd8", "author":"Bart Hamers and J. A. K Suykens", "title":"Coupled Transductive Ensemble Learning of Kernel Models", "venue":"Bart De Moor", "year":"2003", "window":"donated by Quinlan and is one of the Credit Approval Databases which were used in the <b>Statlog</b> project. There 690 observations in this data set with six numerical and eight attributes. The optimal hyperparameters for the Gaussian RBF kernel are \u00a1 \u00b0, \u00be 2 \u00a2 = (9.03e2, 12.15) . All the ensembles were based on 10 submodels. Similar to all the", "mykey":1526},
 {"datasetID":101, "supportID":"97FC8CAD79557A06EBD7B79951BFEDC4855EC5B4", "rexaID":"74b68cad54c62b62b7ad64911291e2e0b0620dd8", "author":"Bart Hamers and J. A. K Suykens", "title":"Coupled Transductive Ensemble Learning of Kernel Models", "venue":"Bart De Moor", "year":"2003", "window":"second layer we will always use the sparse formulation (7), with number of support models SM \u00b7 q. This sparse solution also improves the performance as observed in the experiments. 5.2.1 <b>Tic-Tac-Toe</b> Data Set The Tic-Tac-Toe Endgame database is a UCI (Blake and Merz (1998)) data set contributed by Aha, encoding the complete set of possible board configurations at the end of the tictac-toe games. The", "mykey":1527},
 {"datasetID":14, "supportID":"981CC13ED4946CBCA0911E170D39B43756AEA32B", "rexaID":"79b9012d7063a4c0e98d98ebd63d63044c8da997", "author":"W. Nick Street", "title":"A Neural Network Model for Prognostic Prediction", "venue":"ICML", "year":"1998", "window":"of the models to separate cases with favorable and unfavorable prognoses (see Section 3.3). 3 Experimental Results Computational experiments were performed on two very different <b>breast</b> <b>cancer</b> data sets. The first is known as Wisconsin Prognostic Breast Cancer (WPBC) and is characterized by a small number of cases, relatively high dimensionality, very precise values and almost no missing data. The", "mykey":1528},
 {"datasetID":17, "supportID":"981CC13ED4946CBCA0911E170D39B43756AEA32B", "rexaID":"79b9012d7063a4c0e98d98ebd63d63044c8da997", "author":"W. Nick Street", "title":"A Neural Network Model for Prognostic Prediction", "venue":"ICML", "year":"1998", "window":"of the models to separate cases with favorable and unfavorable prognoses (see Section 3.3). 3 Experimental Results Computational experiments were performed on two very different <b>breast</b> <b>cancer</b> data sets. The first is known as <b>Wisconsin</b> Prognostic Breast Cancer (WPBC) and is characterized by a small number of cases, relatively high dimensionality, very precise values and almost no missing data. The", "mykey":1529},
 {"datasetID":15, "supportID":"981CC13ED4946CBCA0911E170D39B43756AEA32B", "rexaID":"79b9012d7063a4c0e98d98ebd63d63044c8da997", "author":"W. Nick Street", "title":"A Neural Network Model for Prognostic Prediction", "venue":"ICML", "year":"1998", "window":"of the models to separate cases with favorable and unfavorable prognoses (see Section 3.3). 3 Experimental Results Computational experiments were performed on two very different <b>breast</b> <b>cancer</b> data sets. The first is known as <b>Wisconsin</b> Prognostic Breast Cancer (WPBC) and is characterized by a small number of cases, relatively high dimensionality, very precise values and almost no missing data. The", "mykey":1530},
 {"datasetID":16, "supportID":"981CC13ED4946CBCA0911E170D39B43756AEA32B", "rexaID":"79b9012d7063a4c0e98d98ebd63d63044c8da997", "author":"W. Nick Street", "title":"A Neural Network Model for Prognostic Prediction", "venue":"ICML", "year":"1998", "window":"of the models to separate cases with favorable and unfavorable prognoses (see Section 3.3). 3 Experimental Results Computational experiments were performed on two very different <b>breast</b> <b>cancer</b> data sets. The first is known as <b>Wisconsin</b> Prognostic Breast Cancer (WPBC) and is characterized by a small number of cases, relatively high dimensionality, very precise values and almost no missing data. The", "mykey":1531},
 {"datasetID":59, "supportID":"9834EC21D1762AFD5EC24E571C5FF019BCCD9314", "rexaID":"5dc7f8afb0d062322bf401745db703534477a4a0", "author":"Shailesh Kumar and Melba Crawford and Joydeep Ghosh", "title":"A versatile framework for labelling imagery with a large number of classes", "venue":"Department of Electrical and Computer Engineering", "year":"", "window":"for distinguishing some other pair of classes. Section 2 describes the Bayesian pairwise classifier (BPC) architecture with feature selection. Experimental results on the 26 class <b>letter recognition</b> dataset and, more importantly, the 11 class remote sensing dataset are presented in section 3. 2 Pairwise Classifier Architecture A C class problem is first decomposed into a set of C 2 # two class problems", "mykey":1532},
 {"datasetID":105, "supportID":"9835B022A780977AE6A87B762AB51D920622E75E", "rexaID":"c8866c27be62693c5cedc3d78b570b889b75fa72", "author":"Jonathan Eckstein and Peter L. Hammer and Ying Liu and Mikhail Nediak and Bruno Simeone", "title":"The Maximum Box Problem and its Application to Data Analysis", "venue":"RUTCOR Rutgers Center for Operations Research Rutgers University", "year":"2002", "window":"1. Table 1 also indicates the number of observations for which some data values are missing. In our experiments, we removed the observations with missing values, with the exception of the <b>voting data</b> set, where almost half of the observations contained missing values. In this data set, there are sixteen attributes, and all of them are binary. In this case, we have substituted the missing binary", "mykey":1533},
 {"datasetID":44, "supportID":"983C9D00A186B96FD243ED8F256126FD5AD747D8", "rexaID":"20b8d5b9285a402f14753a766e96889e990a99f3", "author":"Bob Ricks and Dan Ventura", "title":"Training a Quantum Neural Network", "venue":"NIPS", "year":"2003", "window":"an epoch refers to finding and fixing the weight of a single node. We also tried the randomized search algorithm for a few real-world machine learning problems: lenses, <b>Hayes</b> <b>Roth</b> and the iris datasets [19]. The lenses data set is a data set that tries to predict whether people will need soft contact lenses, hard contact lenses or no contacts. The iris dataset details features of three different", "mykey":1534},
 {"datasetID":53, "supportID":"983C9D00A186B96FD243ED8F256126FD5AD747D8", "rexaID":"20b8d5b9285a402f14753a766e96889e990a99f3", "author":"Bob Ricks and Dan Ventura", "title":"Training a Quantum Neural Network", "venue":"NIPS", "year":"2003", "window":"an epoch refers to finding and fixing the weight of a single node. We also tried the randomized search algorithm for a few real-world machine learning problems: lenses, Hayes-Roth and the <b>iris</b> datasets [19]. The lenses data set is a data set that tries to predict whether people will need soft contact lenses, hard contact lenses or no contacts. The iris dataset details features of three different", "mykey":1535},
 {"datasetID":58, "supportID":"983C9D00A186B96FD243ED8F256126FD5AD747D8", "rexaID":"20b8d5b9285a402f14753a766e96889e990a99f3", "author":"Bob Ricks and Dan Ventura", "title":"Training a Quantum Neural Network", "venue":"NIPS", "year":"2003", "window":"an epoch refers to finding and fixing the weight of a single node. We also tried the randomized search algorithm for a few real-world machine learning problems: <b>lenses</b>  Hayes-Roth and the iris datasets [19]. The lenses data set is a data set that tries to predict whether people will need soft contact lenses, hard contact lenses or no contacts. The iris dataset details features of three different", "mykey":1536},
 {"datasetID":56, "supportID":"9872A1F4737BD87CE3D0838E6BBF87E0192D660C", "rexaID":"944b9d70eb0a01d18c91109dfeb566936461a194", "author":"Alexander K. Seewald", "title":"Meta-Learning for Stacked Classification", "venue":"Austrian Research Institute for Artificial Intelligence", "year":"", "window":"using only seven folds produces the exact same result. When removing the base-classifier dependent features, IBk is still the best classifier with an additional error on <b>labor</b>  the smallest dataset. In this case MLR, another high-bias and global learner, is equally good. So wemaytentatively conclude that for this meta-dataset, there seems to be no single feature which can predict the", "mykey":1537},
 {"datasetID":151, "supportID":"989F68889B2F714489E9F1599337A63A4022C253", "rexaID":"d18b9cca12173a8ca3d5a781cadcf847442930f4", "author":"Carlotta Domeniconi and Jing Peng and Dimitrios Gunopulos", "title":"An Adaptive Metric Machine for Pattern Classification", "venue":"NIPS", "year":"2000", "window":"consists of q = 4 measurements made on each of N = 100 iris plants of J = 2 species; 2. <b>Sonar</b> data. This data set consists of q = 60 frequency measurements made on each of N = 208 data of J = 2 classes (``mines'' and ``rocks''); 3. Vowel data. This example has q = 10 measurements and 11 classes. There are total", "mykey":1538},
 {"datasetID":42, "supportID":"989F68889B2F714489E9F1599337A63A4022C253", "rexaID":"d18b9cca12173a8ca3d5a781cadcf847442930f4", "author":"Carlotta Domeniconi and Jing Peng and Dimitrios Gunopulos", "title":"An Adaptive Metric Machine for Pattern Classification", "venue":"NIPS", "year":"2000", "window":"N = 208 data of J = 2 classes (``mines'' and ``rocks''); 3. Vowel data. This example has q = 10 measurements and 11 classes. There are total of N = 528 samples in this example; 4. <b>Glass</b> data. This data set consists of q = 9 chemical attributes measured for each of N = 214 data of J = 6 classes; 5. Image data. This data set consists of 40 texture images that are manually classified into 15 classes. The", "mykey":1539},
 {"datasetID":53, "supportID":"989F68889B2F714489E9F1599337A63A4022C253", "rexaID":"d18b9cca12173a8ca3d5a781cadcf847442930f4", "author":"Carlotta Domeniconi and Jing Peng and Dimitrios Gunopulos", "title":"An Adaptive Metric Machine for Pattern Classification", "venue":"NIPS", "year":"2000", "window":"used were taken from the UCI Machine Learning Database Repository [10], except for the unreleased image data set. They are: 1. <b>Iris</b> data. This data set consists of q = 4 measurements made on each of N = 100 iris plants of J = 2 species; 2. Sonar data. This data set consists of q = 60 frequency measurements", "mykey":1540},
 {"datasetID":60, "supportID":"989F68889B2F714489E9F1599337A63A4022C253", "rexaID":"d18b9cca12173a8ca3d5a781cadcf847442930f4", "author":"Carlotta Domeniconi and Jing Peng and Dimitrios Gunopulos", "title":"An Adaptive Metric Machine for Pattern Classification", "venue":"NIPS", "year":"2000", "window":"consists of q = 16 numerical attributes and J = 26 classes; 8. <b>Liver</b> data. This data set consists of 345 instances, represented by q = 6 numerical attributes, and J = 2 classes; and 9. Lung data. This example has 32 instances having q = 56 numerical features and J = 3 classes. Results:", "mykey":1541},
 {"datasetID":2, "supportID":"98D6E2E3F83BB855FAEF19CF78C0CD69E6F4683F", "rexaID":"8931d2a4a8256ea88abea3ea3ac820fd421ce0b1", "author":"Stephen D. Bay and Michael J. Pazzani", "title":"Detecting Group Differences: Mining Contrast Sets", "venue":"Data Min. Knowl. Discov, 5", "year":"2001", "window":"3 This program is available from http://fuzzy.cs.Uni-Magdeburg.de/#borgelt/.Version 1.8 of his program is incorporated in the data mining tool Clementine. 18 issues. We used the following datasets which are summarized in Table 2. <b>Adult</b>  The Adult Census data contains information extracted from the 1994 CurrentPopulation Survey. There are variables such as age, working class, education, sex,", "mykey":1542},
 {"datasetID":20, "supportID":"98D6E2E3F83BB855FAEF19CF78C0CD69E6F4683F", "rexaID":"8931d2a4a8256ea88abea3ea3ac820fd421ce0b1", "author":"Stephen D. Bay and Michael J. Pazzani", "title":"Detecting Group Differences: Mining Contrast Sets", "venue":"Data Min. Knowl. Discov, 5", "year":"2001", "window":"sized divisions by frequency (e.g. income) or interval width (e.g. age). Finally,we further randomly sampled the data to obtain a 1 in 1000 sample. Federal <b>Census</b> data is one of the most difficult data sets to mine because of the long average record width coupled with the high number of popular attribute-value pairs which occur frequently in many records. These two factors combine to result in many", "mykey":1543},
 {"datasetID":127, "supportID":"98D6E2E3F83BB855FAEF19CF78C0CD69E6F4683F", "rexaID":"8931d2a4a8256ea88abea3ea3ac820fd421ce0b1", "author":"Stephen D. Bay and Michael J. Pazzani", "title":"Detecting Group Differences: Mining Contrast Sets", "venue":"Data Min. Knowl. Discov, 5", "year":"2001", "window":"example, if we are mining at a support difference of 10% and group A has a support of 11% we still need to mine group B as long as its support is non-zero. STUCCO was very fast and did well on all data sets, even on Mushroom and <b>IPUMS</b> which are among the most difficult data sets for mining algorithms. STUCCO was slower than Apriori on UCI Admissions by a factor of approximately three. This is probably", "mykey":1544},
 {"datasetID":73, "supportID":"98D6E2E3F83BB855FAEF19CF78C0CD69E6F4683F", "rexaID":"8931d2a4a8256ea88abea3ea3ac820fd421ce0b1", "author":"Stephen D. Bay and Michael J. Pazzani", "title":"Detecting Group Differences: Mining Contrast Sets", "venue":"Data Min. Knowl. Discov, 5", "year":"2001", "window":"The Adult Census data contains information extracted from the 1994 CurrentPopulation Survey. There are variables such as age, working class, education, sex, hours worked, salary, etc. <b>Mushroom</b>  This data set describes mushrooms and their physical properties such as shape, odor, habitat, etc. Mushroom is not a true observational data set as the examples are not drawn from individual instances but rather", "mykey":1545},
 {"datasetID":14, "supportID":"996D122EE2F4B1B9300F3A35CCD5424ABFCBF7FC", "rexaID":"bcdb5e21043b224589c8ba0c670a49d10f5f5580", "author":"Igor Fischer and Jan Poland", "title":"Amplifying the Block Matrix Structure for Spectral Clustering", "venue":"Telecommunications Lab", "year":"2005", "window":"are common benchmark sets with real-world data (Murphy & Aha, 1994): the iris, the wine and the <b>breast</b> <b>cancer</b> data set. Both our methods perform very well on iris and breast cancer. However, the wine data set is too sparse for context-dependent method: only 178 points in 13 dimensions, giving the conductivity too", "mykey":1546},
 {"datasetID":53, "supportID":"996D122EE2F4B1B9300F3A35CCD5424ABFCBF7FC", "rexaID":"bcdb5e21043b224589c8ba0c670a49d10f5f5580", "author":"Igor Fischer and Jan Poland", "title":"Amplifying the Block Matrix Structure for Spectral Clustering", "venue":"Telecommunications Lab", "year":"2005", "window":"are common benchmark sets with real-world data (Murphy & Aha, 1994): the <b>iris</b>  the wine and the breast cancer data set. Both our methods perform very well on iris and breast cancer. However, the wine data set is too sparse for context-dependent method: only 178 points in 13 dimensions, giving the conductivity too", "mykey":1547},
 {"datasetID":109, "supportID":"996D122EE2F4B1B9300F3A35CCD5424ABFCBF7FC", "rexaID":"bcdb5e21043b224589c8ba0c670a49d10f5f5580", "author":"Igor Fischer and Jan Poland", "title":"Amplifying the Block Matrix Structure for Spectral Clustering", "venue":"Telecommunications Lab", "year":"2005", "window":"are common benchmark sets with real-world data (Murphy & Aha, 1994): the iris, the <b>wine</b> and the breast cancer data set. Both our methods perform very well on iris and breast cancer. However, the wine data set is too sparse for context-dependent method: only 178 points in 13 dimensions, giving the conductivity too", "mykey":1548},
 {"datasetID":53, "supportID":"996E461480D5974D242CD3AC5C6CE544311D43FA", "rexaID":"546cbe749d666efb8ff699628e535e0fd083f6bc", "author":"Norbert Jankowski", "title":"Survey of Neural Transfer Functions", "venue":"Department of Computer Methods, Nicholas Copernicus University", "year":"", "window":"sphere defined by this metric. The influence of input renormalization (using Minkovsky distance functions) on the shapes of decision borders is illustrated in Fig. 30 for the classical <b>Iris</b> flowers dataset (only the last two input features, x 3 and x 4 are shown, for description of the data cf. [89]). Dramatic changes in the shapes of decision borders for different Minkovsky metrices are observed.", "mykey":1549},
 {"datasetID":67, "supportID":"996E461480D5974D242CD3AC5C6CE544311D43FA", "rexaID":"546cbe749d666efb8ff699628e535e0fd083f6bc", "author":"Norbert Jankowski", "title":"Survey of Neural Transfer Functions", "venue":"Department of Computer Methods, Nicholas Copernicus University", "year":"", "window":"results of RBF are not similar to the results of MLP. In some cases crossvalidation errors were twice as large using MLP than RBF (for example for the <b>DNA</b> dataset RBF gave 4.1% of error and was the best of all methods while MLP gave 8.8 % of errors and was on 12-th position, while for the Belgian Power data the situation is reversed, with MLP giving 1.7% of", "mykey":1550},
 {"datasetID":14, "supportID":"9A4AB71FA8007EBC766C93E25181A097B2216D38", "rexaID":"5193dfc0a9d39b5f86fe360d6beff81aa9b7390e", "author":"Nikunj C. Oza and Stuart J. Russell", "title":"Experimental comparisons of online and batch versions of bagging and boosting", "venue":"KDD", "year":"2001", "window":"We tested bagging and boosting with decision trees only on some of the smaller datasets (Promoters, Balance, <b>Breast</b> <b>Cancer</b>  Car Evaluation) because the lossless decision tree algorithm is too expensive with larger datasets in online mode. Bagging and online bagging perform comparably", "mykey":1551},
 {"datasetID":17, "supportID":"9A4AB71FA8007EBC766C93E25181A097B2216D38", "rexaID":"5193dfc0a9d39b5f86fe360d6beff81aa9b7390e", "author":"Nikunj C. Oza and Stuart J. Russell", "title":"Experimental comparisons of online and batch versions of bagging and boosting", "venue":"KDD", "year":"2001", "window":"learning and its effect on ensemble performance. 6. ACKNOWLEDGEMENTS The <b>Wisconsin</b> <b>Breast</b> <b>Cancer</b> dataset was obtained from the University of Wisconsin Hospitals, Madison from Dr. William H. Wolberg. The Forest Covertype is Copyrighted 1998 by Jock A. Blackard and Colorado State University. 7.", "mykey":1552},
 {"datasetID":15, "supportID":"9A4AB71FA8007EBC766C93E25181A097B2216D38", "rexaID":"5193dfc0a9d39b5f86fe360d6beff81aa9b7390e", "author":"Nikunj C. Oza and Stuart J. Russell", "title":"Experimental comparisons of online and batch versions of bagging and boosting", "venue":"KDD", "year":"2001", "window":"learning and its effect on ensemble performance. 6. ACKNOWLEDGEMENTS The <b>Wisconsin</b> <b>Breast</b> <b>Cancer</b> dataset was obtained from the University of Wisconsin Hospitals, Madison from Dr. William H. Wolberg. The Forest Covertype is Copyrighted 1998 by Jock A. Blackard and Colorado State University. 7.", "mykey":1553},
 {"datasetID":16, "supportID":"9A4AB71FA8007EBC766C93E25181A097B2216D38", "rexaID":"5193dfc0a9d39b5f86fe360d6beff81aa9b7390e", "author":"Nikunj C. Oza and Stuart J. Russell", "title":"Experimental comparisons of online and batch versions of bagging and boosting", "venue":"KDD", "year":"2001", "window":"learning and its effect on ensemble performance. 6. ACKNOWLEDGEMENTS The <b>Wisconsin</b> <b>Breast</b> <b>Cancer</b> dataset was obtained from the University of Wisconsin Hospitals, Madison from Dr. William H. Wolberg. The Forest Covertype is Copyrighted 1998 by Jock A. Blackard and Colorado State University. 7.", "mykey":1554},
 {"datasetID":19, "supportID":"9A4AB71FA8007EBC766C93E25181A097B2216D38", "rexaID":"5193dfc0a9d39b5f86fe360d6beff81aa9b7390e", "author":"Nikunj C. Oza and Stuart J. Russell", "title":"Experimental comparisons of online and batch versions of bagging and boosting", "venue":"KDD", "year":"2001", "window":"for which we ran the bagging and boosting algorithms with decision trees was the <b>Car</b> Evaluation dataset from the UCI Repository. Figure 5 shows the learning curve. Batch and online bagging with decision trees perform almost identically (and always significantly better than a single decision tree).", "mykey":1555},
 {"datasetID":20, "supportID":"9A4AB71FA8007EBC766C93E25181A097B2216D38", "rexaID":"5193dfc0a9d39b5f86fe360d6beff81aa9b7390e", "author":"Nikunj C. Oza and Stuart J. Russell", "title":"Experimental comparisons of online and batch versions of bagging and boosting", "venue":"KDD", "year":"2001", "window":"used in our experiments. For the Soybean and <b <b>Census</b> Income</b> datasets, we have given the sizes of the supplied training and test sets. For the remaining datasets, we have given the sizes of the training and test sets in our #ve-fold crossvalidation runs. Data Set", "mykey":1556},
 {"datasetID":117, "supportID":"9A4AB71FA8007EBC766C93E25181A097B2216D38", "rexaID":"5193dfc0a9d39b5f86fe360d6beff81aa9b7390e", "author":"Nikunj C. Oza and Stuart J. Russell", "title":"Experimental comparisons of online and batch versions of bagging and boosting", "venue":"KDD", "year":"2001", "window":"used in our experiments. For the Soybean and <b>Census</b> <b>Income</b> datasets, we have given the sizes of the supplied training and test sets. For the remaining datasets, we have given the sizes of the training and test sets in our #ve-fold crossvalidation runs. Data Set", "mykey":1557},
 {"datasetID":31, "supportID":"9A4AB71FA8007EBC766C93E25181A097B2216D38", "rexaID":"5193dfc0a9d39b5f86fe360d6beff81aa9b7390e", "author":"Nikunj C. Oza and Stuart J. Russell", "title":"Experimental comparisons of online and batch versions of bagging and boosting", "venue":"KDD", "year":"2001", "window":"[2], two datasets (Census Income and Forest <b>Covertype</b>  from the UCI KDD archive [1], and three synthetic datasets. We give their sizes and numbers of attributes and classes in Table 1. All three of our synthetic", "mykey":1558},
 {"datasetID":90, "supportID":"9A4AB71FA8007EBC766C93E25181A097B2216D38", "rexaID":"5193dfc0a9d39b5f86fe360d6beff81aa9b7390e", "author":"Nikunj C. Oza and Stuart J. Russell", "title":"Experimental comparisons of online and batch versions of bagging and boosting", "venue":"KDD", "year":"2001", "window":"only the first ten examples before being 2 Recall that we test base model hm on the training examples in order to adjust their weights before using them to generate base model hm+1 . Table 1: The datasets used in our experiments. For the <b>Soybean</b> and Census Income datasets, we have given the sizes of the supplied training and test sets. For the remaining datasets, we have given the sizes of the", "mykey":1559},
 {"datasetID":91, "supportID":"9A4AB71FA8007EBC766C93E25181A097B2216D38", "rexaID":"5193dfc0a9d39b5f86fe360d6beff81aa9b7390e", "author":"Nikunj C. Oza and Stuart J. Russell", "title":"Experimental comparisons of online and batch versions of bagging and boosting", "venue":"KDD", "year":"2001", "window":"only the first ten examples before being 2 Recall that we test base model hm on the training examples in order to adjust their weights before using them to generate base model hm+1 . Table 1: The datasets used in our experiments. For the <b>Soybean</b> and Census Income datasets, we have given the sizes of the supplied training and test sets. For the remaining datasets, we have given the sizes of the", "mykey":1560},
 {"datasetID":53, "supportID":"9A5CF4BB1AECD4BD3AEAE921DA32C0D4C98AFB76", "rexaID":"9acaa3dfebccac9d79abb08f7eced65482be1631", "author":"Inderjit S. Dhillon and Dharmendra S. Modha and W. Scott Spangler", "title":"Class visualization of high-dimensional data with applications", "venue":"Department of Computer Sciences, University of Texas", "year":"2002", "window":"to preserve the inter-class structure present in the original multidimensional space R d . The underlying theory, supported by several illustrations on the <b>Iris</b> plant and ISOLET speech reognition data sets [Blake et al., 1998], is presented in Section 2. Class-similarity graphs to enhance each individual two-dimensional projection of the data (see Section 3). 2 Class tours that show sequences of the", "mykey":1561},
 {"datasetID":54, "supportID":"9A5CF4BB1AECD4BD3AEAE921DA32C0D4C98AFB76", "rexaID":"9acaa3dfebccac9d79abb08f7eced65482be1631", "author":"Inderjit S. Dhillon and Dharmendra S. Modha and W. Scott Spangler", "title":"Class visualization of high-dimensional data with applications", "venue":"Department of Computer Sciences, University of Texas", "year":"2002", "window":"to preserve the inter-class structure present in the original multidimensional space R d . The underlying theory, supported by several illustrations on the Iris plant and <b>ISOLET</b> speech reognition data sets [Blake et al., 1998], is presented in Section 2. Class-similarity graphs to enhance each individual two-dimensional projection of the data (see Section 3). 2 Class tours that show sequences of the", "mykey":1562},
 {"datasetID":1, "supportID":"9A8276E97FC149884AD95CD5A9AAE312A351FFE0", "rexaID":"3fe33086239c6beec412b11fde015871a85c6011", "author":"Sally Jo Cunningham", "title":"Dataset cataloging metadata for machine learning applications and research", "venue":"Department of Computer Science University of Waikato", "year":"", "window":"At UCI, for example, the documentation records range from the highly specific (for example, the <b>abalone</b> dataset record in Figure 1a) to the nearly non-existent (the \"undocumented databases\" directory at UCI, as typified by the economic sanctions data description in Figure 1b). This variability is not", "mykey":1563},
 {"datasetID":153, "supportID":"9A8276E97FC149884AD95CD5A9AAE312A351FFE0", "rexaID":"3fe33086239c6beec412b11fde015871a85c6011", "author":"Sally Jo Cunningham", "title":"Dataset cataloging metadata for machine learning applications and research", "venue":"Department of Computer Science University of Waikato", "year":"", "window":";;; -*- Mode: LISP; Syntax: common-lisp; Base: 10 -*- ;;;totally undocumented <b>sanctions</b> database. ;;;Pazzani AAAI-86 or EWSL-88 (a) portion of abalone data description (b) <b>economic sanctions</b> dataset header Figure 1. Content independent catalog samples from the UCI ML dataset repository content-dependent: In comparison to content-independent metadata, content-dependent descriptors are directly", "mykey":1564},
 {"datasetID":52, "supportID":"9ABD1A7995047CAF7ABB79B07682B1FFE981D9BD", "rexaID":"1570e3547e5d0131eee5b25f40359409b8d8c396", "author":"Rajesh Parekh and Jihoon Yang and Vasant Honavar", "title":"Constructive Neural-Network Learning Algorithms for Pattern Classification", "venue":"", "year":"", "window":"outputs represents the number of output classes, and attributes describes the type of input attributes of the patterns. The real-world datasets <b>ionosphere</b>  pima, segmentation, and vehicle are available at the UCI Machine Learning Repository [32] while the 3-circles dataset was artificially generated. The 3-circles dataset comprises of 1800", "mykey":1565},
 {"datasetID":149, "supportID":"9ABD1A7995047CAF7ABB79B07682B1FFE981D9BD", "rexaID":"1570e3547e5d0131eee5b25f40359409b8d8c396", "author":"Rajesh Parekh and Jihoon Yang and Vasant Honavar", "title":"Constructive Neural-Network Learning Algorithms for Pattern Classification", "venue":"", "year":"", "window":"outputs represents the number of output classes, and attributes describes the type of input attributes of the patterns. The real-world datasets ionosphere, pima, segmentation, and <b>vehicle</b> are available at the UCI Machine Learning Repository [32] while the 3-circles dataset was artificially generated. The 3-circles dataset comprises of 1800", "mykey":1566},
 {"datasetID":2, "supportID":"9ACFD6C46DC7B85BEFED23535BDBEB734B1AD849", "rexaID":"03a71aaf988c71c8022be08734da8e376f7fe037", "author":"Omid Madani and David M. Pennock and Gary William Flake", "title":"Co-Validation: Using Model Disagreement to Validate Classification Algorithms", "venue":"Yahoo! Research Labs", "year":"", "window":"unlabeled data does not tend to wildly underestimate error, even though it's theoretically possible. 3 Experiments We conducted experiments on the 20 Newsgroups and Reuters-21578 test categorization datasets 1 , and the Votes, Chess, <b>Adult</b>  and Optics datasets from the UCI collection [BKM98]. We chose 1 Available from http://www.ics.uci.edu/ and http://www.daviddlewis.com/resources/testcollections/ two", "mykey":1567},
 {"datasetID":23, "supportID":"9ACFD6C46DC7B85BEFED23535BDBEB734B1AD849", "rexaID":"03a71aaf988c71c8022be08734da8e376f7fe037", "author":"Omid Madani and David M. Pennock and Gary William Flake", "title":"Co-Validation: Using Model Disagreement to Validate Classification Algorithms", "venue":"Yahoo! Research Labs", "year":"", "window":"unlabeled data does not tend to wildly underestimate error, even though it's theoretically possible. 3 Experiments We conducted experiments on the 20 Newsgroups and Reuters-21578 test categorization datasets 1 , and the Votes, <b>Chess</b>  Adult, and Optics datasets from the UCI collection [BKM98]. We chose 1 Available from http://www.ics.uci.edu/ and http://www.daviddlewis.com/resources/testcollections/ two", "mykey":1568},
 {"datasetID":21, "supportID":"9ACFD6C46DC7B85BEFED23535BDBEB734B1AD849", "rexaID":"03a71aaf988c71c8022be08734da8e376f7fe037", "author":"Omid Madani and David M. Pennock and Gary William Flake", "title":"Co-Validation: Using Model Disagreement to Validate Classification Algorithms", "venue":"Yahoo! Research Labs", "year":"", "window":"unlabeled data does not tend to wildly underestimate error, even though it's theoretically possible. 3 Experiments We conducted experiments on the 20 Newsgroups and Reuters-21578 test categorization datasets 1 , and the Votes, <b>Chess</b>  Adult, and Optics datasets from the UCI collection [BKM98]. We chose 1 Available from http://www.ics.uci.edu/ and http://www.daviddlewis.com/resources/testcollections/ two", "mykey":1569},
 {"datasetID":22, "supportID":"9ACFD6C46DC7B85BEFED23535BDBEB734B1AD849", "rexaID":"03a71aaf988c71c8022be08734da8e376f7fe037", "author":"Omid Madani and David M. Pennock and Gary William Flake", "title":"Co-Validation: Using Model Disagreement to Validate Classification Algorithms", "venue":"Yahoo! Research Labs", "year":"", "window":"unlabeled data does not tend to wildly underestimate error, even though it's theoretically possible. 3 Experiments We conducted experiments on the 20 Newsgroups and Reuters-21578 test categorization datasets 1 , and the Votes, <b>Chess</b>  Adult, and Optics datasets from the UCI collection [BKM98]. We chose 1 Available from http://www.ics.uci.edu/ and http://www.daviddlewis.com/resources/testcollections/ two", "mykey":1570},
 {"datasetID":137, "supportID":"9ACFD6C46DC7B85BEFED23535BDBEB734B1AD849", "rexaID":"03a71aaf988c71c8022be08734da8e376f7fe037", "author":"Omid Madani and David M. Pennock and Gary William Flake", "title":"Co-Validation: Using Model Disagreement to Validate Classification Algorithms", "venue":"Yahoo! Research Labs", "year":"", "window":"unlabeled data does not tend to wildly underestimate error, even though it's theoretically possible. 3 Experiments We conducted experiments on the 20 Newsgroups and <b>Reuters</b> 21578 test categorization datasets 1 , and the Votes, Chess, Adult, and Optics datasets from the UCI collection [BKM98]. We chose 1 Available from http://www.ics.uci.edu/ and http://www.daviddlewis.com/resources/testcollections/ two", "mykey":1571},
 {"datasetID":5, "supportID":"9AE8D5A6870E2F629B3CE39D2090C56D07727E1B", "rexaID":"e3fb3565af764647303c58f32b57a23f9eae18c0", "author":"Krista Lagus and Esa Alhoniemi and Jeremias Seppa and Antti Honkela and Arno Wagner", "title":"INDEPENDENT VARIABLE GROUP ANALYSIS IN LEARNING COMPACT REPRESENTATIONS FOR DATA", "venue":"Neural Networks Research Centre, Helsinki University of Technology", "year":"", "window":"models optimized carefully using the IVGA implementation. The model search of our IVGA implementation was able to discover the best grouping, i.e., the one with the smallest cost. 3.2. <b>Arrhythmia</b> data set The identification of different types of heart problems, namely cardiac arrhythmias, is carried out based on electrocardiography measurings from a large number of electrodes. We used a freely", "mykey":1572},
 {"datasetID":45, "supportID":"9AE8D5A6870E2F629B3CE39D2090C56D07727E1B", "rexaID":"e3fb3565af764647303c58f32b57a23f9eae18c0", "author":"Krista Lagus and Esa Alhoniemi and Jeremias Seppa and Antti Honkela and Arno Wagner", "title":"INDEPENDENT VARIABLE GROUP ANALYSIS IN LEARNING COMPACT REPRESENTATIONS FOR DATA", "venue":"Neural Networks Research Centre, Helsinki University of Technology", "year":"", "window":"models optimized carefully using the IVGA implementation. The model search of our IVGA implementation was able to discover the best grouping, i.e., the one with the smallest cost. 3.2. Arrhythmia data set The identification of different types of <b>heart</b> problems, namely cardiac arrhythmias, is carried out based on electrocardiography measurings from a large number of electrodes. We used a freely", "mykey":1573},
 {"datasetID":155, "supportID":"9B0ED36480B43C38D03240BD6A9B05CF89AD707B", "rexaID":"f91a9d851efd2169d5f16f8bfd5c7b9d2b81655c", "author":"C. esar and Cesar Guerra-Salcedo and Darrell Whitley", "title":"Feature Selection Mechanisms for Ensemble Creation : A Genetic Search Perspective", "venue":"Department of Computer Science Colorado State University", "year":"", "window":"Features Classes Train Size Test Size LandSat 36 6 4435 2000 DNA 180 39 2000 1186 Segment 19 7 210 2100 <b>Cloud</b> 204 10 1000 633 Table 1: Dataset employed for the experiments. In the DNA dataset the attributes values are 0 or 1. In the Segment and the Cloud dataset the attributes values are floats. In the LandSat dataset the attribute values", "mykey":1574},
 {"datasetID":67, "supportID":"9B0ED36480B43C38D03240BD6A9B05CF89AD707B", "rexaID":"f91a9d851efd2169d5f16f8bfd5c7b9d2b81655c", "author":"C. esar and Cesar Guerra-Salcedo and Darrell Whitley", "title":"Feature Selection Mechanisms for Ensemble Creation : A Genetic Search Perspective", "venue":"Department of Computer Science Colorado State University", "year":"", "window":"Features Classes Train Size Test Size LandSat 36 6 4435 2000 <b>DNA</b> 180 39 2000 1186 Segment 19 7 210 2100 Cloud 204 10 1000 633 Table 1: Dataset employed for the experiments. In the DNA dataset the attributes values are 0 or 1. In the Segment and the Cloud dataset the attributes values are floats. In the LandSat dataset the attribute values", "mykey":1575},
 {"datasetID":146, "supportID":"9B0ED36480B43C38D03240BD6A9B05CF89AD707B", "rexaID":"f91a9d851efd2169d5f16f8bfd5c7b9d2b81655c", "author":"C. esar and Cesar Guerra-Salcedo and Darrell Whitley", "title":"Feature Selection Mechanisms for Ensemble Creation : A Genetic Search Perspective", "venue":"Department of Computer Science Colorado State University", "year":"", "window":"the RSM method. To classify an unseen case x, each classifier in the ensemble votes on the class for x. The class with the most votes is the class predicted by the ensemble (majority-vote scheme). Dataset Features Classes Train Size Test Size <b>LandSat</b> 36 6 4435 2000 DNA 180 39 2000 1186 Segment 19 7 210 2100 Cloud 204 10 1000 633 Table 1: Dataset employed for the experiments. In the DNA dataset the", "mykey":1576},
 {"datasetID":98, "supportID":"9B0ED36480B43C38D03240BD6A9B05CF89AD707B", "rexaID":"f91a9d851efd2169d5f16f8bfd5c7b9d2b81655c", "author":"C. esar and Cesar Guerra-Salcedo and Darrell Whitley", "title":"Feature Selection Mechanisms for Ensemble Creation : A Genetic Search Perspective", "venue":"Department of Computer Science Colorado State University", "year":"", "window":"1993) and a table-based classifier called Euclidean Decision Tables (EDT) (Guerra-Salcedo & Whitley 1998). Setups and Results A series of experiments were carried out using publicly available datasets provided by the Project <b>Statlog</b> 1 the UCI machine learning repository (C. Blake & Merz 1998) and by Richard Bankert of the Naval Research Laboratory. Table 1 shows the datasets employed for this", "mykey":1577},
 {"datasetID":14, "supportID":"9B6CF5D98E3C9C398773066E1C56EA9E8424097E", "rexaID":"8afa6796645ce4b0642db26c822cf6bfa8cc4d0d", "author":"Wl odzisl/aw Duch and Rudy Setiono and Jacek M. Zurada", "title":"Computational intelligence methods for rule-based data understanding", "venue":"", "year":"", "window":"data. Large number of rules will usually lead to poor generalization, and the insight into the knowledge hidden in the data will be lost. C. Wisconsin <b>breast</b> <b>cancer</b> data. The Wisconsin breast cancer dataset [132] is one of the favorite benchmark datasets for testing classifiers (Table V). Properties of cancer cells were collected for 699 cases, with 458 benign (65.5%) and 241 (34.5%) malignant cases of", "mykey":1578},
 {"datasetID":17, "supportID":"9B6CF5D98E3C9C398773066E1C56EA9E8424097E", "rexaID":"8afa6796645ce4b0642db26c822cf6bfa8cc4d0d", "author":"Wl odzisl/aw Duch and Rudy Setiono and Jacek M. Zurada", "title":"Computational intelligence methods for rule-based data understanding", "venue":"", "year":"", "window":"data. Large number of rules will usually lead to poor generalization, and the insight into the knowledge hidden in the data will be lost. C. <b>Wisconsin</b> <b>breast</b> <b>cancer</b> data. The Wisconsin breast cancer dataset [132] is one of the favorite benchmark datasets for testing classifiers (Table V). Properties of cancer cells were collected for 699 cases, with 458 benign (65.5%) and 241 (34.5%) malignant cases of", "mykey":1579},
 {"datasetID":15, "supportID":"9B6CF5D98E3C9C398773066E1C56EA9E8424097E", "rexaID":"8afa6796645ce4b0642db26c822cf6bfa8cc4d0d", "author":"Wl odzisl/aw Duch and Rudy Setiono and Jacek M. Zurada", "title":"Computational intelligence methods for rule-based data understanding", "venue":"", "year":"", "window":"data. Large number of rules will usually lead to poor generalization, and the insight into the knowledge hidden in the data will be lost. C. <b>Wisconsin</b> <b>breast</b> <b>cancer</b> data. The Wisconsin breast cancer dataset [132] is one of the favorite benchmark datasets for testing classifiers (Table V). Properties of cancer cells were collected for 699 cases, with 458 benign (65.5%) and 241 (34.5%) malignant cases of", "mykey":1580},
 {"datasetID":16, "supportID":"9B6CF5D98E3C9C398773066E1C56EA9E8424097E", "rexaID":"8afa6796645ce4b0642db26c822cf6bfa8cc4d0d", "author":"Wl odzisl/aw Duch and Rudy Setiono and Jacek M. Zurada", "title":"Computational intelligence methods for rule-based data understanding", "venue":"", "year":"", "window":"data. Large number of rules will usually lead to poor generalization, and the insight into the knowledge hidden in the data will be lost. C. <b>Wisconsin</b> <b>breast</b> <b>cancer</b> data. The Wisconsin breast cancer dataset [132] is one of the favorite benchmark datasets for testing classifiers (Table V). Properties of cancer cells were collected for 699 cases, with 458 benign (65.5%) and 241 (34.5%) malignant cases of", "mykey":1581},
 {"datasetID":34, "supportID":"9B6CF5D98E3C9C398773066E1C56EA9E8424097E", "rexaID":"8afa6796645ce4b0642db26c822cf6bfa8cc4d0d", "author":"Wl odzisl/aw Duch and Rudy Setiono and Jacek M. Zurada", "title":"Computational intelligence methods for rule-based data understanding", "venue":"", "year":"", "window":"vector. Prototype-based rules are little known so far and computationally more costly to find, but certainly for some data, they may be simple and accurate. D. <b>Diabetes</b>  The \"Pima Indian Diabetes\" dataset [118] is also frequently used as a benchmark data [89], [135], [134], [136]. All patients were females, at least 21 years old, of Pima Indian heritage. 768 cases have been collected, 500 (65.1%)", "mykey":1582},
 {"datasetID":53, "supportID":"9B6CF5D98E3C9C398773066E1C56EA9E8424097E", "rexaID":"8afa6796645ce4b0642db26c822cf6bfa8cc4d0d", "author":"Wl odzisl/aw Duch and Rudy Setiono and Jacek M. Zurada", "title":"Computational intelligence methods for rule-based data understanding", "venue":"", "year":"", "window":"larger input uncertainties do not change in subsequent minimizations. VIII. EXTRACTION OF RULES -- ILLUSTRATIVE EXAMPLE The process of rule extraction is illustrated here using the well-known <b>Iris</b> dataset, provided by Fisher in 1936. The data PROCEEDINGS OF IEEE, VOL. XX, NO. YY, 2003 17 have been obtained from the UCI machine learning repository [118]. The Iris data have 150 vectors evenly", "mykey":1583},
 {"datasetID":73, "supportID":"9B6CF5D98E3C9C398773066E1C56EA9E8424097E", "rexaID":"8afa6796645ce4b0642db26c822cf6bfa8cc4d0d", "author":"Wl odzisl/aw Duch and Rudy Setiono and Jacek M. Zurada", "title":"Computational intelligence methods for rule-based data understanding", "venue":"", "year":"", "window":"activation of the first neuron. Adding a second neuron and training it on the remaining cases generates two additional rules, R 3 handling 40 cases and R 4 handling only 8 cases. For the <b>mushroom</b> dataset, SSV tree has found a 100% accurate solution that can be described as four logical rules using only five attributes. The first two rules are identical to the rules given above, but the remaining two", "mykey":1584},
 {"datasetID":79, "supportID":"9B6CF5D98E3C9C398773066E1C56EA9E8424097E", "rexaID":"8afa6796645ce4b0642db26c822cf6bfa8cc4d0d", "author":"Wl odzisl/aw Duch and Rudy Setiono and Jacek M. Zurada", "title":"Computational intelligence methods for rule-based data understanding", "venue":"", "year":"", "window":"vector. Prototype-based rules are little known so far and computationally more costly to find, but certainly for some data, they may be simple and accurate. D. <b>Diabetes</b>  The  <b>Pima</b> Indian Diabetes\" dataset [118] is also frequently used as a benchmark data [89], [135], [134], [136]. All patients were females, at least 21 years old, of Pima Indian heritage. 768 cases have been collected, 500 (65.1%)", "mykey":1585},
 {"datasetID":148, "supportID":"9B6CF5D98E3C9C398773066E1C56EA9E8424097E", "rexaID":"8afa6796645ce4b0642db26c822cf6bfa8cc4d0d", "author":"Wl odzisl/aw Duch and Rudy Setiono and Jacek M. Zurada", "title":"Computational intelligence methods for rule-based data understanding", "venue":"", "year":"", "window":"Indeed, visual inspection of the data using multidimensional scaling would show many paired vectors, including 26 identical pairs of vectors in the training data. G. NASA <b>Shuttle</b> The Shuttle dataset from NASA contains nine continuous numerical attributes related to the positions of radiators in the Space Shuttle [118]. There are 43500 training vectors and 14500 test vectors, divided into seven", "mykey":1586},
 {"datasetID":98, "supportID":"9B6CF5D98E3C9C398773066E1C56EA9E8424097E", "rexaID":"8afa6796645ce4b0642db26c822cf6bfa8cc4d0d", "author":"Wl odzisl/aw Duch and Rudy Setiono and Jacek M. Zurada", "title":"Computational intelligence methods for rule-based data understanding", "venue":"", "year":"", "window":"diabetes. Eight attributes describe age, number of times pregnant, body mass index, plasma glucose concentration, diastolic blood pressure, diabetes pedigree function, and other medical tests. This dataset was used in the <b>Statlog</b> project [89], with the best 10-fold cross-validation accuracy around 77.7% obtained by logistic discriminant analysis. Our estimation of variance on cross-validation", "mykey":1587},
 {"datasetID":102, "supportID":"9B6CF5D98E3C9C398773066E1C56EA9E8424097E", "rexaID":"8afa6796645ce4b0642db26c822cf6bfa8cc4d0d", "author":"Wl odzisl/aw Duch and Rudy Setiono and Jacek M. Zurada", "title":"Computational intelligence methods for rule-based data understanding", "venue":"", "year":"", "window":"Simple logical rules are also quite competitive in this case, allowing for understanding of important factors that determine the diagnosis. E. The hypothyroid data. This is a somewhat larger medical dataset [118], containing screening tests for <b>thyroid</b> problems. The training data have 3772 medical records collected in the first year, and the test data have 3428 cases collected in the next year of the", "mykey":1588},
 {"datasetID":124, "supportID":"9B73EFEA8762F7B4EC036E0887D32F3D7B2ECF75", "rexaID":"9182a5f903de880381f975a70365a46fe603bdf3", "author":"Xiaofeng He and Partha Niyogi", "title":"Locality Preserving Projections", "venue":"NIPS", "year":"2003", "window":"Images Much research [5][6][7] has suggested that the human <b>face</b> images reside on a manifold embedded in the image space. In this subsection, we applied LPP to images of faces. The same <b>face image</b> dataset used in [5] is used for this experiment. Fig. 2 shows the mapping results. The images of faces are mapped into the 2-dimensional plane described by the first two coordinates of the Locality", "mykey":1589},
 {"datasetID":42, "supportID":"9B7F1C9CCB6F45393F9F47AE01BF7E0DFF955FB7", "rexaID":"8e36921ac4908ffda6ec3eeef41d8c840c9fd884", "author":"Francesco Masulli and Giorgio Valentini", "title":"Quantitative Evaluation of Dependence among Outputs in ECOC Classifiers Using Mutual Information Based Measures", "venue":"Universitdi Genova DISI - Dipartimento di Informatica e Scienze dell'Informazione INFM - Istituto Nazionale per la Fisica della Materia", "year":"", "window":"the dependence among codeword bits errors [12, 8]. In our experimentation we evaluate this dependence in ECOC monolithic and ECOC PND learning machines. 3.2 The data In our experiments we have used data sets from the UCI repository of Irvine  <b>glass</b>  letter, optdigits) [11] and a synthetic data set (d5) made up by five threedimensional classes, each composed by two normal distributed disjoint clusters", "mykey":1590},
 {"datasetID":14, "supportID":"9B9DD22D0660646FD6F2B09E8BD50E2A5EC172A2", "rexaID":"095d7064837557bdfbca12fb9c12dbaaeb3a8b0d", "author":"Adam H. Cannon and Lenore J. Cowen and Carey E. Priebe", "title":"Approximate Distance Classification", "venue":"Department of Mathematical Sciences The Johns Hopkins University", "year":"", "window":"data before implementing the ADC classification algorithm. Here, only the raw data has been analyzed using the same procedure described above. 5 Conclusions Results on the Wisconsin <b>breast</b> <b>cancer</b> data set and the Fisher iris data set compare very well with previous work on these data. The Pima Indian diabetes results are also nearly competitive with previous work. In all three cases it should be", "mykey":1591},
 {"datasetID":17, "supportID":"9B9DD22D0660646FD6F2B09E8BD50E2A5EC172A2", "rexaID":"095d7064837557bdfbca12fb9c12dbaaeb3a8b0d", "author":"Adam H. Cannon and Lenore J. Cowen and Carey E. Priebe", "title":"Approximate Distance Classification", "venue":"Department of Mathematical Sciences The Johns Hopkins University", "year":"", "window":"data before implementing the ADC classification algorithm. Here, only the raw data has been analyzed using the same procedure described above. 5 Conclusions Results on the <b>Wisconsin</b> <b>breast</b> <b>cancer</b> data set and the Fisher iris data set compare very well with previous work on these data. The Pima Indian diabetes results are also nearly competitive with previous work. In all three cases it should be", "mykey":1592},
 {"datasetID":15, "supportID":"9B9DD22D0660646FD6F2B09E8BD50E2A5EC172A2", "rexaID":"095d7064837557bdfbca12fb9c12dbaaeb3a8b0d", "author":"Adam H. Cannon and Lenore J. Cowen and Carey E. Priebe", "title":"Approximate Distance Classification", "venue":"Department of Mathematical Sciences The Johns Hopkins University", "year":"", "window":"data before implementing the ADC classification algorithm. Here, only the raw data has been analyzed using the same procedure described above. 5 Conclusions Results on the <b>Wisconsin</b> <b>breast</b> <b>cancer</b> data set and the Fisher iris data set compare very well with previous work on these data. The Pima Indian diabetes results are also nearly competitive with previous work. In all three cases it should be", "mykey":1593},
 {"datasetID":16, "supportID":"9B9DD22D0660646FD6F2B09E8BD50E2A5EC172A2", "rexaID":"095d7064837557bdfbca12fb9c12dbaaeb3a8b0d", "author":"Adam H. Cannon and Lenore J. Cowen and Carey E. Priebe", "title":"Approximate Distance Classification", "venue":"Department of Mathematical Sciences The Johns Hopkins University", "year":"", "window":"data before implementing the ADC classification algorithm. Here, only the raw data has been analyzed using the same procedure described above. 5 Conclusions Results on the <b>Wisconsin</b> <b>breast</b> <b>cancer</b> data set and the Fisher iris data set compare very well with previous work on these data. The Pima Indian diabetes results are also nearly competitive with previous work. In all three cases it should be", "mykey":1594},
 {"datasetID":53, "supportID":"9B9DD22D0660646FD6F2B09E8BD50E2A5EC172A2", "rexaID":"095d7064837557bdfbca12fb9c12dbaaeb3a8b0d", "author":"Adam H. Cannon and Lenore J. Cowen and Carey E. Priebe", "title":"Approximate Distance Classification", "venue":"Department of Mathematical Sciences The Johns Hopkins University", "year":"", "window":"data before implementing the ADC classification algorithm. Here, only the raw data has been analyzed using the same procedure described above. 5 Conclusions Results on the Wisconsin breast cancer data set and the Fisher <b>iris</b> data set compare very well with previous work on these data. The Pima Indian diabetes results are also nearly competitive with previous work. In all three cases it should be", "mykey":1595},
 {"datasetID":1, "supportID":"9BE2B7EC87D830A0639C61890D935B8AB0346C86", "rexaID":"f7fdf9dbb5f98a218956025550c1f603b3cb24f2", "author":"Alexander G. Gray and Bernd Fischer and Johann Schumann and Wray L. Buntine", "title":"Automatic Derivation of Statistical Algorithms: The EM Family and Beyond", "venue":"NIPS", "year":"2002", "window":"extension of our running example, integrating several features, yields a Gaussian Bayes classifier model G 2 . G 2 has been successfully tested on various standard benchmarks [1], e.g., the <b>Abalone</b> dataset. Currently, the number of expected classes has to be given in advance. Mixture models and EM. A wide range of k-Gaussian mixture models can be handled by AUTOBAYES, ranging from the simple 1D (M 1 )", "mykey":1596},
 {"datasetID":2, "supportID":"9C28A6E37710D22B8DA58DAA9EC0D6EC76F7C988", "rexaID":"beeb203c082359f4e141e1767a14f09449a5a717", "author":"Jie Cheng and Russell Greiner", "title":"Learning Bayesian Belief Network Classifiers: Algorithms and System", "venue":"Canadian Conference on AI", "year":"2001", "window":"used in the experiments. Dataset Attributes. Classes Instances Train Test <b>Adult</b> 13 2 32561 16281 Nursery 8 5 8640 4320 Mushroom 22 2 5416 2708 Chess 36 2 2130 1066 DNA 60 3 2000 1186 The experiments were carried out using our", "mykey":1597},
 {"datasetID":23, "supportID":"9C28A6E37710D22B8DA58DAA9EC0D6EC76F7C988", "rexaID":"beeb203c082359f4e141e1767a14f09449a5a717", "author":"Jie Cheng and Russell Greiner", "title":"Learning Bayesian Belief Network Classifiers: Algorithms and System", "venue":"Canadian Conference on AI", "year":"2001", "window":"Wrapper(multi-net) with ordering = W-MN-O, Wrapper(multi-net) with feature selection = W-MN-FS and Wrapper(multi-net) with feature selection with ordering = WMN-FS-O. The ordering for the <b>Chess</b> data set is the reversed order of the features that appear in the data set since it is more reasonable, the ordering we use for other data sets are simply the order of the features that appear in the data", "mykey":1598},
 {"datasetID":21, "supportID":"9C28A6E37710D22B8DA58DAA9EC0D6EC76F7C988", "rexaID":"beeb203c082359f4e141e1767a14f09449a5a717", "author":"Jie Cheng and Russell Greiner", "title":"Learning Bayesian Belief Network Classifiers: Algorithms and System", "venue":"Canadian Conference on AI", "year":"2001", "window":"Wrapper(multi-net) with ordering = W-MN-O, Wrapper(multi-net) with feature selection = W-MN-FS and Wrapper(multi-net) with feature selection with ordering = WMN-FS-O. The ordering for the <b>Chess</b> data set is the reversed order of the features that appear in the data set since it is more reasonable, the ordering we use for other data sets are simply the order of the features that appear in the data", "mykey":1599},
 {"datasetID":22, "supportID":"9C28A6E37710D22B8DA58DAA9EC0D6EC76F7C988", "rexaID":"beeb203c082359f4e141e1767a14f09449a5a717", "author":"Jie Cheng and Russell Greiner", "title":"Learning Bayesian Belief Network Classifiers: Algorithms and System", "venue":"Canadian Conference on AI", "year":"2001", "window":"Wrapper(multi-net) with ordering = W-MN-O, Wrapper(multi-net) with feature selection = W-MN-FS and Wrapper(multi-net) with feature selection with ordering = WMN-FS-O. The ordering for the <b>Chess</b> data set is the reversed order of the features that appear in the data set since it is more reasonable, the ordering we use for other data sets are simply the order of the features that appear in the data", "mykey":1600},
 {"datasetID":14, "supportID":"9C2A4F4C83FA132ED6DF0243C83B5FAB869E444E", "rexaID":"0dccc15e45577745f11d643e8dab5db77827831a", "author":"Ismail Taha and Joydeep Ghosh", "title":"Symbolic Interpretation of Artificial Neural Networks", "venue":"IEEE Trans. Knowl. Data Eng, 11", "year":"1999", "window":"and universal approach. A rule evaluation technique that orders extracted rules based on three performance measures is then proposed. The three techniques are applied to the iris and <b>breast</b> <b>cancer</b> data sets. The extracted rules are evaluated qualitatively and quantitatively, and compared with those obtained by other approaches. Index Terms: rule extraction, hybrid systems, knowledge refinement, neural", "mykey":1601},
 {"datasetID":53, "supportID":"9C2A4F4C83FA132ED6DF0243C83B5FAB869E444E", "rexaID":"0dccc15e45577745f11d643e8dab5db77827831a", "author":"Ismail Taha and Joydeep Ghosh", "title":"Symbolic Interpretation of Artificial Neural Networks", "venue":"IEEE Trans. Knowl. Data Eng, 11", "year":"1999", "window":"and universal approach. A rule evaluation technique that orders extracted rules based on three performance measures is then proposed. The three techniques are applied to the <b>iris</b> and breast cancer data sets. The extracted rules are evaluated qualitatively and quantitatively, and compared with those obtained by other approaches. Index Terms: rule extraction, hybrid systems, knowledge refinement, neural", "mykey":1602},
 {"datasetID":67, "supportID":"9C2A4F4C83FA132ED6DF0243C83B5FAB869E444E", "rexaID":"0dccc15e45577745f11d643e8dab5db77827831a", "author":"Ismail Taha and Joydeep Ghosh", "title":"Symbolic Interpretation of Artificial Neural Networks", "venue":"IEEE Trans. Knowl. Data Eng, 11", "year":"1999", "window":"that have been used as benchmarks for rule extraction approaches are the Monk [43], Mushroom [32] and the <b>DNA</b> <b>promoter</b> [47] data sets. All three of these data sets inputs are symbolic/discrete by nature. Since we want to test more general problems that may include continuous valued variables, Iris and Breast-Cancer were preferred", "mykey":1603},
 {"datasetID":73, "supportID":"9C2A4F4C83FA132ED6DF0243C83B5FAB869E444E", "rexaID":"0dccc15e45577745f11d643e8dab5db77827831a", "author":"Ismail Taha and Joydeep Ghosh", "title":"Symbolic Interpretation of Artificial Neural Networks", "venue":"IEEE Trans. Knowl. Data Eng, 11", "year":"1999", "window":"that have been used as benchmarks for rule extraction approaches are the Monk [43], <b>Mushroom</b> [32] and the DNA promoter [47] data sets. All three of these data sets inputs are symbolic/discrete by nature. Since we want to test more general problems that may include continuous valued variables, Iris and Breast-Cancer were preferred", "mykey":1604},
 {"datasetID":120, "supportID":"9C408B6979792C651477A6439FC4166EED001959", "rexaID":"4487f47affc309f0d984645b44cdc8e1f42c2472", "author":"Gaurav Marwah and Lois C. Boggess", "title":"Artificial Immune Systems for Classification : Some Issues", "venue":"Department of Computer Science Mississippi State University", "year":"", "window":"data. The classes of antigen occurring more frequently were allocated more resources and those occurring less frequently were allocated fewer resources. Table 5: Accuracy Rates For <b>E</b> <b>coli</b> And Yeast Data Sets Using Different Methods For Resource Allocation. Method used for resource allocation Accuracy (E.Coli) Accuracy (Yeast) Half the resources for in class ARBs and the other half for out of class", "mykey":1605},
 {"datasetID":39, "supportID":"9C408B6979792C651477A6439FC4166EED001959", "rexaID":"4487f47affc309f0d984645b44cdc8e1f42c2472", "author":"Gaurav Marwah and Lois C. Boggess", "title":"Artificial Immune Systems for Classification : Some Issues", "venue":"Department of Computer Science Mississippi State University", "year":"", "window":"data. The classes of antigen occurring more frequently were allocated more resources and those occurring less frequently were allocated fewer resources. Table 5: Accuracy Rates For E <b>coli</b> And Yeast Data Sets Using Different Methods For Resource Allocation. Method used for resource allocation Accuracy (E.Coli) Accuracy (Yeast) Half the resources for in class ARBs and the other half for out of class", "mykey":1606},
 {"datasetID":53, "supportID":"9C408B6979792C651477A6439FC4166EED001959", "rexaID":"4487f47affc309f0d984645b44cdc8e1f42c2472", "author":"Gaurav Marwah and Lois C. Boggess", "title":"Artificial Immune Systems for Classification : Some Issues", "venue":"Department of Computer Science Mississippi State University", "year":"", "window":"satisfying some stimulation threshold, but the stimulation threshold for out of class ARBs was somewhat relaxed as compared to in class ARBs. Table 4 shows the accuracy rates obtained for the <b>iris</b> data set using the approaches just described. Five way cross validation was performed to achieve these results. Table 4: Accuracy Rates For Iris Data Set Using Different Approaches For ARB Pool Organization", "mykey":1607},
 {"datasetID":110, "supportID":"9C408B6979792C651477A6439FC4166EED001959", "rexaID":"4487f47affc309f0d984645b44cdc8e1f42c2472", "author":"Gaurav Marwah and Lois C. Boggess", "title":"Artificial Immune Systems for Classification : Some Issues", "venue":"Department of Computer Science Mississippi State University", "year":"", "window":"the accuracy of the classifier varies depending on the characteristics of the problem. The need for these alternatives was realized while testing AIRS on the well-known and publicly available <b>yeast</b> data set, which appears to be a difficult classification problem. The data set was obtained from the repository of the University of California at Irvine (Blake and Merz, 1998) and contained 1484 instances", "mykey":1608},
 {"datasetID":42, "supportID":"9C55846F1A0EF7FDD40EF79C88B0EEAB52E2ABD3", "rexaID":"77faaac3f85d52abea60e7152460894b085b63ec", "author":"Pramod Viswanath and M. Narasimha Murty and Shalabh Bhatnagar", "title":"A pattern synthesis technique to reduce the curse of dimensionality effect", "venue":"E-mail", "year":"", "window":"We performed experiments with five different datasets, viz., OCR, WINE, THYROID, <b>GLASS</b> and PENDIGITS, respectively. Except the OCR dataset, all others are from the UCI Repository [16]. OCR dataset is also used in [17, 18]. The properties of the", "mykey":1609},
 {"datasetID":102, "supportID":"9C55846F1A0EF7FDD40EF79C88B0EEAB52E2ABD3", "rexaID":"77faaac3f85d52abea60e7152460894b085b63ec", "author":"Pramod Viswanath and M. Narasimha Murty and Shalabh Bhatnagar", "title":"A pattern synthesis technique to reduce the curse of dimensionality effect", "venue":"E-mail", "year":"", "window":"We performed experiments with five different datasets, viz., OCR, WINE, <b>THYROID</b>  GLASS and PENDIGITS, respectively. Except the OCR dataset, all others are from the UCI Repository [16]. OCR dataset is also used in [17, 18]. The properties of the", "mykey":1610},
 {"datasetID":109, "supportID":"9C55846F1A0EF7FDD40EF79C88B0EEAB52E2ABD3", "rexaID":"77faaac3f85d52abea60e7152460894b085b63ec", "author":"Pramod Viswanath and M. Narasimha Murty and Shalabh Bhatnagar", "title":"A pattern synthesis technique to reduce the curse of dimensionality effect", "venue":"E-mail", "year":"", "window":"We performed experiments with five different datasets, viz., OCR, <b>WINE</b>  THYROID, GLASS and PENDIGITS, respectively. Except the OCR dataset, all others are from the UCI Repository [16]. OCR dataset is also used in [17, 18]. The properties of the", "mykey":1611},
 {"datasetID":2, "supportID":"9CA9342B849271C2DD4DBCA4D00EF029972A7388", "rexaID":"b2c60bdf066b03909792bd5d64891f3607b948c8", "author":"Rich Caruana and Alexandru Niculescu-Mizil and Geoff Crew and Alex Ksikes", "title":"Ensemble selection from libraries of models", "venue":"ICML", "year":"2004", "window":"but each ensemble is just a weighted average of models, so the average of a set of ensembles also is a simple weighted average of the base-level models. Bagging is discussed in Section 5.3. 3. Data Sets We experiment with seven problems: <b>ADULT</b>  COVER TYPE, LETTER.p1, and LETTER.p2 from the UCI Repository (Blake & Merz, 1998), MEDIS, a pneumonia data set, SLAC, data from collaborators at the", "mykey":1612},
 {"datasetID":90, "supportID":"9CA9342B849271C2DD4DBCA4D00EF029972A7388", "rexaID":"b2c60bdf066b03909792bd5d64891f3607b948c8", "author":"Rich Caruana and Alexandru Niculescu-Mizil and Geoff Crew and Alex Ksikes", "title":"Ensemble selection from libraries of models", "venue":"ICML", "year":"2004", "window":"1-13 as class 0 and letters 14-26 as class 1, yielding a di\u00c6cult, but balanced, problem. HYPER SPECT was converted to binary by treating the large confusable class <b>Soybean</b> Mintil as class 1. These data sets were selected because they are large enough to allow moderate size train and validation sets, and still have data left for large final test sets. For our experiments, we used training sets of 5000", "mykey":1613},
 {"datasetID":91, "supportID":"9CA9342B849271C2DD4DBCA4D00EF029972A7388", "rexaID":"b2c60bdf066b03909792bd5d64891f3607b948c8", "author":"Rich Caruana and Alexandru Niculescu-Mizil and Geoff Crew and Alex Ksikes", "title":"Ensemble selection from libraries of models", "venue":"ICML", "year":"2004", "window":"1-13 as class 0 and letters 14-26 as class 1, yielding a di\u00c6cult, but balanced, problem. HYPER SPECT was converted to binary by treating the large confusable class <b>Soybean</b> Mintil as class 1. These data sets were selected because they are large enough to allow moderate size train and validation sets, and still have data left for large final test sets. For our experiments, we used training sets of 5000", "mykey":1614},
 {"datasetID":151, "supportID":"9CDB1DA34B184C085D4C62BB84EEF470C90D0DA7", "rexaID":"9f311a1b1421973488b673c39c8545ef006570cc", "author":"Alain Rakotomamonjy", "title":"Leave-One-Out errors in Bipartite Ranking SVM", "venue":"PSI CNRS FRE2645 INSA de Rouen Avenue de l'universite", "year":"", "window":"test set and the approximated bound have been evaluated. Presented results are the average results for 20 di\u00aeerent trials of the random split. Figure (4) presents the results that we achieved for datasets <b>sonar</b> and ionosphere. In one case, we can see that the LOPO approximated bound gives interesting result since the true test AUC plot has a similar behaviour to the 17 10 -2 10 -1 10 0 10 1 10 2 10", "mykey":1615},
 {"datasetID":52, "supportID":"9CDB1DA34B184C085D4C62BB84EEF470C90D0DA7", "rexaID":"9f311a1b1421973488b673c39c8545ef006570cc", "author":"Alain Rakotomamonjy", "title":"Leave-One-Out errors in Bipartite Ranking SVM", "venue":"PSI CNRS FRE2645 INSA de Rouen Avenue de l'universite", "year":"", "window":"test set and the approximated bound have been evaluated. Presented results are the average results for 20 di\u00aeerent trials of the random split. Figure (4) presents the results that we achieved for datasets sonar and <b>ionosphere</b>  In one case, we can see that the LOPO approximated bound gives interesting result since the true test AUC plot has a similar behaviour to the 17 10 -2 10 -1 10 0 10 1 10 2 10", "mykey":1616},
 {"datasetID":67, "supportID":"9D13BB714BDB97A3738D36D44CD280788707BF51", "rexaID":"f23deedb8c4f89030e936b0d119459b41278db96", "author":"Andreas L. Prodromidis", "title":"On the Management of Distributed Learning Agents Ph.D. Thesis Proposal CUCS-032-97", "venue":"Department of Computer Science Columbia University", "year":"1998", "window":"of real credit card transactions and two <b>molecular biology</b> sequence analysis data sets, were used in our experiments. The credit card data sets were provided by the Chase and First Union Banks, members of FSTC (Financial Services Technology Consortium) and the molecular biology", "mykey":1617},
 {"datasetID":68, "supportID":"9D13BB714BDB97A3738D36D44CD280788707BF51", "rexaID":"f23deedb8c4f89030e936b0d119459b41278db96", "author":"Andreas L. Prodromidis", "title":"On the Management of Distributed Learning Agents Ph.D. Thesis Proposal CUCS-032-97", "venue":"Department of Computer Science Columbia University", "year":"1998", "window":"of real credit card transactions and two <b>molecular biology</b> sequence analysis data sets, were used in our experiments. The credit card data sets were provided by the Chase and First Union Banks, members of FSTC (Financial Services Technology Consortium) and the molecular biology", "mykey":1618},
 {"datasetID":69, "supportID":"9D13BB714BDB97A3738D36D44CD280788707BF51", "rexaID":"f23deedb8c4f89030e936b0d119459b41278db96", "author":"Andreas L. Prodromidis", "title":"On the Management of Distributed Learning Agents Ph.D. Thesis Proposal CUCS-032-97", "venue":"Department of Computer Science Columbia University", "year":"1998", "window":"of real credit card transactions and two <b>molecular biology</b> sequence analysis data sets, were used in our experiments. The credit card data sets were provided by the Chase and First Union Banks, members of FSTC (Financial Services Technology Consortium) and the molecular biology", "mykey":1619},
 {"datasetID":154, "supportID":"9D13BB714BDB97A3738D36D44CD280788707BF51", "rexaID":"f23deedb8c4f89030e936b0d119459b41278db96", "author":"Andreas L. Prodromidis", "title":"On the Management of Distributed Learning Agents Ph.D. Thesis Proposal CUCS-032-97", "venue":"Department of Computer Science Columbia University", "year":"1998", "window":"class label (fraud/legitimate transaction). Some of the fields are arithmetic and the rest categorical, i.e. numbers were used to represent a few discrete categories. The secondary <b>protein</b> structure data set (SS) [36], courtesy of Qian and Sejnowski, contains 21,625 sequences of amino acids and secondary structures at the corresponding positions. There are three structures (classes) and 20 amino acids", "mykey":1620},
 {"datasetID":102, "supportID":"9D13BB714BDB97A3738D36D44CD280788707BF51", "rexaID":"f23deedb8c4f89030e936b0d119459b41278db96", "author":"Andreas L. Prodromidis", "title":"On the Management of Distributed Learning Agents Ph.D. Thesis Proposal CUCS-032-97", "venue":"Department of Computer Science Columbia University", "year":"1998", "window":"+ Marmalade.cs + Mango.cs Control & Data messages Transfer of Learning & Classifier Agents Database Configuration Database Configuration Strawberry.cs Data Site - 2 Mango.cs DATA SITES: <b>thyroid</b> DATASET = META_LEARNER = Bayes CROSS_VALIDATION_FOLD = 2 META_LEARNING_FOLD = 2 META_LEARNING_LEVEL = 1 IMAGE_URL = http://www.cs.... Configuration File LEARNER = ID3 The JAM architecture with 3 datasites", "mykey":1621},
 {"datasetID":14, "supportID":"9D593E9D43C528632757643EDC5462878315CAAB", "rexaID":"ce75ba6b3cf7315e2578b0181306beb521c91fbd", "author":"Paul D. Wilson and Tony R. Martinez", "title":"Combining Cross-Validation and Confidence to Measure Fitness", "venue":"fonix corporation Brigham Young University", "year":"", "window":"at the bottom of Table 1, CVC had a significantly higher average generalization accuracy on this set of classification tasks than both the static and LCV methods at a 99% confidence level or higher. Dataset Anneal Australian <b>Breast</b> <b>Cancer</b> WI) Bridges Crx Echocardiogram Flag Glass Heart Heart(Cleveland) Heart(Hungarian) Heart(Long Beach) Heart(More) Heart(Swiss) Hepatitis Horse Colic Image Segmentation", "mykey":1622},
 {"datasetID":18, "supportID":"9D593E9D43C528632757643EDC5462878315CAAB", "rexaID":"ce75ba6b3cf7315e2578b0181306beb521c91fbd", "author":"Paul D. Wilson and Tony R. Martinez", "title":"Combining Cross-Validation and Confidence to Measure Fitness", "venue":"fonix corporation Brigham Young University", "year":"", "window":"at the bottom of Table 1, CVC had a significantly higher average generalization accuracy on this set of classification tasks than both the static and LCV methods at a 99% confidence level or higher. Dataset Anneal Australian Breast Cancer(WI) <b>Bridges</b> Crx Echocardiogram Flag Glass Heart Heart(Cleveland) Heart(Hungarian) Heart(Long Beach) Heart(More) Heart(Swiss) Hepatitis Horse Colic Image Segmentation", "mykey":1623},
 {"datasetID":42, "supportID":"9DF31559B1B02D78AABA0E740C15B9A845133FCF", "rexaID":"78b94c51025c63bb39ea776347fe151e203eaa24", "author":"Francesco Masulli", "title":"An experimental analysis of the dependence among codeword bit errors in ECOC learning machines", "venue":"and Giorgio Valentini b,c", "year":"2003", "window":"machine, varying the number of hidden units between 5 to 50, yielding to 11#20 = 220 evaluations of I E ; I SE ; #R and # S both for ECOC monolithic and ECOC PND learning machines. For the UCI data sets <b>glass</b>  letter and optdigits we used only 2 different structures, using, respectively, 5 and 9, 120 and 140, 60 and 70 hidden units, yielding to 2 # 20 = 40 evaluations of the mutual information", "mykey":1624},
 {"datasetID":2, "supportID":"9E0F82D7B8F989F2E66E95C1CCF6902475477945", "rexaID":"da53589d7ad993c65a1857401ea6f7b0b025f41a", "author":"Petri Kontkanen and Jussi Lahtinen and Petri Myllymaki and Tomi Silander and Henry Tirri", "title":"USING BAYESIAN NETWORKS FOR VISUALIZING HIGH-DIMENSIONAL DATA", "venue":"Complex Systems Computation Group (CoSCo)", "year":"", "window":"via the CoSCo group home page. 5 Australian Credit Balance Scale Connect-4 German Credit Thyroid Disease Vehicle Silhouettes Figure 1: Examples of the two-dimensional visualizations obtained. 6 dataset size #attrs. #classes <b>Adult</b> 32561 15 2 Australian Credit 690 15 2 Balance Scale 625 5 3 Breast Cancer (Wisconsin) 699 11 2 Breast Cancer 286 10 2 Connect-4 67557 43 3 Credit Screening 690 16 2 Pima", "mykey":1625},
 {"datasetID":109, "supportID":"9E0F82D7B8F989F2E66E95C1CCF6902475477945", "rexaID":"da53589d7ad993c65a1857401ea6f7b0b025f41a", "author":"Petri Kontkanen and Jussi Lahtinen and Petri Myllymaki and Tomi Silander and Henry Tirri", "title":"USING BAYESIAN NETWORKS FOR VISUALIZING HIGH-DIMENSIONAL DATA", "venue":"Complex Systems Computation Group (CoSCo)", "year":"", "window":"8124 23 2 PostoperativePatient 90 9 3 Thyroid Disease 215 6 3 Tic-Tac-Toe Endgame 958 10 2 Vehicle Silhouettes 846 19 4 Congressional Voting Records 435 17 2 <b>Wine</b> Recognition 178 14 3 Table 1: The datasets used in the experiments. For estimating the quality of the visualizations produced, we used the validation scheme described in the previous section. The prediction methods used are listed in Table", "mykey":1626},
 {"datasetID":151, "supportID":"9E1FA5B9196F81C37C01592F5FFE69CD2BF7D1FD", "rexaID":"2aefeeb5c89fa6bd9fe7606fee20e1285438d5a5", "author":"Chris Drummond and Robert C. Holte", "title":"Explicitly representing expected cost: an alternative to ROC representation", "venue":"KDD", "year":"2000", "window":"classifier for a particular PCF(+) value is not necessarily the one produced by a training set with the same PCF(+) characteristics is illustrated in figure 17, which shows ROC curves for the <b>sonar</b> data set from the UCI collection [1]. The points represented by circles, and connected by solid lines, were generated using C4.5 (release 7 using information gain) modified to account for costs (by altering", "mykey":1627},
 {"datasetID":23, "supportID":"9EB298EE67D30F80D82CF2564FED3A741A769833", "rexaID":"00c705fd319a8886a15103ab320040ae719c4283", "author":"Ron Kohavi and Dan Sommerfield", "title":"Feature Subset Selection Using the Wrapper Method: Overfitting and Dynamic Search Space Topology", "venue":"KDD", "year":"1995", "window":"in error. The execution time on a Sparc20 for feature subset selection using ID3 ranged from under five minutes for breast-cancer (Wisconsin), cleve, heart, and vote to about an hour for most datasets. DNA took 29 hours, followed by <b>chess</b> at four hours. The DNA run took so long because of ever increasing estimates that did not really improve the test-set accuracy. 7 Conclusions We reviewed the", "mykey":1628},
 {"datasetID":21, "supportID":"9EB298EE67D30F80D82CF2564FED3A741A769833", "rexaID":"00c705fd319a8886a15103ab320040ae719c4283", "author":"Ron Kohavi and Dan Sommerfield", "title":"Feature Subset Selection Using the Wrapper Method: Overfitting and Dynamic Search Space Topology", "venue":"KDD", "year":"1995", "window":"in error. The execution time on a Sparc20 for feature subset selection using ID3 ranged from under five minutes for breast-cancer (Wisconsin), cleve, heart, and vote to about an hour for most datasets. DNA took 29 hours, followed by <b>chess</b> at four hours. The DNA run took so long because of ever increasing estimates that did not really improve the test-set accuracy. 7 Conclusions We reviewed the", "mykey":1629},
 {"datasetID":22, "supportID":"9EB298EE67D30F80D82CF2564FED3A741A769833", "rexaID":"00c705fd319a8886a15103ab320040ae719c4283", "author":"Ron Kohavi and Dan Sommerfield", "title":"Feature Subset Selection Using the Wrapper Method: Overfitting and Dynamic Search Space Topology", "venue":"KDD", "year":"1995", "window":"in error. The execution time on a Sparc20 for feature subset selection using ID3 ranged from under five minutes for breast-cancer (Wisconsin), cleve, heart, and vote to about an hour for most datasets. DNA took 29 hours, followed by <b>chess</b> at four hours. The DNA run took so long because of ever increasing estimates that did not really improve the test-set accuracy. 7 Conclusions We reviewed the", "mykey":1630},
 {"datasetID":45, "supportID":"9EB298EE67D30F80D82CF2564FED3A741A769833", "rexaID":"00c705fd319a8886a15103ab320040ae719c4283", "author":"Ron Kohavi and Dan Sommerfield", "title":"Feature Subset Selection Using the Wrapper Method: Overfitting and Dynamic Search Space Topology", "venue":"KDD", "year":"1995", "window":"in error. The execution time on a Sparc20 for feature subset selection using ID3 ranged from under five minutes for breast-cancer (Wisconsin), cleve, <b>heart</b>  and vote to about an hour for most datasets. DNA took 29 hours, followed by chess at four hours. The DNA run took so long because of ever increasing estimates that did not really improve the test-set accuracy. 7 Conclusions We reviewed the", "mykey":1631},
 {"datasetID":67, "supportID":"9EB298EE67D30F80D82CF2564FED3A741A769833", "rexaID":"00c705fd319a8886a15103ab320040ae719c4283", "author":"Ron Kohavi and Dan Sommerfield", "title":"Feature Subset Selection Using the Wrapper Method: Overfitting and Dynamic Search Space Topology", "venue":"KDD", "year":"1995", "window":"in error. The execution time on a Sparc20 for feature subset selection using ID3 ranged from under five minutes for breast-cancer (Wisconsin), cleve, heart, and vote to about an hour for most datasets. <b>DNA</b> took 29 hours, followed by chess at four hours. The DNA run took so long because of ever increasing estimates that did not really improve the test-set accuracy. 7 Conclusions We reviewed the", "mykey":1632},
 {"datasetID":42, "supportID":"9EBB3354F855D1C4C2D5F2391F5B9A6ED7873338", "rexaID":"50339570de3deefebea8a48dc4100e6fe883a16f", "author":"Francesco Masulli and Giorgio Valentini", "title":"Comparing Decomposition Methods for Classification", "venue":"Istituto Nazionale per la Fisica della Materia DISI - Dipartimento di Informatica e Scienze dell'Informazione", "year":"", "window":"OPC bold driver CC grad. desc. CC bold driver ECOC BCH grad. desc. ECOC BCH bold driver (a) (b) Figure 1: Comparison of performances of different decomposition methods on <b>glass</b> (a) and optdigits (b) data sets of the UCI. Table 4: Standard PWC (left) and CC (right) decomposition matrices. 0 B B B B B @ +1 1 0 0 +1 0 1 0 +1 0 0 1 0 +1 1 0 0 +1 0 1 0 0 +1 1 1 C C C C C A 0 B B B B B @ +1 +1 1 1 +1 1 +1 1", "mykey":1633},
 {"datasetID":39, "supportID":"9EE3A758A8D77E3549BAEADF666284E7A626FA76", "rexaID":"1c1424dac83eee4c354e1de77ef9daf5f3fa5742", "author":"Charles X. Ling and Qiang Yang and Jianning Wang and Shichao Zhang", "title":"Decision trees with minimal costs", "venue":"ICML", "year":"2004", "window":"total cost.   Aimed at minimizing the total cost of test and  misclassification, our new decision-tree algorithm has  several desirable features. We will discuss these features  below, using the dataset  <b>Ecoli</b>  as an example (Blake &  Merz 1998). This dataset, after pre-processing, has 332  labelled examples, which are described by six attributes.  The numerical attributes are first discretized", "mykey":1634},
 {"datasetID":7, "supportID":"9F27F44CBA9D780CBF52065C4CE07C5BBA5AD197", "rexaID":"7125dc87ceddf952f1840074b40ac0526dcc03dc", "author":"Marcus Hutter and Marco Zaffalon", "title":"Distribution of Mutual Information from Complete and Incomplete Data", "venue":"CoRR, csLG/0403025", "year":"2004", "window":"0 100 200 300 400 500 600 700 800 900 1000 1100 Instance number Aver. number of excluded features (Spam) F FF BF Figure 4: Average number of attributes excluded by the different filters on the Spam data set. Name #feat. #inst. #m.d. mode freq. <b>Audiology</b> 69 226 317 0.212 Crx 15 690 67 0.555 Horse-colic 18 368 1281 0.630 Hypothyroidloss 23 3163 1980 0.952 Soybean-large 35 683 2337 0.135 Table 3:", "mykey":1635},
 {"datasetID":8, "supportID":"9F27F44CBA9D780CBF52065C4CE07C5BBA5AD197", "rexaID":"7125dc87ceddf952f1840074b40ac0526dcc03dc", "author":"Marcus Hutter and Marco Zaffalon", "title":"Distribution of Mutual Information from Complete and Incomplete Data", "venue":"CoRR, csLG/0403025", "year":"2004", "window":"0 100 200 300 400 500 600 700 800 900 1000 1100 Instance number Aver. number of excluded features (Spam) F FF BF Figure 4: Average number of attributes excluded by the different filters on the Spam data set. Name #feat. #inst. #m.d. mode freq. <b>Audiology</b> 69 226 317 0.212 Crx 15 690 67 0.555 Horse-colic 18 368 1281 0.630 Hypothyroidloss 23 3163 1980 0.952 Soybean-large 35 683 2337 0.135 Table 3:", "mykey":1636},
 {"datasetID":23, "supportID":"9F27F44CBA9D780CBF52065C4CE07C5BBA5AD197", "rexaID":"7125dc87ceddf952f1840074b40ac0526dcc03dc", "author":"Marcus Hutter and Marco Zaffalon", "title":"Distribution of Mutual Information from Complete and Incomplete Data", "venue":"CoRR, csLG/0403025", "year":"2004", "window":"The remaining cases are described by means of the following figures. Figure 2 shows that FF allowed the naive Bayes to significantly do better predictions than F for the greatest part of the <b>Chess</b> data set. The maximum difference in prediction accuracy is obtained at instance 422, where the accuracies are 0.889 and 0.832 for the cases FF and F, respectively. Figure 2 does not report the BF case,", "mykey":1637},
 {"datasetID":21, "supportID":"9F27F44CBA9D780CBF52065C4CE07C5BBA5AD197", "rexaID":"7125dc87ceddf952f1840074b40ac0526dcc03dc", "author":"Marcus Hutter and Marco Zaffalon", "title":"Distribution of Mutual Information from Complete and Incomplete Data", "venue":"CoRR, csLG/0403025", "year":"2004", "window":"The remaining cases are described by means of the following figures. Figure 2 shows that FF allowed the naive Bayes to significantly do better predictions than F for the greatest part of the <b>Chess</b> data set. The maximum difference in prediction accuracy is obtained at instance 422, where the accuracies are 0.889 and 0.832 for the cases FF and F, respectively. Figure 2 does not report the BF case,", "mykey":1638},
 {"datasetID":22, "supportID":"9F27F44CBA9D780CBF52065C4CE07C5BBA5AD197", "rexaID":"7125dc87ceddf952f1840074b40ac0526dcc03dc", "author":"Marcus Hutter and Marco Zaffalon", "title":"Distribution of Mutual Information from Complete and Incomplete Data", "venue":"CoRR, csLG/0403025", "year":"2004", "window":"The remaining cases are described by means of the following figures. Figure 2 shows that FF allowed the naive Bayes to significantly do better predictions than F for the greatest part of the <b>Chess</b> data set. The maximum difference in prediction accuracy is obtained at instance 422, where the accuracies are 0.889 and 0.832 for the cases FF and F, respectively. Figure 2 does not report the BF case,", "mykey":1639},
 {"datasetID":63, "supportID":"9F27F44CBA9D780CBF52065C4CE07C5BBA5AD197", "rexaID":"7125dc87ceddf952f1840074b40ac0526dcc03dc", "author":"Marcus Hutter and Marco Zaffalon", "title":"Distribution of Mutual Information from Complete and Incomplete Data", "venue":"CoRR, csLG/0403025", "year":"2004", "window":"on a number of different domains. For example, Shuttle-small reports Distribution of Mutual Information 19 data on diagnosing failures of the space shuttle; <b>Lymphography</b> and Hypothyroid are medical data sets; Spam is a body of e-mails that can be spam or non-spam; etc. Name #feat. #inst. mode freq. Australian 36 690 0.555 Chess 36 3196 0.520 Crx 15 653 0.547 German-org 17 1000 0.700 Hypothyroid 23 2238", "mykey":1640},
 {"datasetID":60, "supportID":"9F99E6F0A06D692FFF731EA175DAE71AEB4C2047", "rexaID":"6c5be33e1f8da0388cb66c628ccad3a0b00eab57", "author":"Guido Lindner and Rudi Studer", "title":"AST: Support for Algorithm Selection with a CBR Approach", "venue":"PKDD", "year":"1999", "window":"with the results of C5.0 on these datasets which are glass2 and <b>liver</b> from the UCI repository. Besides the results of applying the different algorithms on these applications the user gets a degree of similarity of the selected cases. This", "mykey":1641},
 {"datasetID":98, "supportID":"9F99E6F0A06D692FFF731EA175DAE71AEB4C2047", "rexaID":"6c5be33e1f8da0388cb66c628ccad3a0b00eab57", "author":"Guido Lindner and Rudi Studer", "title":"AST: Support for Algorithm Selection with a CBR Approach", "venue":"PKDD", "year":"1999", "window":"the MLT project with its Consultant system [Consortium, 1993] as well as the <b>Statlog</b> project [Michie et al., 1994]), aiming at comparing the performance of a fixed set of algorithms on several data sets. In the Statlog project 23 algorithms were evaluated on 21 data sets. A similar perspective on model selection can be found in [Kohavi et al., 1997], where these ideas form the background", "mykey":1642},
 {"datasetID":111, "supportID":"A09432D834D6F43DF8FE22A6FCE4E9B241ABE717", "rexaID":"4d3a15f17c5158aad69c6133ae41ed25b6e037a6", "author":"Eibe Frank and Stefan Kramer", "title":"Ensembles of nested dichotomies for multi-class problems", "venue":"ICML", "year":"2004", "window":"prim.-tumor 339 3.9 0 17 22 segment 2310 0.0 19 0 7 soybean 683 9.8 0 35 19 splice 3190 0.0 0 61 3 vehicle 846 0.0 18 0 4 vowel 990 0.0 10 3 11 waveform 5000 0.0 40 0 3 <b>zoo</b> 101 0.0 1 15 7 Table 1. Datasets used for the experiments differences in accuracy by using the corrected resampled t-test at the 5% significance level. This test has been shown to have Type I error at the significance level and", "mykey":1643},
 {"datasetID":150, "supportID":"A0C968B9AE31C39DC2DEE8D213A25B07C277E341", "rexaID":"86b2d2adbc4692e9aeafc750d7e02ef28799ac8f", "author":"Steve Whittaker and Loren G. Terveen and Bonnie A. Nardi", "title":"Let's stop pushing the envelope and start addressing it: a reference task agenda for HCI", "venue":"a Senior Research Scientist in the Human Computer Interaction Department of AT&T LabsResearch", "year":"", "window":"(Marcus, 1992, Price, 1991, Stern, 1990, Wayne,  1989). A dataset consists of a publicly available <b>corpus</b> of spoken sentences,  divided into training and test sentences. The initial task was to recognise the  individual sentences in the corpus. There was no", "mykey":1644},
 {"datasetID":42, "supportID":"A0EF5CFFF9CED79D4A44168B5BF1EDFAD46FF222", "rexaID":"d4664ad584fcc55802b2bffeb0d57f8e62eca0e2", "author":"Ron Kohavi and Brian Frasca", "title":"Useful Feature Subsets and Rough Set Reducts", "venue":"the Third International Workshop on Rough Sets and Soft Computing", "year":"", "window":"taken from Holte's paper, C4.5 has a 3.5% higher accuracy. The average accuracy for Holte-II is 82.7%, and 86.2% for C4.5. If we ignore the two <b>glass</b> datasets on which Holte-II does poorly, the difference shrinks to 1.3%. Thus even on data with continuous features that have not been discretized, Holte-II does reasonably close to C4.5. Moreover, the", "mykey":1645},
 {"datasetID":53, "supportID":"A0EF5CFFF9CED79D4A44168B5BF1EDFAD46FF222", "rexaID":"d4664ad584fcc55802b2bffeb0d57f8e62eca0e2", "author":"Ron Kohavi and Brian Frasca", "title":"Useful Feature Subsets and Rough Set Reducts", "venue":"the Third International Workshop on Rough Sets and Soft Computing", "year":"", "window":"bears no resemblance to Holte's 1R algorithm. 1993), stopping after a predetermined number of non-improving node expansions. Figure 2 shows the search through the feature subsets in the <b>IRIS</b> dataset. The number in brackets denotes the order the nodes are visited. The bootstrap estimate is given with one standard deviation of the accuracy after the +=Gamma sign. The estimated real accuracy (on", "mykey":1646},
 {"datasetID":70, "supportID":"A0EF5CFFF9CED79D4A44168B5BF1EDFAD46FF222", "rexaID":"d4664ad584fcc55802b2bffeb0d57f8e62eca0e2", "author":"Ron Kohavi and Brian Frasca", "title":"Useful Feature Subsets and Rough Set Reducts", "venue":"the Third International Workshop on Rough Sets and Soft Computing", "year":"", "window":"tic-tac-toe, breast-cancer, chess, mushroom, vote, and vote1, Holte-II has an average accuracy of 93.6%, much better than C4.5's average accuracy of 82.2%. If we ignore <b>Monk</b> 1, Monk 2, and parity---datasets that C4.5 does very badly on---the average accuracy for Holte-II is 91.2% and 88.5% for C4.5. Holte's 1R program (Holte 1993) built one-rules, that is, rules that test a single attribute, and was", "mykey":1647},
 {"datasetID":101, "supportID":"A0EF5CFFF9CED79D4A44168B5BF1EDFAD46FF222", "rexaID":"d4664ad584fcc55802b2bffeb0d57f8e62eca0e2", "author":"Ron Kohavi and Brian Frasca", "title":"Useful Feature Subsets and Rough Set Reducts", "venue":"the Third International Workshop on Rough Sets and Soft Computing", "year":"", "window":"increasing the running time considerably is to dynamically decide on the number of bootstrap samples needed. An abstract description of this problem is described in (Kohavi 1994). For the artificial datasets: Monk 1-3, parity, and <b>tic-tac-toe</b>  the test sets include the space of all possible instances, and therefore the test set accuracy is the actual real accuracy. For the real datasets, our accuracy", "mykey":1648},
 {"datasetID":98, "supportID":"A10711987D7DD4255301498C714506343693E114", "rexaID":"b88612f75e6cd1cee2efb08360bfb0ebf5295a97", "author":"Yishay Mansour", "title":"Pessimistic decision tree pruning based on tree size", "venue":"Computer Science Dept. Tel-Aviv University", "year":"", "window":"<b>statlog</b>  Comparative testing and evaluation of statistical and logical learning algorithms for large-scale applications in classification, prediction and control. ftp:ftp.ncc.up.pt/pub/statlog /datasets, (See also: Machine Learning, Neural and Statistical Classification, ed. Michie, Spiegelhalter and Taylor). [VC71] V. N. Vapnik and A. Ya. Chervonenkis. On the uniform convergence of relative", "mykey":1649},
 {"datasetID":150, "supportID":"A11C08B9232A0435A237390AE8476AF4583A3E0E", "rexaID":"86b2d2adbc4692e9aeafc750d7e02ef28799ac8f", "author":"Steve Whittaker and Loren G. Terveen and Bonnie A. Nardi", "title":"Let's stop pushing the envelope and start addressing it: a reference task agenda for HCI", "venue":"a Senior Research Scientist in the Human Computer Interaction Department of AT&T LabsResearch", "year":"", "window":"(Marcus, 1992, Price, 1991, Stern, 1990, Wayne, 1989). A dataset consists of a <b>corpus</b> of spoken sentences defined and made available to the researchers in advance of the bakeoff. The data contains both Reference task agenda . . . 16 \"training data\" -- sentences", "mykey":1650},
 {"datasetID":34, "supportID":"A1219D14D53EE0B93C6D2C8118EDA251B1C00FB7", "rexaID":"e28683d93b8fd976add91a9a13a588202d32fcea", "author":"Peter L. Hammer and Alexander Kogan and Bruno Simeone and Sandor Szedm'ak", "title":"R u t c o r Research R e p o r t", "venue":"Rutgers Center for Operations Research Rutgers University", "year":"2001", "window":"are obtained by lexicographically Page 28 RRR 7-2001 Figure 1: Cost of Classification Inaccuracy for # = 0 0 5 10 15 20 25 30 Credit Breast Cancer Boston Housing <b>Diabetes</b> Heart Disease Oil Voting Datasets Mean Cost LAD StrongSpanned StrongPrime Prime Figure 2: Cost of Classification Inaccuracy for # = 0.5 0 5 10 15 20 25 30 35 40 Credit Breast Cancer Boston Housing Diabetes Heart Disease Oil Voting", "mykey":1651},
 {"datasetID":45, "supportID":"A1219D14D53EE0B93C6D2C8118EDA251B1C00FB7", "rexaID":"e28683d93b8fd976add91a9a13a588202d32fcea", "author":"Peter L. Hammer and Alexander Kogan and Bruno Simeone and Sandor Szedm'ak", "title":"R u t c o r Research R e p o r t", "venue":"Rutgers Center for Operations Research Rutgers University", "year":"2001", "window":"are obtained by lexicographically Page 28 RRR 7-2001 Figure 1: Cost of Classification Inaccuracy for # = 0 0 5 10 15 20 25 30 Credit Breast Cancer Boston Housing Diabetes <b>Heart</b> Disease Oil Voting Datasets Mean Cost LAD StrongSpanned StrongPrime Prime Figure 2: Cost of Classification Inaccuracy for # = 0.5 0 5 10 15 20 25 30 35 40 Credit Breast Cancer Boston Housing Diabetes Heart Disease Oil Voting", "mykey":1652},
 {"datasetID":48, "supportID":"A1219D14D53EE0B93C6D2C8118EDA251B1C00FB7", "rexaID":"e28683d93b8fd976add91a9a13a588202d32fcea", "author":"Peter L. Hammer and Alexander Kogan and Bruno Simeone and Sandor Szedm'ak", "title":"R u t c o r Research R e p o r t", "venue":"Rutgers Center for Operations Research Rutgers University", "year":"2001", "window":"are obtained by lexicographically Page 28 RRR 7-2001 Figure 1: Cost of Classification Inaccuracy for # = 0 0 5 10 15 20 25 30 Credit Breast Cancer Boston <b>Housing</b> Diabetes Heart Disease Oil Voting Datasets Mean Cost LAD StrongSpanned StrongPrime Prime Figure 2: Cost of Classification Inaccuracy for # = 0.5 0 5 10 15 20 25 30 35 40 Credit Breast Cancer Boston Housing Diabetes Heart Disease Oil Voting", "mykey":1653},
 {"datasetID":25, "supportID":"A1AE583A2D5FF8684C5C9CD12BF517171550FA70", "rexaID":"cac8f7b952b6dc917dd68834c825e7f48373cafa", "author":"Zoubin Ghahramani and Michael I. Jordan", "title":"Factorial Hidden Markov Models", "venue":"Machine Learning, 29", "year":"1997", "window":"of EM). Factorial HMMs of varying sizes (K ranging from 2 to 6; M ranging from 2 to 9) were also trained on the same data. To 18 Z. GHAHRAMANI AND M.I. JORDAN Table 2. Attributes in the <b>Bach</b> chorale data set. The key signature and time signature attributes were constant over the duration of the chorale. All attributes were treated as real numbers and modeled as linear-Gaussian observations (4a).", "mykey":1654},
 {"datasetID":53, "supportID":"A26F0C396A9553D40BC6E4438697DD6455AAA25A", "rexaID":"546cbe749d666efb8ff699628e535e0fd083f6bc", "author":"Norbert Jankowski", "title":"Survey of Neural Transfer Functions", "venue":"Department of Computer Methods, Nicholas Copernicus University", "year":"", "window":"sphere defined by this metric. The influence of input renormalization (using Minkovsky distance functions) on the shapes of decision borders is illustrated in Fig. 30 for the classical <b>Iris</b> flowers dataset (only the last two input features, x 3 and x 4 are shown, for description of the data cf. [89]). Dramatic changes in the shapes of decision borders for different Minkovsky metrices are observed.", "mykey":1655},
 {"datasetID":67, "supportID":"A26F0C396A9553D40BC6E4438697DD6455AAA25A", "rexaID":"546cbe749d666efb8ff699628e535e0fd083f6bc", "author":"Norbert Jankowski", "title":"Survey of Neural Transfer Functions", "venue":"Department of Computer Methods, Nicholas Copernicus University", "year":"", "window":"results of RBF are not similar to the results of MLP. In some cases crossvalidation errors were twice as large using MLP than RBF (for example for the <b>DNA</b> dataset RBF gave 4.1% of error and was the best of all methods while MLP gave 8.8 % of errors and was on 12-th position, while for the Belgian Power data the situation is reversed, with MLP giving 1.7% of", "mykey":1656},
 {"datasetID":14, "supportID":"A28BF0579224A1160B52DA11AAA4D904006540AA", "rexaID":"631197638c7e0317c98e1a8d98e5fce8921aa758", "author":"Yongmei Wang and Ian H. Witten", "title":"Modeling for Optimal Probability Prediction", "venue":"ICML", "year":"2002", "window":"(Heart Disease, Cleveland), German (Statlog Project, German Credit), Ionosphere, Pima (Pima Indian Diabetes), Spambase, and WDBC (Winsconsin <b>Breast</b> <b>Cancer</b>  WDBC). The eighth is the Crab dataset from Agresti (1996). Some of the datasets were modified slightly: some attributes and instances were deleted to eliminate missing values, multi-class problems were transformed into binary ones, a", "mykey":1657},
 {"datasetID":94, "supportID":"A28BF0579224A1160B52DA11AAA4D904006540AA", "rexaID":"631197638c7e0317c98e1a8d98e5fce8921aa758", "author":"Yongmei Wang and Ian H. Witten", "title":"Modeling for Optimal Probability Prediction", "venue":"ICML", "year":"2002", "window":"runs of ten-fold cross-validation. According to both the negative log-likelihood and the classification rate, the estimator New provides either the best or nearly the best results for six of the datasets. For the other two  <b>Spambase</b> and WDBC), its results are intermediate and comparable with other estimators. Along with this, it also reduces the model dimensionality, which the MLE can never do. 5.", "mykey":1658},
 {"datasetID":52, "supportID":"A2A253709B76F2BB4038A43BF641540B6D64DBEB", "rexaID":"492fa3ac5e4f32e8d4bb1be1225c4fcf8739a553", "author":"Colin Campbell and Nello Cristianini and Alex J. Smola", "title":"Query Learning with Large Margin Classifiers", "venue":"ICML", "year":"2000", "window":"with a training set of 200 (Figure 2) there are an average 56 support vectors against an average 60 queries made. Real World Data. In Figure 5 we plot the corresponding curves for the <b>ionosphere</b> data set from the UCI Repository (Blake, Keogh & Merz, 1998). The ionosphere data set had a sparsity ratio of 0.29 so the advantages of selective sampling are clear. A plot of the averaged distance to the", "mykey":1659},
 {"datasetID":98, "supportID":"A3151BB9874AE321C671EBEE0526F2770E529FAD", "rexaID":"f894a06c44cd7ad6cf65caefba1dbe69d29763ec", "author":"Ron Kohavi and George John and Richard Long and David Manley and Karl Pfleger", "title":"Appears in Tools with AI '94", "venue":"Computer Science Department Stanford University", "year":"", "window":"but they are not an integrated environment, and are not very efficient. <b>StatLog</b> [19] is an ESPRIT project studying the behavior of over twenty algorithms (mostly in the MLToolbox), on over twenty datasets. StatLog is an instance of a good experimental study, but does not provide the tools to aid researchers in performing similar studies. Wray Buntine has recently suggested a unified approach to some", "mykey":1660},
 {"datasetID":34, "supportID":"A361D55BDBF9E41C66EDE1F12AF7CFFD55BA6FA8", "rexaID":"26619ba87b875d7e1b11e285a01de5485a24ff88", "author":"Zhihua Zhang and James T. Kwok and Dit-Yan Yeung", "title":"Parametric Distance Metric Learning with Label Information", "venue":"IJCAI", "year":"2003", "window":"(Numbers in bold indicate the better results). data set Euclidean metric learned metric <b>diabetes</b> 459/638 480/638 soybean 37/37 37/37 wine 85/118 117/118 WBC 412/469 446/469 ionosphere 168/251 221/251 iris 107/120 110/120 14 5 Conclusion We have proposed", "mykey":1661},
 {"datasetID":132, "supportID":"A376826468BEFB8BF97058DCB6FC00BB55F1BC6F", "rexaID":"2a9ec1d82b2c5a5478143d84928613ca7a9c3585", "author":"Harsha Nagesh and Sanjay Goil and Alok N. Choudhary", "title":"Adaptive Grids for Clustering Massive Data Sets", "venue":"Department of Energy ASCI", "year":"", "window":"which cannot be known a-priori for real data sets. 3. Each <b>Movie</b> Recommendation Data We applied MAFIA to a massive data set which contained movie ratings collected by DEC Systems Research Center [18] over a period of 18 months. During this period", "mykey":1662},
 {"datasetID":4, "supportID":"A37D1BCD1A7A46081EADA6AC96E787D4EBE10722", "rexaID":"4133f6bdbdc248e6e8cac9083cf934be0dd0fa31", "author":"W. Nick Street and Yoo-Hyon Kim", "title":"A streaming ensemble algorithm (SEA) for large-scale classification", "venue":"KDD", "year":"2001", "window":"cases, 25.7% of which belong to class 1. Predictive features include nuclear grade, tumor extent, tumor size, lymph node status, and number of lymph nodes examined. # <b>anonymous</b> <b>Web</b> browsing: This data set records browsing patterns for 32,710 anonymous visitors to the Microsoft Web site. We created a classification problem in a manner similar to Breese, et al. [3] by choosing to predict whether a", "mykey":1663},
 {"datasetID":14, "supportID":"A37D1BCD1A7A46081EADA6AC96E787D4EBE10722", "rexaID":"4133f6bdbdc248e6e8cac9083cf934be0dd0fa31", "author":"W. Nick Street and Yoo-Hyon Kim", "title":"A streaming ensemble algorithm (SEA) for large-scale classification", "venue":"KDD", "year":"2001", "window":"contains 44,848 instances of which 29.3% are in the over 50k\" class. # SEER <b>breast</b> <b>cancer</b>  The breast cancer data set from the Surveillance, Epidemiology, and End Results (SEER) program [6] of the National Institutes of Health contains follow-up data on over 44,000 breast cancer patients. The cases were filtered to", "mykey":1664},
 {"datasetID":67, "supportID":"A380817FB24FF67AE988D4CB2326573F6B7CCB02", "rexaID":"dedfa4527706631a09382e23555649c7b3e37ebd", "author":"Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin", "title":"Linear dimensionalityreduction using relevance weighted LDA", "venue":"School of Electrical and Electronic Engineering Nanyang Technological University", "year":"2005", "window":"to compare LDA, aPAC, WLDR, EWLDR. The six data sets are landsat, optdigits, vehicle, <b>DNA</b>  thyroid disease and vowel data sets. Landsat. The Landsat data set is generated from landsat multi-spectral scanner image data. It has 36 dimensions, 4435", "mykey":1665},
 {"datasetID":80, "supportID":"A380817FB24FF67AE988D4CB2326573F6B7CCB02", "rexaID":"dedfa4527706631a09382e23555649c7b3e37ebd", "author":"Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin", "title":"Linear dimensionalityreduction using relevance weighted LDA", "venue":"School of Electrical and Electronic Engineering Nanyang Technological University", "year":"2005", "window":"is generated from landsat multi-spectral scanner image data. It has 36 dimensions, 4435 training samples and 2000 testing samples belonging to 6 classes. Optdigits. This is a 60-dimensional data set on <b>optical</b> <b>recognition</b> of 10 handwritten digits. It has separate training and testing sets with 3823 and 1797 samples, respectively. Vehicle. This data set involves classification of a given", "mykey":1666},
 {"datasetID":81, "supportID":"A380817FB24FF67AE988D4CB2326573F6B7CCB02", "rexaID":"dedfa4527706631a09382e23555649c7b3e37ebd", "author":"Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin", "title":"Linear dimensionalityreduction using relevance weighted LDA", "venue":"School of Electrical and Electronic Engineering Nanyang Technological University", "year":"2005", "window":"is generated from landsat multi-spectral scanner image data. It has 36 dimensions, 4435 training samples and 2000 testing samples belonging to 6 classes. Optdigits. This is a 60-dimensional data set on optical <b>recognition</b> of 10 <b>handwritten</b> <b>digits</b>  It has separate training and testing sets with 3823 and 1797 samples, respectively. Vehicle. This data set involves classification of a given", "mykey":1667},
 {"datasetID":146, "supportID":"A380817FB24FF67AE988D4CB2326573F6B7CCB02", "rexaID":"dedfa4527706631a09382e23555649c7b3e37ebd", "author":"Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin", "title":"Linear dimensionalityreduction using relevance weighted LDA", "venue":"School of Electrical and Electronic Engineering Nanyang Technological University", "year":"2005", "window":"to compare LDA, aPAC, WLDR, EWLDR. The six data sets are <b>landsat</b>  optdigits, vehicle, DNA, thyroid disease and vowel data sets. Landsat. The Landsat data set is generated from landsat multi-spectral scanner image data. It has 36 dimensions, 4435", "mykey":1668},
 {"datasetID":149, "supportID":"A380817FB24FF67AE988D4CB2326573F6B7CCB02", "rexaID":"dedfa4527706631a09382e23555649c7b3e37ebd", "author":"Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin", "title":"Linear dimensionalityreduction using relevance weighted LDA", "venue":"School of Electrical and Electronic Engineering Nanyang Technological University", "year":"2005", "window":"to compare LDA, aPAC, WLDR, EWLDR. The six data sets are landsat, optdigits, <b>vehicle</b>  DNA, thyroid disease and vowel data sets. Landsat. The Landsat data set is generated from landsat multi-spectral scanner image data. It has 36 dimensions, 4435", "mykey":1669},
 {"datasetID":102, "supportID":"A380817FB24FF67AE988D4CB2326573F6B7CCB02", "rexaID":"dedfa4527706631a09382e23555649c7b3e37ebd", "author":"Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin", "title":"Linear dimensionalityreduction using relevance weighted LDA", "venue":"School of Electrical and Electronic Engineering Nanyang Technological University", "year":"2005", "window":"to compare LDA, aPAC, WLDR, EWLDR. The six data sets are landsat, optdigits, vehicle, DNA, <b>thyroid</b> disease and vowel data sets. Landsat. The Landsat data set is generated from landsat multi-spectral scanner image data. It has 36 dimensions, 4435", "mykey":1670},
 {"datasetID":34, "supportID":"A3A1E2640DDAEED9C586CE43F217DC03D9C3B00E", "rexaID":"d64d2705cabed449e8cb2ecc3c3c77c54ee71051", "author":"Jeroen Eggermont and Joost N. Kok and Walter A. Kosters", "title":"Genetic Programming for data classification: partitioning the search space", "venue":"SAC", "year":"2004", "window":"The results of our refined gp algorithm using the gain ratio criterion are again worse than those of our clustering and other refined gp algorithms. The Pima Indians <b>diabetes</b> Data Set On the Pima Indians diabetes data set (see Table 5) the refined gp algorithms using the gain criterion are again better than those using the gain ratio criterion. If we compare the results of our", "mykey":1671},
 {"datasetID":45, "supportID":"A3A1E2640DDAEED9C586CE43F217DC03D9C3B00E", "rexaID":"d64d2705cabed449e8cb2ecc3c3c77c54ee71051", "author":"Jeroen Eggermont and Joost N. Kok and Walter A. Kosters", "title":"Genetic Programming for data classification: partitioning the search space", "venue":"SAC", "year":"2004", "window":"using different the sets of internal nodes. The same behavior is seen for k = 4 and k = 5. In all cases the discovered decision trees differ syntactically per fold and random seed. The <b>Heart</b> Disease Data Set The results on the Heart disease data set are displayed in Table 6. All our gp algorithms show a large improvement in misclassification performance over our simple gp algorithm. In all but two cases", "mykey":1672},
 {"datasetID":52, "supportID":"A3A1E2640DDAEED9C586CE43F217DC03D9C3B00E", "rexaID":"d64d2705cabed449e8cb2ecc3c3c77c54ee71051", "author":"Jeroen Eggermont and Joost N. Kok and Walter A. Kosters", "title":"Genetic Programming for data classification: partitioning the search space", "venue":"SAC", "year":"2004", "window":"algorithm has a much smaller standard deviation. When we look at the number of clusters or maximum number of partitions we see that a maximum of 2 clusters or partitions is clearly the best for this data set. The <b>Ionosphere</b> Data Set If we look at the results on the Ionosphere data set in Table 7 we see that using the gain ratio instead of the gain criterion with our refined gp algorithms greatly", "mykey":1673},
 {"datasetID":53, "supportID":"A3A1E2640DDAEED9C586CE43F217DC03D9C3B00E", "rexaID":"d64d2705cabed449e8cb2ecc3c3c77c54ee71051", "author":"Jeroen Eggermont and Joost N. Kok and Walter A. Kosters", "title":"Genetic Programming for data classification: partitioning the search space", "venue":"SAC", "year":"2004", "window":"is disappointing as only our clustering gp algorithm with 3 clusters per numerical valued attribute manages to really outperform our simple gp but still performs much worse than C4.5. The <b>Iris</b> Data Set If we look at the results of our gp algorithms on the Iris data set in Table 8 we see that by far the best performance is achieved by our clustering gp algorithm with 3 clusters per numerical valued", "mykey":1674},
 {"datasetID":79, "supportID":"A3A1E2640DDAEED9C586CE43F217DC03D9C3B00E", "rexaID":"d64d2705cabed449e8cb2ecc3c3c77c54ee71051", "author":"Jeroen Eggermont and Joost N. Kok and Walter A. Kosters", "title":"Genetic Programming for data classification: partitioning the search space", "venue":"SAC", "year":"2004", "window":"The results of our refined gp algorithm using the gain ratio criterion are again worse than those of our clustering and other refined gp algorithms. The <b>Pima</b> <b>Indians</b> <b>diabetes</b> Data Set On the Pima Indians diabetes data set (see Table 5) the refined gp algorithms using the gain criterion are again better than those using the gain ratio criterion. If we compare the results of our", "mykey":1675},
 {"datasetID":143, "supportID":"A3A1E2640DDAEED9C586CE43F217DC03D9C3B00E", "rexaID":"d64d2705cabed449e8cb2ecc3c3c77c54ee71051", "author":"Jeroen Eggermont and Joost N. Kok and Walter A. Kosters", "title":"Genetic Programming for data classification: partitioning the search space", "venue":"SAC", "year":"2004", "window":"used in the experiments data set records attributes classes <b>Australian credit</b> (statlog) 690 14 2 German credit (statlog) 1000 23 2 Pima Indians diabetes 768 8 2 Heart disease (statlog) 270 13 2 Ionosphere 351 34 2 Iris 150 4 3 .", "mykey":1676},
 {"datasetID":144, "supportID":"A3A1E2640DDAEED9C586CE43F217DC03D9C3B00E", "rexaID":"d64d2705cabed449e8cb2ecc3c3c77c54ee71051", "author":"Jeroen Eggermont and Joost N. Kok and Walter A. Kosters", "title":"Genetic Programming for data classification: partitioning the search space", "venue":"SAC", "year":"2004", "window":"C4.5 or our simple gp. A positive aspect of the refined gp algorithms using the gain ratio criterion is that the standard deviations are lower than for our other algorithms. Table 4: <b>German credit</b> data set results algorithm k average s.d. best worst rank clustering gp 2 27.8 0.7 26.3 28.8 4 clustering gp 3 28.0 0.8 27.0 29.8 6 clustering gp 4 27.9 0.9 26.7 29.4 5 clustering gp 5 28.4 0.8 26.9 29.5 11", "mykey":1677},
 {"datasetID":2, "supportID":"A4DBBC3271F90C9CDD0798CCF4A330EF0887956D", "rexaID":"b2c60bdf066b03909792bd5d64891f3607b948c8", "author":"Rich Caruana and Alexandru Niculescu-Mizil and Geoff Crew and Alex Ksikes", "title":"Ensemble selection from libraries of models", "venue":"ICML", "year":"2004", "window":"but each ensemble is just a weighted average of models, so the average of a set of ensembles also is a simple weighted average of the base-level models. Bagging is discussed in Section 5.3. 3. Data Sets We experiment with seven problems: <b>ADULT</b>  COVER TYPE, LETTER.p1, and LETTER.p2 from the UCI Repository (Blake & Merz, 1998), MEDIS, a pneumonia data set, SLAC, data from collaborators at the", "mykey":1678},
 {"datasetID":90, "supportID":"A4DBBC3271F90C9CDD0798CCF4A330EF0887956D", "rexaID":"b2c60bdf066b03909792bd5d64891f3607b948c8", "author":"Rich Caruana and Alexandru Niculescu-Mizil and Geoff Crew and Alex Ksikes", "title":"Ensemble selection from libraries of models", "venue":"ICML", "year":"2004", "window":"1-13 as class 0 and letters 14-26 as class 1, yielding a di\u00c6cult, but balanced, problem. HYPER SPECT was converted to binary by treating the large confusable class <b>Soybean</b> Mintil as class 1. These data sets were selected because they are large enough to allow moderate size train and validation sets, and still have data left for large final test sets. For our experiments, we used training sets of 5000", "mykey":1679},
 {"datasetID":91, "supportID":"A4DBBC3271F90C9CDD0798CCF4A330EF0887956D", "rexaID":"b2c60bdf066b03909792bd5d64891f3607b948c8", "author":"Rich Caruana and Alexandru Niculescu-Mizil and Geoff Crew and Alex Ksikes", "title":"Ensemble selection from libraries of models", "venue":"ICML", "year":"2004", "window":"1-13 as class 0 and letters 14-26 as class 1, yielding a di\u00c6cult, but balanced, problem. HYPER SPECT was converted to binary by treating the large confusable class <b>Soybean</b> Mintil as class 1. These data sets were selected because they are large enough to allow moderate size train and validation sets, and still have data left for large final test sets. For our experiments, we used training sets of 5000", "mykey":1680},
 {"datasetID":34, "supportID":"A4ECC6C7345A85C80DEEFC233EE5E8A22A1343BD", "rexaID":"5f8eb537fc397ad5e506b2eae4f6676b48990ba6", "author":"Thomas G. Dietterich", "title":"Approximate Statistical Test For Comparing Supervised Classification Learning Algorithms", "venue":"Neural Computation, 10", "year":"1998", "window":"measured on the 10,000 calibration examples) matched the average performance of C4.5 to within 0.1%. For the Pima Indians <b>Diabetes</b> data set, we drew 1000 data sets of size 300 from the 768 available examples. For each of these data sets, the remaining 468 examples were retained for calibration. Each of the 1000 data sets of size 300 was", "mykey":1681},
 {"datasetID":59, "supportID":"A4ECC6C7345A85C80DEEFC233EE5E8A22A1343BD", "rexaID":"5f8eb537fc397ad5e506b2eae4f6676b48990ba6", "author":"Thomas G. Dietterich", "title":"Approximate Statistical Test For Comparing Supervised Classification Learning Algorithms", "venue":"Neural Computation, 10", "year":"1998", "window":"1 (Quinlan, 1993) and the first nearest-neighbor (NN) algorithm (Dasarathy, 1991). We then selected three difficult problems: the EXP6 problem developed by Kong (1995), the <b>Letter Recognition</b> data set (Frey & Slate, 1991), and the Pima Indians Diabetes Task (Merz & Murphy, 1996). Of course, C4.5 and NN do not have the same performance on these data sets. In EXP6 and Letter Recognition, NN", "mykey":1682},
 {"datasetID":79, "supportID":"A4ECC6C7345A85C80DEEFC233EE5E8A22A1343BD", "rexaID":"5f8eb537fc397ad5e506b2eae4f6676b48990ba6", "author":"Thomas G. Dietterich", "title":"Approximate Statistical Test For Comparing Supervised Classification Learning Algorithms", "venue":"Neural Computation, 10", "year":"1998", "window":"measured on the 10,000 calibration examples) matched the average performance of C4.5 to within 0.1%. For the <b>Pima</b> <b>Indians</b> <b>Diabetes</b> data set, we drew 1000 data sets of size 300 from the 768 available examples. For each of these data sets, the remaining 468 examples were retained for calibration. Each of the 1000 data sets of size 300 was", "mykey":1683},
 {"datasetID":151, "supportID":"A4F29992C910C8F8CF9193CE2767482FAE39507F", "rexaID":"482c2024eea88014e9cff136b42fc98cccf6dad4", "author":"Richard Maclin and David W. Opitz", "title":"An Empirical Evaluation of Bagging and Boosting", "venue":"AAAI/IAAI", "year":"1997", "window":"outperforms using a single classifier, but significantly outperforms Bagging (e.g., kr-vskp, letter, segmentation, and vehicle). Ada-Boosting's results are even more extreme. For certain data sets (kr-vs-kp, letter, <b>sonar</b> , Ada-Boosting produces a significant gain over any other method (including Arcing). On other data sets Ada-Boosting produces results that are even worse than using a", "mykey":1684},
 {"datasetID":42, "supportID":"A4F29992C910C8F8CF9193CE2767482FAE39507F", "rexaID":"482c2024eea88014e9cff136b42fc98cccf6dad4", "author":"Richard Maclin and David W. Opitz", "title":"An Empirical Evaluation of Bagging and Boosting", "venue":"AAAI/IAAI", "year":"1997", "window":"that can be drawn from the results is that both the Simple Ensemble and Bagging approaches almost always produces better performance than just training a single classifier. For some of these data sets (e.g., <b>glass</b>  kr-vs-kp, letter, segmentation, soybean, and vehicle) the gains in performance are quite significant. One aspect many of these data sets share is that they involve predictions for", "mykey":1685},
 {"datasetID":14, "supportID":"A51711DC61213AACF7D5B86D9353D9D7E6CF222B", "rexaID":"b19579eae108f0efb0d9adf97e480280f8e4f7a8", "author":"Krzysztof Grabczewski and Wl/odzisl/aw Duch", "title":"Heterogeneous Forests of Decision Trees", "venue":"ICANN", "year":"2002", "window":"< 1.10531) then primary hypothyroid 2. if TSH } 6.05 # FTI } 64.72 # on_thyroxine = 0 # thyroid_surgery = 0 # TT4 < 150.5 then compensated hypothyroid 3. else healthy. The Wisconsin <b>breast</b> <b>cancer</b> dataset contains 699 instances, with 458 benign (65.5%) and 241 (34.5%) malignant cases. Each instance is described by 9 attributes with integer value in the range 1-10 and a binary class label. For 16", "mykey":1686},
 {"datasetID":17, "supportID":"A51711DC61213AACF7D5B86D9353D9D7E6CF222B", "rexaID":"b19579eae108f0efb0d9adf97e480280f8e4f7a8", "author":"Krzysztof Grabczewski and Wl/odzisl/aw Duch", "title":"Heterogeneous Forests of Decision Trees", "venue":"ICANN", "year":"2002", "window":"< 1.10531) then primary hypothyroid 2. if TSH } 6.05 # FTI } 64.72 # on_thyroxine = 0 # thyroid_surgery = 0 # TT4 < 150.5 then compensated hypothyroid 3. else healthy. The <b>Wisconsin</b> <b>breast</b> <b>cancer</b> dataset contains 699 instances, with 458 benign (65.5%) and 241 (34.5%) malignant cases. Each instance is described by 9 attributes with integer value in the range 1-10 and a binary class label. For 16", "mykey":1687},
 {"datasetID":15, "supportID":"A51711DC61213AACF7D5B86D9353D9D7E6CF222B", "rexaID":"b19579eae108f0efb0d9adf97e480280f8e4f7a8", "author":"Krzysztof Grabczewski and Wl/odzisl/aw Duch", "title":"Heterogeneous Forests of Decision Trees", "venue":"ICANN", "year":"2002", "window":"< 1.10531) then primary hypothyroid 2. if TSH } 6.05 # FTI } 64.72 # on_thyroxine = 0 # thyroid_surgery = 0 # TT4 < 150.5 then compensated hypothyroid 3. else healthy. The <b>Wisconsin</b> <b>breast</b> <b>cancer</b> dataset contains 699 instances, with 458 benign (65.5%) and 241 (34.5%) malignant cases. Each instance is described by 9 attributes with integer value in the range 1-10 and a binary class label. For 16", "mykey":1688},
 {"datasetID":16, "supportID":"A51711DC61213AACF7D5B86D9353D9D7E6CF222B", "rexaID":"b19579eae108f0efb0d9adf97e480280f8e4f7a8", "author":"Krzysztof Grabczewski and Wl/odzisl/aw Duch", "title":"Heterogeneous Forests of Decision Trees", "venue":"ICANN", "year":"2002", "window":"< 1.10531) then primary hypothyroid 2. if TSH } 6.05 # FTI } 64.72 # on_thyroxine = 0 # thyroid_surgery = 0 # TT4 < 150.5 then compensated hypothyroid 3. else healthy. The <b>Wisconsin</b> <b>breast</b> <b>cancer</b> dataset contains 699 instances, with 458 benign (65.5%) and 241 (34.5%) malignant cases. Each instance is described by 9 attributes with integer value in the range 1-10 and a binary class label. For 16", "mykey":1689},
 {"datasetID":146, "supportID":"A599425781BE332FD01CC264603E8AA3F6C4944C", "rexaID":"82c18d900119b86bada0621e2774e8fcd0b47140", "author":"Xavier Giannakopoulos and Juha Karhunen and Erkki Oja", "title":"A COMPARISON OF NEURAL ICA ALGORITHMS USING REAL-WORLD DATA", "venue":"IDSIA", "year":"", "window":"roughly optimal [12]. On-line estimation of kurtosis was added also to this algorithm. 4. COMPARISON METHODS We have made experiments with crab data, <b>satellite</b> data, and MEG artefact data. These data sets will be brieAEy discussed in context with experimental results. Because we are now dealing with real-world data, the assumptions made on the ICA model (1) may not hold, or hold only approximately.", "mykey":1690},
 {"datasetID":151, "supportID":"A5BF048C34C3C6916162F8F241BAB1F8336025EE", "rexaID":"d18b9cca12173a8ca3d5a781cadcf847442930f4", "author":"Carlotta Domeniconi and Jing Peng and Dimitrios Gunopulos", "title":"An Adaptive Metric Machine for Pattern Classification", "venue":"NIPS", "year":"2000", "window":"consists of q = 4 measurements made on each of N = 100 iris plants of J = 2 species; 2. <b>Sonar</b> data. This data set consists of q = 60 frequency measurements made on each of N = 208 data of J = 2 classes (``mines'' and ``rocks''); 3. Vowel data. This example has q = 10 measurements and 11 classes. There are total", "mykey":1691},
 {"datasetID":42, "supportID":"A5BF048C34C3C6916162F8F241BAB1F8336025EE", "rexaID":"d18b9cca12173a8ca3d5a781cadcf847442930f4", "author":"Carlotta Domeniconi and Jing Peng and Dimitrios Gunopulos", "title":"An Adaptive Metric Machine for Pattern Classification", "venue":"NIPS", "year":"2000", "window":"N = 208 data of J = 2 classes (``mines'' and ``rocks''); 3. Vowel data. This example has q = 10 measurements and 11 classes. There are total of N = 528 samples in this example; 4. <b>Glass</b> data. This data set consists of q = 9 chemical attributes measured for each of N = 214 data of J = 6 classes; 5. Image data. This data set consists of 40 texture images that are manually classified into 15 classes. The", "mykey":1692},
 {"datasetID":53, "supportID":"A5BF048C34C3C6916162F8F241BAB1F8336025EE", "rexaID":"d18b9cca12173a8ca3d5a781cadcf847442930f4", "author":"Carlotta Domeniconi and Jing Peng and Dimitrios Gunopulos", "title":"An Adaptive Metric Machine for Pattern Classification", "venue":"NIPS", "year":"2000", "window":"used were taken from the UCI Machine Learning Database Repository [10], except for the unreleased image data set. They are: 1. <b>Iris</b> data. This data set consists of q = 4 measurements made on each of N = 100 iris plants of J = 2 species; 2. Sonar data. This data set consists of q = 60 frequency measurements", "mykey":1693},
 {"datasetID":60, "supportID":"A5BF048C34C3C6916162F8F241BAB1F8336025EE", "rexaID":"d18b9cca12173a8ca3d5a781cadcf847442930f4", "author":"Carlotta Domeniconi and Jing Peng and Dimitrios Gunopulos", "title":"An Adaptive Metric Machine for Pattern Classification", "venue":"NIPS", "year":"2000", "window":"consists of q = 16 numerical attributes and J = 26 classes; 8. <b>Liver</b> data. This data set consists of 345 instances, represented by q = 6 numerical attributes, and J = 2 classes; and 9. Lung data. This example has 32 instances having q = 56 numerical features and J = 3 classes. Results:", "mykey":1694},
 {"datasetID":2, "supportID":"A638CBF63DD28B9E5B2D1F003CBBA209D7BF5AF9", "rexaID":"a702c0617d7187cfc988819d79f0eb2a42ba3f19", "author":"Thomas Serafini and G. Zanghirati and Del Zanna and T. Serafini and Gaetano Zanghirati and Luca Zanni", "title":"DIPARTIMENTO DI MATEMATICA", "venue":"Gradient Projection Methods for", "year":"2003", "window":"the MNIST database of handwritten digits [24] and the UCI <b>Adult</b> data set [27]. These experiments are carried out on a Compaq XP1000 workstation at 667MHz with 1GB of RAM, with standard C codes. All the considered methods compute the projection on the special feasible", "mykey":1695},
 {"datasetID":81, "supportID":"A638CBF63DD28B9E5B2D1F003CBBA209D7BF5AF9", "rexaID":"a702c0617d7187cfc988819d79f0eb2a42ba3f19", "author":"Thomas Serafini and G. Zanghirati and Del Zanna and T. Serafini and Gaetano Zanghirati and Luca Zanni", "title":"DIPARTIMENTO DI MATEMATICA", "venue":"Gradient Projection Methods for", "year":"2003", "window":"and show how the GVPMs can be a valuable alternative to both SPGM(# (1,2) ) and AL-SPGMs. To evaluate the above methods on some QP problems of the form (2) we train Gaussian SVMs on two real-world data sets: the MNIST database of <b>handwritten</b> <b>digits</b> [24] and the UCI Adult data set [27]. These experiments are carried out on a Compaq XP1000 workstation at 667MHz with 1GB of RAM, with standard C codes.", "mykey":1696},
 {"datasetID":14, "supportID":"A69C85F6BF728B015E2FDA44CDBBA454FCFF30ED", "rexaID":"271b9c67f2a11a31962a436a41aaa5ed148dda6e", "author":"Lorne Mason and Peter L. Bartlett and Jonathan Baxter", "title":"Direct Optimization of Margins Improves Generalization in Combined Classifiers", "venue":"NIPS", "year":"1998", "window":"sets were reduced in size to makeoverfitting more likely, so that complexity regularization with DOOM could haveaneffect. (The details are given in the full version [MBB98].) In three of the datasets (Credit Application, Wisconsin <b>Breast</b> <b>Cancer</b> and Pima Indians Diabetes), AdaBoost gained no advantage from using more than a single classifier. In these datasets, the number of classifiers was", "mykey":1697},
 {"datasetID":17, "supportID":"A69C85F6BF728B015E2FDA44CDBBA454FCFF30ED", "rexaID":"271b9c67f2a11a31962a436a41aaa5ed148dda6e", "author":"Lorne Mason and Peter L. Bartlett and Jonathan Baxter", "title":"Direct Optimization of Margins Improves Generalization in Combined Classifiers", "venue":"NIPS", "year":"1998", "window":"sets were reduced in size to makeoverfitting more likely, so that complexity regularization with DOOM could haveaneffect. (The details are given in the full version [MBB98].) In three of the datasets (Credit Application, <b>Wisconsin</b> <b>Breast</b> <b>Cancer</b> and Pima Indians Diabetes), AdaBoost gained no advantage from using more than a single classifier. In these datasets, the number of classifiers was", "mykey":1698},
 {"datasetID":15, "supportID":"A69C85F6BF728B015E2FDA44CDBBA454FCFF30ED", "rexaID":"271b9c67f2a11a31962a436a41aaa5ed148dda6e", "author":"Lorne Mason and Peter L. Bartlett and Jonathan Baxter", "title":"Direct Optimization of Margins Improves Generalization in Combined Classifiers", "venue":"NIPS", "year":"1998", "window":"sets were reduced in size to makeoverfitting more likely, so that complexity regularization with DOOM could haveaneffect. (The details are given in the full version [MBB98].) In three of the datasets (Credit Application, <b>Wisconsin</b> <b>Breast</b> <b>Cancer</b> and Pima Indians Diabetes), AdaBoost gained no advantage from using more than a single classifier. In these datasets, the number of classifiers was", "mykey":1699},
 {"datasetID":16, "supportID":"A69C85F6BF728B015E2FDA44CDBBA454FCFF30ED", "rexaID":"271b9c67f2a11a31962a436a41aaa5ed148dda6e", "author":"Lorne Mason and Peter L. Bartlett and Jonathan Baxter", "title":"Direct Optimization of Margins Improves Generalization in Combined Classifiers", "venue":"NIPS", "year":"1998", "window":"sets were reduced in size to makeoverfitting more likely, so that complexity regularization with DOOM could haveaneffect. (The details are given in the full version [MBB98].) In three of the datasets (Credit Application, <b>Wisconsin</b> <b>Breast</b> <b>Cancer</b> and Pima Indians Diabetes), AdaBoost gained no advantage from using more than a single classifier. In these datasets, the number of classifiers was", "mykey":1700},
 {"datasetID":151, "supportID":"A69C85F6BF728B015E2FDA44CDBBA454FCFF30ED", "rexaID":"271b9c67f2a11a31962a436a41aaa5ed148dda6e", "author":"Lorne Mason and Peter L. Bartlett and Jonathan Baxter", "title":"Direct Optimization of Margins Improves Generalization in Combined Classifiers", "venue":"NIPS", "year":"1998", "window":"the light curveisDOOMwith` selected by cross-validation. The test errors for both algorithms are marked on the vertical axis at margin 0. can be seen in Figure 3 (Credit Application and <b>Sonar</b> data sets), the generalization performance of the combined classifier produced by DOOM can be as good as or better than that of the classifier produced by AdaBoost, despite having dramatically worse minimum", "mykey":1701},
 {"datasetID":27, "supportID":"A69C85F6BF728B015E2FDA44CDBBA454FCFF30ED", "rexaID":"271b9c67f2a11a31962a436a41aaa5ed148dda6e", "author":"Lorne Mason and Peter L. Bartlett and Jonathan Baxter", "title":"Direct Optimization of Margins Improves Generalization in Combined Classifiers", "venue":"NIPS", "year":"1998", "window":"sets were reduced in size to makeoverfitting more likely, so that complexity regularization with DOOM could haveaneffect. (The details are given in the full version [MBB98].) In three of the datasets  <b>Credit Application</b>  Wisconsin Breast Cancer and Pima Indians Diabetes), AdaBoost gained no advantage from using more than a single classifier. In these datasets, the number of classifiers was", "mykey":1702},
 {"datasetID":52, "supportID":"A69C85F6BF728B015E2FDA44CDBBA454FCFF30ED", "rexaID":"271b9c67f2a11a31962a436a41aaa5ed148dda6e", "author":"Lorne Mason and Peter L. Bartlett and Jonathan Baxter", "title":"Direct Optimization of Margins Improves Generalization in Combined Classifiers", "venue":"NIPS", "year":"1998", "window":"classifier produced by DOOM can be as good as or better than that of the classifier produced by AdaBoost, despite having dramatically worse minimum training margin. Conversely, Figure 3  <b>Ionosphere</b> data set) shows that improved generalization performance can be associated with an improved minimum margin. The margin distributions also show that there is a balance to be found between training error and", "mykey":1703},
 {"datasetID":2, "supportID":"A6AEF373C1542D18063EA15E5F0881E83C98005D", "rexaID":"fc0a66d3a7336b6eabad919a7389c68cc37f2564", "author":"Ayhan Demiriz and Kristin P. Bennett and John Shawe and I. Nouretdinov V.", "title":"Linear Programming Boosting via Column Generation", "venue":"Dept. of Decision Sciences and Eng. Systems, Rensselaer Polytechnic Institute", "year":"", "window":"of the final set of weak hypotheses. This is just a very simple method of boosting multiclass problems. Further investigation of LP multiclass approaches is needed. We ran experiments on larger datasets: Forest, <b>Adult</b>  USPS, and Optdigits from UCI[14]. Forest is a 54-dimension dataset with seven possible classes. The data are divided into 11340 training, 3780 validation, and 565892 testing", "mykey":1704},
 {"datasetID":14, "supportID":"A6AEF373C1542D18063EA15E5F0881E83C98005D", "rexaID":"fc0a66d3a7336b6eabad919a7389c68cc37f2564", "author":"Ayhan Demiriz and Kristin P. Bennett and John Shawe and I. Nouretdinov V.", "title":"Linear Programming Boosting via Column Generation", "venue":"Dept. of Decision Sciences and Eng. Systems, Rensselaer Polytechnic Institute", "year":"", "window":"criterion for stopping when an optimal ensemble is found that is reached in relatively few iterations. It uses few weak hypotheses. There are only 81 possible stumps on the <b>Breast</b> <b>Cancer</b> dataset (nine attributes having nine possible values), so clearly AdaBoost may require the same tree to be generated multiple times. LPBoost generates a weak hypothesis only once and can alter the weight on", "mykey":1705},
 {"datasetID":45, "supportID":"A6AEF373C1542D18063EA15E5F0881E83C98005D", "rexaID":"fc0a66d3a7336b6eabad919a7389c68cc37f2564", "author":"Ayhan Demiriz and Kristin P. Bennett and John Shawe and I. Nouretdinov V.", "title":"Linear Programming Boosting via Column Generation", "venue":"Dept. of Decision Sciences and Eng. Systems, Rensselaer Polytechnic Institute", "year":"", "window":"and the stopping criteria. Both methods were allowed the same maximum number of iterations. 8.1 Boosting Decision Tree Stumps We used decision tree stumps as base hypotheses on the following six datasets: Cancer (9,699), Diagnostic (30,569), <b>Heart</b> (13,297), Ionosphere (34,351), Musk (166,476), and Sonar (60,208). The number of features and number of points in each dataset are shown, respectively,", "mykey":1706},
 {"datasetID":48, "supportID":"A6AEF373C1542D18063EA15E5F0881E83C98005D", "rexaID":"fc0a66d3a7336b6eabad919a7389c68cc37f2564", "author":"Ayhan Demiriz and Kristin P. Bennett and John Shawe and I. Nouretdinov V.", "title":"Linear Programming Boosting via Column Generation", "venue":"Dept. of Decision Sciences and Eng. Systems, Rensselaer Polytechnic Institute", "year":"", "window":"used in decision tree stumps experiments, we use four additional UCI datasets here. These are the House(16,435), <b>Housing</b> 13,506) 3 , Pima(8,768), and Spam(57,4601) datasets. As in the decision tree stumps experiments, we report results from 10-fold CV. Since the best # value", "mykey":1707},
 {"datasetID":80, "supportID":"A6AEF373C1542D18063EA15E5F0881E83C98005D", "rexaID":"fc0a66d3a7336b6eabad919a7389c68cc37f2564", "author":"Ayhan Demiriz and Kristin P. Bennett and John Shawe and I. Nouretdinov V.", "title":"Linear Programming Boosting via Column Generation", "venue":"Dept. of Decision Sciences and Eng. Systems, Rensselaer Polytechnic Institute", "year":"", "window":"with missing values. The default handling in C4.5 has been used for missing values. USPS and Optdigits are <b>optical</b> character <b>recognition</b> datasets. USPS has 256 dimensions without missing value. Out of 7291 original training points, we use 1822 points as training data and the rest 5469 as validation data. There are 2007 test points. Optdigits", "mykey":1708},
 {"datasetID":26, "supportID":"A74EFB1B547DE1B8E73A40EDC9A84EB09F30970F", "rexaID":"dfce0da4e5e3d501a92e3be7e4b0cc43d46fd057", "author":"Alan Burton and Paul H J Kelly", "title":"Performance Prediction of Paging Workloads Using Lightweight Tracing", "venue":"IPDPS", "year":"2003", "window":"set and a set of pre-defined classes, C4.5 [11] attempts to generate a function which maps the data items in a database into the predefined classes, by constructing an optimal decision tree. The data set used in these experiments was the  <b>Connect-4</b> Opening Database' [9]. # mSQL. This experiment involved running part of the AS 3 AP[13] SQL benchmark on version 2.0.3 of mSQL[6], a lightweight database", "mykey":1709},
 {"datasetID":34, "supportID":"A784F30E5815862D3F19B6DF2552F36548466623", "rexaID":"d64d2705cabed449e8cb2ecc3c3c77c54ee71051", "author":"Jeroen Eggermont and Joost N. Kok and Walter A. Kosters", "title":"Genetic Programming for data classification: partitioning the search space", "venue":"SAC", "year":"2004", "window":"our refined gp algorithm using the gain criterion performs a lot better than our simple gp algorithm and C4.5. If we look at the other two data set sets (German Credit and Pima Indians <b>Diabetes</b> , we see these are also the data sets on which our simple gp performs better than or close to C4.5. It is at the moment unclear why on these two data", "mykey":1710},
 {"datasetID":45, "supportID":"A784F30E5815862D3F19B6DF2552F36548466623", "rexaID":"d64d2705cabed449e8cb2ecc3c3c77c54ee71051", "author":"Jeroen Eggermont and Joost N. Kok and Walter A. Kosters", "title":"Genetic Programming for data classification: partitioning the search space", "venue":"SAC", "year":"2004", "window":"14.3 8.5 9.1 We performed 10 independent runs for our two gp algorithms to obtain the results presented in Table 4. When available from the literature the results of C4.5 are reported. For three data sets (Australian credit, <b>Heart</b> disease and German credit) no results were reported for C4.5. In those three instances, marked with a #, we applied C4.5 to the data set ourselves. The best results for", "mykey":1711},
 {"datasetID":52, "supportID":"A784F30E5815862D3F19B6DF2552F36548466623", "rexaID":"d64d2705cabed449e8cb2ecc3c3c77c54ee71051", "author":"Jeroen Eggermont and Joost N. Kok and Walter A. Kosters", "title":"Genetic Programming for data classification: partitioning the search space", "venue":"SAC", "year":"2004", "window":"records attributes classes Australian credit (statlog) 690 14 2 German credit (statlog) 1000 23 2 Pima Indians diabetes 768 8 2 Heart disease (statlog) 270 13 2 <b>Ionosphere</b> 351 34 2 total data set is divided into n parts. Each part is chosen once as the test set while the other ncross-validation. We will mention the results of C4.5 as reported by Freund and Shapire [4], in order to compare", "mykey":1712},
 {"datasetID":79, "supportID":"A784F30E5815862D3F19B6DF2552F36548466623", "rexaID":"d64d2705cabed449e8cb2ecc3c3c77c54ee71051", "author":"Jeroen Eggermont and Joost N. Kok and Walter A. Kosters", "title":"Genetic Programming for data classification: partitioning the search space", "venue":"SAC", "year":"2004", "window":"our refined gp algorithm using the gain criterion performs a lot better than our simple gp algorithm and C4.5. If we look at the other two data set sets (German Credit and <b>Pima</b> <b>Indians</b> <b>Diabetes</b> , we see these are also the data sets on which our simple gp performs better than or close to C4.5. It is at the moment unclear why on these two data", "mykey":1713},
 {"datasetID":143, "supportID":"A784F30E5815862D3F19B6DF2552F36548466623", "rexaID":"d64d2705cabed449e8cb2ecc3c3c77c54ee71051", "author":"Jeroen Eggermont and Joost N. Kok and Walter A. Kosters", "title":"Genetic Programming for data classification: partitioning the search space", "venue":"SAC", "year":"2004", "window":"used in the experiments data set records attributes classes <b>Australian credit</b> (statlog) 690 14 2 German credit (statlog) 1000 23 2 Pima Indians diabetes 768 8 2 Heart disease (statlog) 270 13 2 Ionosphere 351 34 2 total data set is", "mykey":1714},
 {"datasetID":144, "supportID":"A784F30E5815862D3F19B6DF2552F36548466623", "rexaID":"d64d2705cabed449e8cb2ecc3c3c77c54ee71051", "author":"Jeroen Eggermont and Joost N. Kok and Walter A. Kosters", "title":"Genetic Programming for data classification: partitioning the search space", "venue":"SAC", "year":"2004", "window":"our refined gp algorithm using the gain criterion performs a lot better than our simple gp algorithm and C4.5. If we look at the other two data set sets  <b>German Credit</b> and Pima Indians Diabetes), we see these are also the data sets on which our simple gp performs better than or close to C4.5. It is at the moment unclear why on these two data", "mykey":1715},
 {"datasetID":98, "supportID":"A784F30E5815862D3F19B6DF2552F36548466623", "rexaID":"d64d2705cabed449e8cb2ecc3c3c77c54ee71051", "author":"Jeroen Eggermont and Joost N. Kok and Walter A. Kosters", "title":"Genetic Programming for data classification: partitioning the search space", "venue":"SAC", "year":"2004", "window":"records attributes classes Australian credit  <b>statlog</b>  690 14 2 German credit (statlog) 1000 23 2 Pima Indians diabetes 768 8 2 Heart disease (statlog) 270 13 2 Ionosphere 351 34 2 total data set is divided into n parts. Each part is chosen once as the test set while the other ncross-validation. We will mention the results of C4.5 as reported by Freund and Shapire [4], in order to compare", "mykey":1716},
 {"datasetID":53, "supportID":"A7AE71206FBB3DCF5718BF6B8CAD9C2E208E6289", "rexaID":"fdc570273fa15597fa6f45ce2a2d2129f56e2c56", "author":"Asa Ben-Hur and David Horn and Hava T. Siegelmann and Vladimir Vapnik", "title":"A Support Vector Method for Hierarchical Clustering", "venue":"Faculty of IE and Management Technion", "year":"", "window":"cost of a decrease in efficiency, which makes our algorithm useful even for very large data-sets. To compare the performance of our algorithm with other hierarchical algorithms we ran it on the <b>Iris</b> data set [15], which is a standard benchmark in the pattern recognition literature. It can be obtained from the UCI repository [16]. The data set contains 150 instances each containing four measurements of", "mykey":1717},
 {"datasetID":90, "supportID":"A7B4B2BC9F5EA5E387DCDE8239C7E68DCC8E87F4", "rexaID":"3a36418db6efd6a24b911127405da46f5a2dd8a9", "author":"Subramani Mani and Marco Porta and Suzanne McDermott", "title":"Building Bayesian Network Models in Medicine: the MENTOR Experience", "venue":"Center for Biomedical Informatics University of Pittsburgh", "year":"2002", "window":"Our validation tests using LED, ALARM and <b>SOYBEAN</b> which are small to large artificial datasets used for Machine Learning research and available from the University of California at the Irvine Machine Learning repository [MuAh94] gave a mean accuracy of 80% over ten runs. The range was from", "mykey":1718},
 {"datasetID":91, "supportID":"A7B4B2BC9F5EA5E387DCDE8239C7E68DCC8E87F4", "rexaID":"3a36418db6efd6a24b911127405da46f5a2dd8a9", "author":"Subramani Mani and Marco Porta and Suzanne McDermott", "title":"Building Bayesian Network Models in Medicine: the MENTOR Experience", "venue":"Center for Biomedical Informatics University of Pittsburgh", "year":"2002", "window":"Our validation tests using LED, ALARM and <b>SOYBEAN</b> which are small to large artificial datasets used for Machine Learning research and available from the University of California at the Irvine Machine Learning repository [MuAh94] gave a mean accuracy of 80% over ten runs. The range was from", "mykey":1719},
 {"datasetID":19, "supportID":"A7B9ECDA37828776A14147E3F7B137D41E2E71A3", "rexaID":"80d25f1152aed1f55fc47bd0f450312debe7617e", "author":"Jos'e L. Balc'azar", "title":"Rules with Bounded Negations and the Coverage Inference Scheme", "venue":"Dept. LSI, UPC", "year":"", "window":"is analyzed and proved to have reasonable algebraic properties; and examples are given where, from a single rule, many other rules can be found through this scheme. 3. Experiments The <b>Car</b> Evaluation Dataset [BR] in the UCI repository [BM] has seven categorical columns (six attributes and a class), which we preprocessed into a single boolean attribute per value of each categorical attribute, thus", "mykey":1720},
 {"datasetID":30, "supportID":"A7B9ECDA37828776A14147E3F7B137D41E2E71A3", "rexaID":"80d25f1152aed1f55fc47bd0f450312debe7617e", "author":"Jos'e L. Balc'azar", "title":"Rules with Bounded Negations and the Coverage Inference Scheme", "venue":"Dept. LSI, UPC", "year":"", "window":"a car labelled ``acceptable'' for five or more persons must not have low safety. Both rules hold at a low support close to the threshold but very high confidence. The <b>Contraceptive</b> Method Choice dataset [[LLS]] has 1473 instances with 10 attributes each, one of them being the class. The data comes from a national survey in Indonesia, and consists of sociodemographical and personal information about", "mykey":1721},
 {"datasetID":73, "supportID":"A7B9ECDA37828776A14147E3F7B137D41E2E71A3", "rexaID":"80d25f1152aed1f55fc47bd0f450312debe7617e", "author":"Jos'e L. Balc'azar", "title":"Rules with Bounded Negations and the Coverage Inference Scheme", "venue":"Dept. LSI, UPC", "year":"", "window":"being able to generate exactly the same set of rules that would be found at the chosen support and confidence. We describe next such a notion of cover, due to [CS], who applied it to a synthetic dataset and to the <b>Mushroom</b> database. We describe the results of employing this cover strategy on the databases Car (which is close to synthetic) and Contraceptive Method Choice, with real-world data coming", "mykey":1722},
 {"datasetID":14, "supportID":"A7DB9035146BC452B4E076775FAA495213AD48D6", "rexaID":"ab6823c2bce78649401e56bfb2d46bb279535879", "author":"Chiranjib Bhattacharyya", "title":"Robust Classification of noisy data using Second Order Cone Programming approach", "venue":"Dept. Computer Science and Automation, Indian Institute of Science", "year":"", "window":"website[9]. Ionosphere, sonar and wiconsin <b>breast</b> <b>cancer</b> were the three different datasets. The ionosphere dataset contains 34 dimensional observations, which are obtained from radar signals, while the sonar dataset contains 60 dimensional observation vectors. The wisconsin dataset", "mykey":1723},
 {"datasetID":151, "supportID":"A7DB9035146BC452B4E076775FAA495213AD48D6", "rexaID":"ab6823c2bce78649401e56bfb2d46bb279535879", "author":"Chiranjib Bhattacharyya", "title":"Robust Classification of noisy data using Second Order Cone Programming approach", "venue":"Dept. Computer Science and Automation, Indian Institute of Science", "year":"", "window":"downloaded from UCI machine learning dataset website[9]. Ionosphere, <b>sonar</b> and wiconsin breast cancer were the three different datasets. The ionosphere dataset contains 34 dimensional observations, which are obtained from radar signals, while", "mykey":1724},
 {"datasetID":52, "supportID":"A7DB9035146BC452B4E076775FAA495213AD48D6", "rexaID":"ab6823c2bce78649401e56bfb2d46bb279535879", "author":"Chiranjib Bhattacharyya", "title":"Robust Classification of noisy data using Second Order Cone Programming approach", "venue":"Dept. Computer Science and Automation, Indian Institute of Science", "year":"", "window":"downloaded from UCI machine learning dataset website[9]. <b>Ionosphere</b>  sonar and wiconsin breast cancer were the three different datasets. The ionosphere dataset contains 34 dimensional observations, which are obtained from radar signals, while", "mykey":1725},
 {"datasetID":53, "supportID":"A7F8167058ACAB55A99F3DEDF3CAA4F647DBCFA9", "rexaID":"bcac678940e7598fd6e6b5d1fc9cb5a2895c05f6", "author":"Rudy Setiono and Huan Liu", "title":"Fragmentation Problem and Automated Feature Construction", "venue":"School of Computing National University of Singapore", "year":"", "window":"[21] which has 9 binary features x 1 ; x 2 ; : : : ; x 9 . The 512 instances are labeled as follows: (a) Class 1: x 1 x 2 x 3 + x 1 x 2 + x 7 x 8 x 9 + x 7 x 9 , (b) Class 2: Otherwise. ffl <b>Iris</b> dataset [6] which has 150 instances described by 4 continuous attributes: sepal length (A 1 ), sepal width (A 2 ), petal length (A 3 ), and petal width (A 4 ). Each pattern belongs to one of the 3 possible", "mykey":1726},
 {"datasetID":52, "supportID":"A84DE556662B1E538D65BFEC06B2D335AC46CD65", "rexaID":"3cb3fbd5512e3cd12111b598fece53fcb42c484b", "author":"Christos Dimitrakakis and Samy Bengioy", "title":"Online Policy Adaptation for Ensemble Classifiers", "venue":"IDIAP", "year":"", "window":"0.5 0.6 0.7 0.8 0.9 1 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1 1 expert 2 experts 4 experts 8 experts 16 experts 32 experts Figure 1: Cumulative margin distribution for RL on the <b>ionosphere</b> dataset, with an increasing number of experts. MLP Boost MOE RL 7.28% 1.21% 4.84% 2.43% 32.8% 16.2% 31.6% 29.1% 15.0% 16.2% 13.7% 15.0% 5.96% 5.96% 5.96% 3.08% 4.10% 2.52% 4.55% 3.73% 2.63% 1.42% 2.13% 2.3%", "mykey":1727},
 {"datasetID":94, "supportID":"A84DE556662B1E538D65BFEC06B2D335AC46CD65", "rexaID":"3cb3fbd5512e3cd12111b598fece53fcb42c484b", "author":"Christos Dimitrakakis and Samy Bengioy", "title":"Online Policy Adaptation for Ensemble Classifiers", "venue":"IDIAP", "year":"", "window":"2.72% 3.10% 2.80% 2.69% 8.33% 6.48% 7.75% 7.41% 56.1% 61.9% 68.1% 48.3% Table 1: Classification error on the UCI breast, forest, heart, ionosphere, letter, optdigits, pendigits, <b>spambase</b> and vowel datasets using 32 experts. 7 times out of 9 respectively. For each dataset we have also calculated the cumulative margin distribution resulting from equation (1). For the RL mixture there was a constant", "mykey":1728},
 {"datasetID":74, "supportID":"A8E1FF05E34912E04613670AD6B98E75EC4BF9CF", "rexaID":"476d22372039041e04dd57d8eb40a2666f04fb50", "author":"Qingping Tao and Stephen Scott and N. V. Vinodchandran and Thomas T. Osugi", "title":"SVM-based generalized multiple-instance learning via approximate box counting", "venue":"ICML", "year":"2004", "window":"results of our new kernel on applications such as content-based image retrieval, prediction of drug affinity to bind to multiple sites simultaneously, protein sequence identification, and the <b>Musk</b> data sets. Finally, we conclude in Section 7. 2. Notation and Definitions Let X denote {0, . . . , s} d (though our results trivially generalize to X = Q d i=1 {0, . . . , s i }). Let BX denote the set of", "mykey":1729},
 {"datasetID":75, "supportID":"A8E1FF05E34912E04613670AD6B98E75EC4BF9CF", "rexaID":"476d22372039041e04dd57d8eb40a2666f04fb50", "author":"Qingping Tao and Stephen Scott and N. V. Vinodchandran and Thomas T. Osugi", "title":"SVM-based generalized multiple-instance learning via approximate box counting", "venue":"ICML", "year":"2004", "window":"results of our new kernel on applications such as content-based image retrieval, prediction of drug affinity to bind to multiple sites simultaneously, protein sequence identification, and the <b>Musk</b> data sets. Finally, we conclude in Section 7. 2. Notation and Definitions Let X denote {0, . . . , s} d (though our results trivially generalize to X = Q d i=1 {0, . . . , s i }). Let BX denote the set of", "mykey":1730},
 {"datasetID":154, "supportID":"A8E1FF05E34912E04613670AD6B98E75EC4BF9CF", "rexaID":"476d22372039041e04dd57d8eb40a2666f04fb50", "author":"Qingping Tao and Stephen Scott and N. V. Vinodchandran and Thomas T. Osugi", "title":"SVM-based generalized multiple-instance learning via approximate box counting", "venue":"ICML", "year":"2004", "window":"results of our new kernel on applications such as content-based image retrieval, prediction of drug affinity to bind to multiple sites simultaneously, <b>protein</b> sequence identification, and the Musk data sets. Finally, we conclude in Section 7. 2. Notation and Definitions Let X denote {0, . . . , s} d (though our results trivially generalize to X = Q d i=1 {0, . . . , s i }). Let BX denote the set of", "mykey":1731},
 {"datasetID":42, "supportID":"A91062CAA36819ED58BC819A3D9D76EF6C7F542C", "rexaID":"6f2537b9354cfbd865cc2057f04a6216cbafd89c", "author":"Ethem Alpaydin", "title":"Voting over Multiple Condensed Nearest Neighbors", "venue":"Artif. Intell. Rev, 11", "year":"1997", "window":"do not contribute much. Whether an additional subset pays off the additional complexity and memory is a trade-off that needs to be resolved depending on the particular application at hand. In three datasets, VOWEL, THYROID, and <b>GLASS</b>  we do not seem to gain anything by voting. The VOWEL database defines a quite difficult problem and is very noisy; the optimal k is 7. The result with 7-NN is the", "mykey":1732},
 {"datasetID":53, "supportID":"A91062CAA36819ED58BC819A3D9D76EF6C7F542C", "rexaID":"6f2537b9354cfbd865cc2057f04a6216cbafd89c", "author":"Ethem Alpaydin", "title":"Voting over Multiple Condensed Nearest Neighbors", "venue":"Artif. Intell. Rev, 11", "year":"1997", "window":"accuracy goes higher but the variance also decreases. This indicates better generalization and is the clear advantage of voting. Complete results are given in Table 4. Results for the <b>IRIS</b> and WINE datasets are similar and are omitted. When one increases the number of voting subsets, after a certain number, new subsets do not contribute much. Whether an additional subset pays off the additional", "mykey":1733},
 {"datasetID":102, "supportID":"A91062CAA36819ED58BC819A3D9D76EF6C7F542C", "rexaID":"6f2537b9354cfbd865cc2057f04a6216cbafd89c", "author":"Ethem Alpaydin", "title":"Voting over Multiple Condensed Nearest Neighbors", "venue":"Artif. Intell. Rev, 11", "year":"1997", "window":"are given in Table 2. OCR is a handwritten digit database (Guyon et al., 1989). Others are available from the UCI Repository (Murphy, 1994). In the OCR, VOWEL, and <b>THYROID</b> datasets, the training and test sets are separated. In others, we chose the training set small for not to have too large accuracy with NN thus leaving space for improvement. Euclidean distance is used as", "mykey":1734},
 {"datasetID":109, "supportID":"A91062CAA36819ED58BC819A3D9D76EF6C7F542C", "rexaID":"6f2537b9354cfbd865cc2057f04a6216cbafd89c", "author":"Ethem Alpaydin", "title":"Voting over Multiple Condensed Nearest Neighbors", "venue":"Artif. Intell. Rev, 11", "year":"1997", "window":"accuracy goes higher but the variance also decreases. This indicates better generalization and is the clear advantage of voting. Complete results are given in Table 4. Results for the IRIS and <b>WINE</b> datasets are similar and are omitted. When one increases the number of voting subsets, after a certain number, new subsets do not contribute much. Whether an additional subset pays off the additional", "mykey":1735},
 {"datasetID":2, "supportID":"A9581E5B9CDB67A124DACC66C2C98C723A9F67E3", "rexaID":"1383360e8bb2cfd7a98219f867869a9f6d7e0db0", "author":"Wei-Chun Kao and Kai-Min Chung and Lucas Assun and Chih-Jen Lin", "title":"Decomposition Methods for Linear Support Vector Machines", "venue":"Neural Computation, 16", "year":"2004", "window":"used are in Table 3.2. The four small problems are from the statlog collection (Michie, Spiegelhalter, and Taylor 1994). The problem <b>adult</b> is compiled by Platt (1998) from the UCI \"adult\" data set (Blake and Merz 1998). Problem web is also from Platt. Problem ijcnn is from the first problem of IJCNN challenge 2001 (Prokhorov 2001). Note that we use the winner's transformation of the raw data", "mykey":1736},
 {"datasetID":67, "supportID":"A9581E5B9CDB67A124DACC66C2C98C723A9F67E3", "rexaID":"1383360e8bb2cfd7a98219f867869a9f6d7e0db0", "author":"Wei-Chun Kao and Kai-Min Chung and Lucas Assun and Chih-Jen Lin", "title":"Decomposition Methods for Linear Support Vector Machines", "venue":"Neural Computation, 16", "year":"2004", "window":"in (Keerthi and Lin 2003), due to the di\u00b1culty on solving linear SVMs, Algorithm 1 is only tested on small two-class problems. Here, we would like to evaluate this algorithm on large multi-class data sets. We consider problems <b>dna</b>  satimage, letter, and shuttle, which were originally from the statlog collection (Michie, Spiegelhalter, and Taylor 1994) and were used in (Hsu and Lin 2002a). Except", "mykey":1737},
 {"datasetID":98, "supportID":"A9581E5B9CDB67A124DACC66C2C98C723A9F67E3", "rexaID":"1383360e8bb2cfd7a98219f867869a9f6d7e0db0", "author":"Wei-Chun Kao and Kai-Min Chung and Lucas Assun and Chih-Jen Lin", "title":"Decomposition Methods for Linear Support Vector Machines", "venue":"Neural Computation, 16", "year":"2004", "window":"D. J. Spiegelhalter, and C. C. Taylor (1994). Machine Learning, Neural and Statistical Classification. Englewood Cli\u00aes, N.J.: Prentice Hall. Data available at http://www.ncc.up.pt/liacc/ML <b>statlog</b> datasets.html. Osuna, E., R. Freund, and F. Girosi (1997). Training support vector machines: An application to face detection. In Proceedings of CVPR'97, New York, NY, pp. 130--136. IEEE. Platt, J. C.", "mykey":1738},
 {"datasetID":48, "supportID":"A98B150AF00D3C2E6EB4DB595206448BF8975A80", "rexaID":"3ef07572a3c1cda9a7f2e4243ad683964cf7a084", "author":"H. Altay Guvenir and Ilhan Uysal", "title":"Regression on feature projections", "venue":"a Department of Computer Engineering, Bilkent University", "year":"1999", "window":"q i 2 ^ t q i jj 13 In order to compare the RFP algorithm with KNN and Rules learning algorithms, we used abalone, auto-mpg, buying, country, cpu, electric, \u00afare, <b>housing</b>  read and servo real world datasets for function approximation (available at http://funapp.cs.bilkent.edu.tr [11]). The information about the number of instances, number and type of features and presence of missing values are", "mykey":1739},
 {"datasetID":87, "supportID":"A98B150AF00D3C2E6EB4DB595206448BF8975A80", "rexaID":"3ef07572a3c1cda9a7f2e4243ad683964cf7a084", "author":"H. Altay Guvenir and Ilhan Uysal", "title":"Regression on feature projections", "venue":"a Department of Computer Engineering, Bilkent University", "year":"1999", "window":"q i 2 ^ t q i jj 13 In order to compare the RFP algorithm with KNN and Rules learning algorithms, we used abalone, auto-mpg, buying, country, cpu, electric, \u00afare, housing, read and <b>servo</b> real world datasets for function approximation (available at http://funapp.cs.bilkent.edu.tr [11]). The information about the number of instances, number and type of features and presence of missing values are", "mykey":1740},
 {"datasetID":7, "supportID":"AA2FF28E197CF43670227D2525E174BFB358F79B", "rexaID":"b6069e488f10098d50e684d245a5574aa0c50c58", "author":"D. Randall Wilson and Roel Martinez", "title":"Improved Center Point Selection for Probabilistic Neural Networks", "venue":"Proceedings of the International Conference on Artificial Neural Networks and Genetic Algorithms", "year":"", "window":"reduction in size can be even more dramatic when there are more instances available. This is especially true when the number of instances is large compared to the complexity of the decision surface. Dataset Anneal <b>Audiology</b> Australian Breast Cancer (WI) Bridges Crx Echocardiogram Flag Heart (Hungarian) Heart (More) Heart Heart (Swiss) Hepatitis Horse-Colic Iris Liver-Bupa Pima-Indians-Diabetes", "mykey":1741},
 {"datasetID":8, "supportID":"AA2FF28E197CF43670227D2525E174BFB358F79B", "rexaID":"b6069e488f10098d50e684d245a5574aa0c50c58", "author":"D. Randall Wilson and Roel Martinez", "title":"Improved Center Point Selection for Probabilistic Neural Networks", "venue":"Proceedings of the International Conference on Artificial Neural Networks and Genetic Algorithms", "year":"", "window":"reduction in size can be even more dramatic when there are more instances available. This is especially true when the number of instances is large compared to the complexity of the decision surface. Dataset Anneal <b>Audiology</b> Australian Breast Cancer (WI) Bridges Crx Echocardiogram Flag Heart (Hungarian) Heart (More) Heart Heart (Swiss) Hepatitis Horse-Colic Iris Liver-Bupa Pima-Indians-Diabetes", "mykey":1742},
 {"datasetID":14, "supportID":"AA2FF28E197CF43670227D2525E174BFB358F79B", "rexaID":"b6069e488f10098d50e684d245a5574aa0c50c58", "author":"D. Randall Wilson and Roel Martinez", "title":"Improved Center Point Selection for Probabilistic Neural Networks", "venue":"Proceedings of the International Conference on Artificial Neural Networks and Genetic Algorithms", "year":"", "window":"reduction in size can be even more dramatic when there are more instances available. This is especially true when the number of instances is large compared to the complexity of the decision surface. Dataset Anneal Audiology Australian <b>Breast</b> <b>Cancer</b> (WI) Bridges Crx Echocardiogram Flag Heart (Hungarian) Heart (More) Heart Heart (Swiss) Hepatitis Horse-Colic Iris Liver-Bupa Pima-Indians-Diabetes", "mykey":1743},
 {"datasetID":38, "supportID":"AA2FF28E197CF43670227D2525E174BFB358F79B", "rexaID":"b6069e488f10098d50e684d245a5574aa0c50c58", "author":"D. Randall Wilson and Roel Martinez", "title":"Improved Center Point Selection for Probabilistic Neural Networks", "venue":"Proceedings of the International Conference on Artificial Neural Networks and Genetic Algorithms", "year":"", "window":"it retained almost two-thirds of the instances while suffering a large drop in accuracy compared to the other two models. However, in the <b>Echocardiogram</b> dataset, the RPNN used only 9% of the data while improving generalization accuracy by over 12%. Future research will focus on identifying characteristics of applications that help determine whether the RPNN", "mykey":1744},
 {"datasetID":74, "supportID":"AAFE3B91FCBE0DB5491BFB4F62FE3A788ACC316E", "rexaID":"39b9817d095c310d94387745c74f7d429c8ec204", "author":"Zhi-Hua Zhou and Hua Zhou", "title":"Multi-Instance Learning: A Survey", "venue":"National Laboratory for Novel Software Technology", "year":"", "window":"and relational learning. 24 7 Discussion The most serious problem encumbering the advance of multi-instance learning is that there is only one popularly used real-world benchmark data, i.e. the <b>Musk</b> data sets. Although some application data have been used in some works, they can hardly act as benchmarks for some reasons. For example, the COREL image database has been used by Maron and Ratan [18], Yang", "mykey":1745},
 {"datasetID":75, "supportID":"AAFE3B91FCBE0DB5491BFB4F62FE3A788ACC316E", "rexaID":"39b9817d095c310d94387745c74f7d429c8ec204", "author":"Zhi-Hua Zhou and Hua Zhou", "title":"Multi-Instance Learning: A Survey", "venue":"National Laboratory for Novel Software Technology", "year":"", "window":"and relational learning. 24 7 Discussion The most serious problem encumbering the advance of multi-instance learning is that there is only one popularly used real-world benchmark data, i.e. the <b>Musk</b> data sets. Although some application data have been used in some works, they can hardly act as benchmarks for some reasons. For example, the COREL image database has been used by Maron and Ratan [18], Yang", "mykey":1746},
 {"datasetID":14, "supportID":"AB5D8810A96E501A8B34418CBC14ABC74D4BB28F", "rexaID":"ca3e1e0bf335a97cedb76be7b64610181e0f6684", "author":"Kristin P. Bennett and Ayhan Demiriz and Richard Maclin", "title":"Exploiting unlabeled data in ensemble methods", "venue":"KDD", "year":"2002", "window":"experiments we used simple multilayer perceptrons with a single layer of hidden units. The networks were trained using backpropagation with a learning rate of 0.15 and a momentum value of 0.90. The datasets for the experiments are <b>breast</b> <b>cancer</b> wisconsin, pima-indians diabetes, and letter-recognition drawn from the UCI Machine Learning repository [3]. The number of units in the hidden layer for the", "mykey":1747},
 {"datasetID":17, "supportID":"AB5D8810A96E501A8B34418CBC14ABC74D4BB28F", "rexaID":"ca3e1e0bf335a97cedb76be7b64610181e0f6684", "author":"Kristin P. Bennett and Ayhan Demiriz and Richard Maclin", "title":"Exploiting unlabeled data in ensemble methods", "venue":"KDD", "year":"2002", "window":"experiments we used simple multilayer perceptrons with a single layer of hidden units. The networks were trained using backpropagation with a learning rate of 0.15 and a momentum value of 0.90. The datasets for the experiments are <b>breast</b> <b>cancer</b> <b>wisconsin</b>  pima-indians diabetes, and letter-recognition drawn from the UCI Machine Learning repository [3]. The number of units in the hidden layer for the", "mykey":1748},
 {"datasetID":15, "supportID":"AB5D8810A96E501A8B34418CBC14ABC74D4BB28F", "rexaID":"ca3e1e0bf335a97cedb76be7b64610181e0f6684", "author":"Kristin P. Bennett and Ayhan Demiriz and Richard Maclin", "title":"Exploiting unlabeled data in ensemble methods", "venue":"KDD", "year":"2002", "window":"experiments we used simple multilayer perceptrons with a single layer of hidden units. The networks were trained using backpropagation with a learning rate of 0.15 and a momentum value of 0.90. The datasets for the experiments are <b>breast</b> <b>cancer</b> <b>wisconsin</b>  pima-indians diabetes, and letter-recognition drawn from the UCI Machine Learning repository [3]. The number of units in the hidden layer for the", "mykey":1749},
 {"datasetID":16, "supportID":"AB5D8810A96E501A8B34418CBC14ABC74D4BB28F", "rexaID":"ca3e1e0bf335a97cedb76be7b64610181e0f6684", "author":"Kristin P. Bennett and Ayhan Demiriz and Richard Maclin", "title":"Exploiting unlabeled data in ensemble methods", "venue":"KDD", "year":"2002", "window":"experiments we used simple multilayer perceptrons with a single layer of hidden units. The networks were trained using backpropagation with a learning rate of 0.15 and a momentum value of 0.90. The datasets for the experiments are <b>breast</b> <b>cancer</b> <b>wisconsin</b>  pima-indians diabetes, and letter-recognition drawn from the UCI Machine Learning repository [3]. The number of units in the hidden layer for the", "mykey":1750},
 {"datasetID":34, "supportID":"AB5D8810A96E501A8B34418CBC14ABC74D4BB28F", "rexaID":"ca3e1e0bf335a97cedb76be7b64610181e0f6684", "author":"Kristin P. Bennett and Ayhan Demiriz and Richard Maclin", "title":"Exploiting unlabeled data in ensemble methods", "venue":"KDD", "year":"2002", "window":"for the experiments are breast-cancer-wisconsin, pima-indians <b>diabetes</b>  and letter-recognition drawn from the UCI Machine Learning repository [3]. The number of units in the hidden layer for the datasets was 5 for the breast-cancer and diabetes datasets and 40 in the letter-recognition dataset. The number of training epochs was set to 20 for breastcancer, 30 for diabetes, and 30 for letter", "mykey":1751},
 {"datasetID":59, "supportID":"AB5D8810A96E501A8B34418CBC14ABC74D4BB28F", "rexaID":"ca3e1e0bf335a97cedb76be7b64610181e0f6684", "author":"Kristin P. Bennett and Ayhan Demiriz and Richard Maclin", "title":"Exploiting unlabeled data in ensemble methods", "venue":"KDD", "year":"2002", "window":"ufraction 0.10 superv ufraction 0.10 semi_sup ufraction 0.25 superv ufraction 0.25 semi_sup ufraction 0.5 superv ufraction 0.5 semi_sup Figure 1: Neural network results for the <b>Letter Recognition</b> dataset using networks with 5, 10 and 20 hidden units. Results shown are for 10, 25, and 50 percent of the data marked as unlabeled (ufractions of 0.10, 0.25, and 0.5) for AdaBoost (superv) and ASSEMBLE", "mykey":1752},
 {"datasetID":151, "supportID":"AB71A2A0545C8150289E459D642BB91771DC60D4", "rexaID":"7a13c34f809076592c23c1e2c9c6a9a72464d911", "author":"Ayhan Demiriz and Kristin P. Bennett and Mark J. Embrechts", "title":"A Genetic Algorithm Approach for Semi-Supervised Clustering", "venue":"E-Business Department, Verizon Inc.", "year":"2002", "window":"the transduction result (using both labeled and unlabeled data) cases. Note that on the center cluster transduction does work appropriately. The inductive 1 The (k, \u00af, \u00ae) values applied for each dataset were bright (15, 0.01,0.99), <b>sonar</b> (7,0.1,1), heart (7,0.25,0.75), ionosphere (7, 0.01,0.99), house (7,0.1,0.9), housing (11,0.01, 0.99), diagnostic (11,0.4,0.6), pima (11,0.01,0.99) and", "mykey":1753},
 {"datasetID":53, "supportID":"AB71A2A0545C8150289E459D642BB91771DC60D4", "rexaID":"7a13c34f809076592c23c1e2c9c6a9a72464d911", "author":"Ayhan Demiriz and Kristin P. Bennett and Mark J. Embrechts", "title":"A Genetic Algorithm Approach for Semi-Supervised Clustering", "venue":"E-Business Department, Verizon Inc.", "year":"2002", "window":"506 points), House Votes (16 variables, 435 points), Breast Cancer Diagnostic (30 variables, 569 points), Pima Diabetes ( 8 variables, 769 points), and <b>Iris</b> ( 4 variables, 150 points). The datasets have categorical dependent variables except Housing. The continuous dependent variable for this dataset was categorized at the level of 21.5. Iris is a three class problem. The other datasets are", "mykey":1754},
 {"datasetID":34, "supportID":"AB73B18C24486B8AB786ACE3D38A490E20D425EE", "rexaID":"0b088e37f14f4bd30ad6c550b543306c6363eaa7", "author":"Marina Skurichina and Robert P W Duin", "title":"Boosting in Linear Discriminant Analysis", "venue":"Multiple Classifier Systems", "year":"2000", "window":"(Data II) with 225 and 126 objects belonging to the first and the second data class, respectively. The second is the 8-dimensional <b>diabetes</b> data set (Data III) consisting of 500 and 268 objects from the first and the second data class, respectively. These two data sets were also used in [8], when studying bagging and boosting for decision trees.", "mykey":1755},
 {"datasetID":52, "supportID":"AB73B18C24486B8AB786ACE3D38A490E20D425EE", "rexaID":"0b088e37f14f4bd30ad6c550b543306c6363eaa7", "author":"Marina Skurichina and Robert P W Duin", "title":"Boosting in Linear Discriminant Analysis", "venue":"Multiple Classifier Systems", "year":"2000", "window":"are taken from the UCI Repository [14]. The first is the 34dimensional <b>ionosphere</b> data set (Data II) with 225 and 126 objects belonging to the first and the second data class, respectively. The second is the 8-dimensional diabetes data set (Data III) consisting of 500 and 268 objects from", "mykey":1756},
 {"datasetID":14, "supportID":"AB7BC67A4A55EE0FFAA6E5A0F6EC971E0C8E420E", "rexaID":"79b9012d7063a4c0e98d98ebd63d63044c8da997", "author":"W. Nick Street", "title":"A Neural Network Model for Prognostic Prediction", "venue":"ICML", "year":"1998", "window":"of the models to separate cases with favorable and unfavorable prognoses (see Section 3.3). 3 Experimental Results Computational experiments were performed on two very different <b>breast</b> <b>cancer</b> data sets. The first is known as Wisconsin Prognostic Breast Cancer (WPBC) and is characterized by a small number of cases, relatively high dimensionality, very precise values and almost no missing data. The", "mykey":1757},
 {"datasetID":17, "supportID":"AB7BC67A4A55EE0FFAA6E5A0F6EC971E0C8E420E", "rexaID":"79b9012d7063a4c0e98d98ebd63d63044c8da997", "author":"W. Nick Street", "title":"A Neural Network Model for Prognostic Prediction", "venue":"ICML", "year":"1998", "window":"of the models to separate cases with favorable and unfavorable prognoses (see Section 3.3). 3 Experimental Results Computational experiments were performed on two very different <b>breast</b> <b>cancer</b> data sets. The first is known as <b>Wisconsin</b> Prognostic Breast Cancer (WPBC) and is characterized by a small number of cases, relatively high dimensionality, very precise values and almost no missing data. The", "mykey":1758},
 {"datasetID":15, "supportID":"AB7BC67A4A55EE0FFAA6E5A0F6EC971E0C8E420E", "rexaID":"79b9012d7063a4c0e98d98ebd63d63044c8da997", "author":"W. Nick Street", "title":"A Neural Network Model for Prognostic Prediction", "venue":"ICML", "year":"1998", "window":"of the models to separate cases with favorable and unfavorable prognoses (see Section 3.3). 3 Experimental Results Computational experiments were performed on two very different <b>breast</b> <b>cancer</b> data sets. The first is known as <b>Wisconsin</b> Prognostic Breast Cancer (WPBC) and is characterized by a small number of cases, relatively high dimensionality, very precise values and almost no missing data. The", "mykey":1759},
 {"datasetID":16, "supportID":"AB7BC67A4A55EE0FFAA6E5A0F6EC971E0C8E420E", "rexaID":"79b9012d7063a4c0e98d98ebd63d63044c8da997", "author":"W. Nick Street", "title":"A Neural Network Model for Prognostic Prediction", "venue":"ICML", "year":"1998", "window":"of the models to separate cases with favorable and unfavorable prognoses (see Section 3.3). 3 Experimental Results Computational experiments were performed on two very different <b>breast</b> <b>cancer</b> data sets. The first is known as <b>Wisconsin</b> Prognostic Breast Cancer (WPBC) and is characterized by a small number of cases, relatively high dimensionality, very precise values and almost no missing data. The", "mykey":1760},
 {"datasetID":2, "supportID":"ABA0E12B6B3C3EF0B88DD575BBEF633AC2B35358", "rexaID":"4c8e8cf6857f1f1bc9b43679d241b096513ee6f2", "author":"Ron Kohavi and Barry G. Becker and Dan Sommerfield", "title":"Improving Simple Bayes", "venue":"Data Mining and Visualization Group Silicon Graphics, Inc", "year":"", "window":"especially larger ones, such as segment, mushroom, letter, and <b>adult</b> for a total of 37. The specific datasets are shown below in Table 2. Our main concern with estimating accuracy is that the estimate should be precise. Therefore, we ran different inducers on these datasets in two forms. If the dataset was", "mykey":1761},
 {"datasetID":67, "supportID":"ABA0E12B6B3C3EF0B88DD575BBEF633AC2B35358", "rexaID":"4c8e8cf6857f1f1bc9b43679d241b096513ee6f2", "author":"Ron Kohavi and Barry G. Becker and Dan Sommerfield", "title":"Improving Simple Bayes", "venue":"Data Mining and Visualization Group Silicon Graphics, Inc", "year":"", "window":"was large or artificial, indicating that a single test set would yield accurate estimates, we used a training-set/test-set as defined in the source for the dataset (e.g., Statlog defined the splits for <b>DNA</b>  letter, satimage; CART defined the training size for waveform and led24) or a 2/3, 1/3 split, and ran the inducer once; otherwise, we performed 10-fold", "mykey":1762},
 {"datasetID":73, "supportID":"ABA0E12B6B3C3EF0B88DD575BBEF633AC2B35358", "rexaID":"4c8e8cf6857f1f1bc9b43679d241b096513ee6f2", "author":"Ron Kohavi and Barry G. Becker and Dan Sommerfield", "title":"Improving Simple Bayes", "venue":"Data Mining and Visualization Group Silicon Graphics, Inc", "year":"", "window":"that had significant differences. We can see that frequency counts (Nomatches-0) performs generally worse than No-matches-PC, except on the cars and <b>mushroom</b> datasets where it performs significantly better. LaplaceGamma m seems to take the best of both worlds. It tracks No-matches-PC on most datasets, except cars and mushroom where it tracks No-matches-0 well.", "mykey":1763},
 {"datasetID":98, "supportID":"ABA0E12B6B3C3EF0B88DD575BBEF633AC2B35358", "rexaID":"4c8e8cf6857f1f1bc9b43679d241b096513ee6f2", "author":"Ron Kohavi and Barry G. Becker and Dan Sommerfield", "title":"Improving Simple Bayes", "venue":"Data Mining and Visualization Group Silicon Graphics, Inc", "year":"", "window":"was large or artificial, indicating that a single test set would yield accurate estimates, we used a training-set/test-set as defined in the source for the dataset (e.g., <b>Statlog</b> defined the splits for DNA, letter, satimage; CART defined the training size for waveform and led24) or a 2/3, 1/3 split, and ran the inducer once; otherwise, we performed 10-fold", "mykey":1764},
 {"datasetID":151, "supportID":"ABB6B8F33845E2E76689FD838F46BA7AFF8AF56A", "rexaID":"63fb13048c106214f91a999900e7efd5a3dcdef1", "author":"Chris Drummond and Robert C. Holte", "title":"Exploiting the Cost (In)sensitivity of Decision Tree Splitting Criteria", "venue":"ICML", "year":"2000", "window":"<b>sonar</b> a classifier with a normalized expected cost that is totally independent of misclassification costs and priors. Figure 15 shows cost curves for the different splitting criteria on the sonar data set. For a given splitting criterion the classifier corresponding to the intersection would be a horizontal line through the highest point on the cost curve. In all cases this cost-insensitive", "mykey":1765},
 {"datasetID":34, "supportID":"ABB6B8F33845E2E76689FD838F46BA7AFF8AF56A", "rexaID":"63fb13048c106214f91a999900e7efd5a3dcdef1", "author":"Chris Drummond and Robert C. Holte", "title":"Exploiting the Cost (In)sensitivity of Decision Tree Splitting Criteria", "venue":"ICML", "year":"2000", "window":"for DKM but is very dependent on the ratio for accuracy. Figure 7 shows the range of points generated by the middle eight of the twelve ratios using an unpruned decision tree on the <b>diabetes</b> data set. The limits of this range are indicated by the numbers. The dashed line is accuracy, points are well spread out across ROC space. For DKM the spread is much narrower, consistent with a low", "mykey":1766},
 {"datasetID":98, "supportID":"ABBD40EBE6410B2875C2F33A0142D0E94CA8A582", "rexaID":"b4f5fa80a6f5e06084e3e518c67de340a8a6aead", "author":"Wl/odzisl/aw Duch", "title":"Support Vector Neural Training", "venue":"Index Terms--", "year":"", "window":"has been re-analyzed with a number of methods available in the Ghostminer package [11]. Many other results for this dataset may be found in the <b>Statlog</b> book [13]. Best results (Table I) were achieved with the k-Nearest Neighbors classifier with small k (automatic selection using crossvalidation tests found optimal k=3),", "mykey":1767},
 {"datasetID":14, "supportID":"AC7E05133014FD03F82D4B2A37095F5344D00191", "rexaID":"4ce4c96181e2836dd80a71c2efebc7fb030c55d8", "author":"Baback Moghaddam and Gregory Shakhnarovich", "title":"Boosted Dyadic Kernel Discriminants", "venue":"NIPS", "year":"2002", "window":"the number of support vectors for the SVM, and #k.ev. the number of kernel evaluations required by a boosted hypercuts classifier. Means and standard deviations in 30 trials are reported for each data set. WBC,WPBC,WDBC are Wisconsin <b>Breast</b> <b>Cancer</b>  Prognosis and Diagnosis data sets, respectively. In each experiment, the data set was randomly partitioned into training, validation and test sets of", "mykey":1768},
 {"datasetID":17, "supportID":"AC7E05133014FD03F82D4B2A37095F5344D00191", "rexaID":"4ce4c96181e2836dd80a71c2efebc7fb030c55d8", "author":"Baback Moghaddam and Gregory Shakhnarovich", "title":"Boosted Dyadic Kernel Discriminants", "venue":"NIPS", "year":"2002", "window":"the number of support vectors for the SVM, and #k.ev. the number of kernel evaluations required by a boosted hypercuts classifier. Means and standard deviations in 30 trials are reported for each data set. WBC,WPBC,WDBC are <b>Wisconsin</b> <b>Breast</b> <b>Cancer</b>  Prognosis and Diagnosis data sets, respectively. In each experiment, the data set was randomly partitioned into training, validation and test sets of", "mykey":1769},
 {"datasetID":15, "supportID":"AC7E05133014FD03F82D4B2A37095F5344D00191", "rexaID":"4ce4c96181e2836dd80a71c2efebc7fb030c55d8", "author":"Baback Moghaddam and Gregory Shakhnarovich", "title":"Boosted Dyadic Kernel Discriminants", "venue":"NIPS", "year":"2002", "window":"the number of support vectors for the SVM, and #k.ev. the number of kernel evaluations required by a boosted hypercuts classifier. Means and standard deviations in 30 trials are reported for each data set. WBC,WPBC,WDBC are <b>Wisconsin</b> <b>Breast</b> <b>Cancer</b>  Prognosis and Diagnosis data sets, respectively. In each experiment, the data set was randomly partitioned into training, validation and test sets of", "mykey":1770},
 {"datasetID":16, "supportID":"AC7E05133014FD03F82D4B2A37095F5344D00191", "rexaID":"4ce4c96181e2836dd80a71c2efebc7fb030c55d8", "author":"Baback Moghaddam and Gregory Shakhnarovich", "title":"Boosted Dyadic Kernel Discriminants", "venue":"NIPS", "year":"2002", "window":"the number of support vectors for the SVM, and #k.ev. the number of kernel evaluations required by a boosted hypercuts classifier. Means and standard deviations in 30 trials are reported for each data set. WBC,WPBC,WDBC are <b>Wisconsin</b> <b>Breast</b> <b>Cancer</b>  Prognosis and Diagnosis data sets, respectively. In each experiment, the data set was randomly partitioned into training, validation and test sets of", "mykey":1771},
 {"datasetID":45, "supportID":"AC7E05133014FD03F82D4B2A37095F5344D00191", "rexaID":"4ce4c96181e2836dd80a71c2efebc7fb030c55d8", "author":"Baback Moghaddam and Gregory Shakhnarovich", "title":"Boosted Dyadic Kernel Discriminants", "venue":"NIPS", "year":"2002", "window":"and k-Nearest Neighbor (k-NN). We chose sets large enough for reasonable training/validation/test partitioning, and that represent binary (or easily converted to binary) classification problems. Dataset N d k-NN SVM #SV Hypercuts #k.ev. <b>Heart</b> 90 13 .196 #.042 .202 #.038 62 #10 .202 #.030 50 #12 Ionosphere 120 34 .168 #.024 .064 #.018 73 #7 .083 #.022 63 #7 WBC 200 9 .034 #.011 .032 #.008 50 #26", "mykey":1772},
 {"datasetID":42, "supportID":"ACA6B8A99FA95E18808D75A357D999F747ED38EC", "rexaID":"b154b00ca44ffdbc56ab144aa90deab78e1f0de3", "author":"Jitender S. Deogun and Vijay V. Raghavan and Hayri Sever", "title":"Exploiting Upper Approximation in the Rough Set Methodology", "venue":"KDD", "year":"1995", "window":"does not match to known concepts we use 5NNR classification scheme with Euclidean distance function to determine the closest known concept. The difference between two values of an attribute are data set Size Attr. Training Test 1. <b>Glass</b> 9 66 148 2. Breast cancer 9 211 488 3. Parity 5+10 15 226 524 4. Iris 4 45 105 5. Monk 1 6 124 432 6. Monk 2 6 169 432 7. Monk 3 6 122 432 8. Vote 16 132 303 9.", "mykey":1773},
 {"datasetID":90, "supportID":"ACA6B8A99FA95E18808D75A357D999F747ED38EC", "rexaID":"b154b00ca44ffdbc56ab144aa90deab78e1f0de3", "author":"Jitender S. Deogun and Vijay V. Raghavan and Hayri Sever", "title":"Exploiting Upper Approximation in the Rough Set Methodology", "venue":"KDD", "year":"1995", "window":"that SBS + UC relatively performed much worse than UC was small <b>soybean</b> data set. When we continued our experiment on this data set with different `-reduct that was 20th and 21st features of soybean's training set, we obtained accuracy of 96.1%, which was much better than that", "mykey":1774},
 {"datasetID":91, "supportID":"ACA6B8A99FA95E18808D75A357D999F747ED38EC", "rexaID":"b154b00ca44ffdbc56ab144aa90deab78e1f0de3", "author":"Jitender S. Deogun and Vijay V. Raghavan and Hayri Sever", "title":"Exploiting Upper Approximation in the Rough Set Methodology", "venue":"KDD", "year":"1995", "window":"that SBS + UC relatively performed much worse than UC was small <b>soybean</b> data set. When we continued our experiment on this data set with different `-reduct that was 20th and 21st features of soybean's training set, we obtained accuracy of 96.1%, which was much better than that", "mykey":1775},
 {"datasetID":101, "supportID":"AD47527C92AFE3025C157D5436379755D51E8020", "rexaID":"4c690aed8465fe41ec10e225d9de6dbaeafedc2a", "author":"Michael Bain", "title":"Structured Features from Concept Lattices for Unsupervised Learning and Classification", "venue":"Australian Joint Conference on Artificial Intelligence", "year":"2002", "window":"for problems of constructive induction (e.g. [11]), the <b>tic-tac-toe</b> data set from the UCI repository. The problem is to predict whether each of 958 legal endgame boards for tic-tac-toe is won for `x'. The results are shown in Table 3. Accuracy (%) Tree size Attributes", "mykey":1776},
 {"datasetID":111, "supportID":"AD47527C92AFE3025C157D5436379755D51E8020", "rexaID":"4c690aed8465fe41ec10e225d9de6dbaeafedc2a", "author":"Michael Bain", "title":"Structured Features from Concept Lattices for Unsupervised Learning and Classification", "venue":"Australian Joint Conference on Artificial Intelligence", "year":"2002", "window":"pre-conditions of any operator and (c) it constructs a (conjunctive) definition as a single definite clause. In a first experiment we applied Conduce to a task of unsupervised learning. The  <b>zoo</b>  data set contains 101 instances. Each is described using 17 attributes and a unique name, such as aardvark, ostrich, seasnake, wasp, etc. It is an artificial data set and is not supposed to be taxonomically", "mykey":1777},
 {"datasetID":42, "supportID":"AE2071BC88FAA6BF9590282025159B0A6424ABC0", "rexaID":"17b1afb4bd706435fa92313e1e85a5d6009a42f4", "author":"Ping Zhong and Masao Fukushima", "title":"Second Order Cone Programming Formulations for Robust Multi-class Classification", "venue":"", "year":"", "window":"problem as follows: max \u00ae,\u00be,\u00bf e T \u00ae- (\u00be + \u00bf) s.t. \u00af E T \u00ae = 0, \u00ae \u00b7 (1 - \u00ba)e, (38) \u00be - \u00bf = \u00ba, \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 2 4 - 1 p 2(K+1) ~ A T \u00ae \u00bf 3 5 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b7 \u00be. Table 1: Description of Iris, Wine and <b>Glass</b> datasets. name dimension (N) #classes (K) #examples (L) Iris 4 3 150 Wine 13 3 178 Glass 9 6 214 14 Table 2: Results for Iris, Wine and Glass datasets with noise (\u00bd = 0.3, \u00b7 = 2, \u00ba = 0.05). R a Robust (I)", "mykey":1778},
 {"datasetID":53, "supportID":"AE2071BC88FAA6BF9590282025159B0A6424ABC0", "rexaID":"17b1afb4bd706435fa92313e1e85a5d6009a42f4", "author":"Ping Zhong and Masao Fukushima", "title":"Second Order Cone Programming Formulations for Robust Multi-class Classification", "venue":"", "year":"", "window":"problem as follows: max \u00ae,\u00be,\u00bf e T \u00ae- (\u00be + \u00bf) s.t. \u00af E T \u00ae = 0, \u00ae \u00b7 (1 - \u00ba)e, (38) \u00be - \u00bf = \u00ba, \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 2 4 - 1 p 2(K+1) ~ A T \u00ae \u00bf 3 5 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b7 \u00be. Table 1: Description of <b>Iris</b>  Wine and Glass datasets. name dimension (N) #classes (K) #examples (L) Iris 4 3 150 Wine 13 3 178 Glass 9 6 214 14 Table 2: Results for Iris, Wine and Glass datasets with noise (\u00bd = 0.3, \u00b7 = 2, \u00ba = 0.05). R a Robust (I)", "mykey":1779},
 {"datasetID":109, "supportID":"AE2071BC88FAA6BF9590282025159B0A6424ABC0", "rexaID":"17b1afb4bd706435fa92313e1e85a5d6009a42f4", "author":"Ping Zhong and Masao Fukushima", "title":"Second Order Cone Programming Formulations for Robust Multi-class Classification", "venue":"", "year":"", "window":"problem as follows: max \u00ae,\u00be,\u00bf e T \u00ae- (\u00be + \u00bf) s.t. \u00af E T \u00ae = 0, \u00ae \u00b7 (1 - \u00ba)e, (38) \u00be - \u00bf = \u00ba, \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 2 4 - 1 p 2(K+1) ~ A T \u00ae \u00bf 3 5 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b0 \u00b7 \u00be. Table 1: Description of Iris, <b>Wine</b> and Glass datasets. name dimension (N) #classes (K) #examples (L) Iris 4 3 150 Wine 13 3 178 Glass 9 6 214 14 Table 2: Results for Iris, Wine and Glass datasets with noise (\u00bd = 0.3, \u00b7 = 2, \u00ba = 0.05). R a Robust (I)", "mykey":1780},
 {"datasetID":121, "supportID":"AEBFCD088646B6ED3D0E4896C24086B57967E6A9", "rexaID":"c58fd4c0c5b8fefc00686150d5af26f6966807ef", "author":"Stephen D. Bay and Dennis F. Kibler and Michael J. Pazzani and Padhraic Smyth", "title":"The UCI KDD Archive of Large Data Sets for Data Mining Research and Experimentation", "venue":"SIGKDD Explorations, 2", "year":"2000", "window":"dealing with four different topic areas (music bands, bio-medical, goats, and sheep). Time series and Sequence data which consists of a consecutively ordered set of observations, such as the <b>EEG</b> data set in the archive. Time series measure changes in the value of a continuous variable such as stock prices or economic indicators whereas sequence data records an ordered set of categorical variables", "mykey":1781},
 {"datasetID":122, "supportID":"AEBFCD088646B6ED3D0E4896C24086B57967E6A9", "rexaID":"c58fd4c0c5b8fefc00686150d5af26f6966807ef", "author":"Stephen D. Bay and Dennis F. Kibler and Michael J. Pazzani and Padhraic Smyth", "title":"The UCI KDD Archive of Large Data Sets for Data Mining Research and Experimentation", "venue":"SIGKDD Explorations, 2", "year":"2000", "window":"table in more detail (i.e., the casts and people involved with making the movie). Spatial data which represents a set of observations located on a 2 or 3 dimensional grid. For example, the <b>El</b> <b>Nino</b> Dataset in the archive contains oceanographic and surface meteorological readings taken from a series of buoys positioned throughout the equatorial Pacific. Text Data such as webpages or newspaper articles.", "mykey":1782},
 {"datasetID":125, "supportID":"AEBFCD088646B6ED3D0E4896C24086B57967E6A9", "rexaID":"c58fd4c0c5b8fefc00686150d5af26f6966807ef", "author":"Stephen D. Bay and Dennis F. Kibler and Michael J. Pazzani and Padhraic Smyth", "title":"The UCI KDD Archive of Large Data Sets for Data Mining Research and Experimentation", "venue":"SIGKDD Explorations, 2", "year":"2000", "window":"currently in the archive. Classification: predict the value of a categorical target variable. For example, the <b>Insurance</b> <b>Benchmark</b> data set was used to predict which customers were interested in buying an insurance policy based on product usage data and demographic information. Regression: predict the value of a continuous target", "mykey":1783},
 {"datasetID":130, "supportID":"AEBFCD088646B6ED3D0E4896C24086B57967E6A9", "rexaID":"c58fd4c0c5b8fefc00686150d5af26f6966807ef", "author":"Stephen D. Bay and Dennis F. Kibler and Michael J. Pazzani and Padhraic Smyth", "title":"The UCI KDD Archive of Large Data Sets for Data Mining Research and Experimentation", "venue":"SIGKDD Explorations, 2", "year":"2000", "window":"Fan, Lee, Stolfo, and Miller [2] worked on the problem of reducing operational costs (the cost of running the system) for a real-time network intrusion detection system. They used the <b>KDD</b> <b>CUP</b> <b>1999</b> data set which is based on processed tcpdump files and includes intrusions such as probing, denial of service, illegal local access, and illegal root access. They developed multiple rule sets to identify", "mykey":1784},
 {"datasetID":140, "supportID":"AEBFCD088646B6ED3D0E4896C24086B57967E6A9", "rexaID":"c58fd4c0c5b8fefc00686150d5af26f6966807ef", "author":"Stephen D. Bay and Dennis F. Kibler and Michael J. Pazzani and Padhraic Smyth", "title":"The UCI KDD Archive of Large Data Sets for Data Mining Research and Experimentation", "venue":"SIGKDD Explorations, 2", "year":"2000", "window":"and surface meteorological readings taken from a series of buoys positioned throughout the equatorial Pacific. Text Data such as webpages or newspaper articles. For example, the <b>Syskill</b> and <b>Webert</b> data set contains webpages dealing with four different topic areas (music bands, bio-medical, goats, and sheep). Time series and Sequence data which consists of a consecutively ordered set of observations,", "mykey":1785},
 {"datasetID":2, "supportID":"AEF4B8E2DB45268409949AE1458C0F138A82C022", "rexaID":"81ff15994633ecdb2b9fd5a6d11d985c2b215442", "author":"Ramesh Natarajan and Edwin P D Pednault", "title":"Segmented Regression Estimators for Massive Data Sets", "venue":"SDM", "year":"2002", "window":"Figure 1. A comparison of the lift on the Fingerhut data for the \"Consolidated Payout Model\"using the LRT methodology (left), and for the \"Response Model\"using the NBT methodology (right). 6.3 <b>Adult</b> data set This is a standard data set from [4] with the 32561 training and 16281 test data and about 7% missing value records in the training data. The data has 6 continuous and 8 nominal features and the", "mykey":1786},
 {"datasetID":98, "supportID":"AEF4B8E2DB45268409949AE1458C0F138A82C022", "rexaID":"81ff15994633ecdb2b9fd5a6d11d985c2b215442", "author":"Ramesh Natarajan and Edwin P D Pednault", "title":"Segmented Regression Estimators for Massive Data Sets", "venue":"SDM", "year":"2002", "window":"Problems, SIAM, Philadelphia (1996). [4] C. Blake, E. Keogh and C. Merz, UCI repository of machine learning databases. (http://www.ics.uci.edu/ mlearn). [5] P. Brazdil and J. Gama, <b>statlog</b> project datasets, http://www.nccp.up.pt/liacc/ML/statlog. [6] L. Breiman, J. Friedman, R. Olshen and C. Stone, Classification and Regression Trees, Wadsworth, Belmont CA (1984). [7] G. H. Golub and C. F. Van Loan,", "mykey":1787},
 {"datasetID":7, "supportID":"AF087B9CEAFE5E48B31EEE610AD1B8A9213FD6EC", "rexaID":"4ccb84298ff6f0a62f8263c57259cc114cb1b328", "author":"Mohammed Waleed Kadous", "title":"Expanding the Scope of Concept Learning Using Metafeatures", "venue":"School of Computer Science and Engineering, University of New South Wales", "year":"", "window":"a custom learner works, but is labour-intensive. Relational learning techniques tend to be very sensitive to noise and to the particular clausal representation selected. They are typically 1 These datasets are: arrythmia, <b>audiology</b>  bach chorales, echocardiogram, isolet, mobile robots, waveform. unable to process large data sets in a reasonable time frame, and/or require the user to set limits on the", "mykey":1788},
 {"datasetID":8, "supportID":"AF087B9CEAFE5E48B31EEE610AD1B8A9213FD6EC", "rexaID":"4ccb84298ff6f0a62f8263c57259cc114cb1b328", "author":"Mohammed Waleed Kadous", "title":"Expanding the Scope of Concept Learning Using Metafeatures", "venue":"School of Computer Science and Engineering, University of New South Wales", "year":"", "window":"a custom learner works, but is labour-intensive. Relational learning techniques tend to be very sensitive to noise and to the particular clausal representation selected. They are typically 1 These datasets are: arrythmia, <b>audiology</b>  bach chorales, echocardiogram, isolet, mobile robots, waveform. unable to process large data sets in a reasonable time frame, and/or require the user to set limits on the", "mykey":1789},
 {"datasetID":25, "supportID":"AF087B9CEAFE5E48B31EEE610AD1B8A9213FD6EC", "rexaID":"4ccb84298ff6f0a62f8263c57259cc114cb1b328", "author":"Mohammed Waleed Kadous", "title":"Expanding the Scope of Concept Learning Using Metafeatures", "venue":"School of Computer Science and Engineering, University of New South Wales", "year":"", "window":"a custom learner works, but is labour-intensive. Relational learning techniques tend to be very sensitive to noise and to the particular clausal representation selected. They are typically 1 These datasets are: arrythmia, audiology, <b>bach</b> <b>chorales</b>  echocardiogram, isolet, mobile robots, waveform. unable to process large data sets in a reasonable time frame, and/or require the user to set limits on the", "mykey":1790},
 {"datasetID":66, "supportID":"AF087B9CEAFE5E48B31EEE610AD1B8A9213FD6EC", "rexaID":"4ccb84298ff6f0a62f8263c57259cc114cb1b328", "author":"Mohammed Waleed Kadous", "title":"Expanding the Scope of Concept Learning Using Metafeatures", "venue":"School of Computer Science and Engineering, University of New South Wales", "year":"", "window":"are: arrythmia, audiology, bach chorales, echocardiogram, isolet, <b>mobile</b> <b>robots</b>  waveform. unable to process large data sets in a reasonable time frame, and/or require the user to set limits on the search such as refinement rules (Cohen, 1995). Furthermore, their most powerful feature { the use of relations { is rarely", "mykey":1791},
 {"datasetID":107, "supportID":"AF087B9CEAFE5E48B31EEE610AD1B8A9213FD6EC", "rexaID":"4ccb84298ff6f0a62f8263c57259cc114cb1b328", "author":"Mohammed Waleed Kadous", "title":"Expanding the Scope of Concept Learning Using Metafeatures", "venue":"School of Computer Science and Engineering, University of New South Wales", "year":"", "window":"are: arrythmia, audiology, bach chorales, echocardiogram, isolet, mobile robots, <b>waveform</b>  unable to process large data sets in a reasonable time frame, and/or require the user to set limits on the search such as refinement rules (Cohen, 1995). Furthermore, their most powerful feature { the use of relations { is rarely", "mykey":1792},
 {"datasetID":108, "supportID":"AF087B9CEAFE5E48B31EEE610AD1B8A9213FD6EC", "rexaID":"4ccb84298ff6f0a62f8263c57259cc114cb1b328", "author":"Mohammed Waleed Kadous", "title":"Expanding the Scope of Concept Learning Using Metafeatures", "venue":"School of Computer Science and Engineering, University of New South Wales", "year":"", "window":"are: arrythmia, audiology, bach chorales, echocardiogram, isolet, mobile robots, <b>waveform</b>  unable to process large data sets in a reasonable time frame, and/or require the user to set limits on the search such as refinement rules (Cohen, 1995). Furthermore, their most powerful feature { the use of relations { is rarely", "mykey":1793},
 {"datasetID":46, "supportID":"AF358876A37DDFE25CC129863FF2C9DF0715DBC4", "rexaID":"ecfe10b847878abd1208e57bbbc4040fb87bebbc", "author":"Christophe Giraud and Tony Martinez", "title":"ADYNAMIC INCREMENTAL NETWORK THAT LEARNS BY DISCRIMINATION", "venue":"AA", "year":"", "window":"shuttle-exp consists of the complete set of 278 instances resulting from expanding the 15 rules of the shuttle-landing-control (shuttle-l-c) dataset. 6 Reported results for <b>hepatitis</b> and shuttle-exp were gathered using 10-way cross validation. Results for the Monk problems used the provided training and test sets, and results for shuttle-l-c", "mykey":1794},
 {"datasetID":88, "supportID":"AF358876A37DDFE25CC129863FF2C9DF0715DBC4", "rexaID":"ecfe10b847878abd1208e57bbbc4040fb87bebbc", "author":"Christophe Giraud and Tony Martinez", "title":"ADYNAMIC INCREMENTAL NETWORK THAT LEARNS BY DISCRIMINATION", "venue":"AA", "year":"", "window":"<b>shuttle</b> exp consists of the complete set of 278 instances resulting from expanding the 15 rules of the shuttle <b>landing</b> <b>control</b> (shuttle-l-c) dataset. 6 Reported results for hepatitis and shuttle-exp were gathered using 10-way cross validation. Results for the Monk problems used the provided training and test sets, and results for shuttle-l-c", "mykey":1795},
 {"datasetID":148, "supportID":"AF358876A37DDFE25CC129863FF2C9DF0715DBC4", "rexaID":"ecfe10b847878abd1208e57bbbc4040fb87bebbc", "author":"Christophe Giraud and Tony Martinez", "title":"ADYNAMIC INCREMENTAL NETWORK THAT LEARNS BY DISCRIMINATION", "venue":"AA", "year":"", "window":"non-Boolean inputs have their values translated into their equivalent binary form. Only one form of self-deletion, namely, complete discriminant deletion [10], is implemented in the simulations. The dataset <b>shuttle</b> exp consists of the complete set of 278 instances resulting from expanding the 15 rules of the shuttle-landing-control (shuttle-l-c) dataset. 6 Reported results for hepatitis and shuttle-exp", "mykey":1796},
 {"datasetID":48, "supportID":"AF8B7A720B4ED8EAEB319A0BFAC1585CE798F6D5", "rexaID":"9628f95b0cc07019875b0659d10aab8e2904bfca", "author":"Jianping Wu and Zhi-Hua Zhou and Cheng-The Chen", "title":"Ensemble of GA based Selective Neural Network Ensembles", "venue":"National Laboratory for Novel Software Technology Nanjing University", "year":"", "window":"mean squared error and corresponding standard deviation is also recorded. Experimental results are shown in Tab l e 1 . Statistical tests show that on the Friedman#1, Boston <b>Housing</b>  and Ozone data sets, GASEN's generalization error is significantly lower than that of the simple ensemble method, and e-GASEN attains still lower generalization errors than GASEN. On the Servo data set, GASEN is", "mykey":1797},
 {"datasetID":87, "supportID":"AF8B7A720B4ED8EAEB319A0BFAC1585CE798F6D5", "rexaID":"9628f95b0cc07019875b0659d10aab8e2904bfca", "author":"Jianping Wu and Zhi-Hua Zhou and Cheng-The Chen", "title":"Ensemble of GA based Selective Neural Network Ensembles", "venue":"National Laboratory for Novel Software Technology Nanjing University", "year":"", "window":"GASEN's generalization error is significantly lower than that of the simple ensemble method, and e-GASEN attains still lower generalization errors than GASEN. On the <b>Servo</b> data set, GASEN is slightly inferior to simple ensemble. The e-GASEN method's performance, however, has no significant difference with that of the simple ensemble method. From the aforementioned statistics", "mykey":1798},
 {"datasetID":1, "supportID":"AF9DA0F1C2BCE97A5F6AA2240504B48A64670947", "rexaID":"1b77c2b6fd8a261af286cf411879f9f520824bd6", "author":"Marc Sebban and Richard Nock and St\u00e9phane Lallich", "title":"Stopping Criterion for Boosting-Based Data Reduction Techniques: from Binary to Multiclass Problem", "venue":"Journal of Machine Learning Research, 3", "year":"2002", "window":"In order to assess the relevance of our multiclass statistical test, we 879 SEBBAN, NOCK AND LALLICH DATASET # CLASSES |LS| # FEATURES WAVES 3 500 21 <b>ABALONE</b> 3 1000 8 GLASS 6 214 9 BALANCE 3 625 4 IRIS 3 150 4 LED 10 500 7 LED+17 10 500 24 DERMATOLOGY 6 366 34 Table 4: Multiclass classification problems. 1", "mykey":1799},
 {"datasetID":19, "supportID":"AF9DA0F1C2BCE97A5F6AA2240504B48A64670947", "rexaID":"1b77c2b6fd8a261af286cf411879f9f520824bd6", "author":"Marc Sebban and Richard Nock and St\u00e9phane Lallich", "title":"Stopping Criterion for Boosting-Based Data Reduction Techniques: from Binary to Multiclass Problem", "venue":"Journal of Machine Learning Research, 3", "year":"2002", "window":"(Balance, Echocardiogram, German, Horse Colic, Led, Pima and Vehicle) see important improvements, ranging from 1% to } 5%. In contrast, only one dataset sees significant accuracy decrease  <b>Car</b>  96.0% vs. 93.9%). 9. Conclusions and Future Research This paper explores a method for prototype selection based on boosting, and gives statistical criteria", "mykey":1800},
 {"datasetID":38, "supportID":"AF9DA0F1C2BCE97A5F6AA2240504B48A64670947", "rexaID":"1b77c2b6fd8a261af286cf411879f9f520824bd6", "author":"Marc Sebban and Richard Nock and St\u00e9phane Lallich", "title":"Stopping Criterion for Boosting-Based Data Reduction Techniques: from Binary to Multiclass Problem", "venue":"Journal of Machine Learning Research, 3", "year":"2002", "window":"a weighted decision rule provides better results than the unweighted rule. Among them, 7 datasets (Balance, <b>Echocardiogram</b>  German, Horse Colic, Led, Pima and Vehicle) see important improvements, ranging from 1% to } 5%. In contrast, only one dataset sees significant accuracy decrease (Car,", "mykey":1801},
 {"datasetID":47, "supportID":"AF9DA0F1C2BCE97A5F6AA2240504B48A64670947", "rexaID":"1b77c2b6fd8a261af286cf411879f9f520824bd6", "author":"Marc Sebban and Richard Nock and St\u00e9phane Lallich", "title":"Stopping Criterion for Boosting-Based Data Reduction Techniques: from Binary to Multiclass Problem", "venue":"Journal of Machine Learning Research, 3", "year":"2002", "window":"a weighted decision rule provides better results than the unweighted rule. Among them, 7 datasets (Balance, Echocardiogram, German, <b>Horse</b> <b>Colic</b>  Led, Pima and Vehicle) see important improvements, ranging from 1% to } 5%. In contrast, only one dataset sees significant accuracy decrease (Car,", "mykey":1802},
 {"datasetID":120, "supportID":"AFAB54B1327B2E5D26ADDB8EBFD61CCB8596CE5D", "rexaID":"985a564327523fe48327882fdbb7869ef7c35d9f", "author":"Paul Horton and Kenta Nakai", "title":"Better Prediction of Protein Cellular Localization Sites with the it k Nearest Neighbors Classifier", "venue":"ISMB", "year":"1997", "window":"used have been submitted to the UCI Machine Learning Data Repository (Murphy & Aha 1996) and are described in (Horton & Nakai 1996), (Nakai & Kanehisa 1991), and (Nakai & Kanehisa 1992). We used two datasets: an <b>E</b> <b>coli</b> dataset with 336 proteins sequences labeled according to 8 classes (localization sites) and a yeast dataset with 1462 sequences labeled according to 10 classes. The occurrence of classes", "mykey":1803},
 {"datasetID":39, "supportID":"AFAB54B1327B2E5D26ADDB8EBFD61CCB8596CE5D", "rexaID":"985a564327523fe48327882fdbb7869ef7c35d9f", "author":"Paul Horton and Kenta Nakai", "title":"Better Prediction of Protein Cellular Localization Sites with the it k Nearest Neighbors Classifier", "venue":"ISMB", "year":"1997", "window":"used have been submitted to the UCI Machine Learning Data Repository (Murphy & Aha 1996) and are described in (Horton & Nakai 1996), (Nakai & Kanehisa 1991), and (Nakai & Kanehisa 1992). We used two datasets: an E <b>coli</b> dataset with 336 proteins sequences labeled according to 8 classes (localization sites) and a yeast dataset with 1462 sequences labeled according to 10 classes. The occurrence of classes", "mykey":1804},
 {"datasetID":110, "supportID":"AFAB54B1327B2E5D26ADDB8EBFD61CCB8596CE5D", "rexaID":"985a564327523fe48327882fdbb7869ef7c35d9f", "author":"Paul Horton and Kenta Nakai", "title":"Better Prediction of Protein Cellular Localization Sites with the it k Nearest Neighbors Classifier", "venue":"ISMB", "year":"1997", "window":"with 336 proteins sequences labeled according to 8 classes (localization sites) and a <b>yeast</b> dataset with 1462 sequences labeled according to 10 classes. The occurrence of classes for the datasets are summarized in tables 1 and 2. The 1462 yeast sequences were obtained by removing 12 sequences", "mykey":1805},
 {"datasetID":42, "supportID":"AFB87B9DA94FE907CFC4109DEBE5D7157B996EEF", "rexaID":"81ad8e5b8306aee758f09fd5c1caa8a23c63c2d6", "author":"Ping Zhong and Masao Fukushima", "title":"A Regularized Nonsmooth Newton Method for Multi-class Support Vector Machines", "venue":"", "year":"2005", "window":"the starting point of the next (k + 1)th iteration. The parameters \u00ba 1 and \u00ba 2 in (3) are both set 0.01. In Algorithm 3.1, we replaced the standard Armijo-rule in (S.3) by 10 Table 1: Six benchmark datasets from UCI name iris wine <b>glass</b> vowel vehicle segment #pts 150 178 214 528 846 2310 {fiats|flats} 4 13 9 10 18 19 #cls 3 3 6 11 4 7 #pts: the number of training data; {fiats|flats}: the number of", "mykey":1806},
 {"datasetID":53, "supportID":"AFB87B9DA94FE907CFC4109DEBE5D7157B996EEF", "rexaID":"81ad8e5b8306aee758f09fd5c1caa8a23c63c2d6", "author":"Ping Zhong and Masao Fukushima", "title":"A Regularized Nonsmooth Newton Method for Multi-class Support Vector Machines", "venue":"", "year":"2005", "window":"the starting point of the next (k + 1)th iteration. The parameters \u00ba 1 and \u00ba 2 in (3) are both set 0.01. In Algorithm 3.1, we replaced the standard Armijo-rule in (S.3) by 10 Table 1: Six benchmark datasets from UCI name <b>iris</b> wine glass vowel vehicle segment #pts 150 178 214 528 846 2310 {fiats|flats} 4 13 9 10 18 19 #cls 3 3 6 11 4 7 #pts: the number of training data; {fiats|flats}: the number of", "mykey":1807},
 {"datasetID":149, "supportID":"AFB87B9DA94FE907CFC4109DEBE5D7157B996EEF", "rexaID":"81ad8e5b8306aee758f09fd5c1caa8a23c63c2d6", "author":"Ping Zhong and Masao Fukushima", "title":"A Regularized Nonsmooth Newton Method for Multi-class Support Vector Machines", "venue":"", "year":"2005", "window":"the starting point of the next (k + 1)th iteration. The parameters \u00ba 1 and \u00ba 2 in (3) are both set 0.01. In Algorithm 3.1, we replaced the standard Armijo-rule in (S.3) by 10 Table 1: Six benchmark datasets from UCI name iris wine glass vowel <b>vehicle</b> segment #pts 150 178 214 528 846 2310 {fiats|flats} 4 13 9 10 18 19 #cls 3 3 6 11 4 7 #pts: the number of training data; {fiats|flats}: the number of", "mykey":1808},
 {"datasetID":109, "supportID":"AFB87B9DA94FE907CFC4109DEBE5D7157B996EEF", "rexaID":"81ad8e5b8306aee758f09fd5c1caa8a23c63c2d6", "author":"Ping Zhong and Masao Fukushima", "title":"A Regularized Nonsmooth Newton Method for Multi-class Support Vector Machines", "venue":"", "year":"2005", "window":"the starting point of the next (k + 1)th iteration. The parameters \u00ba 1 and \u00ba 2 in (3) are both set 0.01. In Algorithm 3.1, we replaced the standard Armijo-rule in (S.3) by 10 Table 1: Six benchmark datasets from UCI name iris <b>wine</b> glass vowel vehicle segment #pts 150 178 214 528 846 2310 {fiats|flats} 4 13 9 10 18 19 #cls 3 3 6 11 4 7 #pts: the number of training data; {fiats|flats}: the number of", "mykey":1809},
 {"datasetID":98, "supportID":"AFD8D1FA52EF0B5B3015FB2180C1E867F82C195E", "rexaID":"b9b24d88e45ac7034e22363aac1347ca65caffc6", "author":"I\u00f1aki Inza and Pedro Larraaga and Ramon Etxeberria and Basilio Sierra", "title":"Feature Subset Selection by Bayesian networks based optimization", "venue":"Dept. of Computer Science and Artificial Intelligence. University of the Basque Country", "year":"", "window":"come from the UCI repository [66]. Image dataset comes from the <b>Statlog</b> project [83]. LED24 (Breiman et al. [15]) is a well known artificial dataset with 7 equally relevant and 17 irrelevant binary features. We designed another artificial domain,", "mykey":1810},
 {"datasetID":107, "supportID":"AFD8D1FA52EF0B5B3015FB2180C1E867F82C195E", "rexaID":"b9b24d88e45ac7034e22363aac1347ca65caffc6", "author":"I\u00f1aki Inza and Pedro Larraaga and Ramon Etxeberria and Basilio Sierra", "title":"Feature Subset Selection by Bayesian networks based optimization", "venue":"Dept. of Computer Science and Artificial Intelligence. University of the Basque Country", "year":"", "window":"principal reason of `overfitting' was the low amount of training instances. To study this issue for FSS-EBNA, we have carried out a set of experiments with different training sizes of <b>Waveform</b> 40 dataset [15] with Naive-Bayes classification algorithm [19]: training sizes of 100; 200; 400; 800 and 1; 600 samples and tested over a fixed test set with 3; 200 instances. Figure 7 summarizes the set of", "mykey":1811},
 {"datasetID":108, "supportID":"AFD8D1FA52EF0B5B3015FB2180C1E867F82C195E", "rexaID":"b9b24d88e45ac7034e22363aac1347ca65caffc6", "author":"I\u00f1aki Inza and Pedro Larraaga and Ramon Etxeberria and Basilio Sierra", "title":"Feature Subset Selection by Bayesian networks based optimization", "venue":"Dept. of Computer Science and Artificial Intelligence. University of the Basque Country", "year":"", "window":"principal reason of `overfitting' was the low amount of training instances. To study this issue for FSS-EBNA, we have carried out a set of experiments with different training sizes of <b>Waveform</b> 40 dataset [15] with Naive-Bayes classification algorithm [19]: training sizes of 100; 200; 400; 800 and 1; 600 samples and tested over a fixed test set with 3; 200 instances. Figure 7 summarizes the set of", "mykey":1812},
 {"datasetID":19, "supportID":"B038A9BFF06CE8A4834DD19EE798670609F6CD53", "rexaID":"d2ad474fa4c9b346e9ac8f41900cea7d4917c7ac", "author":"Marc Sebban and Richard Nock and Jean-Hugues Chauchat and Ricco Rakotomalala", "title":"Impact of learning set quality and size on decision tree performances", "venue":"Int. J. Comput. Syst. Signal, 1", "year":"2000", "window":"in detail. Actually, we note that for 3 datasets  <b>Car</b>  Pima and Xd6),thedecisiontreeisso reduced that no rule is induced, resulting in a decision rule in favor of the majority class in LS. This explains why accuracy falls much, respectively", "mykey":1813},
 {"datasetID":34, "supportID":"B0BB832B18C475FC69611727FE1AB0B48C51ED7E", "rexaID":"a957b37bbe915165b7cb9fdb01d643a3c40079c4", "author":"Prem Melville and Raymond J. Mooney", "title":"Diverse ensembles for active learning", "venue":"ICML", "year":"2004", "window":"In particular, we used a sample size of two for the primary dataset, and three for breast-w, soybean, <b>diabetes</b>  vowel and credit-g. The primary aim of active learning is to reduce the amount of training data needed to induce an accurate model. To evaluate this, we", "mykey":1814},
 {"datasetID":90, "supportID":"B0BB832B18C475FC69611727FE1AB0B48C51ED7E", "rexaID":"a957b37bbe915165b7cb9fdb01d643a3c40079c4", "author":"Prem Melville and Raymond J. Mooney", "title":"Diverse ensembles for active learning", "venue":"ICML", "year":"2004", "window":"In particular, we used a sample size of two for the primary dataset, and three for breast-w, <b>soybean</b>  diabetes, vowel and credit-g. The primary aim of active learning is to reduce the amount of training data needed to induce an accurate model. To evaluate this, we", "mykey":1815},
 {"datasetID":91, "supportID":"B0BB832B18C475FC69611727FE1AB0B48C51ED7E", "rexaID":"a957b37bbe915165b7cb9fdb01d643a3c40079c4", "author":"Prem Melville and Raymond J. Mooney", "title":"Diverse ensembles for active learning", "venue":"ICML", "year":"2004", "window":"In particular, we used a sample size of two for the primary dataset, and three for breast-w, <b>soybean</b>  diabetes, vowel and credit-g. The primary aim of active learning is to reduce the amount of training data needed to induce an accurate model. To evaluate this, we", "mykey":1816},
 {"datasetID":137, "supportID":"B0C909FDF4190A485B1745D0F1048F4763C4B89A", "rexaID":"1543ea7033e6a4fc2ab140096210d7821d7018fc", "author":"Vijay S. Iyengar and Chidanand Apt and Tong Zhang", "title":"Active learning using adaptive resampling", "venue":"KDD", "year":"2000", "window":"the ALAR method are shown in Figure 5. Both ALAR-3-nn and ALAR-vote-E achieve the accuracy goal with only 8000 labeled instances. The last benchmark used is the Mod-Apte split of the <b>Reuters</b> data set available from [20]. Only the top ten categories are considered. For eachofthemwe solve the binary classi#cation problem of being in or out of that category.Weused the notion of information gain", "mykey":1817},
 {"datasetID":53, "supportID":"B0FE295CD2435880B5A31D93F5C173EC8E158C7C", "rexaID":"91c9c07720ed14d74e8cda2ec09b5b6789dda2b2", "author":"Daniel C. St and Ralph W. Wilkerson and Cihan H. Dagli", "title":"RULE SET QUALITY MEASURES FOR INDUCTIVE LEARNING ALGORITHMS", "venue":"proceedings of the Artificial Neural Networks In Engineering Conference 1996 (ANNIE", "year":"1996", "window":"distribution of the 148 instances among the four classes \"normal\" with 2 instances, \"metastases\" with 81 instances, \"malign\" with 61 instances, and \"fibrosis\" with 4 instances. The <b>Iris</b> data set, developed by R. A. Fisher (1936), lists the measurements of four characteristics of Iris flowers: petal length, petal width, sepal length, and sepal width. The set includes the measurements of 50", "mykey":1818},
 {"datasetID":120, "supportID":"B0FFA8725C65C4A0C1E61FF9F154253F8EE7941B", "rexaID":"9b1e690eee045665891881f050354585bd20bd54", "author":"Aik Choon Tan and David Gilbert", "title":"An Empirical Comparison of Supervised Machine Learning Techniques in Bioinformatics", "venue":"APBC", "year":"2003", "window":"interested readers should refer to the cited papers for details. <b>E</b> <b>coli</b> data set -- The objective of this data set is to predict the cellular localisation sites of E.coli proteins (Horton and Nakai, 1996). There are 8 different cellular sites, which are cytoplasm (cp), inner", "mykey":1819},
 {"datasetID":39, "supportID":"B0FFA8725C65C4A0C1E61FF9F154253F8EE7941B", "rexaID":"9b1e690eee045665891881f050354585bd20bd54", "author":"Aik Choon Tan and David Gilbert", "title":"An Empirical Comparison of Supervised Machine Learning Techniques in Bioinformatics", "venue":"APBC", "year":"2003", "window":"interested readers should refer to the cited papers for details. E <b>coli</b> data set -- The objective of this data set is to predict the cellular localisation sites of E.coli proteins (Horton and Nakai, 1996). There are 8 different cellular sites, which are cytoplasm (cp), inner", "mykey":1820},
 {"datasetID":67, "supportID":"B0FFA8725C65C4A0C1E61FF9F154253F8EE7941B", "rexaID":"9b1e690eee045665891881f050354585bd20bd54", "author":"Aik Choon Tan and David Gilbert", "title":"An Empirical Comparison of Supervised Machine Learning Techniques in Bioinformatics", "venue":"APBC", "year":"2003", "window":"with the addition of nuclear localisation information. <b>Promoter</b> data set. The task of the classifier is to predict whether a DNA sequence from E.coli is either a promoter or not (Towell et al., 1990). The input data is a 57-nucleotide sequence (A, C, T or G). HIV data", "mykey":1821},
 {"datasetID":154, "supportID":"B0FFA8725C65C4A0C1E61FF9F154253F8EE7941B", "rexaID":"9b1e690eee045665891881f050354585bd20bd54", "author":"Aik Choon Tan and David Gilbert", "title":"An Empirical Comparison of Supervised Machine Learning Techniques in Bioinformatics", "venue":"APBC", "year":"2003", "window":"The task of the classifier is to predict whether a DNA sequence from E.coli is either a promoter or not (Towell et al., 1990). The input data is a 57-nucleotide sequence (A, C, T or G). HIV data set -- The data set contains 362 octamer <b>protein</b> sequences each of which needs to be classified as an HIV protease cleavable site or uncleavable site (Cai and Chou, 1998). Data set E.coli Yeast", "mykey":1822},
 {"datasetID":110, "supportID":"B0FFA8725C65C4A0C1E61FF9F154253F8EE7941B", "rexaID":"9b1e690eee045665891881f050354585bd20bd54", "author":"Aik Choon Tan and David Gilbert", "title":"An Empirical Comparison of Supervised Machine Learning Techniques in Bioinformatics", "venue":"APBC", "year":"2003", "window":"of predicted lipoproteins and 3 different scoring functions on the amino acid contents whether predicted as a outer membrane or inner membrane, cleavable or uncleavable sequence signal. <b>Yeast</b> data set -- The objective is similar to the E.coli data, which is to determine the cellular localisation of the yeast proteins (Horton and Nakai, 1996). There are 10 different sites, which include: CYT", "mykey":1823},
 {"datasetID":23, "supportID":"B1466AC895EAB055C95EEC6CAD0260085FCD6C7B", "rexaID":"3fa8ab593f1fd58f33109aae497ca1fb67732c35", "author":"Marco Zaffalon and Marcus Hutter", "title":"Robust Feature Selection by Mutual Information Distributions", "venue":"CoRR, csAI/0206006", "year":"2002", "window":"The remaining cases are described by means of the following figures. Figure 2 shows that FF allowed the naive Bayes to significantly do better predictions than F for the greatest part of the <b>Chess</b> data set. The maximum di\u00aeerence in prediction accuracy is obtained at instance 422, where the accuracies are 0.889 and 0.832 for the cases FF and F, respectively. Figure 2 does not report the BF case,", "mykey":1824},
 {"datasetID":21, "supportID":"B1466AC895EAB055C95EEC6CAD0260085FCD6C7B", "rexaID":"3fa8ab593f1fd58f33109aae497ca1fb67732c35", "author":"Marco Zaffalon and Marcus Hutter", "title":"Robust Feature Selection by Mutual Information Distributions", "venue":"CoRR, csAI/0206006", "year":"2002", "window":"The remaining cases are described by means of the following figures. Figure 2 shows that FF allowed the naive Bayes to significantly do better predictions than F for the greatest part of the <b>Chess</b> data set. The maximum di\u00aeerence in prediction accuracy is obtained at instance 422, where the accuracies are 0.889 and 0.832 for the cases FF and F, respectively. Figure 2 does not report the BF case,", "mykey":1825},
 {"datasetID":22, "supportID":"B1466AC895EAB055C95EEC6CAD0260085FCD6C7B", "rexaID":"3fa8ab593f1fd58f33109aae497ca1fb67732c35", "author":"Marco Zaffalon and Marcus Hutter", "title":"Robust Feature Selection by Mutual Information Distributions", "venue":"CoRR, csAI/0206006", "year":"2002", "window":"The remaining cases are described by means of the following figures. Figure 2 shows that FF allowed the naive Bayes to significantly do better predictions than F for the greatest part of the <b>Chess</b> data set. The maximum di\u00aeerence in prediction accuracy is obtained at instance 422, where the accuracies are 0.889 and 0.832 for the cases FF and F, respectively. Figure 2 does not report the BF case,", "mykey":1826},
 {"datasetID":63, "supportID":"B1466AC895EAB055C95EEC6CAD0260085FCD6C7B", "rexaID":"3fa8ab593f1fd58f33109aae497ca1fb67732c35", "author":"Marco Zaffalon and Marcus Hutter", "title":"Robust Feature Selection by Mutual Information Distributions", "venue":"CoRR, csAI/0206006", "year":"2002", "window":"on a number of di\u00aeerent domains. For example, Shuttle-small reports data on diagnosing failures of the space shuttle; <b>Lymphography</b> and Hypothyroid are medical data sets; Spam is a body of e-mails that can be spam or non-spam; etc. The data sets presenting non-nominal features have been pre-discretized by MLC++ [KJL + 94], default options. This step may remove some", "mykey":1827},
 {"datasetID":53, "supportID":"B19BCB2705D3A970E1A2235C7F042EBCA49A13F5", "rexaID":"b8a6771d62cfaaffe0071e498102432d8d800573", "author":"Geoffrey Holmes and Bernhard Pfahringer and Richard Kirkby and Eibe Frank and Mark A. Hall", "title":"Multiclass Alternating Decision Trees", "venue":"ECML", "year":"2002", "window":"cleveland 81.72 80.36hepatitis 79.78 77.65 ionosphere 90.49 89.72 labor 84.67 87.5 + promoters 86.8 87.3 sick-euthyroid 97.71 97.85 + sonar 76.65 74.1296.18Table 3. Wrapping two-class ADTree results dataset 1vs1 1vsRest Random Exhaustive <b>iris</b> 95.13 95.33 95.33 95.33 balance-scale 83.94 85.06 + 85.06 + 85.06 + hypothyroid 99.61 99.63 99.64 99.64 anneal 99.01 98.96 99.05 99.19 + zoo 90.38 93.45 + 95.05 +", "mykey":1828},
 {"datasetID":90, "supportID":"B19BCB2705D3A970E1A2235C7F042EBCA49A13F5", "rexaID":"b8a6771d62cfaaffe0071e498102432d8d800573", "author":"Geoffrey Holmes and Bernhard Pfahringer and Richard Kirkby and Eibe Frank and Mark A. Hall", "title":"Multiclass Alternating Decision Trees", "venue":"ECML", "year":"2002", "window":"(class sizes 3-13) but struggles against two of the later datasets. For <b>soybean</b> 1-against-1 uses a tree of size 1710, and for primary-tumor it uses a tree of size 2310. Perhaps the most remarkable result is for half-letter where 1against-1 using 780 tests has an", "mykey":1829},
 {"datasetID":91, "supportID":"B19BCB2705D3A970E1A2235C7F042EBCA49A13F5", "rexaID":"b8a6771d62cfaaffe0071e498102432d8d800573", "author":"Geoffrey Holmes and Bernhard Pfahringer and Richard Kirkby and Eibe Frank and Mark A. Hall", "title":"Multiclass Alternating Decision Trees", "venue":"ECML", "year":"2002", "window":"(class sizes 3-13) but struggles against two of the later datasets. For <b>soybean</b> 1-against-1 uses a tree of size 1710, and for primary-tumor it uses a tree of size 2310. Perhaps the most remarkable result is for half-letter where 1against-1 using 780 tests has an", "mykey":1830},
 {"datasetID":53, "supportID":"B1E527F19DDC0343FC7F3A76D97025BFAB15D4F4", "rexaID":"395f41d678ee7dfc42742e514a39927f80c4c538", "author":"Ross J. Micheals and Patrick Grother and P. Jonathon Phillips", "title":"The NIST HumanID Evaluation Framework", "venue":"AVBPA", "year":"2003", "window":"Jonathon's signature therefore contains five sigmembers: one for the <b>iris</b> scan, three for each facial image, and one for the gait video. For the first sigmember, the iris scan, there is a single dataset with a single file that contains the iris data. Three sigmembers, for the facial imagery, each have a single dataset, each with a single file that each contain a facial image. The fifth sigmember,", "mykey":1831},
 {"datasetID":2, "supportID":"B2185A2F61BC311C2571A081EDBA09E230D4961A", "rexaID":"5e9e56b4525a16e039d75d04d32477b118e36b0d", "author":"Christopher R. Palmer and Christos Faloutsos", "title":"Electricity Based External Similarity of Categorical Attributes", "venue":"PAKDD", "year":"2003", "window":"than the distance functions computed by D fr;P . Since D fr;P has been previously evaluated using single link hierarchical clustering, that is the algorithm that we will use here [3]. The three data sets weevaluated are: 1. <b>Adult</b> - a selection of fields from the 1994 census data collected in the United States. There are 32,561 training examples and 16,281 test examples with 6 numeric fields and 8", "mykey":1832},
 {"datasetID":9, "supportID":"B2185A2F61BC311C2571A081EDBA09E230D4961A", "rexaID":"5e9e56b4525a16e039d75d04d32477b118e36b0d", "author":"Christopher R. Palmer and Christos Faloutsos", "title":"Electricity Based External Similarity of Categorical Attributes", "venue":"PAKDD", "year":"2003", "window":"house servant as an outlier, combined Clerical with Other service and combined Sales and Technical support . The final pair of clusterings in parts (g) and (h) show the makes of cars in the <b>Auto</b> data set. The comparison here is more subtle, but the REP clustering has a more natural looking structure and three very distinct clusters for the luxury cars, the family cars and the imports. D fr;P on the", "mykey":1833},
 {"datasetID":74, "supportID":"B2431272E2FC1B89CD41CEE4F841AEAD3D88A197", "rexaID":"7f1ab881783b9b5384c1fd78ca126009a42adbcd", "author":"Hendrik Blockeel and Luc De Raedt", "title":"Lookahead and Discretization in ILP", "venue":"ILP", "year":"1997", "window":"to an interval test in the discrete domain. The three approaches have been used and compared in our experiments. 3.2 Experimental Evaluation We evaluate the effect of discretization on two datasets: the <b>Musk</b> dataset (available at the UCI repository [11]) and the Diterpene dataset, generously provided to us by Steffen Schulze-Kremer and Saso Dzeroski. Both datasets contain nondeterminate", "mykey":1834},
 {"datasetID":75, "supportID":"B2431272E2FC1B89CD41CEE4F841AEAD3D88A197", "rexaID":"7f1ab881783b9b5384c1fd78ca126009a42adbcd", "author":"Hendrik Blockeel and Luc De Raedt", "title":"Lookahead and Discretization in ILP", "venue":"ILP", "year":"1997", "window":"to an interval test in the discrete domain. The three approaches have been used and compared in our experiments. 3.2 Experimental Evaluation We evaluate the effect of discretization on two datasets: the <b>Musk</b> dataset (available at the UCI repository [11]) and the Diterpene dataset, generously provided to us by Steffen Schulze-Kremer and Saso Dzeroski. Both datasets contain nondeterminate", "mykey":1835},
 {"datasetID":90, "supportID":"B2682982F11AD1D83B201E53AE89B071E96EB8E1", "rexaID":"9533c70dee11979650e1d68d349556d5624b7ba7", "author":"I\u00f1aki Inza and Pedro Larraaga and Basilio Sierra", "title":"Bayesian networks for feature subset selection", "venue":"Department of Computer Sciences and Artificial Intelligence", "year":"", "window":"on their stop-generation numbers to extract differences between the behaviour of GA-o and GA-u. Thus, observing Table 6, the one-point crossover is better suited for Horsecolic and <b>Soybean</b> large datasets than the uniform crossover; otherwise, we see the opposite behaviour in Ionosphere and Anneal. By the use of FSS-EBNA, we also avoid this tunning among different crossover operators for each", "mykey":1836},
 {"datasetID":91, "supportID":"B2682982F11AD1D83B201E53AE89B071E96EB8E1", "rexaID":"9533c70dee11979650e1d68d349556d5624b7ba7", "author":"I\u00f1aki Inza and Pedro Larraaga and Basilio Sierra", "title":"Bayesian networks for feature subset selection", "venue":"Department of Computer Sciences and Artificial Intelligence", "year":"", "window":"on their stop-generation numbers to extract differences between the behaviour of GA-o and GA-u. Thus, observing Table 6, the one-point crossover is better suited for Horsecolic and <b>Soybean</b> large datasets than the uniform crossover; otherwise, we see the opposite behaviour in Ionosphere and Anneal. By the use of FSS-EBNA, we also avoid this tunning among different crossover operators for each", "mykey":1837},
 {"datasetID":149, "supportID":"B27601200B051645A7C2F3A810CD3B4613F37780", "rexaID":"a5de010b13b49c05ccdf94f823248eba956d8d6b", "author":"Ronaldo C. Prati and Peter A. Flach", "title":"ROCCER: an Algorithm for Rule Learning Based on ROC Analysis", "venue":"Institute of Mathematics and Computer Science University of S~ ao Paulo", "year":"", "window":"351 64.10 10 Kr-vs-Kp 37 3196 52.22 11 Letter-a 17 20000 96.06 12 New-thyroid 6 215 83.72 13 Nursery 9 12960 97.45 14 Pima 9 768 65.10 15 Satimage 37 6435 90.27 16 <b>Vehicle</b> 19 846 76.48 Table 1: UCI data sets used in our experiments. been modified to incorporate the induction of unordered rule sets and Laplace error correction as evaluation function [Clark and Boswell, 1991] . Ripper [Cohen, 1995]", "mykey":1838},
 {"datasetID":59, "supportID":"B289658F98064462B059E73E565C5365B52EEE46", "rexaID":"ba8302e5db2921603c34aee0c738b103d1948f5f", "author":"Jaakko Peltonen and Arto Klami and Samuel Kaski", "title":"Learning Metrics for Information Visualization", "venue":"Neural Networks Research Centre Helsinki University of Technology", "year":"", "window":"(Table 3). Computing accurate global distances with the graph search (column 'Graph' in Table 3) further improved performance significantly on the Landsat and <b>Letter recognition</b> data sets. On the two other sets the difference between the Sammon-L variants was not significant. The difference between Sammon-L with the graph approximation and Sammon-E is illustrated in Figure 2 on", "mykey":1839},
 {"datasetID":146, "supportID":"B289658F98064462B059E73E565C5365B52EEE46", "rexaID":"ba8302e5db2921603c34aee0c738b103d1948f5f", "author":"Jaakko Peltonen and Arto Klami and Samuel Kaski", "title":"Learning Metrics for Information Visualization", "venue":"Neural Networks Research Centre Helsinki University of Technology", "year":"", "window":"estimates of auxiliary data, at the winner units of test samples. This measure has a slightly unintuitive corollary: since it requires a density estimator, even though traditional SOMs are not Data set n C N <b>Landsat</b> <b>Satellite</b> Data [1] 36 6 6435 Letter Recognition Data [1] 16 26 20000 LVQ PAK (Phoneme) [6] 20 13 3656 TIMIT Data [11] 12 41 14994 Table 1: The data sets and their dimensionality (n),", "mykey":1840},
 {"datasetID":120, "supportID":"B360A43EFFEC793BB362B2A8111F4572A930D8DB", "rexaID":"f006e50cdba5e00538e2b48452e1d0bab8ed02b9", "author":"Mukund Deshpande and George Karypis", "title":"Evaluation of Techniques for Classifying Biological Sequences", "venue":"PAKDD", "year":"2002", "window":"and contains 10,918 sequences of average length 361.6 symbols, it will be referred as M-EI. The <b>E</b> <b>Coli</b> Genome dataset contains sequences from the genome of prokaryote organism Escherichia coli (ecoli) [NTPP01]. To build this dataset the e-coli genome was first split into two sets of sequences, one containing", "mykey":1841},
 {"datasetID":39, "supportID":"B360A43EFFEC793BB362B2A8111F4572A930D8DB", "rexaID":"f006e50cdba5e00538e2b48452e1d0bab8ed02b9", "author":"Mukund Deshpande and George Karypis", "title":"Evaluation of Techniques for Classifying Biological Sequences", "venue":"PAKDD", "year":"2002", "window":"contains sequences from the genome of prokaryote organism Escherichia <b>coli</b>  <b>ecoli</b>  [NTPP01]. To build this dataset the e-coli genome was first split into two sets of sequences, one containing sequences making up the coding region whereas other containing sequences making up the non-coding region. Next a fixed", "mykey":1842},
 {"datasetID":67, "supportID":"B360A43EFFEC793BB362B2A8111F4572A930D8DB", "rexaID":"f006e50cdba5e00538e2b48452e1d0bab8ed02b9", "author":"Mukund Deshpande and George Karypis", "title":"Evaluation of Techniques for Classifying Biological Sequences", "venue":"PAKDD", "year":"2002", "window":"to the alphabet, which is assumed to be present at the beginning of each sequences [DEKG98]. Equation 3 contains these extra states. Figure 1 illustrates an example of sequence classification on a dataset of <b>DNA</b> sequences. Figure 1(a) shows the training sequences with their respective class labels, these sequences are first split into two parts for computing the TPM associated with each class label.", "mykey":1843},
 {"datasetID":68, "supportID":"B360A43EFFEC793BB362B2A8111F4572A930D8DB", "rexaID":"f006e50cdba5e00538e2b48452e1d0bab8ed02b9", "author":"Mukund Deshpande and George Karypis", "title":"Evaluation of Techniques for Classifying Biological Sequences", "venue":"PAKDD", "year":"2002", "window":"contains a total of 3,370 sequences, which is equal to the number of coding and non-coding regions in the e-coli genome. The <b>Protein Structure</b> dataset is made of amino acid sequences and addresses the problem of assigning a secondary structure to a protein sequence. The dataset was created by processing the SWISS-PROT [BA99] database to obtain", "mykey":1844},
 {"datasetID":69, "supportID":"B360A43EFFEC793BB362B2A8111F4572A930D8DB", "rexaID":"f006e50cdba5e00538e2b48452e1d0bab8ed02b9", "author":"Mukund Deshpande and George Karypis", "title":"Evaluation of Techniques for Classifying Biological Sequences", "venue":"PAKDD", "year":"2002", "window":"# Sessions Avg. Length Dataset # Sessions Avg. Length <b>Splice</b> (S-EI) 1, 527 60.0 Peptidias (P-) 1,584 511.3 Exon 762 60.0 cysteine 416 854.3 Intron 765 60.0 metallo 580 512.6 Mouse Genome (MG-GN) 10,918 361.6 serine 775 500.5 exon", "mykey":1845},
 {"datasetID":154, "supportID":"B360A43EFFEC793BB362B2A8111F4572A930D8DB", "rexaID":"f006e50cdba5e00538e2b48452e1d0bab8ed02b9", "author":"Mukund Deshpande and George Karypis", "title":"Evaluation of Techniques for Classifying Biological Sequences", "venue":"PAKDD", "year":"2002", "window":"contains a total of 3,370 sequences, which is equal to the number of coding and non-coding regions in the e-coli genome. The <b>Protein</b> Structure dataset is made of amino acid sequences and addresses the problem of assigning a secondary structure to a protein sequence. The dataset was created by processing the SWISS-PROT [BA99] database to obtain", "mykey":1846},
 {"datasetID":98, "supportID":"B407910FB3CB79A34C3082A189AB4291BC67A4FB", "rexaID":"c7147f5e08903c837fbc854a1979dd207bd90350", "author":"Wl odzisl and aw Duch", "title":"Committees of Undemocratic Competent Models", "venue":"School of Computer Engineering Nanyang Technological University", "year":"", "window":"k=7, Euclidean 94.9 95.3 best single CUC model Dipol92 98.3 95.2 <b>STATLOG</b> Alloc80 93.7 94.3 Statlog Quadratic DA 100 94.1 Statlog LDA 96.6 94.1 Statlog TABLE II COMPARISON OF RESULTS ON THE LETTER DATASET. RESULTS ARE FROM THE STATLOG BOOK OR OUR OWN CALCULATIONS . System Train % Test % Remarks CUC committee 98.5 96.5 Majority committee 95.8 95.4 kNN, k=5, Euclidean 94.8 95.4 best single CUC model", "mykey":1847},
 {"datasetID":46, "supportID":"B438C196B3172382D452A244B613E9A54A8C10C1", "rexaID":"d891d25459c4422e9cf2262272938d1ada438a9e", "author":"Xiaoli Z. Fern and Carla Brodley", "title":"Boosting Lazy Decision Trees", "venue":"ICML", "year":"2003", "window":"but behave less consistently. For three data sets, <b>Hepatitis</b>  Lympho and Monk2, bagging significantly degrades the performance of the base learner. This is possibly caused by the sub-sampling procedure used by bagging to generate different", "mykey":1848},
 {"datasetID":105, "supportID":"B45A1650D79794F9FE887040F4025564D0D7C6A5", "rexaID":"55a0fe71cda785b63ac52449a77593a9b7f5a8ae", "author":"Federico Divina and Elena Marchiori", "title":"Evolutionary Concept Learning", "venue":"GECCO", "year":"2002", "window":"respectively. The three dataset are public domain datasets. The <b>vote dataset</b> contains votes for each of the U.S. House of Representatives Congressmen on the sixteen key votes. The problem is learning a concept for distinguishing", "mykey":1849},
 {"datasetID":150, "supportID":"B505D5151B9E07D6B485294BA0985B51D129BC0B", "rexaID":"6a9871da32f3042a5de9082a36a11d5aecda6df8", "author":"Kai Ming Ting and Boon Toh Low", "title":"Model Combination in the Multiple-Data-Batches Scenario", "venue":"ECML", "year":"1997", "window":"settings are the same as those used in IB1 3 in all experiments. No parameter settings are required for NB*. Our studies employ two artificial domains (i.e., waveform and LED24) and four real-world datasets (i.e., euthyroid, <b>nettalk</b> stress), splice junction and protein coding) obtained from the UCI repository of machine learning databases (Merz & Murphy, 1996). The two noisy artificial domains are", "mykey":1850},
 {"datasetID":69, "supportID":"B505D5151B9E07D6B485294BA0985B51D129BC0B", "rexaID":"6a9871da32f3042a5de9082a36a11d5aecda6df8", "author":"Kai Ming Ting and Boon Toh Low", "title":"Model Combination in the Multiple-Data-Batches Scenario", "venue":"ECML", "year":"1997", "window":"settings are the same as those used in IB1 3 in all experiments. No parameter settings are required for NB*. Our studies employ two artificial domains (i.e., waveform and LED24) and four real-world datasets (i.e., euthyroid, nettalk(stress), <b>splice</b> junction and protein coding) obtained from the UCI repository of machine learning databases (Merz & Murphy, 1996). The two noisy artificial domains are", "mykey":1851},
 {"datasetID":154, "supportID":"B505D5151B9E07D6B485294BA0985B51D129BC0B", "rexaID":"6a9871da32f3042a5de9082a36a11d5aecda6df8", "author":"Kai Ming Ting and Boon Toh Low", "title":"Model Combination in the Multiple-Data-Batches Scenario", "venue":"ECML", "year":"1997", "window":"DNA nucleotide positions and each position can have one of the four base values. The task is to recognize, given a DNA sequence, two types of the splice junction or neither. The <b>protein</b> coding dataset, introduced by Craven and Shavlik (1993), contains DNA nucleotide sequences and its classification task is to differentiate the coding sequences from the non-coding ones. Each sequence has fifteen", "mykey":1852},
 {"datasetID":102, "supportID":"B505D5151B9E07D6B485294BA0985B51D129BC0B", "rexaID":"6a9871da32f3042a5de9082a36a11d5aecda6df8", "author":"Kai Ming Ting and Boon Toh Low", "title":"Model Combination in the Multiple-Data-Batches Scenario", "venue":"ECML", "year":"1997", "window":"are on or off, plus seventeen irrelevant binary attributes. Each attribute value is inverted with a probability of 0.1. The task is to classify the input as one of the ten digits. The euthyroid dataset is one of the sets of <b>Thyroid</b> examples from the Garvan Institute of Medical Research in Sydney described in Quinlan, Compton, Horn and Lazarus (1987). It consists of 3163 case data and diagnoses for", "mykey":1853},
 {"datasetID":107, "supportID":"B505D5151B9E07D6B485294BA0985B51D129BC0B", "rexaID":"6a9871da32f3042a5de9082a36a11d5aecda6df8", "author":"Kai Ming Ting and Boon Toh Low", "title":"Model Combination in the Multiple-Data-Batches Scenario", "venue":"ECML", "year":"1997", "window":"settings are the same as those used in IB1 3 in all experiments. No parameter settings are required for NB*. Our studies employ two artificial domains (i.e., <b>waveform</b> and LED24) and four real-world datasets (i.e., euthyroid, nettalk(stress), splice junction and protein coding) obtained from the UCI repository of machine learning databases (Merz & Murphy, 1996). The two noisy artificial domains are", "mykey":1854},
 {"datasetID":108, "supportID":"B505D5151B9E07D6B485294BA0985B51D129BC0B", "rexaID":"6a9871da32f3042a5de9082a36a11d5aecda6df8", "author":"Kai Ming Ting and Boon Toh Low", "title":"Model Combination in the Multiple-Data-Batches Scenario", "venue":"ECML", "year":"1997", "window":"settings are the same as those used in IB1 3 in all experiments. No parameter settings are required for NB*. Our studies employ two artificial domains (i.e., <b>waveform</b> and LED24) and four real-world datasets (i.e., euthyroid, nettalk(stress), splice junction and protein coding) obtained from the UCI repository of machine learning databases (Merz & Murphy, 1996). The two noisy artificial domains are", "mykey":1855},
 {"datasetID":42, "supportID":"B59A8BF45ADD9A7BAE9E8C978862F8D3C1077B61", "rexaID":"ba94d46c3f1c8a8ccb1dc3fcbeb6afdd963d3f87", "author":"Xiaoli Z. Fern and Carla Brodley", "title":"Solving cluster ensemble problems by bipartite graph partitioning", "venue":"ICML", "year":"2004", "window":"a fair comparison. 7. Experimental Results The goal of the experiments is to evaluate the three graph formulations - IBGF, CBGF and HBGF - given different cluster ensembles. Table 1. Summary of the data sets data set eos <b>glass</b> hrct isolet6 modis #inst. 2398 214 1545 1440 4975 #class 8 6 8 6 10 org. dim. 20 9 183 617 112 rp dim. 5 5 10 10 6 pca dim. --- --- 30 60 6 7.1. Data Sets and Parameter Settings", "mykey":1856},
 {"datasetID":46, "supportID":"B59EFC945ACD390FD067C30FCC412DB19F2C24A3", "rexaID":"118fe8f48c22375803e81b47d91c0bfd96c19999", "author":"Wl/odzisl/aw Duch and Rafal Adamczak and Geerd H. F Diercksen", "title":"Neural Networks from Similarity Based Perspective", "venue":"Department of Computer Methods, Nicholas Copernicus University", "year":"", "window":"85.5% (with 20 neurons), and inserting a new value that does not appear in the data, such as -100, decreased accuracy to 81.5% (using 22 neurons). The same behavior has been observed for <b>Hepatitis</b> dataset taken from the same source. the data contains 155 vectors, 18 attributes, 13 of them are binary, other have integer values. The last attribute has 67 missing values, attribute 16 has 29 missing", "mykey":1857},
 {"datasetID":53, "supportID":"B59EFC945ACD390FD067C30FCC412DB19F2C24A3", "rexaID":"118fe8f48c22375803e81b47d91c0bfd96c19999", "author":"Wl/odzisl/aw Duch and Rafal Adamczak and Geerd H. F Diercksen", "title":"Neural Networks from Similarity Based Perspective", "venue":"Department of Computer Methods, Nicholas Copernicus University", "year":"", "window":"them on a unit sphere defined by this metric. 6 Pedagogical illustration The influence of non-Euclidean distance functions on the decision borders is illustrated here on the classical <b>Iris</b> flowers dataset, containing 50 cases in each of the 3 classes. The flowers are described by 4 measurements (petal and sepal width and length). Two classes, Iris virginica and Iris versicolor, overlap, and therefore", "mykey":1858},
 {"datasetID":46, "supportID":"B604499A7364797F1F1366F10C98FD1580B7FD8B", "rexaID":"15b7c2d67edb6f722c80cf48928bc53c17ad7bef", "author":"Wl odzisl and Rafal Adamczak and Krzysztof Grabczewski", "title":"Optimization of Logical Rules Derived by Neural Procedures", "venue":"Department of Computer Methods, Nicholas Copernicus University", "year":"", "window":"(if there is no test set). All data are from the UCI repository [11], except for the appendicitis, obtained from the authors of [12] paper. <b>Hepatitis</b> dataset contains many missing values and if averages are used meaningless rules are obtained; here only attributes with few missing values were used (no more than 5). NASA shuttle (described below) and the", "mykey":1859},
 {"datasetID":148, "supportID":"B604499A7364797F1F1366F10C98FD1580B7FD8B", "rexaID":"15b7c2d67edb6f722c80cf48928bc53c17ad7bef", "author":"Wl odzisl and Rafal Adamczak and Krzysztof Grabczewski", "title":"Optimization of Logical Rules Derived by Neural Procedures", "venue":"Department of Computer Methods, Nicholas Copernicus University", "year":"", "window":"77.2 4/4 78.0 Diabetes 2/2 75.0 Hepatitis 3/5 88.4 Heart (Cleveland) 4/3 85.5 Hypothyroid 3/5 99.4 Iris 3/1 95.3 3/2 98.0 Mushrooms 2/1 98.5 3/2 99.4 4/4 99.9 5/6 100.0 A. NASA <b>Shuttle</b> The Shuttle dataset from NASA contains 9 continuos numerical attributes related to the positions of radiators in the Space Shuttle. There are 43500 training vectors and 14500 test vectors, divided into 7 classes in a", "mykey":1860},
 {"datasetID":31, "supportID":"B635ADFAE0D08D00162C09C957931DED07495F20", "rexaID":"c4ef47232a73d5e27b14e63096634eb70dcbddce", "author":"Joao Gama and Ricardo Rocha and Pedro Medas", "title":"Accurate decision trees for mining high-speed data streams", "venue":"KDD", "year":"2003", "window":"surprising. They indicate that sub-optimal decisions could contribute to a bias reduction. Other Results on Real data We have done some experiments on real data. We have used the Forest <b>CoverType</b> dataset from the UCI KDD archive. The goal is to predict the forest cover type from cartographic variables. The problem is defined by 54 variables of di\u00aeerent types: continuous and categorical. The dataset", "mykey":1861},
 {"datasetID":57, "supportID":"B635ADFAE0D08D00162C09C957931DED07495F20", "rexaID":"c4ef47232a73d5e27b14e63096634eb70dcbddce", "author":"Joao Gama and Ricardo Rocha and Pedro Medas", "title":"Accurate decision trees for mining high-speed data streams", "venue":"KDD", "year":"2003", "window":"when classifying test examples: classifying using the majority class (VFDTcMC) and classifying using naive Bayes (VFDTcNB) at leaves. The experimental work has been done using the Waveform and <b>LED datasets</b>  These are well known artificial datasets. We have used the two versions of the Waveform dataset available at the UCI repository [1]. Both versions are problems with three classes. The first", "mykey":1862},
 {"datasetID":107, "supportID":"B635ADFAE0D08D00162C09C957931DED07495F20", "rexaID":"c4ef47232a73d5e27b14e63096634eb70dcbddce", "author":"Joao Gama and Ricardo Rocha and Pedro Medas", "title":"Accurate decision trees for mining high-speed data streams", "venue":"KDD", "year":"2003", "window":"when classifying test examples: classifying using the majority class (VFDTcMC) and classifying using naive Bayes (VFDTcNB) at leaves. The experimental work has been done using the <b>Waveform</b> and LED datasets. These are well known artificial datasets. We have used the two versions of the Waveform dataset available at the UCI repository [1]. Both versions are problems with three classes. The first", "mykey":1863},
 {"datasetID":108, "supportID":"B635ADFAE0D08D00162C09C957931DED07495F20", "rexaID":"c4ef47232a73d5e27b14e63096634eb70dcbddce", "author":"Joao Gama and Ricardo Rocha and Pedro Medas", "title":"Accurate decision trees for mining high-speed data streams", "venue":"KDD", "year":"2003", "window":"when classifying test examples: classifying using the majority class (VFDTcMC) and classifying using naive Bayes (VFDTcNB) at leaves. The experimental work has been done using the <b>Waveform</b> and LED datasets. These are well known artificial datasets. We have used the two versions of the Waveform dataset available at the UCI repository [1]. Both versions are problems with three classes. The first", "mykey":1864},
 {"datasetID":2, "supportID":"B648975290D78C64B21D2ECF80EC43DACFDF9F8B", "rexaID":"8931d2a4a8256ea88abea3ea3ac820fd421ce0b1", "author":"Stephen D. Bay and Michael J. Pazzani", "title":"Detecting Group Differences: Mining Contrast Sets", "venue":"Data Min. Knowl. Discov, 5", "year":"2001", "window":"3 This program is available from http://fuzzy.cs.Uni-Magdeburg.de/\u00b8borgelt/. Version 1.8 of his program is incorporated in the data mining tool Clementine. 18 issues. We used the following datasets which are summarized in Table 2. # <b>Adult</b>  The Adult Census data contains information extracted from the 1994 Current Population Survey. There are variables such as age, working class, education,", "mykey":1865},
 {"datasetID":20, "supportID":"B648975290D78C64B21D2ECF80EC43DACFDF9F8B", "rexaID":"8931d2a4a8256ea88abea3ea3ac820fd421ce0b1", "author":"Stephen D. Bay and Michael J. Pazzani", "title":"Detecting Group Differences: Mining Contrast Sets", "venue":"Data Min. Knowl. Discov, 5", "year":"2001", "window":"sized divisions by frequency (e.g. income) or interval width (e.g. age). Finally, we further randomly sampled the data to obtain a 1 in 1000 sample. Federal <b>Census</b> data is one of the most difficult data sets to mine because of the long average record width coupled with the high number of popular attribute-value pairs which occur frequently in many records. These two factors combine to result in many", "mykey":1866},
 {"datasetID":127, "supportID":"B648975290D78C64B21D2ECF80EC43DACFDF9F8B", "rexaID":"8931d2a4a8256ea88abea3ea3ac820fd421ce0b1", "author":"Stephen D. Bay and Michael J. Pazzani", "title":"Detecting Group Differences: Mining Contrast Sets", "venue":"Data Min. Knowl. Discov, 5", "year":"2001", "window":"example, if we are mining at a support difference of 10% and group A has a support of 11% we still need to mine group B as long as its support is non-zero. STUCCO was very fast and did well on all data sets, even on Mushroom and <b>IPUMS</b> which are among the most difficult data sets for mining algorithms. STUCCO was slower than Apriori on UCI Admissions by a factor of approximately three. This is probably", "mykey":1867},
 {"datasetID":73, "supportID":"B648975290D78C64B21D2ECF80EC43DACFDF9F8B", "rexaID":"8931d2a4a8256ea88abea3ea3ac820fd421ce0b1", "author":"Stephen D. Bay and Michael J. Pazzani", "title":"Detecting Group Differences: Mining Contrast Sets", "venue":"Data Min. Knowl. Discov, 5", "year":"2001", "window":"Adult Census data contains information extracted from the 1994 Current Population Survey. There are variables such as age, working class, education, sex, hours worked, salary, etc. # <b>Mushroom</b>  This data set describes mushrooms and their physical properties such as shape, odor, habitat, etc. Mushroom is not a true observational data set as the examples are not drawn from individual instances but rather", "mykey":1868},
 {"datasetID":90, "supportID":"B6DEE95BFA6ABA502B8890829BA48CDD740ABFE2", "rexaID":"01cf09d5e240fbb8da88b4989dfbffece6153160", "author":"Hendrik Blockeel and Luc De Raedt and Jan Ramon", "title":"Top-Down Induction of Clustering Trees", "venue":"ICML", "year":"1998", "window":"those obtained with the supervised learner Tilde. We see that TIC obtains high accuracies for these problems. The only clustering result we know of is for COBWEB, which obtained 100% on the <b>Soybean</b> data set. This difference is not significant. Tilde's ac72 73 74 75 76 77 78 79 80 81 15 20 25 30 35 40 45 50 55 accuracy (%) size validation set (%) accuracy of pruned tree accuracy of unpruned tree 15 20", "mykey":1869},
 {"datasetID":91, "supportID":"B6DEE95BFA6ABA502B8890829BA48CDD740ABFE2", "rexaID":"01cf09d5e240fbb8da88b4989dfbffece6153160", "author":"Hendrik Blockeel and Luc De Raedt and Jan Ramon", "title":"Top-Down Induction of Clustering Trees", "venue":"ICML", "year":"1998", "window":"those obtained with the supervised learner Tilde. We see that TIC obtains high accuracies for these problems. The only clustering result we know of is for COBWEB, which obtained 100% on the <b>Soybean</b> data set. This difference is not significant. Tilde's ac72 73 74 75 76 77 78 79 80 81 15 20 25 30 35 40 45 50 55 accuracy (%) size validation set (%) accuracy of pruned tree accuracy of unpruned tree 15 20", "mykey":1870},
 {"datasetID":27, "supportID":"B742582D060B7B586F98BF8DFE7004A8DFFCD43F", "rexaID":"af26080292d3845212afdc432dfb8a1d1cdb5c8b", "author":"Xiaoming Huo", "title":"FBP: A Frontier-Based Tree-Pruning Algorithm", "venue":"Seoung Bum Kim", "year":"2002", "window":"the mean difference of the CV error between CCP and FBP is (0.0770, 0.2196). As mentioned earlier, we treat this as a \"sanity check\". Table 4: Comparison of the CV Error Rates Between CCP and FBP Data Set CCP FBP Winner Australian <b>Credit Approval</b> 14.13 14.01 FBP Cleveland Heart Disease 21.15 20.89 FBP Congressional Voting Records 4.16 4.12 FBP Wisconsin Breast Cancer 4.56 4.47 FBP Iris Plants 5.20", "mykey":1871},
 {"datasetID":143, "supportID":"B742582D060B7B586F98BF8DFE7004A8DFFCD43F", "rexaID":"af26080292d3845212afdc432dfb8a1d1cdb5c8b", "author":"Xiaoming Huo", "title":"FBP: A Frontier-Based Tree-Pruning Algorithm", "venue":"Seoung Bum Kim", "year":"2002", "window":"the mean difference of the CV error between CCP and FBP is (0.0770, 0.2196). As mentioned earlier, we treat this as a \"sanity check\". Table 4: Comparison of the CV Error Rates Between CCP and FBP Data Set CCP FBP Winner <b>Australian Credit</b> Approval 14.13 14.01 FBP Cleveland Heart Disease 21.15 20.89 FBP Congressional Voting Records 4.16 4.12 FBP Wisconsin Breast Cancer 4.56 4.47 FBP Iris Plants 5.20", "mykey":1872},
 {"datasetID":146, "supportID":"B7DACC8F1FA9D19AC3DD0A715A9F06CAAEC79474", "rexaID":"2e6705a0fe8b335ec6bffc4acbe8a0665f67c8a5", "author":"Jaakko Peltonen and Arto Klami and Samuel Kaski", "title":"Learning More Accurate Metrics for Self-Organizing Maps", "venue":"ICANN", "year":"2002", "window":"Data set Dimensions Classes Samples <b>Landsat</b> <b>Satellite</b> Data * 36 6 6435 Letter Recognition Data * 16 26 20000 Phoneme Data from LVQ PAK [8] 20 14 3656 TIMIT Data from [10] 12 41 14994 Bankruptcy Data used in", "mykey":1873},
 {"datasetID":53, "supportID":"B820BC13DD22658FD3561632089CEE83D75232C0", "rexaID":"32cb39d6ec7d1af5c8f8ad0fff8300527cf9f188", "author":"Edgar Acuna and Alex Rojas", "title":"Ensembles of classifiers based on Kernel density estimators", "venue":"Department of Mathematics University of Puerto Rico", "year":"2000", "window":"has been developed to carry out all our tasks. The results are shown in the table 7. Table 6. Comparison of Bagging using classical and adaptive kernel classifiers Classical Kernel Adaptive Kernel Dataset Single Bagged Improv Single Bagged Improv <b>Iris</b> 4.00 3.33 16.75 4.67 4.00 14.34 Glass 44.97 40.52 9.90 35.20 33.25 5.54 Heart-C 22.09 20.05 9.23 23.60 19.80 16.10 Breast-W 4.34 4.10 5.53 4.88 4.53", "mykey":1874},
 {"datasetID":98, "supportID":"B820BC13DD22658FD3561632089CEE83D75232C0", "rexaID":"32cb39d6ec7d1af5c8f8ad0fff8300527cf9f188", "author":"Edgar Acuna and Alex Rojas", "title":"Ensembles of classifiers based on Kernel density estimators", "venue":"Department of Mathematics University of Puerto Rico", "year":"2000", "window":"All of them have been analyzed already in a combining setup and nine of these datasets were analyzed in the <b>Statlog</b> Project (Michie, et. al. 1994). B) To make an analysis of the bias-variance decomposition for the misclassification error when classifiers based in kernel density", "mykey":1875},
 {"datasetID":48, "supportID":"B8D32A604482E890EDA6131DA3AC40028AD2127E", "rexaID":"fc1871a06c2119d5979c934e3d950ce16246f512", "author":"Zhi-Hua Zhou and Jianping Wu and Weiyu Tang and Zen Chen", "title":"Combining Regression Estimators: GA-Based Selective Neural Network Ensemble", "venue":"International Journal of Computational Intelligence and Applications, 1", "year":"2001", "window":"than that generated by averaging all in most cases. Pairwise one-tailed t-tests also indicate that the generalization ability of GASEN and enumerating is not significantly different on all the data sets except Boston <b>Housing</b> where GASEN is significantly better than enumerating. Considering that enumerating can hardly work when there are lots of individual networks due to its extensive", "mykey":1876},
 {"datasetID":48, "supportID":"B901BD4349D785BD08348428527315C09AA9BC1C", "rexaID":"7c48d99a6eaf10a72fda6d4dd5591be082286c84", "author":"Rudy Setiono and Huan Liu", "title":"A connectionist approach to generating oblique decision trees", "venue":"IEEE Transactions on Systems, Man, and Cybernetics, Part B, 29", "year":"1999", "window":"351 34 continuous 9. Iris 150 4 continuous 10. Pima-diabetes 768 8 continuous 11. Sonar 208 60 continuous 12. Australian 690 14 mixed 13. HeartDisease 297 13 mixed 14. <b>Housing</b> 506 13 mixed Table 1: Dataset Summary. #Data - data size, Type - attribute type, and #A - number of attributes. 3. Apply NN-DT as follows: (a) Construct a network using the algorithm MLNNCA for the training dataset. Stop MLNNCA", "mykey":1877},
 {"datasetID":74, "supportID":"B92890DDD4FD7A4F00DB1BF12E61403C0AEDFC3B", "rexaID":"77fb0a2e40e1c957295bfa6bffb09ffc1d1bebc7", "author":"Giorgio Valentini", "title":"An experimental bias--variance analysis of SVM ensembles based on resampling techniques", "venue":"", "year":"", "window":"Spam RBF-SVM 0.1292 0.1290 0.14 -0.48 1.57 2.22 Poly-SVM 0.1323 0.1318 0.35 2.11 -5.83 -1.19 D-prod SVM 0.1495 0.1389 7.15 -3.16 19.87 16.38 Data set <b>Musk</b> RBF-SVM 0.0898 0.0920 -2.36 -6.72 22.91 13.67 Poly-SVM 0.1225 0.1128 7.92 -10.49 38.17 37.26 D-prod SVM 0.1501 0.1261 15.97 -2.41 34.56 29.38 V. DISCUSSION A. Bias--Variance characteristics of", "mykey":1878},
 {"datasetID":75, "supportID":"B92890DDD4FD7A4F00DB1BF12E61403C0AEDFC3B", "rexaID":"77fb0a2e40e1c957295bfa6bffb09ffc1d1bebc7", "author":"Giorgio Valentini", "title":"An experimental bias--variance analysis of SVM ensembles based on resampling techniques", "venue":"", "year":"", "window":"Spam RBF-SVM 0.1292 0.1290 0.14 -0.48 1.57 2.22 Poly-SVM 0.1323 0.1318 0.35 2.11 -5.83 -1.19 D-prod SVM 0.1495 0.1389 7.15 -3.16 19.87 16.38 Data set <b>Musk</b> RBF-SVM 0.0898 0.0920 -2.36 -6.72 22.91 13.67 Poly-SVM 0.1225 0.1128 7.92 -10.49 38.17 37.26 D-prod SVM 0.1501 0.1261 15.97 -2.41 34.56 29.38 V. DISCUSSION A. Bias--Variance characteristics of", "mykey":1879},
 {"datasetID":146, "supportID":"B92890DDD4FD7A4F00DB1BF12E61403C0AEDFC3B", "rexaID":"77fb0a2e40e1c957295bfa6bffb09ffc1d1bebc7", "author":"Giorgio Valentini", "title":"An experimental bias--variance analysis of SVM ensembles based on resampling techniques", "venue":"", "year":"", "window":"we randomly split all the available data in a training and a test set of about equal size, except for the Grey <b>Landsat</b> data set for which we maintained the original size for both the training and test set. To measure the bias--variance decomposition of error, for each data set we used 100 sets (parameter n = 100 in Sect.", "mykey":1880},
 {"datasetID":107, "supportID":"B92890DDD4FD7A4F00DB1BF12E61403C0AEDFC3B", "rexaID":"77fb0a2e40e1c957295bfa6bffb09ffc1d1bebc7", "author":"Giorgio Valentini", "title":"An experimental bias--variance analysis of SVM ensembles based on resampling techniques", "venue":"", "year":"", "window":"each region is delimited by one or more of four simple polynomial and trigonometric functions 2 . The synthetic data set <b>Waveform</b> is generated from a combination of 2 of 3 \"base\" waves; we reduced the original three classes of Waveform to two, deleting all samples pertaining to class 0. The other data sets are all", "mykey":1881},
 {"datasetID":108, "supportID":"B92890DDD4FD7A4F00DB1BF12E61403C0AEDFC3B", "rexaID":"77fb0a2e40e1c957295bfa6bffb09ffc1d1bebc7", "author":"Giorgio Valentini", "title":"An experimental bias--variance analysis of SVM ensembles based on resampling techniques", "venue":"", "year":"", "window":"each region is delimited by one or more of four simple polynomial and trigonometric functions 2 . The synthetic data set <b>Waveform</b> is generated from a combination of 2 of 3 \"base\" waves; we reduced the original three classes of Waveform to two, deleting all samples pertaining to class 0. The other data sets are all", "mykey":1882},
 {"datasetID":151, "supportID":"B97D3EEB77C2511B8852477BF8D159193337CE84", "rexaID":"e5d994d772cfe5ec4d0f3e6d669f0bc28180a3ae", "author":"Elena Smirnova and Ida G. Sprinkhuizen-Kuyper and I. Nalbantis and b. ERIM and Universiteit Rotterdam", "title":"Unanimous Voting using Support Vector Machines", "venue":"IKAT, Universiteit Maastricht", "year":"", "window":"(polynomial kernel) for which the gain is 0.72 and the <b>Sonar</b> dataset (polynomial kernel) for which the gain is 0.68. 1 Note VS(I + , I - ) = VS(I + f , I - f )  VS(I + n , I - n ) and VS(I + , I - ) = VS(I + f , I - f )NVS. 6 Conclusion This paper proposes a new", "mykey":1883},
 {"datasetID":45, "supportID":"B97D3EEB77C2511B8852477BF8D159193337CE84", "rexaID":"e5d994d772cfe5ec4d0f3e6d669f0bc28180a3ae", "author":"Elena Smirnova and Ida G. Sprinkhuizen-Kuyper and I. Nalbantis and b. ERIM and Universiteit Rotterdam", "title":"Unanimous Voting using Support Vector Machines", "venue":"IKAT, Universiteit Maastricht", "year":"", "window":"the hypothesis space H contains the target hyperplane, the hyperplane is consistent with the training data; i.e., it belongs to the version space [7, 11]. Thus, the unanimous-voting classification Data Set Parameters Cvssvm Avssvm Asvm I <b>Heart</b> Statlog P, E=2.0, C=1730 56.3% 100% 73.0% 0.42 Heart-Statlog RBF, G=0.2 , C=2182 40.7% 100% 73.7 % 0.24 Hepatitis P, E=1.4, C=11.7 80.0% 100% 80.0 % 0.72", "mykey":1884},
 {"datasetID":46, "supportID":"B97D3EEB77C2511B8852477BF8D159193337CE84", "rexaID":"e5d994d772cfe5ec4d0f3e6d669f0bc28180a3ae", "author":"Elena Smirnova and Ida G. Sprinkhuizen-Kuyper and I. Nalbantis and b. ERIM and Universiteit Rotterdam", "title":"Unanimous Voting using Support Vector Machines", "venue":"IKAT, Universiteit Maastricht", "year":"", "window":"using the proportions of correctly and incorrectly classified instances of SVM and VSSVM. All the cases in Table 1 resulted in a considerable information gain. We especially mention the <b>Hepatitis</b> dataset (polynomial kernel) for which the gain is 0.72 and the Sonar dataset (polynomial kernel) for which the gain is 0.68. 1 Note VS(I + , I - ) = VS(I + f , I - f )  VS(I + n , I - n ) and VS(I + , I -", "mykey":1885},
 {"datasetID":145, "supportID":"B97D3EEB77C2511B8852477BF8D159193337CE84", "rexaID":"e5d994d772cfe5ec4d0f3e6d669f0bc28180a3ae", "author":"Elena Smirnova and Ida G. Sprinkhuizen-Kuyper and I. Nalbantis and b. ERIM and Universiteit Rotterdam", "title":"Unanimous Voting using Support Vector Machines", "venue":"IKAT, Universiteit Maastricht", "year":"", "window":"the hypothesis space H contains the target hyperplane, the hyperplane is consistent with the training data; i.e., it belongs to the version space [7, 11]. Thus, the unanimous-voting classification Data Set Parameters Cvssvm Avssvm Asvm I <b>Heart</b> <b>Statlog</b> P, E=2.0, C=1730 56.3% 100% 73.0% 0.42 Heart-Statlog RBF, G=0.2 , C=2182 40.7% 100% 73.7 % 0.24 Hepatitis P, E=1.4, C=11.7 80.0% 100% 80.0 % 0.72", "mykey":1886},
 {"datasetID":98, "supportID":"B97D3EEB77C2511B8852477BF8D159193337CE84", "rexaID":"e5d994d772cfe5ec4d0f3e6d669f0bc28180a3ae", "author":"Elena Smirnova and Ida G. Sprinkhuizen-Kuyper and I. Nalbantis and b. ERIM and Universiteit Rotterdam", "title":"Unanimous Voting using Support Vector Machines", "venue":"IKAT, Universiteit Maastricht", "year":"", "window":"the hypothesis space H contains the target hyperplane, the hyperplane is consistent with the training data; i.e., it belongs to the version space [7, 11]. Thus, the unanimous-voting classification Data Set Parameters Cvssvm Avssvm Asvm I Heart <b>Statlog</b> P, E=2.0, C=1730 56.3% 100% 73.0% 0.42 Heart-Statlog RBF, G=0.2 , C=2182 40.7% 100% 73.7 % 0.24 Hepatitis P, E=1.4, C=11.7 80.0% 100% 80.0 % 0.72", "mykey":1887},
 {"datasetID":2, "supportID":"B9D41056EEAE5DF6FA0E750D70B3DCA00497C614", "rexaID":"1383360e8bb2cfd7a98219f867869a9f6d7e0db0", "author":"Wei-Chun Kao and Kai-Min Chung and Lucas Assun and Chih-Jen Lin", "title":"Decomposition Methods for Linear Support Vector Machines", "venue":"Neural Computation, 16", "year":"2004", "window":"used are in Table 3.2. The four small problems are from the statlog collection (Michie, Spiegelhalter, and Taylor 1994). The problem <b>adult</b> is compiled by Platt (1998) from the UCI \"adult\" data set (Blake and Merz 1998). Problem web is also from Platt. Problem ijcnn is from the first problem of IJCNN challenge 2001 (Prokhorov 2001). Note that we use the winner's transformation of the raw data", "mykey":1888},
 {"datasetID":67, "supportID":"B9D41056EEAE5DF6FA0E750D70B3DCA00497C614", "rexaID":"1383360e8bb2cfd7a98219f867869a9f6d7e0db0", "author":"Wei-Chun Kao and Kai-Min Chung and Lucas Assun and Chih-Jen Lin", "title":"Decomposition Methods for Linear Support Vector Machines", "venue":"Neural Computation, 16", "year":"2004", "window":"in (Keerthi and Lin 2003), due to the difficulty on solving linear SVMs, Algorithm 1 is only tested on small two-class problems. Here, we would like to evaluate this algorithm on large multi-class data sets. We consider problems <b>dna</b>  satimage, letter, and shuttle, which were originally from the statlog collection (Michie, Spiegelhalter, and Taylor 1994) and were used in (Hsu and Lin 2002a). Except", "mykey":1889},
 {"datasetID":98, "supportID":"B9D41056EEAE5DF6FA0E750D70B3DCA00497C614", "rexaID":"1383360e8bb2cfd7a98219f867869a9f6d7e0db0", "author":"Wei-Chun Kao and Kai-Min Chung and Lucas Assun and Chih-Jen Lin", "title":"Decomposition Methods for Linear Support Vector Machines", "venue":"Neural Computation, 16", "year":"2004", "window":"D. J. Spiegelhalter, and C. C. Taylor (1994). Machine Learning, Neural and Statistical Classification. Englewood Cliffs, N.J.: Prentice Hall. Data available at http://www.ncc.up.pt/liacc/ML <b>statlog</b> datasets.html. Osuna, E., R. Freund, and F. Girosi (1997). Training support vector machines: An application to face detection. In Proceedings of CVPR'97, New York, NY, pp. 130--136. IEEE. Platt, J. C.", "mykey":1890},
 {"datasetID":2, "supportID":"BA198BFC0731CA0E9BFE3FF3F9ADC0C5EFBDE590", "rexaID":"beeb203c082359f4e141e1767a14f09449a5a717", "author":"Jie Cheng and Russell Greiner", "title":"Learning Bayesian Belief Network Classifiers: Algorithms and System", "venue":"Canadian Conference on AI", "year":"2001", "window":"used in the experiments. Dataset Attributes. Classes Instances Train Test <b>Adult</b> 13 2 32561 16281 Nursery 8 5 8640 4320 Mushroom 22 2 5416 2708 Chess 36 2 2130 1066 DNA 60 3 2000 1186 The experiments were carried out using our", "mykey":1891},
 {"datasetID":23, "supportID":"BA198BFC0731CA0E9BFE3FF3F9ADC0C5EFBDE590", "rexaID":"beeb203c082359f4e141e1767a14f09449a5a717", "author":"Jie Cheng and Russell Greiner", "title":"Learning Bayesian Belief Network Classifiers: Algorithms and System", "venue":"Canadian Conference on AI", "year":"2001", "window":"Wrapper(multi-net) with ordering = W-MN-O, Wrapper(multi-net) with feature selection = W-MN-FS and Wrapper(multi-net) with feature selection with ordering = W-MNFS-O. The ordering for the <b>Chess</b> data set is the reversed order of the features that appear in the data set since it is more reasonable, the ordering we use for other data sets are simply the order of the features that appear in the data", "mykey":1892},
 {"datasetID":21, "supportID":"BA198BFC0731CA0E9BFE3FF3F9ADC0C5EFBDE590", "rexaID":"beeb203c082359f4e141e1767a14f09449a5a717", "author":"Jie Cheng and Russell Greiner", "title":"Learning Bayesian Belief Network Classifiers: Algorithms and System", "venue":"Canadian Conference on AI", "year":"2001", "window":"Wrapper(multi-net) with ordering = W-MN-O, Wrapper(multi-net) with feature selection = W-MN-FS and Wrapper(multi-net) with feature selection with ordering = W-MNFS-O. The ordering for the <b>Chess</b> data set is the reversed order of the features that appear in the data set since it is more reasonable, the ordering we use for other data sets are simply the order of the features that appear in the data", "mykey":1893},
 {"datasetID":22, "supportID":"BA198BFC0731CA0E9BFE3FF3F9ADC0C5EFBDE590", "rexaID":"beeb203c082359f4e141e1767a14f09449a5a717", "author":"Jie Cheng and Russell Greiner", "title":"Learning Bayesian Belief Network Classifiers: Algorithms and System", "venue":"Canadian Conference on AI", "year":"2001", "window":"Wrapper(multi-net) with ordering = W-MN-O, Wrapper(multi-net) with feature selection = W-MN-FS and Wrapper(multi-net) with feature selection with ordering = W-MNFS-O. The ordering for the <b>Chess</b> data set is the reversed order of the features that appear in the data set since it is more reasonable, the ordering we use for other data sets are simply the order of the features that appear in the data", "mykey":1894},
 {"datasetID":47, "supportID":"BA2656D3FBB0385BA0471FF3F29DDE12B835F6A3", "rexaID":"a32ab1b3da96c9ae515a685b4fcf50e857708f24", "author":"Mukund Deshpande and George Karypis", "title":"Using conjunction of attribute values for classification", "venue":"CIKM", "year":"2002", "window":"We performed our experiments using a 10 way cross validation scheme and computed average accuracy across different runs. We ran our experiments using a support threshold of 1.0% for all the datasets, except hepati, <b>horse</b> where we used a support threshold of 2.0% and for lymph and zoo we used the support threshold of 5.0%. This was done to ensure that the composite features generated are", "mykey":1895},
 {"datasetID":109, "supportID":"BA2656D3FBB0385BA0471FF3F29DDE12B835F6A3", "rexaID":"a32ab1b3da96c9ae515a685b4fcf50e857708f24", "author":"Mukund Deshpande and George Karypis", "title":"Using conjunction of attribute values for classification", "venue":"CIKM", "year":"2002", "window":"7 214 heart 13 0 2 270 hepati 6 13 2 155 horse 7 15 2 368 iris 4 0 3 150 labor 8 8 2 57 led7 0 7 10 3200 lymph 0 18 4 148 pima 8 0 2 768 tic-tac 0 9 2 958 <b>wine</b> 13 0 3 178 zoo 0 16 7 101 Table 1: UCI dataset statistics. We performed our experiments using a 10 way cross validation scheme and computed average accuracy across different runs. We ran our experiments using a support threshold of 1.0% for all", "mykey":1896},
 {"datasetID":111, "supportID":"BA2656D3FBB0385BA0471FF3F29DDE12B835F6A3", "rexaID":"a32ab1b3da96c9ae515a685b4fcf50e857708f24", "author":"Mukund Deshpande and George Karypis", "title":"Using conjunction of attribute values for classification", "venue":"CIKM", "year":"2002", "window":"7 214 heart 13 0 2 270 hepati 6 13 2 155 horse 7 15 2 368 iris 4 0 3 150 labor 8 8 2 57 led7 0 7 10 3200 lymph 0 18 4 148 pima 8 0 2 768 tic-tac 0 9 2 958 wine 13 0 3 178 <b>zoo</b> 0 16 7 101 Table 1: UCI dataset statistics. We performed our experiments using a 10 way cross validation scheme and computed average accuracy across different runs. We ran our experiments using a support threshold of 1.0% for all", "mykey":1897},
 {"datasetID":34, "supportID":"BACDA5F415680E122D8B388966B324F069216840", "rexaID":"26619ba87b875d7e1b11e285a01de5485a24ff88", "author":"Zhihua Zhang and James T. Kwok and Dit-Yan Yeung", "title":"Parametric Distance Metric Learning with Label Information", "venue":"IJCAI", "year":"2003", "window":"(Numbers in bold indicate the better results). data set Euclidean metric learned metric <b>diabetes</b> 459/638 480/638 soybean 37/37 37/37 wine 85/118 117/118 WBC 412/469 446/469 ionosphere 168/251 221/251 iris 107/120 110/120 without changing the clustering", "mykey":1898},
 {"datasetID":58, "supportID":"BAE08FE2873E0FA29D0D150865D45618DEA05FC8", "rexaID":"a8063c7ad67d182a92934e802e1d2e1587a98fe6", "author":"Christophe G. Giraud-Carrier and Tony Martinez", "title":"AN INCREMENTAL LEARNING MODEL FOR COMMONSENSE REASONING", "venue":"Department of Computer Science Brigham Young University", "year":"", "window":"1. If animal has four legs, then animal is a mammal 2. If animal has feathers, then animal is a bird 3. If animal lays eggs, is aquatic, and has fins, then animal is a fish <b>lenses</b> dataset: 1. If patient has low tear production rate, then patient is not fit for contact lenses voting and voting3 datasets: 1. If representative voted 'no' on the 'physician-fee-freeze' issue, then", "mykey":1899},
 {"datasetID":101, "supportID":"BAE08FE2873E0FA29D0D150865D45618DEA05FC8", "rexaID":"a8063c7ad67d182a92934e802e1d2e1587a98fe6", "author":"Christophe G. Giraud-Carrier and Tony Martinez", "title":"AN INCREMENTAL LEARNING MODEL FOR COMMONSENSE REASONING", "venue":"Department of Computer Science Brigham Young University", "year":"", "window":"1. If representative voted 'no' on the 'physician-fee-freeze' issue, then representative is a democrat <b>tic-tac-toe</b> dataset: 1. If top row of grid is 'XXX', then 'X' wins 2. If middle row of grid is 'XXX', then 'X' wins 3. If bottom row of grid is 'XXX', then 'X' wins 4. If top row of grid is 'OOO', then 'O' wins The", "mykey":1900},
 {"datasetID":111, "supportID":"BAE08FE2873E0FA29D0D150865D45618DEA05FC8", "rexaID":"a8063c7ad67d182a92934e802e1d2e1587a98fe6", "author":"Christophe G. Giraud-Carrier and Tony Martinez", "title":"AN INCREMENTAL LEARNING MODEL FOR COMMONSENSE REASONING", "venue":"Department of Computer Science Brigham Young University", "year":"", "window":"cases are reciprocal). APPENDIX B Following are the precepts used in the simulations of Section 4. We give them in terms of features and predicted output. All other attributes are don't-care. <b>zoo</b> dataset: 1. If animal has four legs, then animal is a mammal 2. If animal has feathers, then animal is a bird 3. If animal lays eggs, is aquatic, and has fins, then animal is a fish lenses dataset: 1. If", "mykey":1901},
 {"datasetID":79, "supportID":"BAF9AF8B1F07AFABE39D82C75A46C45CFF91489A", "rexaID":"da335006c9ae6a44b5b680d00b15c00c0e22e0de", "author":"Tao Jiang and Art B. Owen", "title":"Quasi-regression for visualization and interpretation of black box functions", "venue":"Department of Statistics Stanford University", "year":"2002", "window":"is a determination of whether a given woman is diabetic. There are 7 predictors, including medical measurements and personal history. All of the women are <b>Pima</b> <b>Indians</b>  We used the version of this data set found in Ripley (1996). There are @#@# complete cases for training and \u00cf$\u00cf$ for a test set. The number of pregnancies was replaced by \u00d0\u00b1\u00d1$\u00d2##2# \u00a0 number of pregnancies # . Then it and the other", "mykey":1902},
 {"datasetID":5, "supportID":"BB0CAECAC6360753C63756B2BD957B1FB9B8A6D0", "rexaID":"1a387e912f3bd68e66957cdfdebbb068ffc2149a", "author":"Gisele L. Pappa and Alex Alves Freitas and Celso A A Kaestner", "title":"AMultiobjective Genetic Algorithm for Attribute Selection", "venue":"Computing Laboratory Pontificia Universidade Catolica do Parana University of Kent at Canterbury", "year":"", "window":"used in the experiments Data Set # examples # attributes # classes <b>Arrhythmia</b> 452 269 16 Dermatology 366 34 6 Vehicle 846 18 4 Promoters 106 57 2 Ionosphere 351 34 2 Crx 690 15 2 All the experiments were performed with a well-known", "mykey":1903},
 {"datasetID":33, "supportID":"BB0CAECAC6360753C63756B2BD957B1FB9B8A6D0", "rexaID":"1a387e912f3bd68e66957cdfdebbb068ffc2149a", "author":"Gisele L. Pappa and Alex Alves Freitas and Celso A A Kaestner", "title":"AMultiobjective Genetic Algorithm for Attribute Selection", "venue":"Computing Laboratory Pontificia Universidade Catolica do Parana University of Kent at Canterbury", "year":"", "window":"into account the standard deviations. The better performance of the  GA-found  solutions,  by  comparison  with  the  baseline  solution,  is  particularly  significant  in  the  <b>Dermatology</b> and Crx data sets.  Table 3. Results comparing multiobjective FSS with conventional FSS Data Set FSS MOFSS solutions Tree size Error rate Total Dominate Dominated Neutral Arrhythmia 31.4  5.35  37.37   1.42  32.2  ", "mykey":1904},
 {"datasetID":149, "supportID":"BB0CAECAC6360753C63756B2BD957B1FB9B8A6D0", "rexaID":"1a387e912f3bd68e66957cdfdebbb068ffc2149a", "author":"Gisele L. Pappa and Alex Alves Freitas and Celso A A Kaestner", "title":"AMultiobjective Genetic Algorithm for Attribute Selection", "venue":"Computing Laboratory Pontificia Universidade Catolica do Parana University of Kent at Canterbury", "year":"", "window":"none of the MOFSS-found solutions were dominated by the baseline solution (the set of all attributes). In general, MOFSS found more solutions than the GA, except in the <b>Vehicle</b> data set. In 2 data sets (Arrhytmia and Crx) the majority of solutions found by MOFSS dominate the baseline solution. However, in the other 4 data sets the majority of solutions found by MOFSS are neutral.", "mykey":1905},
 {"datasetID":81, "supportID":"BB0D428FDD6709D0601C84D8DBA020326F6F8C9D", "rexaID":"00b6f77ab5353f8974383e14fbef4cd03a846f8a", "author":"Greg Hamerly and Charles Elkan", "title":"Learning the k in k-means", "venue":"NIPS", "year":"2003", "window":"them slow for more than 8 to 12 dimensions. All our code is written in Matlab; X-means is written in C. 3.1 Discovering true clusters in labeled data We tested these algorithms on two real-world datasets for <b>handwritten</b> digit <b>recognition</b>  the NIST dataset [12] and the Pendigits dataset [2]. The goal is to cluster the data without knowledge of the labels and measure how well the clustering captures", "mykey":1906},
 {"datasetID":59, "supportID":"BC2AC20DF2551DDDF69C935BC8276DF6B2841270", "rexaID":"8e5c7f7c811898dd8308bdf011e3bd14e3b43980", "author":"Jaakko Peltonen and Arto Klami and Samuel Kaski", "title":"Improved Learning of Riemannian Metrics for Exploratory Analysis", "venue":"Improved Learning of Riemannian Metrics for Exploratory Analysis. Neural Networks", "year":"2004", "window":"(Table 5). The graph approximation further improved the results on Landsat and <b>Letter Recognition</b> data sets; on the other two sets the difference between the Sammon-L variants was insignificant. Table 5 Indirect measure of the goodness for the Sammon's mappings. Average percentage of correct", "mykey":1907},
 {"datasetID":146, "supportID":"BC2AC20DF2551DDDF69C935BC8276DF6B2841270", "rexaID":"8e5c7f7c811898dd8308bdf011e3bd14e3b43980", "author":"Jaakko Peltonen and Arto Klami and Samuel Kaski", "title":"Improved Learning of Riemannian Metrics for Exploratory Analysis", "venue":"Improved Learning of Riemannian Metrics for Exploratory Analysis. Neural Networks", "year":"2004", "window":"used for empirical comparisons Data set Dimensions Classes Samples <b>Landsat</b> <b>Satellite</b> Data a 36 6 6435 Letter Recognition Data a 16 26 20000 Phoneme Data b 20 13 3656 TIMIT Data from (TIMIT, 1998) 12 41 14994 a from the UCI Machine", "mykey":1908},
 {"datasetID":2, "supportID":"BD333434A79B1E1994E14CF251C6082976FC34F4", "rexaID":"a702c0617d7187cfc988819d79f0eb2a42ba3f19", "author":"Thomas Serafini and G. Zanghirati and Del Zanna and T. Serafini and Gaetano Zanghirati and Luca Zanni", "title":"DIPARTIMENTO DI MATEMATICA", "venue":"Gradient Projection Methods for", "year":"2003", "window":"strategy and evaluate its e\u00aeectiveness on large-scale benchmark problems. All the subsequent computational results are obtained by training Gaussian SVMs on the well known UCI <b>Adult</b>  Web and MNIST data sets [9, 21]; we describe in detail the test problems and the IBM SP4 testing platform in the Appendix. As already observed, the original selection strategy described in [9] falls within Algorithm DT by", "mykey":1909},
 {"datasetID":81, "supportID":"BD333434A79B1E1994E14CF251C6082976FC34F4", "rexaID":"a702c0617d7187cfc988819d79f0eb2a42ba3f19", "author":"Thomas Serafini and G. Zanghirati and Del Zanna and T. Serafini and Gaetano Zanghirati and Luca Zanni", "title":"DIPARTIMENTO DI MATEMATICA", "venue":"Gradient Projection Methods for", "year":"2003", "window":"the largest sized 49749 and a smaller with size 9888. The Gaussian SVM parameters are: C = 5 and \u00be = p 10. . MNIST data set The MNIST database of <b>handwritten</b> <b>digits</b> [12] contains 784-dimensional nonbinary sparse vectors; the size of the database is 60000 and the sparsity level of the inputs is \u00bc 81%. A Gaussian SVM for", "mykey":1910},
 {"datasetID":2, "supportID":"BD4276E3CCDC388CB2A92FC51A140BB8D278978A", "rexaID":"ac8fe867e1d16d4d09f9bd759ba46699055c7ca6", "author":"Yk Huhtala and Juha K\u00e4rkk\u00e4inen and Pasi Porkka and Hannu Toivonen", "title":"Efficient Discovery of Functional and Approximate Dependencies Using Partitions", "venue":"ICDE", "year":"1998", "window":"(shown only in the table). Approximate dependencies could not be discovered in the <b>Adult</b> data set with TANE/MEM due to the lack of main memory. To find out how the number of rows affects the algorithms, we ran a series of experiments with increasing number of rows. The relations were formed by", "mykey":1911},
 {"datasetID":14, "supportID":"BD4276E3CCDC388CB2A92FC51A140BB8D278978A", "rexaID":"ac8fe867e1d16d4d09f9bd759ba46699055c7ca6", "author":"Yk Huhtala and Juha K\u00e4rkk\u00e4inen and Pasi Porkka and Hannu Toivonen", "title":"Efficient Discovery of Functional and Approximate Dependencies Using Partitions", "venue":"ICDE", "year":"1998", "window":"and their descriptions are available on the UCI Machine Learning Repository [13]. The number of rows, columns, and minimal dependencies found (N ) in each database are shown in Table 1. The datasets labeled \"Wisconsin <b>breast</b> <b>cancer</b> Theta n\" are concatenations of n copies of the Wisconsin breast cancer data. The set of dependencies is the same in all of them. To avoid duplicate rows, all", "mykey":1912},
 {"datasetID":17, "supportID":"BD4276E3CCDC388CB2A92FC51A140BB8D278978A", "rexaID":"ac8fe867e1d16d4d09f9bd759ba46699055c7ca6", "author":"Yk Huhtala and Juha K\u00e4rkk\u00e4inen and Pasi Porkka and Hannu Toivonen", "title":"Efficient Discovery of Functional and Approximate Dependencies Using Partitions", "venue":"ICDE", "year":"1998", "window":"and their descriptions are available on the UCI Machine Learning Repository [13]. The number of rows, columns, and minimal dependencies found (N ) in each database are shown in Table 1. The datasets labeled  <b>Wisconsin</b> <b>breast</b> <b>cancer</b> Theta n\" are concatenations of n copies of the Wisconsin breast cancer data. The set of dependencies is the same in all of them. To avoid duplicate rows, all", "mykey":1913},
 {"datasetID":15, "supportID":"BD4276E3CCDC388CB2A92FC51A140BB8D278978A", "rexaID":"ac8fe867e1d16d4d09f9bd759ba46699055c7ca6", "author":"Yk Huhtala and Juha K\u00e4rkk\u00e4inen and Pasi Porkka and Hannu Toivonen", "title":"Efficient Discovery of Functional and Approximate Dependencies Using Partitions", "venue":"ICDE", "year":"1998", "window":"and their descriptions are available on the UCI Machine Learning Repository [13]. The number of rows, columns, and minimal dependencies found (N ) in each database are shown in Table 1. The datasets labeled  <b>Wisconsin</b> <b>breast</b> <b>cancer</b> Theta n\" are concatenations of n copies of the Wisconsin breast cancer data. The set of dependencies is the same in all of them. To avoid duplicate rows, all", "mykey":1914},
 {"datasetID":16, "supportID":"BD4276E3CCDC388CB2A92FC51A140BB8D278978A", "rexaID":"ac8fe867e1d16d4d09f9bd759ba46699055c7ca6", "author":"Yk Huhtala and Juha K\u00e4rkk\u00e4inen and Pasi Porkka and Hannu Toivonen", "title":"Efficient Discovery of Functional and Approximate Dependencies Using Partitions", "venue":"ICDE", "year":"1998", "window":"and their descriptions are available on the UCI Machine Learning Repository [13]. The number of rows, columns, and minimal dependencies found (N ) in each database are shown in Table 1. The datasets labeled  <b>Wisconsin</b> <b>breast</b> <b>cancer</b> Theta n\" are concatenations of n copies of the Wisconsin breast cancer data. The set of dependencies is the same in all of them. To avoid duplicate rows, all", "mykey":1915},
 {"datasetID":23, "supportID":"BD4276E3CCDC388CB2A92FC51A140BB8D278978A", "rexaID":"ac8fe867e1d16d4d09f9bd759ba46699055c7ca6", "author":"Yk Huhtala and Juha K\u00e4rkk\u00e4inen and Pasi Porkka and Hannu Toivonen", "title":"Efficient Discovery of Functional and Approximate Dependencies Using Partitions", "venue":"ICDE", "year":"1998", "window":"out of memory. Table 2 shows performance results for TANE/MEM in the approximate dependency discovery task, for different thresholds ''. Results for the Hepatitis, Wisconsin breast cancer, and <b>Chess</b> data sets are also presented graphically in Figure 3: N '' =N 0 stands for the number of approximate dependencies found relative to the case for functional dependencies; similarly, Time '' =Time 0 denotes", "mykey":1916},
 {"datasetID":21, "supportID":"BD4276E3CCDC388CB2A92FC51A140BB8D278978A", "rexaID":"ac8fe867e1d16d4d09f9bd759ba46699055c7ca6", "author":"Yk Huhtala and Juha K\u00e4rkk\u00e4inen and Pasi Porkka and Hannu Toivonen", "title":"Efficient Discovery of Functional and Approximate Dependencies Using Partitions", "venue":"ICDE", "year":"1998", "window":"out of memory. Table 2 shows performance results for TANE/MEM in the approximate dependency discovery task, for different thresholds ''. Results for the Hepatitis, Wisconsin breast cancer, and <b>Chess</b> data sets are also presented graphically in Figure 3: N '' =N 0 stands for the number of approximate dependencies found relative to the case for functional dependencies; similarly, Time '' =Time 0 denotes", "mykey":1917},
 {"datasetID":22, "supportID":"BD4276E3CCDC388CB2A92FC51A140BB8D278978A", "rexaID":"ac8fe867e1d16d4d09f9bd759ba46699055c7ca6", "author":"Yk Huhtala and Juha K\u00e4rkk\u00e4inen and Pasi Porkka and Hannu Toivonen", "title":"Efficient Discovery of Functional and Approximate Dependencies Using Partitions", "venue":"ICDE", "year":"1998", "window":"out of memory. Table 2 shows performance results for TANE/MEM in the approximate dependency discovery task, for different thresholds ''. Results for the Hepatitis, Wisconsin breast cancer, and <b>Chess</b> data sets are also presented graphically in Figure 3: N '' =N 0 stands for the number of approximate dependencies found relative to the case for functional dependencies; similarly, Time '' =Time 0 denotes", "mykey":1918},
 {"datasetID":46, "supportID":"BD4276E3CCDC388CB2A92FC51A140BB8D278978A", "rexaID":"ac8fe867e1d16d4d09f9bd759ba46699055c7ca6", "author":"Yk Huhtala and Juha K\u00e4rkk\u00e4inen and Pasi Porkka and Hannu Toivonen", "title":"Efficient Discovery of Functional and Approximate Dependencies Using Partitions", "venue":"ICDE", "year":"1998", "window":"out of memory. Table 2 shows performance results for TANE/MEM in the approximate dependency discovery task, for different thresholds ''. Results for the <b>Hepatitis</b>  Wisconsin breast cancer, and Chess data sets are also presented graphically in Figure 3: N '' =N 0 stands for the number of approximate dependencies found relative to the case for functional dependencies; similarly, Time '' =Time 0 denotes", "mykey":1919},
 {"datasetID":63, "supportID":"BD4276E3CCDC388CB2A92FC51A140BB8D278978A", "rexaID":"ac8fe867e1d16d4d09f9bd759ba46699055c7ca6", "author":"Yk Huhtala and Juha K\u00e4rkk\u00e4inen and Pasi Porkka and Hannu Toivonen", "title":"Efficient Discovery of Functional and Approximate Dependencies Using Partitions", "venue":"ICDE", "year":"1998", "window":"decreases slightly (Wisconsin breast cancer), or drops significantly (Hepatitis). The drop is even stronger with the <b>Lymphography</b> data set (shown only in the table). Approximate dependencies could not be discovered in the Adult data set with TANE/MEM due to the lack of main memory. To find out how the number of rows affects the", "mykey":1920},
 {"datasetID":32, "supportID":"BD7CF5AB5B6D4C7C464135A17E1725325B1EB550", "rexaID":"9755b662d1579511b54a4b2c69eea04c30c2142c", "author":"Juan J. Rodr##guez and Carlos J. Alonso and Henrik Bostrom", "title":"Learning First Order Logic Time Series Classifiers: Rules and Boosting", "venue":"Grupo de Sistemas Inteligentes, Departamento de Inform#atica Universidad de Valladolid, Spain", "year":"", "window":"for classification of time series are not easy to find [12]. For this reason we have used four artificial datasets and only one \real world\" dataset: { <b>Cylinder</b>  Bell and Funnel (CBF). This is an artificial problem [12], in which there are there are 3 classes: cylinder, bell and funnel. Figure 2.a shows two", "mykey":1921},
 {"datasetID":107, "supportID":"BD7CF5AB5B6D4C7C464135A17E1725325B1EB550", "rexaID":"9755b662d1579511b54a4b2c69eea04c30c2142c", "author":"Juan J. Rodr##guez and Carlos J. Alonso and Henrik Bostrom", "title":"Learning First Order Logic Time Series Classifiers: Rules and Boosting", "venue":"Grupo de Sistemas Inteligentes, Departamento de Inform#atica Universidad de Valladolid, Spain", "year":"", "window":"[1]. Figure 2.b shows some examples of three of the classes. The data used were obtained from the UCI KDD Archive [4]. The number of examples is 600, with 60 points in each series. { <b>Waveform</b>  This dataset was introduced by [9]. We used the version from the UCI ML Repository [7]. The number of examples is 900 and the number of points in each series is 21. { Wave + Noise. This dataset was generated in", "mykey":1922},
 {"datasetID":108, "supportID":"BD7CF5AB5B6D4C7C464135A17E1725325B1EB550", "rexaID":"9755b662d1579511b54a4b2c69eea04c30c2142c", "author":"Juan J. Rodr##guez and Carlos J. Alonso and Henrik Bostrom", "title":"Learning First Order Logic Time Series Classifiers: Rules and Boosting", "venue":"Grupo de Sistemas Inteligentes, Departamento de Inform#atica Universidad de Valladolid, Spain", "year":"", "window":"[1]. Figure 2.b shows some examples of three of the classes. The data used were obtained from the UCI KDD Archive [4]. The number of examples is 600, with 60 points in each series. { <b>Waveform</b>  This dataset was introduced by [9]. We used the version from the UCI ML Repository [7]. The number of examples is 900 and the number of points in each series is 21. { Wave + Noise. This dataset was generated in", "mykey":1923},
 {"datasetID":34, "supportID":"BE80C65A6EC7C2ACF49476DBB8C6C3D57E55A757", "rexaID":"4e69e64b6902aa2a91b5cda5a69eb6b030ac12c4", "author":"Adil M. Bagirov and John Yearwood", "title":"A new nonsmooth optimization algorithm for clustering", "venue":"Centre for Informatics and Applied Optimization, School of Information Technology and Mathematical Sciences, University of Ballarat", "year":"", "window":"13 768/768 274.73 1.0 13 389/768 115.44 1.5 14 283/768 113.64 2.0 13 215/768 66.59 4.0 11 94/768 18.45 6.0 5 52/768 2.31 8.0 5 38/768 2.02 The results presented in Table 1 show that for the <b>diabetes</b> data set we can take c 2 [0, 4]. Further decrease of c leads to sharp changes in the cluster structure of the data set. We can see that there are dierences in the number of clusters when c 2 [0, 4]. But for", "mykey":1924},
 {"datasetID":45, "supportID":"BE80C65A6EC7C2ACF49476DBB8C6C3D57E55A757", "rexaID":"4e69e64b6902aa2a91b5cda5a69eb6b030ac12c4", "author":"Adil M. Bagirov and John Yearwood", "title":"A new nonsmooth optimization algorithm for clustering", "venue":"Centre for Informatics and Applied Optimization, School of Information Technology and Mathematical Sciences, University of Ballarat", "year":"", "window":"small clusters. We can also see that for c 2 [1.5, 4] the number of instances and CPU time reduce significantly. The results presented in Table 2 show that appropriate values for the <b>heart</b> disease data set are c 2 [0, 1.5], because further decrease in c leads to changes in the cluster structure of the data set. We can again see that these values of c allow significant reduction in the number of", "mykey":1925},
 {"datasetID":50, "supportID":"BE80C65A6EC7C2ACF49476DBB8C6C3D57E55A757", "rexaID":"4e69e64b6902aa2a91b5cda5a69eb6b030ac12c4", "author":"Adil M. Bagirov and John Yearwood", "title":"A new nonsmooth optimization algorithm for clustering", "venue":"Centre for Informatics and Applied Optimization, School of Information Technology and Mathematical Sciences, University of Ballarat", "year":"", "window":"some databases with known classes. We used the diabetes, liver disorder, heart disease, breast cancer, vehicles, synthetic, pen-based recognition of handwritten digits (PBRHD) and <b>image segmentation</b> data sets in numerical experiments. Descriptions of these data sets can be found in [24]. First, we normalized all features. This was done by a nonsingular matrix so that mean values of all features were 1.", "mykey":1926},
 {"datasetID":60, "supportID":"BE80C65A6EC7C2ACF49476DBB8C6C3D57E55A757", "rexaID":"4e69e64b6902aa2a91b5cda5a69eb6b030ac12c4", "author":"Adil M. Bagirov and John Yearwood", "title":"A new nonsmooth optimization algorithm for clustering", "venue":"Centre for Informatics and Applied Optimization, School of Information Technology and Mathematical Sciences, University of Ballarat", "year":"", "window":"that these values of c allow significant reduction in the number of instances and CPU time. From the results presented in Table 3 we can conclude that appropriate values of c for the <b>liver</b> disorder data set are c 2 [0, 2]. Dierences in the number of clusters when c 2 [0, 2] arise because of small clusters which contain less than 5 % of all instances. Thus using results of numerical experiments on these", "mykey":1927},
 {"datasetID":81, "supportID":"BE80C65A6EC7C2ACF49476DBB8C6C3D57E55A757", "rexaID":"4e69e64b6902aa2a91b5cda5a69eb6b030ac12c4", "author":"Adil M. Bagirov and John Yearwood", "title":"A new nonsmooth optimization algorithm for clustering", "venue":"Centre for Informatics and Applied Optimization, School of Information Technology and Mathematical Sciences, University of Ballarat", "year":"", "window":"of all features were 1. In numerical experiments we take = 10 -2 and the initial number of clusters q 0 = 2 with c = 1.5 for pen-based recognition of <b>handwritten</b> <b>digits</b> and the image segmentation data sets whereas for all others we used the entire data set. First we applied Algorithm 3.1 to calculate clusters. Then the k-means algorithm was applied with the same number of clusters as calculated by", "mykey":1928},
 {"datasetID":147, "supportID":"BE80C65A6EC7C2ACF49476DBB8C6C3D57E55A757", "rexaID":"4e69e64b6902aa2a91b5cda5a69eb6b030ac12c4", "author":"Adil M. Bagirov and John Yearwood", "title":"A new nonsmooth optimization algorithm for clustering", "venue":"Centre for Informatics and Applied Optimization, School of Information Technology and Mathematical Sciences, University of Ballarat", "year":"", "window":"some databases with known classes. We used the diabetes, liver disorder, heart disease, breast cancer, vehicles, synthetic, pen-based recognition of handwritten digits (PBRHD) and <b>image segmentation</b> data sets in numerical experiments. Descriptions of these data sets can be found in [24]. First, we normalized all features. This was done by a nonsingular matrix so that mean values of all features were 1.", "mykey":1929},
 {"datasetID":14, "supportID":"BEE1FB17A8C6610F46EC10FE3F8CF67F5242EAA6", "rexaID":"4695569c53cd581fcc193415a8a94a1f92abf607", "author":"Chotirat Ann and Dimitrios Gunopulos", "title":"Scaling up the Naive Bayesian Classifier: Using Decision Trees for Feature Selection", "venue":"Computer Science Department University of California", "year":"", "window":"19 classes. Attributes selected by SBC = 12. Wisconsin <b>Breast</b> <b>Cancer</b> 75 80 85 90 95 100 10203040506070809099 Training Data (%) Accuracy (%) NBC SBC C4.5 Figure 10. Wisconsin Breast Cancer dataset. 699 instances, 9 attributes, 2 classes. Attributes selected by SBC = 4. Congressional Voting Records 80 85 90 95 100 10203040506070809099 Training Data (%) Accuracy (%) NBC SBC C4.5 Figure 11.", "mykey":1930},
 {"datasetID":17, "supportID":"BEE1FB17A8C6610F46EC10FE3F8CF67F5242EAA6", "rexaID":"4695569c53cd581fcc193415a8a94a1f92abf607", "author":"Chotirat Ann and Dimitrios Gunopulos", "title":"Scaling up the Naive Bayesian Classifier: Using Decision Trees for Feature Selection", "venue":"Computer Science Department University of California", "year":"", "window":"19 classes. Attributes selected by SBC = 12. <b>Wisconsin</b> <b>Breast</b> <b>Cancer</b> 75 80 85 90 95 100 10203040506070809099 Training Data (%) Accuracy (%) NBC SBC C4.5 Figure 10. Wisconsin Breast Cancer dataset. 699 instances, 9 attributes, 2 classes. Attributes selected by SBC = 4. Congressional Voting Records 80 85 90 95 100 10203040506070809099 Training Data (%) Accuracy (%) NBC SBC C4.5 Figure 11.", "mykey":1931},
 {"datasetID":15, "supportID":"BEE1FB17A8C6610F46EC10FE3F8CF67F5242EAA6", "rexaID":"4695569c53cd581fcc193415a8a94a1f92abf607", "author":"Chotirat Ann and Dimitrios Gunopulos", "title":"Scaling up the Naive Bayesian Classifier: Using Decision Trees for Feature Selection", "venue":"Computer Science Department University of California", "year":"", "window":"19 classes. Attributes selected by SBC = 12. <b>Wisconsin</b> <b>Breast</b> <b>Cancer</b> 75 80 85 90 95 100 10203040506070809099 Training Data (%) Accuracy (%) NBC SBC C4.5 Figure 10. Wisconsin Breast Cancer dataset. 699 instances, 9 attributes, 2 classes. Attributes selected by SBC = 4. Congressional Voting Records 80 85 90 95 100 10203040506070809099 Training Data (%) Accuracy (%) NBC SBC C4.5 Figure 11.", "mykey":1932},
 {"datasetID":16, "supportID":"BEE1FB17A8C6610F46EC10FE3F8CF67F5242EAA6", "rexaID":"4695569c53cd581fcc193415a8a94a1f92abf607", "author":"Chotirat Ann and Dimitrios Gunopulos", "title":"Scaling up the Naive Bayesian Classifier: Using Decision Trees for Feature Selection", "venue":"Computer Science Department University of California", "year":"", "window":"19 classes. Attributes selected by SBC = 12. <b>Wisconsin</b> <b>Breast</b> <b>Cancer</b> 75 80 85 90 95 100 10203040506070809099 Training Data (%) Accuracy (%) NBC SBC C4.5 Figure 10. Wisconsin Breast Cancer dataset. 699 instances, 9 attributes, 2 classes. Attributes selected by SBC = 4. Congressional Voting Records 80 85 90 95 100 10203040506070809099 Training Data (%) Accuracy (%) NBC SBC C4.5 Figure 11.", "mykey":1933},
 {"datasetID":105, "supportID":"BEE1FB17A8C6610F46EC10FE3F8CF67F5242EAA6", "rexaID":"4695569c53cd581fcc193415a8a94a1f92abf607", "author":"Chotirat Ann and Dimitrios Gunopulos", "title":"Scaling up the Naive Bayesian Classifier: Using Decision Trees for Feature Selection", "venue":"Computer Science Department University of California", "year":"", "window":"9 attributes, 2 classes. Attributes selected by SBC = 4. Congressional Voting Records 80 85 90 95 100 10203040506070809099 Training Data (%) Accuracy (%) NBC SBC C4.5 Figure 11. Congressional <b>Voting dataset</b>  435 instances, 16 attributes, 2 classes. Attributes selected by SBC = 3. To see a clearer picture on the SBC performance, we did an experiment on the same set of data using the Augmented Bayesian", "mykey":1934},
 {"datasetID":39, "supportID":"BEE1FB17A8C6610F46EC10FE3F8CF67F5242EAA6", "rexaID":"4695569c53cd581fcc193415a8a94a1f92abf607", "author":"Chotirat Ann and Dimitrios Gunopulos", "title":"Scaling up the Naive Bayesian Classifier: Using Decision Trees for Feature Selection", "venue":"Computer Science Department University of California", "year":"", "window":"from the UCI databases, 5 of which Na\u00efve Bayesian classifier outperforms C4.5 and the other 5 of which C4.5 outperforms Na\u00efve Bayesian classifier. Table 1. Descriptions of domains used Dataset #Attributes #Classes #Instances <b>Ecoli</b> 8 8 336 GermanCredit 20 2 1,000 KrVsKp 37 2 3,198 Monk 6 2 554 Mushroom 22 2 8,124 Pima 8 2 768 Promoter 57 2 106 Soybean 35 19 307 Wisconsin 9 2 699 Vote 16 2", "mykey":1935},
 {"datasetID":67, "supportID":"BEE1FB17A8C6610F46EC10FE3F8CF67F5242EAA6", "rexaID":"4695569c53cd581fcc193415a8a94a1f92abf607", "author":"Chotirat Ann and Dimitrios Gunopulos", "title":"Scaling up the Naive Bayesian Classifier: Using Decision Trees for Feature Selection", "venue":"Computer Science Department University of California", "year":"", "window":"attributes, 2 classes. Attributes selected by SBC = 5. <b>Promoter</b> Gene Sequences 40 50 60 70 80 90 100 10 20 30 40 50 60 70 80 90 99 Training Data (%) Accuracy (%) NBC SBC C4.5 Figure 8. Gene Promoter dataset. 106 instances, 57 attributes, 2 classes. Attributes selected by SBC = 5. Soybean 30 40 50 60 70 80 90 100 10203040506070809099 Training Data (%) Accuracy (%) NBC SBC C4.5 Figure 9. Soybean-large", "mykey":1936},
 {"datasetID":70, "supportID":"BEE1FB17A8C6610F46EC10FE3F8CF67F5242EAA6", "rexaID":"4695569c53cd581fcc193415a8a94a1f92abf607", "author":"Chotirat Ann and Dimitrios Gunopulos", "title":"Scaling up the Naive Bayesian Classifier: Using Decision Trees for Feature Selection", "venue":"Computer Science Department University of California", "year":"", "window":"3,198 instances, 37 attributes, 2classes. Attributes selected by SBC = 4. <b>Monk</b> 85 88 91 94 97 100 10203040506070809099 Training Data NBC SBC C4.5 Figure 5. Monk dataset (prob.3). 554 instances, 6 attributes, 2 classes. Attributes selected by SBC = 4. Mushroom 90 92 94 96 98 100 10 20 30 40 50 60 70 80 90 99 Training Data (%) Accuracy (%) NBC SBC C4.5 Figure 6.", "mykey":1937},
 {"datasetID":73, "supportID":"BEE1FB17A8C6610F46EC10FE3F8CF67F5242EAA6", "rexaID":"4695569c53cd581fcc193415a8a94a1f92abf607", "author":"Chotirat Ann and Dimitrios Gunopulos", "title":"Scaling up the Naive Bayesian Classifier: Using Decision Trees for Feature Selection", "venue":"Computer Science Department University of California", "year":"", "window":"554 instances, 6 attributes, 2 classes. Attributes selected by SBC = 4. <b>Mushroom</b> 90 92 94 96 98 100 10 20 30 40 50 60 70 80 90 99 Training Data (%) Accuracy (%) NBC SBC C4.5 Figure 6. Mushroom dataset. 8,124 instances, 22 attributes, 2 classes. Attributes selected by SBC = 6. Pima Indians Diabetes 60 65 70 75 80 85 10203040506070809099 Training Data (%) Accuracy (%) NBC SBC C4.5 Figure 7.", "mykey":1938},
 {"datasetID":79, "supportID":"BEE1FB17A8C6610F46EC10FE3F8CF67F5242EAA6", "rexaID":"4695569c53cd581fcc193415a8a94a1f92abf607", "author":"Chotirat Ann and Dimitrios Gunopulos", "title":"Scaling up the Naive Bayesian Classifier: Using Decision Trees for Feature Selection", "venue":"Computer Science Department University of California", "year":"", "window":"instances, 22 attributes, 2 classes. Attributes selected by SBC = 6. <b>Pima</b> <b>Indians</b> Diabetes 60 65 70 75 80 85 10203040506070809099 Training Data (%) Accuracy (%) NBC SBC C4.5 Figure 7. Pima-Indians dataset. 768 instances, 8 attributes, 2 classes. Attributes selected by SBC = 5. Promoter Gene Sequences 40 50 60 70 80 90 100 10 20 30 40 50 60 70 80 90 99 Training Data (%) Accuracy (%) NBC SBC C4.5", "mykey":1939},
 {"datasetID":90, "supportID":"BEE1FB17A8C6610F46EC10FE3F8CF67F5242EAA6", "rexaID":"4695569c53cd581fcc193415a8a94a1f92abf607", "author":"Chotirat Ann and Dimitrios Gunopulos", "title":"Scaling up the Naive Bayesian Classifier: Using Decision Trees for Feature Selection", "venue":"Computer Science Department University of California", "year":"", "window":"106 instances, 57 attributes, 2 classes. Attributes selected by SBC = 5. <b>Soybean</b> 30 40 50 60 70 80 90 100 10203040506070809099 Training Data (%) Accuracy (%) NBC SBC C4.5 Figure 9. Soybean-large dataset. 307 instances, 35 attributes, 19 classes. Attributes selected by SBC = 12. Wisconsin Breast Cancer 75 80 85 90 95 100 10203040506070809099 Training Data (%) Accuracy (%) NBC SBC C4.5 Figure 10.", "mykey":1940},
 {"datasetID":91, "supportID":"BEE1FB17A8C6610F46EC10FE3F8CF67F5242EAA6", "rexaID":"4695569c53cd581fcc193415a8a94a1f92abf607", "author":"Chotirat Ann and Dimitrios Gunopulos", "title":"Scaling up the Naive Bayesian Classifier: Using Decision Trees for Feature Selection", "venue":"Computer Science Department University of California", "year":"", "window":"106 instances, 57 attributes, 2 classes. Attributes selected by SBC = 5. <b>Soybean</b> 30 40 50 60 70 80 90 100 10203040506070809099 Training Data (%) Accuracy (%) NBC SBC C4.5 Figure 9. Soybean-large dataset. 307 instances, 35 attributes, 19 classes. Attributes selected by SBC = 12. Wisconsin Breast Cancer 75 80 85 90 95 100 10203040506070809099 Training Data (%) Accuracy (%) NBC SBC C4.5 Figure 10.", "mykey":1941},
 {"datasetID":144, "supportID":"BEE1FB17A8C6610F46EC10FE3F8CF67F5242EAA6", "rexaID":"4695569c53cd581fcc193415a8a94a1f92abf607", "author":"Chotirat Ann and Dimitrios Gunopulos", "title":"Scaling up the Naive Bayesian Classifier: Using Decision Trees for Feature Selection", "venue":"Computer Science Department University of California", "year":"", "window":"336 instances, 8 attributes, 8 classes. Attributes selected by SBC = 4. <b>German Credit</b> 64 66 68 70 72 74 76 78 10203040506070809099 Training Data (%) Accuracy (%) NBC SBC C4.5 Figure 3. German Credit dataset. 1,000 instances, 20 attributes, 2 classes. Attributes selected by SBC = 6. Chess Endgames (kr-vs-kp) 75 80 85 90 95 100 10 20 30 40 50 60 70 80 90 99 Training Data (%) Accuracy (%) NBC SBC C4.5", "mykey":1942},
 {"datasetID":45, "supportID":"BEFF54AB395765FF766DE728EDC8274A81B83D02", "rexaID":"a56eb6b369952a5e696937ad2d432212563846ba", "author":"Jinyan Li and Xiuzhen Zhang and Guozhu Dong and Kotagiri Ramamohanarao and Qun Sun", "title":"Efficient Mining of High Confidience Association Rules without Support Thresholds", "venue":"PKDD", "year":"1999", "window":"rules and some very high (say 90%) confidence rules using approaches similar to mining top rules. Experimental results using the Mushroom, the Cleveland <b>heart</b> disease, and the Boston housing datasets are reported to evaluate the efficiency of the proposed approach. 1 Introduction Association rules [1] were proposed to capture significant dependence between items in transactional datasets. For", "mykey":1943},
 {"datasetID":48, "supportID":"BEFF54AB395765FF766DE728EDC8274A81B83D02", "rexaID":"a56eb6b369952a5e696937ad2d432212563846ba", "author":"Jinyan Li and Xiuzhen Zhang and Guozhu Dong and Kotagiri Ramamohanarao and Qun Sun", "title":"Efficient Mining of High Confidience Association Rules without Support Thresholds", "venue":"PKDD", "year":"1999", "window":"rules and some very high (say 90%) confidence rules using approaches similar to mining top rules. Experimental results using the Mushroom, the Cleveland heart disease, and the Boston <b>housing</b> datasets are reported to evaluate the efficiency of the proposed approach. 1 Introduction Association rules [1] were proposed to capture significant dependence between items in transactional datasets. For", "mykey":1944},
 {"datasetID":73, "supportID":"BEFF54AB395765FF766DE728EDC8274A81B83D02", "rexaID":"a56eb6b369952a5e696937ad2d432212563846ba", "author":"Jinyan Li and Xiuzhen Zhang and Guozhu Dong and Kotagiri Ramamohanarao and Qun Sun", "title":"Efficient Mining of High Confidience Association Rules without Support Thresholds", "venue":"PKDD", "year":"1999", "window":"we do not enumerate all top rules; we use borders [4] to succinctly represent them instead. The significance of this representation is highlighted in the experimental results of <b>Mushroom</b> dataset, where there exist a huge number of top rules. In addition to top rules, we also address the problems of mining zero-confidence rules and mining very high (say ###\"$%& ) confidence rules with", "mykey":1945},
 {"datasetID":57, "supportID":"BFB5D79BD4B8F7AC8091340BB67360AD476D6018", "rexaID":"e091a00eb65fd6124d7b0190126299db6ee8057e", "author":"Xavier Llor and David E. Goldberg", "title":"Minimal Achievable Error in the LED", "venue":"Illinois Genetic Algorithms Laboratory University of Illinois at Urbana-Champaign", "year":"2002", "window":"S. Mathews Ave, Urbana, IL 61801 {llora@illigal.ge.uiuc.edu, deg@uiuc.edu} Abstract This paper presents a theoretical model to predict the minimal achievable error, given a noise ratio #, in the <b>LED data</b> set problem. The motivation for developing this theoretical model is to understand and explain some of the results that different systems achieve when they solve the LED problem. Moreover, given a new", "mykey":1946},
 {"datasetID":45, "supportID":"BFF061C2A090F56E0296A7D094B318D76F1EC63B", "rexaID":"a11c91c5cf02794b7d1d4bdc0222b3c11dce9cac", "author":"David Page and Soumya Ray", "title":"Skewing: An Efficient Alternative to Lookahead for Decision Tree Induction", "venue":"IJCAI", "year":"2003", "window":"Skewing ID3, No Skewing Figure 8: Five-Variable Hard Targets 50 60 70 80 90 100 200 400 600 800 1000 Accuracy (%) Sample Size ID3 with Skewing ID3, No Skewing Figure 9: Six-Variable Hard Targets Data Set Standard ID3 ID3 with Skewing <b>Heart</b> 71.9 74.5 Voting 94.0 94.2 Voting-2 87.4 88.6 Contra 60.4 61.5 Monks-1 92.6 100.0 Monks-2 86.5 89.3 Monks-3 89.8 91.7 Table 4: Accuracies of ID3 and ID3 with", "mykey":1947},
 {"datasetID":53, "supportID":"C03B759A335C2780ACC82C216D32CF6E1BC2D00A", "rexaID":"cbfc5b79f03770a32505b3342b2e330a1626be7d", "author":"Wl/odzisl/aw Duch and Rafal Adamczak and Krzysztof Grabczewski", "title":"Extraction of crisp logical rules using constrained backpropagation networks", "venue":"Department of Computer Methods, Nicholas Copernicus University", "year":"", "window":"a few cases. The final solution may be presented as a set of rules or as a network of nodes performing logical functions. III. Three examples A. <b>Iris</b> data In the first example the classical Iris dataset was used (all datasets were taken from the UCI machine learning repository [9]). The data has 150 vectors evenly distributed in three classes, called iris-setosa, iris-versicolor and iris-virginica.", "mykey":1948},
 {"datasetID":73, "supportID":"C03B759A335C2780ACC82C216D32CF6E1BC2D00A", "rexaID":"cbfc5b79f03770a32505b3342b2e330a1626be7d", "author":"Wl/odzisl/aw Duch and Rafal Adamczak and Krzysztof Grabczewski", "title":"Extraction of crisp logical rules using constrained backpropagation networks", "venue":"Department of Computer Methods, Nicholas Copernicus University", "year":"", "window":"and four antecedents. We have also tried to derive rules using only 10% of cases for training, achieving identical results. This is the simplest systematic logical description of the <b>mushroom</b> dataset that we know of, although some of these rules have probably been also found by RULEX and TREX algorithms [1]. Analysis of the graph representing possible contributions of the relevant attributes to", "mykey":1949},
 {"datasetID":68, "supportID":"C0B2104BE51DC15544C3B7DE80FCFD568019ABDD", "rexaID":"1b69a36a9cff9bb19b533eff75b2a9924e4a73b2", "author":"Steven Eschrich and Nitesh V. Chawla and Lawrence O. Hall", "title":"Generalization Methods in Bioinformatics", "venue":"BIOKDD", "year":"2002", "window":"Wealsoinvestigate the abilityofover-generalization in each classifier of an ensemble to more accurately predict the non-homologous structures seen within the <b>protein secondary</b> structure prediction dataset. Several key decisions must be made with regard to the ensemble of subsamples algorithm. Random subsampling of the dataset can be done with or without replacement. Subsampling with replacement is", "mykey":1950},
 {"datasetID":78, "supportID":"C0B2104BE51DC15544C3B7DE80FCFD568019ABDD", "rexaID":"1b69a36a9cff9bb19b533eff75b2a9924e4a73b2", "author":"Steven Eschrich and Nitesh V. Chawla and Lawrence O. Hall", "title":"Generalization Methods in Bioinformatics", "venue":"BIOKDD", "year":"2002", "window":"from the UCI Machine Learning Repository [3]. The datasets chosen were <b>page block</b>  pendigits, satimage and letter. The letter dataset was the largest with 20,000 examples. Atenfold cross-validation was performed for each combination of subsample size and", "mykey":1951},
 {"datasetID":154, "supportID":"C0B2104BE51DC15544C3B7DE80FCFD568019ABDD", "rexaID":"1b69a36a9cff9bb19b533eff75b2a9924e4a73b2", "author":"Steven Eschrich and Nitesh V. Chawla and Lawrence O. Hall", "title":"Generalization Methods in Bioinformatics", "venue":"BIOKDD", "year":"2002", "window":"Wealsoinvestigate the abilityofover-generalization in each classifier of an ensemble to more accurately predict the non-homologous structures seen within the <b>protein</b> secondary structure prediction dataset. Several key decisions must be made with regard to the ensemble of subsamples algorithm. Random subsampling of the dataset can be done with or without replacement. Subsampling with replacement is", "mykey":1952},
 {"datasetID":1, "supportID":"C1664239D276062B38DB65251983020A1C4B4AA1", "rexaID":"1b77c2b6fd8a261af286cf411879f9f520824bd6", "author":"Marc Sebban and Richard Nock and St\u00e9phane Lallich", "title":"Stopping Criterion for Boosting-Based Data Reduction Techniques: from Binary to Multiclass Problem", "venue":"Journal of Machine Learning Research, 3", "year":"2002", "window":"are negative (-1). When a class j is tested against the others, the current label of the instance e is the value M(y(e); j), where y(e) 2 f1; 2; ::; cg. Table 4: Multiclass classification problems. Dataset # classes jLSj # features Waves 3 500 21 <b>Abalone</b> 3 1000 8 Glass 6 214 9 Balance 3 625 4 Iris 3 150 4 Led 10 500 7 Led+17 10 500 24 Dermatology 6 366 34 18 Stopping criterion for boosting 1 2 3 4 5 6", "mykey":1953},
 {"datasetID":19, "supportID":"C1664239D276062B38DB65251983020A1C4B4AA1", "rexaID":"1b77c2b6fd8a261af286cf411879f9f520824bd6", "author":"Marc Sebban and Richard Nock and St\u00e9phane Lallich", "title":"Stopping Criterion for Boosting-Based Data Reduction Techniques: from Binary to Multiclass Problem", "venue":"Journal of Machine Learning Research, 3", "year":"2002", "window":"(Balance, Echocardiogram, German, Horse Colic, Led, Pima and Vehicle) see important improvements, ranging from 1% to } 5%. In contrast, only one dataset sees significant accuracy decrease  <b>Car</b>  96.0% vs. 93.9%). 9. Conclusions and Future Research This paper explores a method for prototype selection based on boosting, and gives statistical criteria", "mykey":1954},
 {"datasetID":38, "supportID":"C1664239D276062B38DB65251983020A1C4B4AA1", "rexaID":"1b77c2b6fd8a261af286cf411879f9f520824bd6", "author":"Marc Sebban and Richard Nock and St\u00e9phane Lallich", "title":"Stopping Criterion for Boosting-Based Data Reduction Techniques: from Binary to Multiclass Problem", "venue":"Journal of Machine Learning Research, 3", "year":"2002", "window":"a weighted decision rule provides better results than the unweighted rule. Among them, 7 datasets (Balance, <b>Echocardiogram</b>  German, Horse Colic, Led, Pima and Vehicle) see important improvements, ranging from 1% to } 5%. In contrast, only one dataset sees significant accuracy decrease (Car,", "mykey":1955},
 {"datasetID":47, "supportID":"C1664239D276062B38DB65251983020A1C4B4AA1", "rexaID":"1b77c2b6fd8a261af286cf411879f9f520824bd6", "author":"Marc Sebban and Richard Nock and St\u00e9phane Lallich", "title":"Stopping Criterion for Boosting-Based Data Reduction Techniques: from Binary to Multiclass Problem", "venue":"Journal of Machine Learning Research, 3", "year":"2002", "window":"a weighted decision rule provides better results than the unweighted rule. Among them, 7 datasets (Balance, Echocardiogram, German, <b>Horse</b> <b>Colic</b>  Led, Pima and Vehicle) see important improvements, ranging from 1% to } 5%. In contrast, only one dataset sees significant accuracy decrease (Car,", "mykey":1956},
 {"datasetID":73, "supportID":"C194A1AF2FD5BC1896D3A2CF3ACE5680D6D5ACE8", "rexaID":"535ca91c09f86e524fbd9691fb920e89528e9de7", "author":"Kiri Wagstaff and Claire Cardie", "title":"Clustering with Instance-level Constraints", "venue":"ICML", "year":"2000", "window":"reflects its inherent class structure, i.e. to create one cluster per class. Talavera and B#ejar (1999), for example, use this model to place instances from the <b>mushroom</b> UCI (Blake & Merz, 1998) data set into either a poisonous\" or an edible\" cluster. We focus here on the latter model and propose the use of constraints that express information about the underlying class structure, thereby enabling", "mykey":1957},
 {"datasetID":90, "supportID":"C194A1AF2FD5BC1896D3A2CF3ACE5680D6D5ACE8", "rexaID":"535ca91c09f86e524fbd9691fb920e89528e9de7", "author":"Kiri Wagstaff and Claire Cardie", "title":"Clustering with Instance-level Constraints", "venue":"ICML", "year":"2000", "window":"To test the effect of incorporating constraints, we selected three data sets from the UCI repository  <b>soybean</b>  mushroom, and tictactoe) and a fourth \real-world\" data set, pos. # soybean refers to the soybean-small data set, which consists of 47 instances with 34 nominal", "mykey":1958},
 {"datasetID":91, "supportID":"C194A1AF2FD5BC1896D3A2CF3ACE5680D6D5ACE8", "rexaID":"535ca91c09f86e524fbd9691fb920e89528e9de7", "author":"Kiri Wagstaff and Claire Cardie", "title":"Clustering with Instance-level Constraints", "venue":"ICML", "year":"2000", "window":"To test the effect of incorporating constraints, we selected three data sets from the UCI repository  <b>soybean</b>  mushroom, and tictactoe) and a fourth \real-world\" data set, pos. # soybean refers to the soybean-small data set, which consists of 47 instances with 34 nominal", "mykey":1959},
 {"datasetID":14, "supportID":"C236AE947E41B2A044C181842DF6842A435DF558", "rexaID":"53ac23f963b3607aae9580b356e6b236d2955314", "author":"Wl/odzisl/aw Duch and Rafal/ Adamczak Email:duchraad@phys. uni. torun. pl", "title":"Statistical methods for construction of neural networks", "venue":"Department of Computer Methods, Nicholas Copernicus University", "year":"", "window":"p i (x) - p r (x) around x for which the two distributions cross. The simplest network constructed from FDA solution gives classification error which is as good as the original FDA. For such datasets [12] as Wisconsin <b>breast</b> <b>cancer</b>  hepatitis, Cleveland heart disease or diabetes the network obtains better results already before the learning process starts, but for some datasets this is not the", "mykey":1960},
 {"datasetID":17, "supportID":"C236AE947E41B2A044C181842DF6842A435DF558", "rexaID":"53ac23f963b3607aae9580b356e6b236d2955314", "author":"Wl/odzisl/aw Duch and Rafal/ Adamczak Email:duchraad@phys. uni. torun. pl", "title":"Statistical methods for construction of neural networks", "venue":"Department of Computer Methods, Nicholas Copernicus University", "year":"", "window":"p i (x) - p r (x) around x for which the two distributions cross. The simplest network constructed from FDA solution gives classification error which is as good as the original FDA. For such datasets [12] as <b>Wisconsin</b> <b>breast</b> <b>cancer</b>  hepatitis, Cleveland heart disease or diabetes the network obtains better results already before the learning process starts, but for some datasets this is not the", "mykey":1961},
 {"datasetID":15, "supportID":"C236AE947E41B2A044C181842DF6842A435DF558", "rexaID":"53ac23f963b3607aae9580b356e6b236d2955314", "author":"Wl/odzisl/aw Duch and Rafal/ Adamczak Email:duchraad@phys. uni. torun. pl", "title":"Statistical methods for construction of neural networks", "venue":"Department of Computer Methods, Nicholas Copernicus University", "year":"", "window":"p i (x) - p r (x) around x for which the two distributions cross. The simplest network constructed from FDA solution gives classification error which is as good as the original FDA. For such datasets [12] as <b>Wisconsin</b> <b>breast</b> <b>cancer</b>  hepatitis, Cleveland heart disease or diabetes the network obtains better results already before the learning process starts, but for some datasets this is not the", "mykey":1962},
 {"datasetID":16, "supportID":"C236AE947E41B2A044C181842DF6842A435DF558", "rexaID":"53ac23f963b3607aae9580b356e6b236d2955314", "author":"Wl/odzisl/aw Duch and Rafal/ Adamczak Email:duchraad@phys. uni. torun. pl", "title":"Statistical methods for construction of neural networks", "venue":"Department of Computer Methods, Nicholas Copernicus University", "year":"", "window":"p i (x) - p r (x) around x for which the two distributions cross. The simplest network constructed from FDA solution gives classification error which is as good as the original FDA. For such datasets [12] as <b>Wisconsin</b> <b>breast</b> <b>cancer</b>  hepatitis, Cleveland heart disease or diabetes the network obtains better results already before the learning process starts, but for some datasets this is not the", "mykey":1963},
 {"datasetID":46, "supportID":"C236AE947E41B2A044C181842DF6842A435DF558", "rexaID":"53ac23f963b3607aae9580b356e6b236d2955314", "author":"Wl/odzisl/aw Duch and Rafal/ Adamczak Email:duchraad@phys. uni. torun. pl", "title":"Statistical methods for construction of neural networks", "venue":"Department of Computer Methods, Nicholas Copernicus University", "year":"", "window":"p i (x) - p r (x) around x for which the two distributions cross. The simplest network constructed from FDA solution gives classification error which is as good as the original FDA. For such datasets [12] as Wisconsin breast cancer, <b>hepatitis</b>  Cleveland heart disease or diabetes the network obtains better results already before the learning process starts, but for some datasets this is not the", "mykey":1964},
 {"datasetID":98, "supportID":"C236AE947E41B2A044C181842DF6842A435DF558", "rexaID":"53ac23f963b3607aae9580b356e6b236d2955314", "author":"Wl/odzisl/aw Duch and Rafal/ Adamczak Email:duchraad@phys. uni. torun. pl", "title":"Statistical methods for construction of neural networks", "venue":"Department of Computer Methods, Nicholas Copernicus University", "year":"", "window":"cases give results that are comparable or better than those found by neural networks. A review of different approaches to classification and comparison of performance of 20 methods on 22 real world datasets has been done within the <b>StatLog</b> European Community project [3]. The algorithms that appeared most frequently as the top five were all of statistical nature, including four discriminant approaches:", "mykey":1965},
 {"datasetID":102, "supportID":"C236AE947E41B2A044C181842DF6842A435DF558", "rexaID":"53ac23f963b3607aae9580b356e6b236d2955314", "author":"Wl/odzisl/aw Duch and Rafal/ Adamczak Email:duchraad@phys. uni. torun. pl", "title":"Statistical methods for construction of neural networks", "venue":"Department of Computer Methods, Nicholas Copernicus University", "year":"", "window":"show that it would certainly be worthwhile to try. 3. Logical rules for network construction. Table 1: Classification results for a number of optimized MLP training algorithms applied to the <b>thyroid</b> dataset -- only the best results are shown. BP = Backpropagation. Method Training % Test k-NN (Manhattan) -- 93.8 Bayes rule [14] 97.0 96.1 BP+conjugate gradient 94.6 93.8 Best BP 99.1 97.6 RPROP 99.6 98.0", "mykey":1966},
 {"datasetID":9, "supportID":"C2E1DE1627E3AA365E09DE5AA79DDF335846C699", "rexaID":"5d88347a087b01681bec3121a13b7b29465cdc71", "author":"Mauro Birattari and Gianluca Bontempi and Hugues Bersini", "title":"Lazy Learning Meets the Recursive Least Squares Algorithm", "venue":"NIPS", "year":"1998", "window":"The first five, described by Quinlan (1993), were obtained from the UCI Repository of machine learning databases (Merz & Murphy, 1998), while the 4 Table 1: A summary of the characteristics of the datasets considered. Dataset Housing Cpu Prices <b>Mpg</b> Servo Ozone Number of examples 506 209 159 392 167 330 Number of regressors 13 6 16 7 8 8 Table 2: Mean absolute error on unseen cases. Method Housing Cpu", "mykey":1967},
 {"datasetID":48, "supportID":"C2E1DE1627E3AA365E09DE5AA79DDF335846C699", "rexaID":"5d88347a087b01681bec3121a13b7b29465cdc71", "author":"Mauro Birattari and Gianluca Bontempi and Hugues Bersini", "title":"Lazy Learning Meets the Recursive Least Squares Algorithm", "venue":"NIPS", "year":"1998", "window":"The first five, described by Quinlan (1993), were obtained from the UCI Repository of machine learning databases (Merz & Murphy, 1998), while the 4 Table 1: A summary of the characteristics of the datasets considered. Dataset <b>Housing</b> Cpu Prices Mpg Servo Ozone Number of examples 506 209 159 392 167 330 Number of regressors 13 6 16 7 8 8 Table 2: Mean absolute error on unseen cases. Method Housing Cpu", "mykey":1968},
 {"datasetID":87, "supportID":"C2E1DE1627E3AA365E09DE5AA79DDF335846C699", "rexaID":"5d88347a087b01681bec3121a13b7b29465cdc71", "author":"Mauro Birattari and Gianluca Bontempi and Hugues Bersini", "title":"Lazy Learning Meets the Recursive Least Squares Algorithm", "venue":"NIPS", "year":"1998", "window":"considered. Dataset Housing Cpu Prices Mpg <b>Servo</b> Ozone Number of examples 506 209 159 392 167 330 Number of regressors 13 6 16 7 8 8 Table 2: Mean absolute error on unseen cases. Method Housing Cpu Prices Mpg Servo", "mykey":1969},
 {"datasetID":14, "supportID":"C2EE689A27EF525B05E11475D3DD937C4F3A910E", "rexaID":"283e535d8f512eedabbd803c72a86b891eed8474", "author":"Chun-Nan Hsu and Hilmar Schuschel and Ya-Ting Yang", "title":"The ANNIGMA-Wrapper Approach to Neural Nets Feature Selection for Knowledge Discovery and Data Mining", "venue":"Institute of Information Science", "year":"1999", "window":"the technique presented in [10], where it is used to enhance the effectiveness of feature. An optimal result is the selection of features ``jacketcolor'', ``holding'', and ``bodyshape''. Real-world Datasets <b>Breast</b> <b>Cancer</b> Wisconsin (Cancer) This dataset has 699 instances of 10 features : one is the ID number and 9 others have values within 1 to 10. Each instance has one of the 2 possible classes:", "mykey":1970},
 {"datasetID":17, "supportID":"C2EE689A27EF525B05E11475D3DD937C4F3A910E", "rexaID":"283e535d8f512eedabbd803c72a86b891eed8474", "author":"Chun-Nan Hsu and Hilmar Schuschel and Ya-Ting Yang", "title":"The ANNIGMA-Wrapper Approach to Neural Nets Feature Selection for Knowledge Discovery and Data Mining", "venue":"Institute of Information Science", "year":"1999", "window":"the technique presented in [10], where it is used to enhance the effectiveness of feature. An optimal result is the selection of features ``jacketcolor'', ``holding'', and ``bodyshape''. Real-world Datasets <b>Breast</b> <b>Cancer</b> <b>Wisconsin</b> (Cancer) This dataset has 699 instances of 10 features : one is the ID number and 9 others have values within 1 to 10. Each instance has one of the 2 possible classes:", "mykey":1971},
 {"datasetID":15, "supportID":"C2EE689A27EF525B05E11475D3DD937C4F3A910E", "rexaID":"283e535d8f512eedabbd803c72a86b891eed8474", "author":"Chun-Nan Hsu and Hilmar Schuschel and Ya-Ting Yang", "title":"The ANNIGMA-Wrapper Approach to Neural Nets Feature Selection for Knowledge Discovery and Data Mining", "venue":"Institute of Information Science", "year":"1999", "window":"the technique presented in [10], where it is used to enhance the effectiveness of feature. An optimal result is the selection of features ``jacketcolor'', ``holding'', and ``bodyshape''. Real-world Datasets <b>Breast</b> <b>Cancer</b> <b>Wisconsin</b> (Cancer) This dataset has 699 instances of 10 features : one is the ID number and 9 others have values within 1 to 10. Each instance has one of the 2 possible classes:", "mykey":1972},
 {"datasetID":16, "supportID":"C2EE689A27EF525B05E11475D3DD937C4F3A910E", "rexaID":"283e535d8f512eedabbd803c72a86b891eed8474", "author":"Chun-Nan Hsu and Hilmar Schuschel and Ya-Ting Yang", "title":"The ANNIGMA-Wrapper Approach to Neural Nets Feature Selection for Knowledge Discovery and Data Mining", "venue":"Institute of Information Science", "year":"1999", "window":"the technique presented in [10], where it is used to enhance the effectiveness of feature. An optimal result is the selection of features ``jacketcolor'', ``holding'', and ``bodyshape''. Real-world Datasets <b>Breast</b> <b>Cancer</b> <b>Wisconsin</b> (Cancer) This dataset has 699 instances of 10 features : one is the ID number and 9 others have values within 1 to 10. Each instance has one of the 2 possible classes:", "mykey":1973},
 {"datasetID":105, "supportID":"C2EE689A27EF525B05E11475D3DD937C4F3A910E", "rexaID":"283e535d8f512eedabbd803c72a86b891eed8474", "author":"Chun-Nan Hsu and Hilmar Schuschel and Ya-Ting Yang", "title":"The ANNIGMA-Wrapper Approach to Neural Nets Feature Selection for Knowledge Discovery and Data Mining", "venue":"Institute of Information Science", "year":"1999", "window":"reported the configuration of the network in detail. Among their configuration, the learning rate 2.0 and the range of initial weights 17 [Gamma 0:3; +0:3] are adopted in our experiments. Vote This dataset consists of the <b>voting records</b> of 435 congressmen on 16 issues in the 1984 congress, 2nd session. The votes are classified into ``yea'', ``nay'', and ``unknown''. The classification problem is to", "mykey":1974},
 {"datasetID":151, "supportID":"C2EE689A27EF525B05E11475D3DD937C4F3A910E", "rexaID":"283e535d8f512eedabbd803c72a86b891eed8474", "author":"Chun-Nan Hsu and Hilmar Schuschel and Ya-Ting Yang", "title":"The ANNIGMA-Wrapper Approach to Neural Nets Feature Selection for Knowledge Discovery and Data Mining", "venue":"Institute of Information Science", "year":"1999", "window":"instances of 59 features. The domain of the features are one of fA,G,T,Cg (DNA nucleotides). We set their value to f1,2,3,4g respectively. 53 instances are positive and 53 are negative. <b>Sonar</b> This dataset contains 208 patterns. 111 patterns were obtained by bouncing sonar signals off a metal at various angles and under various conditions. 97 patterns were obtained from rocks under similar conditions.", "mykey":1975},
 {"datasetID":45, "supportID":"C2EE689A27EF525B05E11475D3DD937C4F3A910E", "rexaID":"283e535d8f512eedabbd803c72a86b891eed8474", "author":"Chun-Nan Hsu and Hilmar Schuschel and Ya-Ting Yang", "title":"The ANNIGMA-Wrapper Approach to Neural Nets Feature Selection for Knowledge Discovery and Data Mining", "venue":"Institute of Information Science", "year":"1999", "window":"starting with 0. Unknown values are set to 0.5. <b>heart</b> Disease This dataset concerning heart disease diagnosis contains 4 sub-databases col16 lected from 4 locations. Each database has the same instance format. We used two of them in our experiment: one from Cleveland", "mykey":1976},
 {"datasetID":52, "supportID":"C2EE689A27EF525B05E11475D3DD937C4F3A910E", "rexaID":"283e535d8f512eedabbd803c72a86b891eed8474", "author":"Chun-Nan Hsu and Hilmar Schuschel and Ya-Ting Yang", "title":"The ANNIGMA-Wrapper Approach to Neural Nets Feature Selection for Knowledge Discovery and Data Mining", "venue":"Institute of Information Science", "year":"1999", "window":"concentrated on distinguishing the presence of heart disease (value 1,2 3,4 in the classification) from absence (value 0). Missing values in both databases are all replaced by zero. <b>Ionosphere</b> This dataset has 351 records of 34 features. The goal is to classify two types of radar returns from the ionosphere. ``Good'' radar returns are those showing evidence of some type of structure in the ionosphere.", "mykey":1977},
 {"datasetID":2, "supportID":"C30899E29A0BF072EF74A4AA2D012072F94F031D", "rexaID":"0767f0171b0f74f96978a41f3e947ea91cc9dbd3", "author":"Alexander J. Smola and Vishy Vishwanathan and Eleazar Eskin", "title":"Laplace Propagation", "venue":"NIPS", "year":"2003", "window":"X i h# i ; #i; (15) with the joint minimizer being the average of the individual solutions. 5 Experiments To test our ideas we performed a set of experiments with the widely available Web and <b>Adult</b> datasets from the UCI repository [1]. All experiments were performed on a 2.4 MHz 2 Note that we had to replace the equality with set inclusion due to the fact that c is not everywhere differentiable, hence", "mykey":1978},
 {"datasetID":1, "supportID":"C3C32C222E673C0F029C7AE697F8E22937AE5111", "rexaID":"bc80295973a43d3806ff4dfe83e5724260301c33", "author":"Iztok Savnik and Peter A. Flach", "title":"Discovery of multivalued dependencies from relations", "venue":"Intell. Data Anal, 4", "year":"2000", "window":"which were used in the experiments are available at UCI Machine learning repository [10]. In the case of the datasets Car, Bupa and <b>Abalone</b> we use randomly selected subsets of the original datasets. For each experiment we specify the name of the relation (dataset) r(R), the number of tuples in relation jrj, the", "mykey":1979},
 {"datasetID":19, "supportID":"C3C32C222E673C0F029C7AE697F8E22937AE5111", "rexaID":"bc80295973a43d3806ff4dfe83e5724260301c33", "author":"Iztok Savnik and Peter A. Flach", "title":"Discovery of multivalued dependencies from relations", "venue":"Intell. Data Anal, 4", "year":"2000", "window":"which were used in the experiments are available at UCI Machine learning repository [10]. In the case of the datasets <b>Car</b>  Bupa and Abalone we use randomly selected subsets of the original datasets. For each experiment we specify the name of the relation (dataset) r(R), the number of tuples in relation jrj, the", "mykey":1980},
 {"datasetID":48, "supportID":"C3E10A9CE038D4726A394B8F5F3612457184B7D5", "rexaID":"49b5aef7a515640b1dde8f9f777d5a1f0c014022", "author":"Glenn Fung and M. Murat Dundar and Jinbo Bi and Bharat Rao", "title":"A fast iterative algorithm for fisher discriminant using heterogeneous kernels", "venue":"ICML", "year":"2004", "window":"used in the literature for benchmarking from the UCI Machine Learning Repository (Murphy & Aha, 1992): Ionosphere, Cleveland Heart, Pima Indians, BUPA Liver and Boston <b>Housing</b>  Additionally, a sixth dataset, the colon CAD dataset, relates to colorectal cancer diagnosis using virtual colonoscopy derived from computer tomographic images. We will refer to this dataset as the colon CAD dataset. The", "mykey":1981},
 {"datasetID":52, "supportID":"C3E10A9CE038D4726A394B8F5F3612457184B7D5", "rexaID":"49b5aef7a515640b1dde8f9f777d5a1f0c014022", "author":"Glenn Fung and M. Murat Dundar and Jinbo Bi and Bharat Rao", "title":"A fast iterative algorithm for fisher discriminant using heterogeneous kernels", "venue":"ICML", "year":"2004", "window":"p-values obtained show that there is no significant difference between A-KFD and the the standard KFD where the kernel model is chosen using a cross-validation tuning procedure. Only on two of the datasets, <b>ionosphere</b> and housing there is a small statistically significant difference for the two methods, with the performance of A-KFD being the better of the two for the ionosphere dataset and the", "mykey":1982},
 {"datasetID":60, "supportID":"C3E10A9CE038D4726A394B8F5F3612457184B7D5", "rexaID":"49b5aef7a515640b1dde8f9f777d5a1f0c014022", "author":"Glenn Fung and M. Murat Dundar and Jinbo Bi and Bharat Rao", "title":"A fast iterative algorithm for fisher discriminant using heterogeneous kernels", "venue":"ICML", "year":"2004", "window":"used in the literature for benchmarking from the UCI Machine Learning Repository (Murphy & Aha, 1992): Ionosphere, Cleveland Heart, Pima Indians, BUPA <b>Liver</b> and Boston Housing. Additionally, a sixth dataset, the colon CAD dataset, relates to colorectal cancer diagnosis using virtual colonoscopy derived from computer tomographic images. We will refer to this dataset as the colon CAD dataset. The", "mykey":1983},
 {"datasetID":111, "supportID":"C49FD9128AFB2FC760AD06FFCE7480656E6AD8DB", "rexaID":"4d3a15f17c5158aad69c6133ae41ed25b6e037a6", "author":"Eibe Frank and Stefan Kramer", "title":"Ensembles of nested dichotomies for multi-class problems", "venue":"ICML", "year":"2004", "window":"prim.-tumor 339 3.9 0 17 21 segment 2310 0.0 19 0 7 soybean 683 9.8 0 35 19 splice 3190 0.0 0 61 3 vehicle 846 0.0 18 0 4 vowel 990 0.0 10 3 11 waveform 5000 0.0 40 0 3 <b>zoo</b> 101 0.0 1 15 7 Table 1: Datasets used for the experiments In the first set of experiments, we compared ensembles of nested dichotomies (ENDs) with several other standard multi-class methods. In the second set we varied the number", "mykey":1984},
 {"datasetID":2, "supportID":"C50F5A6C646634EA3588315C71F78DCE3D2FA3AA", "rexaID":"5300c71ae945611a1d091404fc0461a48738e151", "author":"Dmitry Pavlov and Darya Chudova and Padhraic Smyth", "title":"Towards scalable support vector machines using squashing", "venue":"KDD", "year":"2000", "window":"were ``The Microsoft Anonymous Web'' and the ''Forest Cover Type'' datasets available at UCI KDD archive [Bay99] and <b>Adult</b> dataset available at UCI machine learning repository [BM98]. Web data reflects the Web pages of www.microsoft.com that each user visited during one", "mykey":1985},
 {"datasetID":4, "supportID":"C50F5A6C646634EA3588315C71F78DCE3D2FA3AA", "rexaID":"5300c71ae945611a1d091404fc0461a48738e151", "author":"Dmitry Pavlov and Darya Chudova and Padhraic Smyth", "title":"Towards scalable support vector machines using squashing", "venue":"KDD", "year":"2000", "window":"2: Error on the Test Set for Various Models On the Synthetic Data. Baseline 1% srs-SMO 1% squash-SMO 1% boost-SMO full-SMO 50% 41% 34.04% 34.84% 33.54% 8 4.2 Results on Benchmark Data The public datasets were ``The <b>Microsoft</b> <b>Anonymous</b> <b>Web</b> ' and the ''Forest Cover Type'' datasets available at UCI KDD archive [Bay99] and Adult dataset available at UCI machine learning repository [BM98]. Web data", "mykey":1986},
 {"datasetID":14, "supportID":"C59893B2D45A048E403D26DE7FE63409B3941555", "rexaID":"9f9df113476ffbf356892bb497bd2714e6f56d99", "author":"Ismail Taha and Joydeep Ghosh", "title":"Characterization of the Wisconsin Breast cancer Database Using a Hybrid Symbolic-Connectionist System", "venue":"Proceedings of ANNIE", "year":"1996", "window":"and Ordering Procedure 10 3.1 The Rule Ordering Procedure : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 10 4 Output Integration 11 5 Implementation Results 12 5.1 <b>Breast</b> <b>Cancer</b> Data Set: : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 12 5.2 Methodology : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 12 5.3 Rule Extraction and", "mykey":1987},
 {"datasetID":17, "supportID":"C59893B2D45A048E403D26DE7FE63409B3941555", "rexaID":"9f9df113476ffbf356892bb497bd2714e6f56d99", "author":"Ismail Taha and Joydeep Ghosh", "title":"Characterization of the Wisconsin Breast cancer Database Using a Hybrid Symbolic-Connectionist System", "venue":"Proceedings of ANNIE", "year":"1996", "window":"rule extraction techniques, the BIO-RE, Partial-RE, and Full-RE, on the <b>breast</b> <b>cancer</b> problem. The extracted rules are presented in order based on the rule ordering algorithm. 5.1 Breast-Cancer Data Set: The <b>Wisconsin</b> breast cancer data set has nine inputs and two output classes [26, 31]. The input features are: X 1 = Clump Thickness; X 2 = Uniformity of Cell Size; X 3 = Uniformity of Cell Shape; X", "mykey":1988},
 {"datasetID":15, "supportID":"C59893B2D45A048E403D26DE7FE63409B3941555", "rexaID":"9f9df113476ffbf356892bb497bd2714e6f56d99", "author":"Ismail Taha and Joydeep Ghosh", "title":"Characterization of the Wisconsin Breast cancer Database Using a Hybrid Symbolic-Connectionist System", "venue":"Proceedings of ANNIE", "year":"1996", "window":"rule extraction techniques, the BIO-RE, Partial-RE, and Full-RE, on the <b>breast</b> <b>cancer</b> problem. The extracted rules are presented in order based on the rule ordering algorithm. 5.1 Breast-Cancer Data Set: The <b>Wisconsin</b> breast cancer data set has nine inputs and two output classes [26, 31]. The input features are: X 1 = Clump Thickness; X 2 = Uniformity of Cell Size; X 3 = Uniformity of Cell Shape; X", "mykey":1989},
 {"datasetID":16, "supportID":"C59893B2D45A048E403D26DE7FE63409B3941555", "rexaID":"9f9df113476ffbf356892bb497bd2714e6f56d99", "author":"Ismail Taha and Joydeep Ghosh", "title":"Characterization of the Wisconsin Breast cancer Database Using a Hybrid Symbolic-Connectionist System", "venue":"Proceedings of ANNIE", "year":"1996", "window":"rule extraction techniques, the BIO-RE, Partial-RE, and Full-RE, on the <b>breast</b> <b>cancer</b> problem. The extracted rules are presented in order based on the rule ordering algorithm. 5.1 Breast-Cancer Data Set: The <b>Wisconsin</b> breast cancer data set has nine inputs and two output classes [26, 31]. The input features are: X 1 = Clump Thickness; X 2 = Uniformity of Cell Size; X 3 = Uniformity of Cell Shape; X", "mykey":1990},
 {"datasetID":2, "supportID":"C5B700482579FD2AE13D8F38EE57606139DAEF20", "rexaID":"6bacbb21f7a1d6dee581f756cf326e0932446698", "author":"Zhiyuan Chen and Johannes Gehrke and Flip Korn", "title":"Query Optimization In Compressed Database Systems", "venue":"SIGMOD Conference", "year":"2001", "window":"are not compressed. TPC-H data contains 8 tables and 61 attributes, 23 of which are string-valued. The string attributes account for about 60% of the total database size. We also used a 4MB of dataset with US census data, the <b>adult</b> data set [5] for experiments on compression strategies. The adult dataset contains a single table with 14 attributes, 8 of them string-valued, accounting for about 80%", "mykey":1991},
 {"datasetID":20, "supportID":"C5B700482579FD2AE13D8F38EE57606139DAEF20", "rexaID":"6bacbb21f7a1d6dee581f756cf326e0932446698", "author":"Zhiyuan Chen and Johannes Gehrke and Flip Korn", "title":"Query Optimization In Compressed Database Systems", "venue":"SIGMOD Conference", "year":"2001", "window":"are not compressed. TPC-H data contains 8 tables and 61 attributes, 23 of which are string-valued. The string attributes account for about 60% of the total database size. We also used a 4MB of dataset with US <b>census</b> data, the adult data set [5] for experiments on compression strategies. The adult dataset contains a single table with 14 attributes, 8 of them string-valued, accounting for about 80%", "mykey":1992},
 {"datasetID":116, "supportID":"C5B700482579FD2AE13D8F38EE57606139DAEF20", "rexaID":"6bacbb21f7a1d6dee581f756cf326e0932446698", "author":"Zhiyuan Chen and Johannes Gehrke and Flip Korn", "title":"Query Optimization In Compressed Database Systems", "venue":"SIGMOD Conference", "year":"2001", "window":"are not compressed. TPC-H data contains 8 tables and 61 attributes, 23 of which are string-valued. The string attributes account for about 60% of the total database size. We also used a 4MB of dataset with <b>US census</b> data, the adult data set [5] for experiments on compression strategies. The adult dataset contains a single table with 14 attributes, 8 of them string-valued, accounting for about 80%", "mykey":1993},
 {"datasetID":53, "supportID":"C5E4B20ED612B93444217F385FD169BF04C7343E", "rexaID":"104a89897dc4a05cb068858194d1d78ad98d5012", "author":"Sugato Basu", "title":"Semi-Supervised Clustering with Limited Background Knowledge", "venue":"AAAI", "year":"2004", "window":"like stop-word removal, tf-idf weighting, and removal of very high-frequency and very low-frequency words (Dhillon & Modha, 2001). From the UCI collection we selected <b>Iris</b>  which is a well-known dataset having 150 points in 4 dimensions. We used the active pairwise constrained version of KMeans on Iris, and SPKMeans on Classic3-subset. Learning curves with cross validation For all algorithms on", "mykey":1994},
 {"datasetID":109, "supportID":"C5E4B20ED612B93444217F385FD169BF04C7343E", "rexaID":"104a89897dc4a05cb068858194d1d78ad98d5012", "author":"Sugato Basu", "title":"Semi-Supervised Clustering with Limited Background Knowledge", "venue":"AAAI", "year":"2004", "window":"Experiments were conducted on several datasets from the UCI repository: Iris, <b>Wine</b>  and representative randomly sampled subsets from the Pen-Digits and Letter datasets. For Pen-Digits and Letter, we chose two sets of three classes: fI, J, Lg", "mykey":1995},
 {"datasetID":105, "supportID":"C710C98D7FD7350E9857950E11FCA1969956609C", "rexaID":"dbd0f89e43cc274cf80effc519740af54cccef75", "author":"Igor Kononenko and Edvard Simec", "title":"Induction of decision trees using RELIEFF", "venue":"University of Ljubljana, Faculty of electrical engineering & computer science", "year":"", "window":"(table 6) indicates that the attributes are irrelevant to the class. On the <b>VOTE data</b> set the naive Bayesian classifier is the worst, while both versions of Assistant are comparable to the rule based classifier by Smyth et al. (1990). The most interesting results appear in the MESH", "mykey":1996},
 {"datasetID":53, "supportID":"C710C98D7FD7350E9857950E11FCA1969956609C", "rexaID":"dbd0f89e43cc274cf80effc519740af54cccef75", "author":"Igor Kononenko and Edvard Simec", "title":"Induction of decision trees using RELIEFF", "venue":"University of Ljubljana, Faculty of electrical engineering & computer science", "year":"", "window":"for patients suffering from hepatitis. The data was provided by Gail Gong from Carnegie-Mellon University. We also compared the performance of the algorithms on the following non-medical real world data sets (SOYB, <b>IRIS</b>  and VOTE are obtained from the Irvine database (Murphy & Aha, 1991)): SOYB: The famous soybean data set used by Michalski & Chilausky (1980). IRIS: The well known Fisher's problem of", "mykey":1997},
 {"datasetID":90, "supportID":"C710C98D7FD7350E9857950E11FCA1969956609C", "rexaID":"dbd0f89e43cc274cf80effc519740af54cccef75", "author":"Igor Kononenko and Edvard Simec", "title":"Induction of decision trees using RELIEFF", "venue":"University of Ljubljana, Faculty of electrical engineering & computer science", "year":"", "window":"(SOYB, IRIS, and VOTE are obtained from the Irvine database (Murphy & Aha, 1991)): SOYB: The famous <b>soybean</b> data set used by Michalski & Chilausky (1980). IRIS: The well known Fisher's problem of determining the type of iris flower. MESH3,MESH15: The problem of determining the number of elements for each of the", "mykey":1998},
 {"datasetID":91, "supportID":"C710C98D7FD7350E9857950E11FCA1969956609C", "rexaID":"dbd0f89e43cc274cf80effc519740af54cccef75", "author":"Igor Kononenko and Edvard Simec", "title":"Induction of decision trees using RELIEFF", "venue":"University of Ljubljana, Faculty of electrical engineering & computer science", "year":"", "window":"(SOYB, IRIS, and VOTE are obtained from the Irvine database (Murphy & Aha, 1991)): SOYB: The famous <b>soybean</b> data set used by Michalski & Chilausky (1980). IRIS: The well known Fisher's problem of determining the type of iris flower. MESH3,MESH15: The problem of determining the number of elements for each of the", "mykey":1999},
 {"datasetID":151, "supportID":"C775D79C7480F5722D708608369EBD8DFB9CF55E", "rexaID":"5866e91743366edf42eae4a1e4a3266e8839cea9", "author":"Stefan Aeberhard and O. de Vel and Danny Coomans", "title":"New Fast Algorithms for Variable Selection based on Classifier Performance", "venue":"James Cook University", "year":"", "window":"and 59, 71 and 48 objects per class. The classes correspond to three different cultivars and the 13 variables measure 13 different constituents of the three types of resulting wines. The second data set, the <b>sonar</b> data [11], is 60 dimensional with two classes and 111 and 97 objects per class. The two different classes correspond to sonar signals bounced off a metal cylinder and reflected off a", "mykey":2000},
 {"datasetID":109, "supportID":"C775D79C7480F5722D708608369EBD8DFB9CF55E", "rexaID":"5866e91743366edf42eae4a1e4a3266e8839cea9", "author":"Stefan Aeberhard and O. de Vel and Danny Coomans", "title":"New Fast Algorithms for Variable Selection based on Classifier Performance", "venue":"James Cook University", "year":"", "window":"taken to add or eliminate half of the variables. complexities from O(d 4 ) to O(d 3 ). The time complexities of the optimal algorithms for QDA and LDA based variable selection are O(N p d 2 ). Two data sets were used. The first, the <b>wine</b> data [11], is 13 dimensional with three classes and 59, 71 and 48 objects per class. The classes correspond to three different cultivars and the 13 variables measure", "mykey":2001},
 {"datasetID":44, "supportID":"C86F069D2AD0CD6E86D361CA54007DF454A13991", "rexaID":"43adf91a23b576706c55888c9f46d07d424a7938", "author":"Anthony D. Griffiths and Derek Bridge", "title":"A Yardstick for the Evaluation of Case-Based Classifiers", "venue":"Department of Computer Science, University of York", "year":"", "window":"used in Figures 5 and 4 The data set in the UCI repository for the  <b>Hayes</b> <b>Roth</b>  target function described above is incomplete in that it contains instances for only some of the possible descriptions, contains duplications, and contains", "mykey":2002},
 {"datasetID":58, "supportID":"C86F069D2AD0CD6E86D361CA54007DF454A13991", "rexaID":"43adf91a23b576706c55888c9f46d07d424a7938", "author":"Anthony D. Griffiths and Derek Bridge", "title":"A Yardstick for the Evaluation of Case-Based Classifiers", "venue":"Department of Computer Science, University of York", "year":"", "window":"giving 24 possible customer descriptions. These descriptions are classified into three classes according to the suitability of different types of contact <b>lens</b>  The documentation for the data set indicates that a correct description of the target function requires 9 production rules. The distribution of the classification values is highly skewed, giving a relative frequency for the majority", "mykey":2003},
 {"datasetID":80, "supportID":"C93E1E809F7869AA56B5E0DD2E2E02947E6C7084", "rexaID":"01b6c0d942e55f6fbcd5def25529431bf2fa33ff", "author":"Erick Cant\u00fa-Paz and Chandrika Kamath", "title":"Using Evolutionary Algorithms to Induce Oblique Decision Trees", "venue":"Center for Applied Scientific Computing Lawrence Livermore National Laboratory", "year":"", "window":"we experimented with the <b>optical</b> digit <b>recognition</b> data set, which is also available at UCI's ML repository. This data set has 3823 instances in a training set and 1797 in a testing set; each instance is described by 64 numeric attributes. The objective is", "mykey":2004},
 {"datasetID":9, "supportID":"C9498206B4C372BCDBBF1FC7DA3B4E743E131DA6", "rexaID":"3caf773de7b1ad6c9236cbd03763058bc8846e9d", "author":"Dan Pelleg", "title":"Scalable and Practical Probability Density Estimators for Scientific Anomaly Detection", "venue":"School of Computer Science Carnegie Mellon University", "year":"2004", "window":"would be to first try and estimate \u00be (say, using a model with spherical Gaussians) and use the estimate to set the rectangle tails. Experiments on real-life data were done on the  <b>mpg</b>  and \"census\" datasets from the UCI repository (Blake & Merz, 1998). The \"mpg\" data has about 400 records with 7 continuous 2 attributes. Running on this data with the number of components set to three, we get the", "mykey":2005},
 {"datasetID":20, "supportID":"C9498206B4C372BCDBBF1FC7DA3B4E743E131DA6", "rexaID":"3caf773de7b1ad6c9236cbd03763058bc8846e9d", "author":"Dan Pelleg", "title":"Scalable and Practical Probability Density Estimators for Scientific Anomaly Detection", "venue":"School of Computer Science Carnegie Mellon University", "year":"2004", "window":"would be to first try and estimate \u00be (say, using a model with spherical Gaussians) and use the estimate to set the rectangle tails. Experiments on real-life data were done on the \"mpg\" and  <b>census</b>  datasets from the UCI repository (Blake & Merz, 1998). The \"mpg\" data has about 400 records with 7 continuous 2 attributes. Running on this data with the number of components set to three, we get the", "mykey":2006},
 {"datasetID":29, "supportID":"C9498206B4C372BCDBBF1FC7DA3B4E743E131DA6", "rexaID":"3caf773de7b1ad6c9236cbd03763058bc8846e9d", "author":"Dan Pelleg", "title":"Scalable and Practical Probability Density Estimators for Scientific Anomaly Detection", "venue":"School of Computer Science Carnegie Mellon University", "year":"2004", "window":"of points and linearly with the number of clusters. This allows for clustering with tens of thousands of centroids and millions of points using commodity <b>hardware</b>  7 1.1 Introduction Consider a dataset with R records, each having M attributes. Given a constant k, the clustering problem is to partition the data into k subsets such that each subset behaves \"well\" under some measure. For example, we", "mykey":2007},
 {"datasetID":14, "supportID":"C98EAEB264C9E873FF7996140718DD34CEBA917B", "rexaID":"38848f2f66b7de9e40f1576d9dcb59b8361fccac", "author":"G. Ratsch and B. Scholkopf and Alex Smola and Sebastian Mika and T. Onoda and K. -R Muller", "title":"Robust Ensemble Learning for Data Mining", "venue":"GMD FIRST, Kekul#estr", "year":"", "window":"generalization performance of AdaBoost in the low noise regime. However, AdaBoost performs worse than other learning machines on noisy tasks [6, 7], such as the iris and the <b>breast</b> <b>cancer</b> benchmark data sets [5]. The present paper addresses the overfitting problem of AdaBoost in two ways. Primarily, it makes an algorithmic contribution to the problem of constructing regularized boosting algorithms.", "mykey":2008},
 {"datasetID":53, "supportID":"C98EAEB264C9E873FF7996140718DD34CEBA917B", "rexaID":"38848f2f66b7de9e40f1576d9dcb59b8361fccac", "author":"G. Ratsch and B. Scholkopf and Alex Smola and Sebastian Mika and T. Onoda and K. -R Muller", "title":"Robust Ensemble Learning for Data Mining", "venue":"GMD FIRST, Kekul#estr", "year":"", "window":"generalization performance of AdaBoost in the low noise regime. However, AdaBoost performs worse than other learning machines on noisy tasks [6, 7], such as the <b>iris</b> and the breast cancer benchmark data sets [5]. The present paper addresses the overfitting problem of AdaBoost in two ways. Primarily, it makes an algorithmic contribution to the problem of constructing regularized boosting algorithms.", "mykey":2009},
 {"datasetID":20, "supportID":"C99D454ECB2D0CE44C08CF45AFE317FF2E452862", "rexaID":"a158e74cc09e53ef929c43d490e983517f612292", "author":"Eibe Frank and Geoffrey Holmes and Richard Kirkby and Mark A. Hall", "title":"Racing Committees for Large Datasets", "venue":"Discovery Science", "year":"2002", "window":"LogitBoost #Iterations Racing w/o pruning Racing w pruning anonymous 27.00% 60 28.24% 27.56% adult 13.51% 67 14.58% 14.72% shuttle 0.01% 86 0.08% 0.07% <b>census</b> income 4.43% 448 4.90% 4.93% The next dataset we consider is census-income. The first row of Figure 4 shows the results. The most striking aspect is the effect of pruning with small chunk sizes. In this domain the fluctuation in error is", "mykey":2010},
 {"datasetID":117, "supportID":"C99D454ECB2D0CE44C08CF45AFE317FF2E452862", "rexaID":"a158e74cc09e53ef929c43d490e983517f612292", "author":"Eibe Frank and Geoffrey Holmes and Richard Kirkby and Mark A. Hall", "title":"Racing Committees for Large Datasets", "venue":"Discovery Science", "year":"2002", "window":"LogitBoost #Iterations Racing w/o pruning Racing w pruning anonymous 27.00% 60 28.24% 27.56% adult 13.51% 67 14.58% 14.72% shuttle 0.01% 86 0.08% 0.07% <b>census</b> <b>income</b> 4.43% 448 4.90% 4.93% The next dataset we consider is census-income. The first row of Figure 4 shows the results. The most striking aspect is the effect of pruning with small chunk sizes. In this domain the fluctuation in error is", "mykey":2011},
 {"datasetID":1, "supportID":"C9BCF622EC8412DCF06D7FAEF2934109B4199D7C", "rexaID":"48d6beec2a36a87d9d88b6de85dd85a75e5ed24d", "author":"C. Titus Brown and Harry W. Bullen and Sean P. Kelly and Robert K. Xiao and Steven G. Satterfield and John G. Hagedorn and Judith E. Devaney", "title":"Visualization and Data Mining in an 3D Immersive Environment: Summer Project 2003", "venue":"", "year":"", "window":"an overview of the entire museum layout. 45 4.13 <b>abalone</b> The abalone data set was analysed by Sean Kelly. This dataset contains roughly 2000 instances of measurements of abalone shellfish. The biggest issue in visualizing it was the lack of distinct target attribute for", "mykey":2012},
 {"datasetID":9, "supportID":"C9BCF622EC8412DCF06D7FAEF2934109B4199D7C", "rexaID":"48d6beec2a36a87d9d88b6de85dd85a75e5ed24d", "author":"C. Titus Brown and Harry W. Bullen and Sean P. Kelly and Robert K. Xiao and Steven G. Satterfield and John G. Hagedorn and Judith E. Devaney", "title":"Visualization and Data Mining in an 3D Immersive Environment: Summer Project 2003", "venue":"", "year":"", "window":"was analysed by Christian Brown. Overview The Miles Per Gallon  <b>MPG</b>  data set consisted of data regarding the engines of numerous cars. Each car had 8 attributes, intended to be used to predict miles per gallon for each car. The attributes were a mix of both discrete and", "mykey":2013},
 {"datasetID":42, "supportID":"C9BCF622EC8412DCF06D7FAEF2934109B4199D7C", "rexaID":"48d6beec2a36a87d9d88b6de85dd85a75e5ed24d", "author":"C. Titus Brown and Harry W. Bullen and Sean P. Kelly and Robert K. Xiao and Steven G. Satterfield and John G. Hagedorn and Judith E. Devaney", "title":"Visualization and Data Mining in an 3D Immersive Environment: Summer Project 2003", "venue":"", "year":"", "window":"whole museum from abovelack of support for numeric targets by InfoGain. 50 4.14 <b>glass</b> The glass data set was analysed by Sean Kelly. This is a purely numeric dataset containing roughly 200 glass samples with information on amounts of various chemical elements in the samples and what purpose the glass", "mykey":2014},
 {"datasetID":48, "supportID":"C9BCF622EC8412DCF06D7FAEF2934109B4199D7C", "rexaID":"48d6beec2a36a87d9d88b6de85dd85a75e5ed24d", "author":"C. Titus Brown and Harry W. Bullen and Sean P. Kelly and Robert K. Xiao and Steven G. Satterfield and John G. Hagedorn and Judith E. Devaney", "title":"Visualization and Data Mining in an 3D Immersive Environment: Summer Project 2003", "venue":"", "year":"", "window":"We decided to explore labeling systems and layouts more in the future. 19 Figure 4.2: A closeup of two of the MPG graphs. 20 4.3 <b>Housing</b> This data set was analysed by Robert Xiao. Overview The housing data set consisted of data regarding 506 houses in Boston, Massachusetts. Thirteen continuous attributes, including a target variable of median", "mykey":2015},
 {"datasetID":50, "supportID":"C9BCF622EC8412DCF06D7FAEF2934109B4199D7C", "rexaID":"48d6beec2a36a87d9d88b6de85dd85a75e5ed24d", "author":"C. Titus Brown and Harry W. Bullen and Sean P. Kelly and Robert K. Xiao and Steven G. Satterfield and John G. Hagedorn and Judith E. Devaney", "title":"Visualization and Data Mining in an 3D Immersive Environment: Summer Project 2003", "venue":"", "year":"", "window":"be reduced to about three or four. 21 Figure 4.3: The graph of the ChiSquared analysis of the housing data set. 22 4.4 <b>Image Segmentation</b> This data set was analysed by Christian Brown. Overview The Image Segmentation (Seg) data set consisted of data relating numerous analyses of the colors in subdivided", "mykey":2016},
 {"datasetID":60, "supportID":"C9BCF622EC8412DCF06D7FAEF2934109B4199D7C", "rexaID":"48d6beec2a36a87d9d88b6de85dd85a75e5ed24d", "author":"C. Titus Brown and Harry W. Bullen and Sean P. Kelly and Robert K. Xiao and Steven G. Satterfield and John G. Hagedorn and Judith E. Devaney", "title":"Visualization and Data Mining in an 3D Immersive Environment: Summer Project 2003", "venue":"", "year":"", "window":"either upgrading the system to handle a larger number of polygons or creating a series of graphs using different randomly selected data points. 27 Figure 4.5: A long shot of the full tree cover data set display. 28 4.6 BUPA <b>Liver</b> Disorder This data set was analysed by Robert Xiao. Overview This data set consisted of 6 attributes: the results of 5 different blood tests which were thought to be", "mykey":2017},
 {"datasetID":73, "supportID":"C9BCF622EC8412DCF06D7FAEF2934109B4199D7C", "rexaID":"48d6beec2a36a87d9d88b6de85dd85a75e5ed24d", "author":"C. Titus Brown and Harry W. Bullen and Sean P. Kelly and Robert K. Xiao and Steven G. Satterfield and John G. Hagedorn and Judith E. Devaney", "title":"Visualization and Data Mining in an 3D Immersive Environment: Summer Project 2003", "venue":"", "year":"", "window":"whole museum at distance. 54 Figure 4.26: Glass dataset: whole museum from above. 55 4.15 <b>Mushroom</b> The mushroom data set was analysed by Sean kelly. The mushroom dataset reflects all the problems found in the tic-tac-toe data, but on a larger scale. Once", "mykey":2018},
 {"datasetID":78, "supportID":"C9BCF622EC8412DCF06D7FAEF2934109B4199D7C", "rexaID":"48d6beec2a36a87d9d88b6de85dd85a75e5ed24d", "author":"C. Titus Brown and Harry W. Bullen and Sean P. Kelly and Robert K. Xiao and Steven G. Satterfield and John G. Hagedorn and Judith E. Devaney", "title":"Visualization and Data Mining in an 3D Immersive Environment: Summer Project 2003", "venue":"", "year":"", "window":"from Italy. Wines from three different types of grapes are included. There are levels of 13 chemicals provided for each of 178 instances. The target attribute is the type of grape. Figure 4.8: Wine data set in museum environment. 33 4.9 <b>Page Block</b> The Block data set was processed by Harry Bullen. This data set concerns to document analysis. Each instance describes a block of a page in a document. Each", "mykey":2019},
 {"datasetID":89, "supportID":"C9BCF622EC8412DCF06D7FAEF2934109B4199D7C", "rexaID":"48d6beec2a36a87d9d88b6de85dd85a75e5ed24d", "author":"C. Titus Brown and Harry W. Bullen and Sean P. Kelly and Robert K. Xiao and Steven G. Satterfield and John G. Hagedorn and Judith E. Devaney", "title":"Visualization and Data Mining in an 3D Immersive Environment: Summer Project 2003", "venue":"", "year":"", "window":"whole floor from above; extended semicircular layout. 42 4.12 <b>Solar <b>Flare</b> /b> The Solar Flare data set was processed by Harry Bullen. This data set has been made up of only descrete variables. Unfortunately the 3D visualization did not produce interesting results. This is because most of the data", "mykey":2020},
 {"datasetID":94, "supportID":"C9BCF622EC8412DCF06D7FAEF2934109B4199D7C", "rexaID":"48d6beec2a36a87d9d88b6de85dd85a75e5ed24d", "author":"C. Titus Brown and Harry W. Bullen and Sean P. Kelly and Robert K. Xiao and Steven G. Satterfield and John G. Hagedorn and Judith E. Devaney", "title":"Visualization and Data Mining in an 3D Immersive Environment: Summer Project 2003", "venue":"", "year":"", "window":"was to determine what attributes were most meaningful in determining what the block contained. The most important attributes turned out to be the size and shape of the block. Figure 4.9: Page block data set in museum environment. 34 4.10 <b>Spambase</b> The spambase data set was analysed by Sean Kelly. This dataset contains roughly 4000 instances of 58 attributes each, representing e-mail messages. One", "mykey":2021},
 {"datasetID":147, "supportID":"C9BCF622EC8412DCF06D7FAEF2934109B4199D7C", "rexaID":"48d6beec2a36a87d9d88b6de85dd85a75e5ed24d", "author":"C. Titus Brown and Harry W. Bullen and Sean P. Kelly and Robert K. Xiao and Steven G. Satterfield and John G. Hagedorn and Judith E. Devaney", "title":"Visualization and Data Mining in an 3D Immersive Environment: Summer Project 2003", "venue":"", "year":"", "window":"be reduced to about three or four. 21 Figure 4.3: The graph of the ChiSquared analysis of the housing data set. 22 4.4 <b>Image Segmentation</b> This data set was analysed by Christian Brown. Overview The Image Segmentation (Seg) data set consisted of data relating numerous analyses of the colors in subdivided", "mykey":2022},
 {"datasetID":101, "supportID":"C9BCF622EC8412DCF06D7FAEF2934109B4199D7C", "rexaID":"48d6beec2a36a87d9d88b6de85dd85a75e5ed24d", "author":"C. Titus Brown and Harry W. Bullen and Sean P. Kelly and Robert K. Xiao and Steven G. Satterfield and John G. Hagedorn and Judith E. Devaney", "title":"Visualization and Data Mining in an 3D Immersive Environment: Summer Project 2003", "venue":"", "year":"", "window":"the Museum Environment . . . . . . . . . . 7 3.4 Visual Component Tools . . . . . . . . . . . . . . . . . . . . . . 8 3.5 Dataflow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 4 Data Sets Analysed 17 4.1 <b>Tic-Tac-Toe</b> . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 4.2 Miles Per Gallon . . . . . . . . . . . . . . . . . . . . . . . . . . 19 4.3 Housing . . . . . . . . . .", "mykey":2023},
 {"datasetID":109, "supportID":"C9BCF622EC8412DCF06D7FAEF2934109B4199D7C", "rexaID":"48d6beec2a36a87d9d88b6de85dd85a75e5ed24d", "author":"C. Titus Brown and Harry W. Bullen and Sean P. Kelly and Robert K. Xiao and Steven G. Satterfield and John G. Hagedorn and Judith E. Devaney", "title":"Visualization and Data Mining in an 3D Immersive Environment: Summer Project 2003", "venue":"", "year":"", "window":"any patterns in the RGB values of the roads and neighboring terrain due to significant rounding of the RGB values. 31 Figure 4.7: The two-color graph for all clear road images. 32 4.8 <b>Wine</b> The Wine data set was analysed by Harry Bullen. This data set contains the chemical analysis of wines from Italy. Wines from three different types of grapes are included. There are levels of 13 chemicals provided for", "mykey":2024},
 {"datasetID":151, "supportID":"CA0EB2AD2AE47FACD2EACFE2F715C8C9ED5938B3", "rexaID":"b859b5cc14ac44c92ce02385f20204fdf9e32714", "author":"Christos Emmanouilidis and A. Hunter and Dr J. MacIntyre", "title":"A Multiobjective Evolutionary Setting for Feature Selection and a Commonality-Based Crossover Operator", "venue":"Centre for Adaptive Systems, School of Computing, Engineering and Technology University of Sunderland", "year":"", "window":"the individual features distribution during evolution (Figure 5, lower row). Once feature selection is completed, final MLP models are built, based on the training and validation data. The <b>sonar</b> data set and, to a lesser extent the ionoshpere, is so sparse that employing a large number of hidden units seems to lead to overfit. Both MLP and PNN models are tested on the independent evaluation data. 0", "mykey":2025},
 {"datasetID":52, "supportID":"CA0EB2AD2AE47FACD2EACFE2F715C8C9ED5938B3", "rexaID":"b859b5cc14ac44c92ce02385f20204fdf9e32714", "author":"Christos Emmanouilidis and A. Hunter and Dr J. MacIntyre", "title":"A Multiobjective Evolutionary Setting for Feature Selection and a Commonality-Based Crossover Operator", "venue":"Centre for Adaptive Systems, School of Computing, Engineering and Technology University of Sunderland", "year":"", "window":"is that it reduces the effect of the noise in fitness evaluation. 6 Experimental Results We demonstrate how our multiobjective evolutionary algorithm feature selection works on two benchmarking data sets of considerable dimensionality. <b>Ionosphere</b>  This data set [21] consists of 351 patterns, with 34 attributes and one output with two classes, good or bad, with good implying evidence of some type of", "mykey":2026},
 {"datasetID":2, "supportID":"CA3D2D438FA6F35DA3CC4D30D7098D2B6814139D", "rexaID":"ce160518d6a429585aeeb3f7a784c5dfb124b669", "author":"Kuan-ming Lin and Chih-Jen Lin", "title":"A Study on Reduced Support Vector Machines", "venue":"Department of Computer Science and Information Engineering National Taiwan University", "year":"", "window":"for protein secondary structure prediction [26]. Finally, the problem <b>adult</b>  from the UCI \"adult\" data set [1] and compiled by Platt [20], is also included. For the adult dataset, there are several realizations. Here, we only consider the realization with the smallest training set; the full dataset with", "mykey":2027},
 {"datasetID":67, "supportID":"CA3D2D438FA6F35DA3CC4D30D7098D2B6814139D", "rexaID":"ce160518d6a429585aeeb3f7a784c5dfb124b669", "author":"Kuan-ming Lin and Chih-Jen Lin", "title":"A Study on Reduced Support Vector Machines", "venue":"Department of Computer Science and Information Engineering National Taiwan University", "year":"", "window":"votes. The implementation of all methods mentioned above is available upon request. V. EXPERIMENTS In this section we conduct experiments on some commonly used problems. We choose large multiclass datasets from the Statlog collection: <b>dna</b>  satimage, letter, and shuttle [16]. We also consider mnist [9], an important benchmark for handwritten digit recognition. The problem ijcnn1 is from the first", "mykey":2028},
 {"datasetID":68, "supportID":"CA3D2D438FA6F35DA3CC4D30D7098D2B6814139D", "rexaID":"ce160518d6a429585aeeb3f7a784c5dfb124b669", "author":"Kuan-ming Lin and Chih-Jen Lin", "title":"A Study on Reduced Support Vector Machines", "venue":"Department of Computer Science and Information Engineering National Taiwan University", "year":"", "window":"for handwritten digit recognition. The problem ijcnn1 is from the first problem of IJCNN challenge 2001 [21]. Note that we use the winner's transformation of raw data [2]. Problem protein, is a data set for <b>protein secondary</b> structure prediction [26]. Finally, the problem adult, from the UCI \"adult\" data set [1] and compiled by Platt [20], is also included. For the adult dataset, there are several", "mykey":2029},
 {"datasetID":154, "supportID":"CA3D2D438FA6F35DA3CC4D30D7098D2B6814139D", "rexaID":"ce160518d6a429585aeeb3f7a784c5dfb124b669", "author":"Kuan-ming Lin and Chih-Jen Lin", "title":"A Study on Reduced Support Vector Machines", "venue":"Department of Computer Science and Information Engineering National Taiwan University", "year":"", "window":"for handwritten digit recognition. The problem ijcnn1 is from the first problem of IJCNN challenge 2001 [21]. Note that we use the winner's transformation of raw data [2]. Problem <b>protein</b>  is a data set for protein secondary structure prediction [26]. Finally, the problem adult, from the UCI \"adult\" data set [1] and compiled by Platt [20], is also included. For the adult dataset, there are several", "mykey":2030},
 {"datasetID":98, "supportID":"CA3D2D438FA6F35DA3CC4D30D7098D2B6814139D", "rexaID":"ce160518d6a429585aeeb3f7a784c5dfb124b669", "author":"Kuan-ming Lin and Chih-Jen Lin", "title":"A Study on Reduced Support Vector Machines", "venue":"Department of Computer Science and Information Engineering National Taiwan University", "year":"", "window":"votes. The implementation of all methods mentioned above is available upon request. V. EXPERIMENTS In this section we conduct experiments on some commonly used problems. We choose large multiclass datasets from the <b>Statlog</b> collection: dna, satimage, letter, and shuttle [16]. We also consider mnist [9], an important benchmark for handwritten digit recognition. The problem ijcnn1 is from the first", "mykey":2031},
 {"datasetID":14, "supportID":"CA467D091BA5B36ED20FAA7672080E679EE82E01", "rexaID":"877f66dae749b55055910c2f9ad01e4208eb9042", "author":"Michael R. Berthold and Klaus--Peter Huber", "title":"From Radial to Rectangular Basis Functions: A new Approach for Rule Learning from Large Datasets", "venue":"Institut fur Rechnerentwurf und Fehlertoleranz (Prof. D. Schmid) Universitat Karlsruhe", "year":"", "window":"C. Further Results This approach was also applied to the <b>breast</b>  <b>cancer</b> dataset (see [9]), originating from a real world application. This dataset contains about 700 patterns, each pattern described by 9 real--valued attributes. Interestingly the RecBF network trained on the", "mykey":2032},
 {"datasetID":53, "supportID":"CA467D091BA5B36ED20FAA7672080E679EE82E01", "rexaID":"877f66dae749b55055910c2f9ad01e4208eb9042", "author":"Michael R. Berthold and Klaus--Peter Huber", "title":"From Radial to Rectangular Basis Functions: A new Approach for Rule Learning from Large Datasets", "venue":"Institut fur Rechnerentwurf und Fehlertoleranz (Prof. D. Schmid) Universitat Karlsruhe", "year":"", "window":"extracted from a Neural Network trained on the data, rather than from the data itself. In this scenario the Neural Network already took care of the noisy patterns. B. The <b>IRIS</b> -data This very famous dataset from Fisher ([5]) contains three classes of 50 instances each, where each class refers to a type of iris plant. One class is linearly separable from the other two; the latter are not linearly", "mykey":2033},
 {"datasetID":32, "supportID":"CA755E6DE40ACE7FE9DA40FA67942E2E90865836", "rexaID":"6046926fc134386ba1ce22047b0146525a5440b7", "author":"Carlos J. Alonso Gonzalez and Juan J. Rodr and iguez Diez", "title":"Time Series Classification by Boosting Interval Based Literals", "venue":"Grupo de Sistemas Inteligentes Departamento de Informatica Universidad de Valladolid", "year":"", "window":"for classification of time series are not easy to find [9]. For this reason we have used four artificial datasets and only one \"real world\" dataset: <b>Cylinder</b>  Bell and Funnel (CBF). This is an artificial problem, introduced in [12]. The learning task is to distinguish between three classes: cylinder (c), bell", "mykey":2034},
 {"datasetID":107, "supportID":"CA755E6DE40ACE7FE9DA40FA67942E2E90865836", "rexaID":"6046926fc134386ba1ce22047b0146525a5440b7", "author":"Carlos J. Alonso Gonzalez and Juan J. Rodr and iguez Diez", "title":"Time Series Classification by Boosting Interval Based Literals", "venue":"Grupo de Sistemas Inteligentes Departamento de Informatica Universidad de Valladolid", "year":"", "window":"after this time. t 3 is in [n/3, 2n/3]. 6. Downward: y(t) = m+ rs - kx. Figure 3.b shows two examples of three of the classes. The data used was obtained from the UCI KDD Archive [4]. <b>Waveform</b>  This dataset was introduced by [7]. The purpouse is to distinguish between three classes, defined by the evaluation in 1, 2 . . . 21, of the following functions: x 1 (i) = uh 1 (i) + (1x 2 (i) = uh 1 (i) + (1x 3", "mykey":2035},
 {"datasetID":108, "supportID":"CA755E6DE40ACE7FE9DA40FA67942E2E90865836", "rexaID":"6046926fc134386ba1ce22047b0146525a5440b7", "author":"Carlos J. Alonso Gonzalez and Juan J. Rodr and iguez Diez", "title":"Time Series Classification by Boosting Interval Based Literals", "venue":"Grupo de Sistemas Inteligentes Departamento de Informatica Universidad de Valladolid", "year":"", "window":"after this time. t 3 is in [n/3, 2n/3]. 6. Downward: y(t) = m+ rs - kx. Figure 3.b shows two examples of three of the classes. The data used was obtained from the UCI KDD Archive [4]. <b>Waveform</b>  This dataset was introduced by [7]. The purpouse is to distinguish between three classes, defined by the evaluation in 1, 2 . . . 21, of the following functions: x 1 (i) = uh 1 (i) + (1x 2 (i) = uh 1 (i) + (1x 3", "mykey":2036},
 {"datasetID":107, "supportID":"CAF8756A123971D12444BC518713310A647353E6", "rexaID":"339f8af547b087e9b52de872e8731ad168816065", "author":"Bede Liu and Mingzeng Hu and Wynne Hsu", "title":"Multi-level organization and summarization of the discovered rules", "venue":"KDD", "year":"2000", "window":"646 91 0.05 14 kdd 5914 68 0.22 15 mushroom 2398 57 0.08 16 pima 44 12 0.01 17 satimage 7515 191 0.30 18 splice 4302 100 0.15 19 tic-tac 266 11 0.02 20 <b>waveform</b> 1480 106 0.07 Average 1487.7 50 0.07 Dataset Table 1: Experiment results with decision trees No. of decision tree leaves No. of GSE tree leaves 105.2 38.1 doctors said that they could not obtain an overall picture of the domain from the", "mykey":2037},
 {"datasetID":108, "supportID":"CAF8756A123971D12444BC518713310A647353E6", "rexaID":"339f8af547b087e9b52de872e8731ad168816065", "author":"Bede Liu and Mingzeng Hu and Wynne Hsu", "title":"Multi-level organization and summarization of the discovered rules", "venue":"KDD", "year":"2000", "window":"646 91 0.05 14 kdd 5914 68 0.22 15 mushroom 2398 57 0.08 16 pima 44 12 0.01 17 satimage 7515 191 0.30 18 splice 4302 100 0.15 19 tic-tac 266 11 0.02 20 <b>waveform</b> 1480 106 0.07 Average 1487.7 50 0.07 Dataset Table 1: Experiment results with decision trees No. of decision tree leaves No. of GSE tree leaves 105.2 38.1 doctors said that they could not obtain an overall picture of the domain from the", "mykey":2038},
 {"datasetID":90, "supportID":"CB278830C58441C1FCABCCFBC8F7C127B281C01F", "rexaID":"b9e2593b03c2cf4e75d8e23fedff3571357f94d8", "author":"Marco Porta and Subramani Mani and Suzanne McDermott", "title":"MENTOR: Building Bayesian Network Models in Medicine CSCE Technical Report TR-2002-016", "venue":"Department of Computer Science and Engineering University of South Carolina", "year":"2002", "window":"Our validation tests using LED, ALARM and <b>SOYBEAN</b> which are small to large artificial datasets used for Machine Learning research and available from the University of California at 10 the Irvine Machine Learning repository [MuAh94] gave a mean accuracy of 80% over ten runs. The range was", "mykey":2039},
 {"datasetID":91, "supportID":"CB278830C58441C1FCABCCFBC8F7C127B281C01F", "rexaID":"b9e2593b03c2cf4e75d8e23fedff3571357f94d8", "author":"Marco Porta and Subramani Mani and Suzanne McDermott", "title":"MENTOR: Building Bayesian Network Models in Medicine CSCE Technical Report TR-2002-016", "venue":"Department of Computer Science and Engineering University of South Carolina", "year":"2002", "window":"Our validation tests using LED, ALARM and <b>SOYBEAN</b> which are small to large artificial datasets used for Machine Learning research and available from the University of California at 10 the Irvine Machine Learning repository [MuAh94] gave a mean accuracy of 80% over ten runs. The range was", "mykey":2040},
 {"datasetID":48, "supportID":"CBA38ADC105B8B99FD109C5967472AE614A0B0DD", "rexaID":"965bcc99a91fa65151f0985f3813992c6403c2f6", "author":"Tapani Raiko and Harri Valpola", "title":"MISSING VALUES IN NONLINEAR FACTOR ANALYSIS", "venue":"Helsinki University of Technology, Neural Networks Research Centre", "year":"", "window":"The training set contains vectors more similar to the test set now. 4. Training and testing sets are permuted and 10 percent of the values are set to miss independently of any neighbours. The second data set is Boston <b>housing</b> data, which is publicly available at [2]. It concerns housing values in suburbs of Boston. Data set contains 506 vectors of 13 dimensions excluding one binary attribute. Four of", "mykey":2041},
 {"datasetID":14, "supportID":"CC16A0BD95C49368A00330A580BD943E0B0283F9", "rexaID":"705b438dbe9ed18fe23005c774d2993019da030f", "author":"Jarkko Salojarvi and Samuel Kaski and Janne Sinkkonen", "title":"Discriminative clustering in Fisher metrics", "venue":"Neural Networks Research Centre Helsinki University of Technology", "year":"", "window":"and secondly through the density function estimate that generates the metric used to define the Fisherian Voronoi regions. IV. EXPERIMENTS Experiments were run with the Wisconsin <b>breast</b> <b>cancer</b> data set from the UCI machine learning repository [9]. The 569 samples consisted of 30 attributes, measured from malignant and benign tumors. We chose the ordinary k-means as the baseline reference method.", "mykey":2042},
 {"datasetID":17, "supportID":"CC16A0BD95C49368A00330A580BD943E0B0283F9", "rexaID":"705b438dbe9ed18fe23005c774d2993019da030f", "author":"Jarkko Salojarvi and Samuel Kaski and Janne Sinkkonen", "title":"Discriminative clustering in Fisher metrics", "venue":"Neural Networks Research Centre Helsinki University of Technology", "year":"", "window":"and secondly through the density function estimate that generates the metric used to define the Fisherian Voronoi regions. IV. EXPERIMENTS Experiments were run with the <b>Wisconsin</b> <b>breast</b> <b>cancer</b> data set from the UCI machine learning repository [9]. The 569 samples consisted of 30 attributes, measured from malignant and benign tumors. We chose the ordinary k-means as the baseline reference method.", "mykey":2043},
 {"datasetID":15, "supportID":"CC16A0BD95C49368A00330A580BD943E0B0283F9", "rexaID":"705b438dbe9ed18fe23005c774d2993019da030f", "author":"Jarkko Salojarvi and Samuel Kaski and Janne Sinkkonen", "title":"Discriminative clustering in Fisher metrics", "venue":"Neural Networks Research Centre Helsinki University of Technology", "year":"", "window":"and secondly through the density function estimate that generates the metric used to define the Fisherian Voronoi regions. IV. EXPERIMENTS Experiments were run with the <b>Wisconsin</b> <b>breast</b> <b>cancer</b> data set from the UCI machine learning repository [9]. The 569 samples consisted of 30 attributes, measured from malignant and benign tumors. We chose the ordinary k-means as the baseline reference method.", "mykey":2044},
 {"datasetID":16, "supportID":"CC16A0BD95C49368A00330A580BD943E0B0283F9", "rexaID":"705b438dbe9ed18fe23005c774d2993019da030f", "author":"Jarkko Salojarvi and Samuel Kaski and Janne Sinkkonen", "title":"Discriminative clustering in Fisher metrics", "venue":"Neural Networks Research Centre Helsinki University of Technology", "year":"", "window":"and secondly through the density function estimate that generates the metric used to define the Fisherian Voronoi regions. IV. EXPERIMENTS Experiments were run with the <b>Wisconsin</b> <b>breast</b> <b>cancer</b> data set from the UCI machine learning repository [9]. The 569 samples consisted of 30 attributes, measured from malignant and benign tumors. We chose the ordinary k-means as the baseline reference method.", "mykey":2045},
 {"datasetID":34, "supportID":"CC4DCDEC5D2F04B5919B0DAFB383C1C51DD55B30", "rexaID":"87afa910f706df9e28bfa697d6d2eea7c0cb53ef", "author":"Ilya Blayvas and Ron Kimmel", "title":"Efficient Classification via Multiresolution Training Set Approximation", "venue":"CS Dept. Technion", "year":"", "window":"\u00b7 D). 3 Experimental Results The proposed method was implemented in VC++ 6.0 and run on `IBM PC 300 PL' with 600MHZ Pentium III processor and 256MB RAM. It was tested on the Pima Indians <b>Diabetes</b> dataset [10], and a large artificial dataset generated with the DatGen program [11]. The results were compared to the Smooth SVM [12] and Sparse Grids [3]. Figure 7: Partition of 2D feature space for a", "mykey":2046},
 {"datasetID":79, "supportID":"CC4DCDEC5D2F04B5919B0DAFB383C1C51DD55B30", "rexaID":"87afa910f706df9e28bfa697d6d2eea7c0cb53ef", "author":"Ilya Blayvas and Ron Kimmel", "title":"Efficient Classification via Multiresolution Training Set Approximation", "venue":"CS Dept. Technion", "year":"", "window":"\u00b7 D). 3 Experimental Results The proposed method was implemented in VC++ 6.0 and run on `IBM PC 300 PL' with 600MHZ Pentium III processor and 256MB RAM. It was tested on the <b>Pima</b> <b>Indians</b> <b>Diabetes</b> dataset [10], and a large artificial dataset generated with the DatGen program [11]. The results were compared to the Smooth SVM [12] and Sparse Grids [3]. Figure 7: Partition of 2D feature space for a", "mykey":2047},
 {"datasetID":14, "supportID":"CD357542A63AACEAA111F454330F8A9CDC39ED36", "rexaID":"3e9ebff12a232c9f091156827e92c55d259b95f3", "author":"Nikunj C. Oza and Stuart J. Russell", "title":"Online Bagging and Boosting", "venue":"Computer Science Division University of California", "year":"", "window":"Bagging and online bagging performed noticeably better than single decision trees on all except the <b>Breast</b> <b>Cancer</b> dataset. With Naive Bayes, bagging and online bagging never performed noticeably better than Naive Bayes, which we expected because of the stability of Naive Bayes [3]. Boosting and online boosting", "mykey":2048},
 {"datasetID":19, "supportID":"CD357542A63AACEAA111F454330F8A9CDC39ED36", "rexaID":"3e9ebff12a232c9f091156827e92c55d259b95f3", "author":"Nikunj C. Oza and Stuart J. Russell", "title":"Online Bagging and Boosting", "venue":"Computer Science Division University of California", "year":"", "window":"ITI online algorithm [14]; batch and online Naive Bayes algorithms are essentially identical. To illustrate the convergence of batch and online learning, we experimented with the <b>Car</b> Evaluation dataset from the UCI Machine Learning Repository [2]. The dataset has 1728 examples, of which we retained 346 (20%) as a test set and used 200, 400, 600, 800, 1000, 1200, and all the remaining 1382 examples", "mykey":2049},
 {"datasetID":23, "supportID":"CD357542A63AACEAA111F454330F8A9CDC39ED36", "rexaID":"3e9ebff12a232c9f091156827e92c55d259b95f3", "author":"Nikunj C. Oza and Stuart J. Russell", "title":"Online Bagging and Boosting", "venue":"Computer Science Division University of California", "year":"", "window":"AdaBoost performed significantly worse and online boosting performed marginally worse. On the Car Evaluation and <b>Chess</b> datasets, AdaBoost and online boosting performed significantly better than Naive Bayes. On the Nursery dataset, AdaBoost performed significantly better and online boosting performed marginally better. 5", "mykey":2050},
 {"datasetID":21, "supportID":"CD357542A63AACEAA111F454330F8A9CDC39ED36", "rexaID":"3e9ebff12a232c9f091156827e92c55d259b95f3", "author":"Nikunj C. Oza and Stuart J. Russell", "title":"Online Bagging and Boosting", "venue":"Computer Science Division University of California", "year":"", "window":"AdaBoost performed significantly worse and online boosting performed marginally worse. On the Car Evaluation and <b>Chess</b> datasets, AdaBoost and online boosting performed significantly better than Naive Bayes. On the Nursery dataset, AdaBoost performed significantly better and online boosting performed marginally better. 5", "mykey":2051},
 {"datasetID":22, "supportID":"CD357542A63AACEAA111F454330F8A9CDC39ED36", "rexaID":"3e9ebff12a232c9f091156827e92c55d259b95f3", "author":"Nikunj C. Oza and Stuart J. Russell", "title":"Online Bagging and Boosting", "venue":"Computer Science Division University of California", "year":"", "window":"AdaBoost performed significantly worse and online boosting performed marginally worse. On the Car Evaluation and <b>Chess</b> datasets, AdaBoost and online boosting performed significantly better than Naive Bayes. On the Nursery dataset, AdaBoost performed significantly better and online boosting performed marginally better. 5", "mykey":2052},
 {"datasetID":76, "supportID":"CD357542A63AACEAA111F454330F8A9CDC39ED36", "rexaID":"3e9ebff12a232c9f091156827e92c55d259b95f3", "author":"Nikunj C. Oza and Stuart J. Russell", "title":"Online Bagging and Boosting", "venue":"Computer Science Division University of California", "year":"", "window":"AdaBoost and online boosting performed significantly better than Naive Bayes. On the <b>Nursery</b> dataset, AdaBoost performed significantly better and online boosting performed marginally better. 5 Conclusions The paper has described online versions of the popular bagging and boosting algorithms and has", "mykey":2053},
 {"datasetID":90, "supportID":"CD357542A63AACEAA111F454330F8A9CDC39ED36", "rexaID":"3e9ebff12a232c9f091156827e92c55d259b95f3", "author":"Nikunj C. Oza and Stuart J. Russell", "title":"Online Bagging and Boosting", "venue":"Computer Science Division University of California", "year":"", "window":"and their performances relative to a single Naive Bayes classifier consistently improved as the sizes of the datasets grew. On the Balance and <b>Soybean</b> datasets, the boosting algorithms performed signi#cantly worse than Naive Bayes. On the Breast Cancer dataset, AdaBoost performed significantly worse and online", "mykey":2054},
 {"datasetID":91, "supportID":"CD357542A63AACEAA111F454330F8A9CDC39ED36", "rexaID":"3e9ebff12a232c9f091156827e92c55d259b95f3", "author":"Nikunj C. Oza and Stuart J. Russell", "title":"Online Bagging and Boosting", "venue":"Computer Science Division University of California", "year":"", "window":"and their performances relative to a single Naive Bayes classifier consistently improved as the sizes of the datasets grew. On the Balance and <b>Soybean</b> datasets, the boosting algorithms performed signi#cantly worse than Naive Bayes. On the Breast Cancer dataset, AdaBoost performed significantly worse and online", "mykey":2055},
 {"datasetID":14, "supportID":"CD6FD34EA5A5C1F7388324FCCEC3B50EE74C6E1C", "rexaID":"42ba5137ab9d88dc1eae5caac18cb6a1818ae700", "author":"Rudy Setiono", "title":"Extracting M-of-N Rules from Trained Neural Networks", "venue":"School of Computing National University of Singapore", "year":"", "window":"of the data were converted to 126 binary inputs before training. In order to reduce computation time, only 2000 randomly selected samples were used. 4. The Wisconsin <b>breast</b> <b>cancer</b> classification dataset [17]. Each of the 699 patterns in the 16 TABLE I: The initial network topology (input, hidden and output units) and the average user time required for training and pruning. Figures in parentheses", "mykey":2056},
 {"datasetID":17, "supportID":"CD6FD34EA5A5C1F7388324FCCEC3B50EE74C6E1C", "rexaID":"42ba5137ab9d88dc1eae5caac18cb6a1818ae700", "author":"Rudy Setiono", "title":"Extracting M-of-N Rules from Trained Neural Networks", "venue":"School of Computing National University of Singapore", "year":"", "window":"of the data were converted to 126 binary inputs before training. In order to reduce computation time, only 2000 randomly selected samples were used. 4. The <b>Wisconsin</b> <b>breast</b> <b>cancer</b> classification dataset [17]. Each of the 699 patterns in the 16 TABLE I: The initial network topology (input, hidden and output units) and the average user time required for training and pruning. Figures in parentheses", "mykey":2057},
 {"datasetID":15, "supportID":"CD6FD34EA5A5C1F7388324FCCEC3B50EE74C6E1C", "rexaID":"42ba5137ab9d88dc1eae5caac18cb6a1818ae700", "author":"Rudy Setiono", "title":"Extracting M-of-N Rules from Trained Neural Networks", "venue":"School of Computing National University of Singapore", "year":"", "window":"of the data were converted to 126 binary inputs before training. In order to reduce computation time, only 2000 randomly selected samples were used. 4. The <b>Wisconsin</b> <b>breast</b> <b>cancer</b> classification dataset [17]. Each of the 699 patterns in the 16 TABLE I: The initial network topology (input, hidden and output units) and the average user time required for training and pruning. Figures in parentheses", "mykey":2058},
 {"datasetID":16, "supportID":"CD6FD34EA5A5C1F7388324FCCEC3B50EE74C6E1C", "rexaID":"42ba5137ab9d88dc1eae5caac18cb6a1818ae700", "author":"Rudy Setiono", "title":"Extracting M-of-N Rules from Trained Neural Networks", "venue":"School of Computing National University of Singapore", "year":"", "window":"of the data were converted to 126 binary inputs before training. In order to reduce computation time, only 2000 randomly selected samples were used. 4. The <b>Wisconsin</b> <b>breast</b> <b>cancer</b> classification dataset [17]. Each of the 699 patterns in the 16 TABLE I: The initial network topology (input, hidden and output units) and the average user time required for training and pruning. Figures in parentheses", "mykey":2059},
 {"datasetID":67, "supportID":"CD6FD34EA5A5C1F7388324FCCEC3B50EE74C6E1C", "rexaID":"42ba5137ab9d88dc1eae5caac18cb6a1818ae700", "author":"Rudy Setiono", "title":"Extracting M-of-N Rules from Trained Neural Networks", "venue":"School of Computing National University of Singapore", "year":"", "window":"M-of-N rule extraction algorithm. This problem has been used by Towell and Shavlik [3] to test their algorithms which extract symbolic rules from knowledge based neural networks. Each sample in the dataset is described by a 60-nucleotide-long <b>DNA</b> sequence. A nucleotide may assume one of the 4 possible values: G (Guanine), T (Thymine), C (Cytosine), or A (Adenine). These values are binary coded as", "mykey":2060},
 {"datasetID":69, "supportID":"CD6FD34EA5A5C1F7388324FCCEC3B50EE74C6E1C", "rexaID":"42ba5137ab9d88dc1eae5caac18cb6a1818ae700", "author":"Rudy Setiono", "title":"Extracting M-of-N Rules from Trained Neural Networks", "venue":"School of Computing National University of Singapore", "year":"", "window":"used in the experiments are publicly available via anonymous ftp from ics.uci.edu [14]. These dataset are: 1. The <b>splice</b> junction dataset. The characteristics of the patterns in this dataset have been described in the previous section. The dataset that consists of 1006 patterns was used. 2. The 3", "mykey":2061},
 {"datasetID":73, "supportID":"CD6FD34EA5A5C1F7388324FCCEC3B50EE74C6E1C", "rexaID":"42ba5137ab9d88dc1eae5caac18cb6a1818ae700", "author":"Rudy Setiono", "title":"Extracting M-of-N Rules from Trained Neural Networks", "venue":"School of Computing National University of Singapore", "year":"", "window":"or (jacket-color is not blue and body-shape is not octagon). Each input attribute value is coded as -1 or 1. Hence, 17 input units are required. The total number of patterns in each of the three datasets is 432. 3. The <b>mushroom</b> classification dataset [16]. The dataset consists of 8124 samples, each of which is described by 22 nominal attributes that are the characteristics of species of mushroom.", "mykey":2062},
 {"datasetID":90, "supportID":"CD89427297DDF91BAE9ED58EB01B3702C9FC74B6", "rexaID":"1f4e06390d7a257b58508b73ed2a689add6a9748", "author":"Bianca Zadrozny", "title":"Reducing multiclass to binary by coupling probability estimates", "venue":"NIPS", "year":"2001", "window":"better. Figure 1 shows how the MSE is lowered at each iteration of the Hastie-Tibshirani algorithm, for the three types of code matrices. Table 3 shows the results of the same experiments on the datasets pendigits and <b>soybean</b>  Again, the MSE is significantly lowered by the iterative procedure, in all cases. For the soybean dataset, using the sparse random matrix, the iterative method again has a", "mykey":2063},
 {"datasetID":91, "supportID":"CD89427297DDF91BAE9ED58EB01B3702C9FC74B6", "rexaID":"1f4e06390d7a257b58508b73ed2a689add6a9748", "author":"Bianca Zadrozny", "title":"Reducing multiclass to binary by coupling probability estimates", "venue":"NIPS", "year":"2001", "window":"better. Figure 1 shows how the MSE is lowered at each iteration of the Hastie-Tibshirani algorithm, for the three types of code matrices. Table 3 shows the results of the same experiments on the datasets pendigits and <b>soybean</b>  Again, the MSE is significantly lowered by the iterative procedure, in all cases. For the soybean dataset, using the sparse random matrix, the iterative method again has a", "mykey":2064},
 {"datasetID":102, "supportID":"CE6E289B8764A8F4DD08C139A74E7417D3AC0B13", "rexaID":"a89b63f8dd0c09e654eaa69a4801b0f52a970016", "author":"Salvatore J. Stolfo and Andreas L. Prodromidis and Shelley Tselepis and Wenke Lee and David W. Fan and Philip K. Chan", "title":"JAM: Java Agents for Meta-Learning over Distributed Databases", "venue":"KDD", "year":"1997", "window":"is a medical database with records (Merz & Murphy 1996), noted by <b>thyroid</b> in the Data Set panel. Other parameters include the host of the CFM, the CrossValidation Fold, the Meta-Learning Fold, the MetaLearning Level, the names of the local learning agent and the local meta-learning", "mykey":2065},
 {"datasetID":42, "supportID":"CEDC9CDA450BF7D7749FF0BB8528E6E638F2E33C", "rexaID":"8be22c7cb4b6b5d2147c5376492279b13ba12e07", "author":"", "title":"Eectiveness of Error Correcting Output Coding methods in ensemble and monolithic learning machines", "venue":"Dipartimento di Informatica, Universitdi Pisa", "year":"", "window":"and composed by normal distributed clusters of data. The set p6 contains 6 classes with no overlapping regions, while the regions of the 9 classes of p9 hardly overlap. <b>Glass</b>  letter and optdigits data sets are from the UCI repository [42]. In the experimentation we have used exhaustive [17] and BCH ECOC generation algorithms [8]. ECOC exhaustive algorithms select among all possible 2 K dichotomies", "mykey":2066},
 {"datasetID":98, "supportID":"CFC31FD28AF7B757D979A7559890A4BE31F04773", "rexaID":"3df4cdc6ac34e8ad8f2800107ccd6373c7e3505e", "author":"Guido Lindner and Rudi Studer", "title":"Algorithm Selection Support for Classification", "venue":"DaimlerChrysler AG, Research & Technology FT3/KL", "year":"", "window":"[Sleeman et al., 1995]. Such an approach is very di\u00c6cult to maintain: each time a new algorithm has to be included one has to recompute all the rules. The <b>Statlog</b> project tried to describe data sets for a meta learning step to generate rules that specify in which case which algorithm is (possibly) applicable. The generated rules use hard boundaries within their condition part. However, instead", "mykey":2067},
 {"datasetID":53, "supportID":"CFF54D1C3089FAE2CDE4296F986137EE7D6EAAF7", "rexaID":"b417429a1fe95e22673736db6f6adc8ecae2ef7d", "author":"Lawrence O. Hall and Nitesh V. Chawla and Kevin W. Bowyer", "title":"Decision Tree Learning on Very Large Data Sets", "venue":"Department of Computer Science and Engineering, ENB 118 University of South Florida", "year":"", "window":"0.6 < Petal-Width <= 1.5 and Petal-Length } 4.9 --} <b>Iris</b> Viginica R5: If 1.5 < Petal-Width <= 1.7 and Petal-Length } 4.9 --} Iris-Versicolor <= 1.7 Figure 1. The C4.5 tree produced on the full Iris dataset and the corresponding rules. The final rules will be ordered by their accuracy taken from the original tree in all cases except for conflict resolution rules for which the accuracy is calculated on", "mykey":2068},
 {"datasetID":2, "supportID":"D05D5496F9011ABD68AF842228671FF3C4DC713D", "rexaID":"0ded09c3d904c9f8eeef9ed0bc3445c2b44a6008", "author":"William W. Cohen and Yoram Singer", "title":"A Simple, Fast, and Effective Rule Learner", "venue":"AT&T Labs--Research Shannon Laboratory", "year":"", "window":"average ranks among these three are 1.8, 2.3, and 1.9. The largest ruleset produced by SLIPPER is 49 rules (for coding). Finally, we evaluated the scalability of the rule learners on several large datasets. We used <b>adult</b>  blackjack, with the addition of 20 irrelevant noise variables; and market3, for which many examples were available. C4rules was not run, since it is known to have scalability", "mykey":2069},
 {"datasetID":67, "supportID":"D0A398F35E1316D1CB00D8F0A940902C4E20CD96", "rexaID":"16ecc3d56b302443c748705dd289fcdcb7f2bba0", "author":"Ron Kohavi and Dan Sommerfield", "title":"To Appear in KDD-98 Targeting Business Users with Decision Table Classifiers", "venue":"Data Mining and Visualization Silicon Graphics, Inc", "year":"", "window":"In fact, it is quite surprising to see how well decision tables perform with five or fewer attributes. Out of 16 natural datasets, only <b>DNA</b> and german used more than five attributes (chess is semi-natural) with DTMaj, and only DNA and letter used more than seven attributes with DTLoc. Such relatively small classifiers can be", "mykey":2070},
 {"datasetID":70, "supportID":"D0A398F35E1316D1CB00D8F0A940902C4E20CD96", "rexaID":"16ecc3d56b302443c748705dd289fcdcb7f2bba0", "author":"Ron Kohavi and Dan Sommerfield", "title":"To Appear in KDD-98 Targeting Business Users with Decision Table Classifiers", "venue":"Data Mining and Visualization Silicon Graphics, Inc", "year":"", "window":"available, mostly natural but also a few artificial ones (m-of-n and the <b>monk</b> problems). The artificial datasets were tested on the given training and test sets. The natural datasets were evaluated using 10fold cross-validation if the file size was less than 3,000 records (to ensure a small standard deviation", "mykey":2071},
 {"datasetID":19, "supportID":"D0E4A339AF59CF076EAA84DB61F2A1DAF10544B2", "rexaID":"f14d3edaeac2280dc4e49948d9d0fc1159bd05ca", "author":"Daniel J. Lizotte and Omid Madani and Russell Greiner", "title":"Budgeted Learning of Naive-Bayes Classifiers", "venue":"UAI", "year":"2003", "window":"at 50 purchases is better than its performance at 50 when the budget is set at 300. Other policies do not take the budget into account. We have observed the same overall patterns on several other datasets that we have tested the policies on so far  <b>CAR</b>  DIABETES, CHESS, BREAST): the performance of SFL is superior or comparable to the performance of other policies, and Biased-Robin is the best", "mykey":2072},
 {"datasetID":105, "supportID":"D0E4A339AF59CF076EAA84DB61F2A1DAF10544B2", "rexaID":"f14d3edaeac2280dc4e49948d9d0fc1159bd05ca", "author":"Daniel J. Lizotte and Omid Madani and Russell Greiner", "title":"Budgeted Learning of Naive-Bayes Classifiers", "venue":"UAI", "year":"2003", "window":"problem with nine features that can take on between two and five values. The relative performances of the policies are closer to each other, but their behaviour is similar to Figure 4(b). The <b>votes dataset</b> (Figure 4(d)) is a binary class problem (democrat vs. republican), with 16 binary features, 435 instances, and a positive class probability of 0.61. In the votes dataset, there is a high proportion", "mykey":2073},
 {"datasetID":73, "supportID":"D0E4A339AF59CF076EAA84DB61F2A1DAF10544B2", "rexaID":"f14d3edaeac2280dc4e49948d9d0fc1159bd05ca", "author":"Daniel J. Lizotte and Omid Madani and Russell Greiner", "title":"Budgeted Learning of Naive-Bayes Classifiers", "venue":"UAI", "year":"2003", "window":"from the UCI Machine Learning Repository [BM98]. These plots show cross validation error (20% of the dataset) on the <b>mushroom</b> and votes datasets of the different policies. Each point is an average of 50 trials where in each trial a random balanced partition of classes was made for training and validation.", "mykey":2074},
 {"datasetID":76, "supportID":"D0E4A339AF59CF076EAA84DB61F2A1DAF10544B2", "rexaID":"f14d3edaeac2280dc4e49948d9d0fc1159bd05ca", "author":"Daniel J. Lizotte and Omid Madani and Russell Greiner", "title":"Budgeted Learning of Naive-Bayes Classifiers", "venue":"UAI", "year":"2003", "window":"75 times, while a non-discriminative feature such as feature 18 is bought an average of only 2 times. For some budgets, the 0/1 error of SFL is nearly half that generated by round robin. The <b>nursery</b> dataset (Figure 4(c)) is a five class problem with nine features that can take on between two and five values. The relative performances of the policies are closer to each other, but their behaviour is", "mykey":2075},
 {"datasetID":40, "supportID":"D0FE18451EDCBDFEE85C34DEB930F22FB2196039", "rexaID":"d4ea16b5ea06d216f7d2be28615fa9c66b98bbea", "author":"George H. John and Ron Kohavi and Karl Pfleger", "title":"Irrelevant Features and the Subset Selection Problem", "venue":"ICML", "year":"1994", "window":"The C4.5 program is the program that comes with Quinlan's book (Quinlan 1992); the ID3 results were obtained by running C4.5 and using the unpruned trees. On the artificial datasets, we used the ``-s -m1'' C4.5 <b>flags</b>  which indicate that subset splits may be used and that splitting should continue until purity. To estimate the accuracy for feature subsets, we used 25-fold", "mykey":2076},
 {"datasetID":53, "supportID":"D0FE18451EDCBDFEE85C34DEB930F22FB2196039", "rexaID":"d4ea16b5ea06d216f7d2be28615fa9c66b98bbea", "author":"George H. John and Ron Kohavi and Karl Pfleger", "title":"Irrelevant Features and the Subset Selection Problem", "venue":"ICML", "year":"1994", "window":"performance was on parity5+5 and CorrAL using stepwise backward elimination, which reduced the error to 0% from 50% and 18.8% respectively. Experiments were also run on the <b>Iris</b>  Thyroid, and Monk1* datasets. The results on these datasets were similar to those reported in this paper. We observed high variance in the 25-fold crossvalidation estimates of the error. Since our algorithms depend on", "mykey":2077},
 {"datasetID":56, "supportID":"D0FE18451EDCBDFEE85C34DEB930F22FB2196039", "rexaID":"d4ea16b5ea06d216f7d2be28615fa9c66b98bbea", "author":"George H. John and Ron Kohavi and Karl Pfleger", "title":"Irrelevant Features and the Subset Selection Problem", "venue":"ICML", "year":"1994", "window":"was divided by Quinlan into 490 training instances and 200 test instances. <b>Labor</b> The dataset contains instances for acceptable and unacceptable contracts. It is a small dataset with 16 features, a training set of 40 instances, and a test set of 17 instances. Our results show that the main", "mykey":2078},
 {"datasetID":102, "supportID":"D0FE18451EDCBDFEE85C34DEB930F22FB2196039", "rexaID":"d4ea16b5ea06d216f7d2be28615fa9c66b98bbea", "author":"George H. John and Ron Kohavi and Karl Pfleger", "title":"Irrelevant Features and the Subset Selection Problem", "venue":"ICML", "year":"1994", "window":"performance was on parity5+5 and CorrAL using stepwise backward elimination, which reduced the error to 0% from 50% and 18.8% respectively. Experiments were also run on the Iris, <b>Thyroid</b>  and Monk1* datasets. The results on these datasets were similar to those reported in this paper. We observed high variance in the 25-fold crossvalidation estimates of the error. Since our algorithms depend on", "mykey":2079},
 {"datasetID":52, "supportID":"D174FB126EF41AB1A0DC96DA273D57E5BDF4A8C5", "rexaID":"5b3417aa2824988405f9ac934b692af30729b447", "author":"Jennifer G. Dy and Carla Brodley", "title":"Feature Selection for Unsupervised Learning", "venue":"Journal of Machine Learning Research, 5", "year":"2004", "window":"features. 25 Dy and Brodley . Feature selection obtained better results than without feature selection. 6.5 Experiments on Real Data We examine the FSSEM variants on the iris, wine, and <b>ionosphere</b> data set from the UCI learning repository (Blake & Merz, 1998), and on a high resolution computed tomography (HRCT) lung image data which we collected from IUPUI medical center (Dy et al., 2003; Dy et al.,", "mykey":2080},
 {"datasetID":53, "supportID":"D174FB126EF41AB1A0DC96DA273D57E5BDF4A8C5", "rexaID":"5b3417aa2824988405f9ac934b692af30729b447", "author":"Jennifer G. Dy and Carla Brodley", "title":"Feature Selection for Unsupervised Learning", "venue":"Journal of Machine Learning Research, 5", "year":"2004", "window":"features. 25 Dy and Brodley . Feature selection obtained better results than without feature selection. 6.5 Experiments on Real Data We examine the FSSEM variants on the <b>iris</b>  wine, and ionosphere data set from the UCI learning repository (Blake & Merz, 1998), and on a high resolution computed tomography (HRCT) lung image data which we collected from IUPUI medical center (Dy et al., 2003; Dy et al.,", "mykey":2081},
 {"datasetID":109, "supportID":"D174FB126EF41AB1A0DC96DA273D57E5BDF4A8C5", "rexaID":"5b3417aa2824988405f9ac934b692af30729b447", "author":"Jennifer G. Dy and Carla Brodley", "title":"Feature Selection for Unsupervised Learning", "venue":"Journal of Machine Learning Research, 5", "year":"2004", "window":"features. 25 Dy and Brodley . Feature selection obtained better results than without feature selection. 6.5 Experiments on Real Data We examine the FSSEM variants on the iris, <b>wine</b>  and ionosphere data set from the UCI learning repository (Blake & Merz, 1998), and on a high resolution computed tomography (HRCT) lung image data which we collected from IUPUI medical center (Dy et al., 2003; Dy et al.,", "mykey":2082},
 {"datasetID":1, "supportID":"D195453A3C57B325A7222FB2AE48F314C13660BF", "rexaID":"5bffb4d591b37fc0c1011681ab2cf075033ca539", "author":"Anton Schwaighofer and Volker Tresp", "title":"Transductive and Inductive Methods for Approximate Gaussian Process Regression", "venue":"NIPS", "year":"2002", "window":"article, we always use the BCM with clustered data. 4 Experimental Comparison In this section we will present an evaluation of the different approximation methods discussed in Sec. 2 and 3 on four data sets. In the <b>ABALONE</b> data set [1] with 4177 examples, the goal is to predict the age of Abalones based on 8 inputs. The KIN8NM data set 4 represents the forward dynamics of an 8 link all-revolute robot", "mykey":2083},
 {"datasetID":53, "supportID":"D1B537E2FF5C9AEA371CBD65DACA7D49A39D8F2C", "rexaID":"e7f14adabe196dbd08024790f0df92a43728d643", "author":"Asa Ben-Hur and David Horn and Hava T. Siegelmann and Vladimir Vapnik", "title":"A Support Vector Method for Clustering", "venue":"NIPS", "year":"2000", "window":"the core regions by an SV method with a global optimal solution. We have found examples where a local maximum is hard to identify by Roberts' method. 3.2 The <b>iris</b> data We ran SVC on the iris data set [9], which is a standard benchmark in the pattern recognition literature. It can be obtained from the UCI repository [10]. The data set contains 150 instances, each containing four measurements of", "mykey":2084},
 {"datasetID":30, "supportID":"D216D97BBB13EC6C228997BB3FF142B1A7CF6117", "rexaID":"2a4fe2f7469eef9d4cc985d6caaa0afe249facf3", "author":"Soumya Ray and David Page", "title":"Generalized Skewing for Functions with Continuous and Nominal Attributes", "venue":"Department of Computer Sciences and Department of Biostatistics and Medical Informatics, University of Wis", "year":"", "window":"1500 1000 500 Accuracy (%) Training Sample Size Gain/Generalized Skewing Gain Figure 11. Hard Nominal Targets, 100-v examples that a subfunction in the target (or the target itself) may be hard. The datasets we use are: <b>Contraceptive</b> Method Choice (CMC), German Credit (German), Cleveland Heart Disease (Heart), voting-records (Voting), pima-indians-diabetes (Diabetes), breast-cancerwisconsin (BCW),", "mykey":2085},
 {"datasetID":2, "supportID":"D27E7103E1B051968BD3E53B93EB98F764898921", "rexaID":"5e9e56b4525a16e039d75d04d32477b118e36b0d", "author":"Christopher R. Palmer and Christos Faloutsos", "title":"Electricity Based External Similarity of Categorical Attributes", "venue":"PAKDD", "year":"2003", "window":"than the distance functions computed by D fr;P . Since D fr;P has been previously evaluated using single link hierarchical clustering, that is the algorithm that we will use here [3]. The three data sets weevaluated are: 1. <b>Adult</b> - a selection of fields from the 1994 census data collected in the United States. There are 32,561 training examples and 16,281 test examples with 6 numeric fields and 8", "mykey":2086},
 {"datasetID":9, "supportID":"D27E7103E1B051968BD3E53B93EB98F764898921", "rexaID":"5e9e56b4525a16e039d75d04d32477b118e36b0d", "author":"Christopher R. Palmer and Christos Faloutsos", "title":"Electricity Based External Similarity of Categorical Attributes", "venue":"PAKDD", "year":"2003", "window":"house servant as an outlier, combined Clerical with Other service and combined Sales and Technical support . The final pair of clusterings in parts (g) and (h) show the makes of cars in the <b>Auto</b> data set. The comparison here is more subtle, but the REP clustering has a more natural looking structure and three very distinct clusters for the luxury cars, the family cars and the imports. D fr;P on the", "mykey":2087},
 {"datasetID":2, "supportID":"D2BFE2207B2FCC01A4C2E6738664F800052BDA10", "rexaID":"819bb37e80fcb500fd159702e1dc2c0fe98f38ac", "author":"Petri Kontkanen and Jussi Lahtinen and Petri Myllymaki and Tomi Silander and Henry Tirri", "title":"Proceedings of Pre- and Post-processing in Machine Learning and Data Mining: Theoretical Aspects and Applications, a workshop within Machine Learning and Applications", "venue":"Complex Systems Computation Group (CoSCo)", "year":"1999", "window":"via the CoSCo group home page. 42 Australian Credit Balance Scale Connect-4 German Credit Thyroid Disease Vehicle Silhouettes Figure 1: Examples of the two-dimensional visualizations obtained. 43 dataset size #attrs. #classes <b>Adult</b> 32561 15 2 Australian Credit 690 15 2 Balance Scale 625 5 3 Breast Cancer (Wisconsin) 699 11 2 Breast Cancer 286 10 2 Connect-4 67557 43 3 Credit Screening 690 16 2 Pima", "mykey":2088},
 {"datasetID":109, "supportID":"D2BFE2207B2FCC01A4C2E6738664F800052BDA10", "rexaID":"819bb37e80fcb500fd159702e1dc2c0fe98f38ac", "author":"Petri Kontkanen and Jussi Lahtinen and Petri Myllymaki and Tomi Silander and Henry Tirri", "title":"Proceedings of Pre- and Post-processing in Machine Learning and Data Mining: Theoretical Aspects and Applications, a workshop within Machine Learning and Applications", "venue":"Complex Systems Computation Group (CoSCo)", "year":"1999", "window":"8124 23 2 Postoperative Patient 90 9 3 Thyroid Disease 215 6 3 Tic-Tac-Toe Endgame 958 10 2 Vehicle Silhouettes 846 19 4 Congressional Voting Records 435 17 2 <b>Wine</b> Recognition 178 14 3 Table 1: The datasets used in the experiments. For estimating the quality of the visualizations produced, we used the validation scheme described in the previous section. The prediction methods used are listed in Table", "mykey":2089},
 {"datasetID":3, "supportID":"D2C145479F672A054BC13DFAEDD2B6D923D0B4A9", "rexaID":"8e674b6eff0f726ba6fff46ef6eaff968dc89f39", "author":"Qingping Tao Ph. D", "title":"MAKING EFFICIENT LEARNING ALGORITHMS WITH EXPONENTIALLY MANY FEATURES", "venue":"Qingping Tao A DISSERTATION Faculty of The Graduate College University of Nebraska In Partial Fulfillment of Requirements", "year":"2004", "window":"(T 0 = n 2 and T s =10n 2 ). M - Metropolis, G - Gibbs, MG - Metropolized Gibbs, PT - Parallel Tempering, BF - Brute Force. Data Sets iris car breast cancer voting auto <b>annealing</b> n 4 6 9 16 25 38 M 5.3 \u00b1 2.1 1.7 \u00b1 0.831.5 \u00b1 5.05.0\u00b1 2.1 12.8 \u00b1 7.5 1.0 \u00b1 0.7 G 6.7 \u00b1 3.81.9 \u00b1 0.8 30.9 \u00b1 5.5 5.0 \u00b1 2.415.6 \u00b1 7.80.6 \u00b1 0.5 MG 6.0 \u00b1 1.7", "mykey":2090},
 {"datasetID":9, "supportID":"D2C145479F672A054BC13DFAEDD2B6D923D0B4A9", "rexaID":"8e674b6eff0f726ba6fff46ef6eaff968dc89f39", "author":"Qingping Tao Ph. D", "title":"MAKING EFFICIENT LEARNING ALGORITHMS WITH EXPONENTIALLY MANY FEATURES", "venue":"Qingping Tao A DISSERTATION Faculty of The Graduate College University of Nebraska In Partial Fulfillment of Requirements", "year":"2004", "window":"(T 0 = n 2 and T s =10n 2 ). M - Metropolis, G - Gibbs, MG - Metropolized Gibbs, PT - Parallel Tempering, BF - Brute Force. Data Sets iris car breast cancer voting <b>auto</b> annealing n 4 6 9 16 25 38 M 5.3 \u00b1 2.1 1.7 \u00b1 0.831.5 \u00b1 5.05.0\u00b1 2.1 12.8 \u00b1 7.5 1.0 \u00b1 0.7 G 6.7 \u00b1 3.81.9 \u00b1 0.8 30.9 \u00b1 5.5 5.0 \u00b1 2.415.6 \u00b1 7.80.6 \u00b1 0.5 MG 6.0 \u00b1 1.7", "mykey":2091},
 {"datasetID":14, "supportID":"D2C145479F672A054BC13DFAEDD2B6D923D0B4A9", "rexaID":"8e674b6eff0f726ba6fff46ef6eaff968dc89f39", "author":"Qingping Tao Ph. D", "title":"MAKING EFFICIENT LEARNING ALGORITHMS WITH EXPONENTIALLY MANY FEATURES", "venue":"Qingping Tao A DISSERTATION Faculty of The Graduate College University of Nebraska In Partial Fulfillment of Requirements", "year":"2004", "window":"(T 0 = n 2 and T s =10n 2 ). M - Metropolis, G - Gibbs, MG - Metropolized Gibbs, PT - Parallel Tempering, BF - Brute Force. Data Sets iris car <b>breast</b> <b>cancer</b> voting auto annealing n 4 6 9 16 25 38 M 5.3 \u00b1 2.1 1.7 \u00b1 0.831.5 \u00b1 5.05.0\u00b1 2.1 12.8 \u00b1 7.5 1.0 \u00b1 0.7 G 6.7 \u00b1 3.81.9 \u00b1 0.8 30.9 \u00b1 5.5 5.0 \u00b1 2.415.6 \u00b1 7.80.6 \u00b1 0.5 MG 6.0 \u00b1 1.7", "mykey":2092},
 {"datasetID":19, "supportID":"D2C145479F672A054BC13DFAEDD2B6D923D0B4A9", "rexaID":"8e674b6eff0f726ba6fff46ef6eaff968dc89f39", "author":"Qingping Tao Ph. D", "title":"MAKING EFFICIENT LEARNING ALGORITHMS WITH EXPONENTIALLY MANY FEATURES", "venue":"Qingping Tao A DISSERTATION Faculty of The Graduate College University of Nebraska In Partial Fulfillment of Requirements", "year":"2004", "window":"(T 0 = n 2 and T s =10n 2 ). M - Metropolis, G - Gibbs, MG - Metropolized Gibbs, PT - Parallel Tempering, BF - Brute Force. Data Sets iris <b>car</b> breast cancer voting auto annealing n 4 6 9 16 25 38 M 5.3 \u00b1 2.1 1.7 \u00b1 0.831.5 \u00b1 5.05.0\u00b1 2.1 12.8 \u00b1 7.5 1.0 \u00b1 0.7 G 6.7 \u00b1 3.81.9 \u00b1 0.8 30.9 \u00b1 5.5 5.0 \u00b1 2.415.6 \u00b1 7.80.6 \u00b1 0.5 MG 6.0 \u00b1 1.7", "mykey":2093},
 {"datasetID":53, "supportID":"D2C145479F672A054BC13DFAEDD2B6D923D0B4A9", "rexaID":"8e674b6eff0f726ba6fff46ef6eaff968dc89f39", "author":"Qingping Tao Ph. D", "title":"MAKING EFFICIENT LEARNING ALGORITHMS WITH EXPONENTIALLY MANY FEATURES", "venue":"Qingping Tao A DISSERTATION Faculty of The Graduate College University of Nebraska In Partial Fulfillment of Requirements", "year":"2004", "window":"(T 0 = n 2 and T s =10n 2 ). M - Metropolis, G - Gibbs, MG - Metropolized Gibbs, PT - Parallel Tempering, BF - Brute Force. Data Sets <b>iris</b> car breast cancer voting auto annealing n 4 6 9 16 25 38 M 5.3 \u00b1 2.1 1.7 \u00b1 0.831.5 \u00b1 5.05.0\u00b1 2.1 12.8 \u00b1 7.5 1.0 \u00b1 0.7 G 6.7 \u00b1 3.81.9 \u00b1 0.8 30.9 \u00b1 5.5 5.0 \u00b1 2.415.6 \u00b1 7.80.6 \u00b1 0.5 MG 6.0 \u00b1 1.7", "mykey":2094},
 {"datasetID":74, "supportID":"D2C145479F672A054BC13DFAEDD2B6D923D0B4A9", "rexaID":"8e674b6eff0f726ba6fff46ef6eaff968dc89f39", "author":"Qingping Tao Ph. D", "title":"MAKING EFFICIENT LEARNING ALGORITHMS WITH EXPONENTIALLY MANY FEATURES", "venue":"Qingping Tao A DISSERTATION Faculty of The Graduate College University of Nebraska In Partial Fulfillment of Requirements", "year":"2004", "window":"67 4.5.1 Content-Based Image Retrieval . . . ................ 67 4.5.2 Identifying Trx-fold Proteins .................... 69 4.5.3 Multi-Site Drug Binding Affinity . . ................ 69 4.5.4 <b>Musk</b> Data Sets ............................ 70 4.6 Conclusions.................................. 71 5 Extended Kernels for Generalized Multiple-Instance Learning 73 5.1 A Count-based Kernel for GMIL . . .", "mykey":2095},
 {"datasetID":75, "supportID":"D2C145479F672A054BC13DFAEDD2B6D923D0B4A9", "rexaID":"8e674b6eff0f726ba6fff46ef6eaff968dc89f39", "author":"Qingping Tao Ph. D", "title":"MAKING EFFICIENT LEARNING ALGORITHMS WITH EXPONENTIALLY MANY FEATURES", "venue":"Qingping Tao A DISSERTATION Faculty of The Graduate College University of Nebraska In Partial Fulfillment of Requirements", "year":"2004", "window":"67 4.5.1 Content-Based Image Retrieval . . . ................ 67 4.5.2 Identifying Trx-fold Proteins .................... 69 4.5.3 Multi-Site Drug Binding Affinity . . ................ 69 4.5.4 <b>Musk</b> Data Sets ............................ 70 4.6 Conclusions.................................. 71 5 Extended Kernels for Generalized Multiple-Instance Learning 73 5.1 A Count-based Kernel for GMIL . . .", "mykey":2096},
 {"datasetID":154, "supportID":"D2C145479F672A054BC13DFAEDD2B6D923D0B4A9", "rexaID":"8e674b6eff0f726ba6fff46ef6eaff968dc89f39", "author":"Qingping Tao Ph. D", "title":"MAKING EFFICIENT LEARNING ALGORITHMS WITH EXPONENTIALLY MANY FEATURES", "venue":"Qingping Tao A DISSERTATION Faculty of The Graduate College University of Nebraska In Partial Fulfillment of Requirements", "year":"2004", "window":"all sequences to 8-dimensional profiles based on the numeric properties of Kim et al. [24] and used them as inputs to the multiple-instance learning algorithm. GMIL-1 is not suitable for the <b>protein</b> data set. As an example why this is a problem, consider the case where 5 clusters is used to built the grid on the 9-dimensional space ( 8dimensional profiles and an additional dimension for identifying the", "mykey":2097},
 {"datasetID":7, "supportID":"D31BEE63C05B00FE2C2ACB117FA3EA55BAC72971", "rexaID":"81a75649d5acc1cc428ca756dac221bac3c8fe01", "author":"Geoffrey I. Webb", "title":"OPUS: An Efficient Admissible Algorithm for Unordered Search", "venue":"J. Artif. Intell. Res. (JAIR, 3", "year":"1995", "window":"B. C. 465,058 * 1,211,211 * * * Execution terminated after exceeding the 24 CPU hour limit. # Only three of ten runs completed. Table 4: Number of nodes explored under best-first fixed-order search Data set Runs Minimum Mean sd <b>Audiology</b> 0 --- --- --- House Votes 84 10 451,038 1,319,911 624,957 Lenses 10 51 64 9 Lymphography 10 597,842 2,251,652 1,454,583 Monk 1 10 463 788 225 Monk 2 10 4,283 5,895 931", "mykey":2098},
 {"datasetID":8, "supportID":"D31BEE63C05B00FE2C2ACB117FA3EA55BAC72971", "rexaID":"81a75649d5acc1cc428ca756dac221bac3c8fe01", "author":"Geoffrey I. Webb", "title":"OPUS: An Efficient Admissible Algorithm for Unordered Search", "venue":"J. Artif. Intell. Res. (JAIR, 3", "year":"1995", "window":"B. C. 465,058 * 1,211,211 * * * Execution terminated after exceeding the 24 CPU hour limit. # Only three of ten runs completed. Table 4: Number of nodes explored under best-first fixed-order search Data set Runs Minimum Mean sd <b>Audiology</b> 0 --- --- --- House Votes 84 10 451,038 1,319,911 624,957 Lenses 10 51 64 9 Lymphography 10 597,842 2,251,652 1,454,583 Monk 1 10 463 788 225 Monk 2 10 4,283 5,895 931", "mykey":2099},
 {"datasetID":14, "supportID":"D31BEE63C05B00FE2C2ACB117FA3EA55BAC72971", "rexaID":"81a75649d5acc1cc428ca756dac221bac3c8fe01", "author":"Geoffrey I. Webb", "title":"OPUS: An Efficient Admissible Algorithm for Unordered Search", "venue":"J. Artif. Intell. Res. (JAIR, 3", "year":"1995", "window":"Tic Tac Toe) disabling other pruning had little or no e\u00aeect under best-first or depth-first search. The largest e\u00aeects are 2.5 fold increases for the Soybean Large and Wisconsin <b>Breast</b> <b>Cancer</b> data sets under best-first search and for the Audiology, Soybean Large and Wisconsin Breast Cancer data sets under depth-first search. From these results it is apparent that while there are some data sets", "mykey":2100},
 {"datasetID":17, "supportID":"D31BEE63C05B00FE2C2ACB117FA3EA55BAC72971", "rexaID":"81a75649d5acc1cc428ca756dac221bac3c8fe01", "author":"Geoffrey I. Webb", "title":"OPUS: An Efficient Admissible Algorithm for Unordered Search", "venue":"J. Artif. Intell. Res. (JAIR, 3", "year":"1995", "window":"Tic Tac Toe) disabling other pruning had little or no e\u00aeect under best-first or depth-first search. The largest e\u00aeects are 2.5 fold increases for the Soybean Large and <b>Wisconsin</b> <b>Breast</b> <b>Cancer</b> data sets under best-first search and for the Audiology, Soybean Large and Wisconsin Breast Cancer data sets under depth-first search. From these results it is apparent that while there are some data sets", "mykey":2101},
 {"datasetID":15, "supportID":"D31BEE63C05B00FE2C2ACB117FA3EA55BAC72971", "rexaID":"81a75649d5acc1cc428ca756dac221bac3c8fe01", "author":"Geoffrey I. Webb", "title":"OPUS: An Efficient Admissible Algorithm for Unordered Search", "venue":"J. Artif. Intell. Res. (JAIR, 3", "year":"1995", "window":"Tic Tac Toe) disabling other pruning had little or no e\u00aeect under best-first or depth-first search. The largest e\u00aeects are 2.5 fold increases for the Soybean Large and <b>Wisconsin</b> <b>Breast</b> <b>Cancer</b> data sets under best-first search and for the Audiology, Soybean Large and Wisconsin Breast Cancer data sets under depth-first search. From these results it is apparent that while there are some data sets", "mykey":2102},
 {"datasetID":16, "supportID":"D31BEE63C05B00FE2C2ACB117FA3EA55BAC72971", "rexaID":"81a75649d5acc1cc428ca756dac221bac3c8fe01", "author":"Geoffrey I. Webb", "title":"OPUS: An Efficient Admissible Algorithm for Unordered Search", "venue":"J. Artif. Intell. Res. (JAIR, 3", "year":"1995", "window":"Tic Tac Toe) disabling other pruning had little or no e\u00aeect under best-first or depth-first search. The largest e\u00aeects are 2.5 fold increases for the Soybean Large and <b>Wisconsin</b> <b>Breast</b> <b>Cancer</b> data sets under best-first search and for the Audiology, Soybean Large and Wisconsin Breast Cancer data sets under depth-first search. From these results it is apparent that while there are some data sets", "mykey":2103},
 {"datasetID":58, "supportID":"D31BEE63C05B00FE2C2ACB117FA3EA55BAC72971", "rexaID":"81a75649d5acc1cc428ca756dac221bac3c8fe01", "author":"Geoffrey I. Webb", "title":"OPUS: An Efficient Admissible Algorithm for Unordered Search", "venue":"J. Artif. Intell. Res. (JAIR, 3", "year":"1995", "window":"the 24 CPU hour limit. may turn out to be a better choice than the other, leading to the exploration of fewer nodes. To test the plausibility of this explanation, OPUS o was run again on the <b>Lenses</b> data set with Step 8 altered to ensure that where two siblings have equal optimistic value they are ordered in the same order as was employed with no optimistic reordering. This resulted in the exploration", "mykey":2104},
 {"datasetID":63, "supportID":"D31BEE63C05B00FE2C2ACB117FA3EA55BAC72971", "rexaID":"81a75649d5acc1cc428ca756dac221bac3c8fe01", "author":"Geoffrey I. Webb", "title":"OPUS: An Efficient Admissible Algorithm for Unordered Search", "venue":"J. Artif. Intell. Res. (JAIR, 3", "year":"1995", "window":"of the search space below a poor choice of node can do much to minimize the damage done by that poor choice, even when there is no backtracking as is the case for depth-first search. For five data sets (House Votes 84, <b>Lymphography</b>  Mushroom, Primary Tumor and Soybean Large), disabling optimistic pruning has little e\u00aeect under best-first search. Disabling optimistic pruning always has large e\u00aeect", "mykey":2105},
 {"datasetID":70, "supportID":"D31BEE63C05B00FE2C2ACB117FA3EA55BAC72971", "rexaID":"81a75649d5acc1cc428ca756dac221bac3c8fe01", "author":"Geoffrey I. Webb", "title":"OPUS: An Efficient Admissible Algorithm for Unordered Search", "venue":"J. Artif. Intell. Res. (JAIR, 3", "year":"1995", "window":"with respect to minimizing the number of nodes expanded under depth-first search. 458 An Efficient Admissible Algorithm for Unordered Search Nonetheless, for only one search task, the <b>Monk</b> 2 data set, does OPUS o explore more nodes under depth-first search (16,345) than an alternative (both no optimistic reordering and fixed-order search that explore 12,879 and 12,791 nodes respectively). These", "mykey":2106},
 {"datasetID":73, "supportID":"D31BEE63C05B00FE2C2ACB117FA3EA55BAC72971", "rexaID":"81a75649d5acc1cc428ca756dac221bac3c8fe01", "author":"Geoffrey I. Webb", "title":"OPUS: An Efficient Admissible Algorithm for Unordered Search", "venue":"J. Artif. Intell. Res. (JAIR, 3", "year":"1995", "window":"of the search space below a poor choice of node can do much to minimize the damage done by that poor choice, even when there is no backtracking as is the case for depth-first search. For five data sets (House Votes 84, Lymphography, <b>Mushroom</b>  Primary Tumor and Soybean Large), disabling optimistic pruning has little e\u00aeect under best-first search. Disabling optimistic pruning always has large e\u00aeect", "mykey":2107},
 {"datasetID":83, "supportID":"D31BEE63C05B00FE2C2ACB117FA3EA55BAC72971", "rexaID":"81a75649d5acc1cc428ca756dac221bac3c8fe01", "author":"Geoffrey I. Webb", "title":"OPUS: An Efficient Admissible Algorithm for Unordered Search", "venue":"J. Artif. Intell. Res. (JAIR, 3", "year":"1995", "window":"I am especially indebted to my anonymous reviewers whose insightful, extensive and detailed comments greatly improved the quality of this paper. The Breast Cancer, Lymphography and <b>Primary</b> <b>Tumor</b> data sets were provided by the Ljubljana Oncology Institute, Slovenia. Thanks to the UCI Repository, its maintainers, Patrick Murphy and David Aha, and its donors, for providing access to the data sets used", "mykey":2108},
 {"datasetID":90, "supportID":"D31BEE63C05B00FE2C2ACB117FA3EA55BAC72971", "rexaID":"81a75649d5acc1cc428ca756dac221bac3c8fe01", "author":"Geoffrey I. Webb", "title":"OPUS: An Efficient Admissible Algorithm for Unordered Search", "venue":"J. Artif. Intell. Res. (JAIR, 3", "year":"1995", "window":"Tic Tac Toe) disabling other pruning had little or no e\u00aeect under best-first or depth-first search. The largest e\u00aeects are 2.5 fold increases for the <b>Soybean</b> Large and Wisconsin Breast Cancer data sets under best-first search and for the Audiology, Soybean Large and Wisconsin Breast Cancer data sets under depth-first search. From these results it is apparent that while there are some data sets", "mykey":2109},
 {"datasetID":91, "supportID":"D31BEE63C05B00FE2C2ACB117FA3EA55BAC72971", "rexaID":"81a75649d5acc1cc428ca756dac221bac3c8fe01", "author":"Geoffrey I. Webb", "title":"OPUS: An Efficient Admissible Algorithm for Unordered Search", "venue":"J. Artif. Intell. Res. (JAIR, 3", "year":"1995", "window":"Tic Tac Toe) disabling other pruning had little or no e\u00aeect under best-first or depth-first search. The largest e\u00aeects are 2.5 fold increases for the <b>Soybean</b> Large and Wisconsin Breast Cancer data sets under best-first search and for the Audiology, Soybean Large and Wisconsin Breast Cancer data sets under depth-first search. From these results it is apparent that while there are some data sets", "mykey":2110},
 {"datasetID":34, "supportID":"D3541C0A2924D0F436ECDD3CBD35BAF9BE45178F", "rexaID":"87afa910f706df9e28bfa697d6d2eea7c0cb53ef", "author":"Ilya Blayvas and Ron Kimmel", "title":"Efficient Classification via Multiresolution Training Set Approximation", "venue":"CS Dept. Technion", "year":"", "window":"\u00b7 D). 3 Experimental Results The proposed method was implemented in VC++ 6.0 and run on `IBM PC 300 PL' with 600MHZ Pentium III processor and 256MB RAM. It was tested on the Pima Indians <b>Diabetes</b> dataset [10], and a large artificial dataset generated with the DatGen program [11]. The results were compared to the Smooth SVM [12] and Sparse Grids [3]. Figure 7: Partition of 2D feature space for a", "mykey":2111},
 {"datasetID":79, "supportID":"D3541C0A2924D0F436ECDD3CBD35BAF9BE45178F", "rexaID":"87afa910f706df9e28bfa697d6d2eea7c0cb53ef", "author":"Ilya Blayvas and Ron Kimmel", "title":"Efficient Classification via Multiresolution Training Set Approximation", "venue":"CS Dept. Technion", "year":"", "window":"\u00b7 D). 3 Experimental Results The proposed method was implemented in VC++ 6.0 and run on `IBM PC 300 PL' with 600MHZ Pentium III processor and 256MB RAM. It was tested on the <b>Pima</b> <b>Indians</b> <b>Diabetes</b> dataset [10], and a large artificial dataset generated with the DatGen program [11]. The results were compared to the Smooth SVM [12] and Sparse Grids [3]. Figure 7: Partition of 2D feature space for a", "mykey":2112},
 {"datasetID":14, "supportID":"D38C96C89194F0B3FCA53E8C9C89AAC0D5F0FDAC", "rexaID":"ff82c050c05e8845f7a81bb4e723cf41fbd2efec", "author":"Saher Esmeir and Shaul Markovitch", "title":"Lookahead-based algorithms for anytime induction of decision trees", "venue":"ICML", "year":"2004", "window":"LSID3 produces classifiers of higher accuracy than C4.5. For the more difficult concepts, the advantage of LSID3 is substantial. However, for some datasets, such as <b>Breast</b> <b>Cancer</b> and Monks-3, C4.5 produces trees that are both smaller and more accurate. These results confirm our expectations: the problems addressed by LSID3 and C4.5 are di\u00aeerent. While", "mykey":2113},
 {"datasetID":101, "supportID":"D38C96C89194F0B3FCA53E8C9C89AAC0D5F0FDAC", "rexaID":"ff82c050c05e8845f7a81bb4e723cf41fbd2efec", "author":"Saher Esmeir and Shaul Markovitch", "title":"Lookahead-based algorithms for anytime induction of decision trees", "venue":"ICML", "year":"2004", "window":"that when additional time is available LSID3 should be preferred over ID3-k for the task of learning this concept. Figure 6 shows the performance of both algorithms when applied on the <b>Tic-tac-toe</b> dataset. In this case, the average run time of ID3 is 0.004 second, while C4.5 finished in 0.008 second. LSID3 dominates ID3-k at any point of time, both in terms of accuracy and size. ID3-k is clearly not", "mykey":2114},
 {"datasetID":7, "supportID":"D39896F8AEEB2BB375D3CD93182975288E55147F", "rexaID":"ccde1df96c7ca5add43e1578b912d95bb86da659", "author":"Wai Lam and Kin Keung and Charles X. Ling", "title":"PR 1527", "venue":"Department of Systems Engineering and Engineering Management, The Chinese University of Hong Kong", "year":"2001", "window":"and their codes Data set Code Automobile Ab Auto-Mpg Am <b>Audiology</b> Au Balance-scale Ba Breast-cancer-w Bc Car Ca Credit screening Cs Ecoli Ec Glass1 Gl Hepati He Ionosphere Io Iris Ir Letter Le Liver Li Monk-1 M1 Monk-2 M2", "mykey":2115},
 {"datasetID":8, "supportID":"D39896F8AEEB2BB375D3CD93182975288E55147F", "rexaID":"ccde1df96c7ca5add43e1578b912d95bb86da659", "author":"Wai Lam and Kin Keung and Charles X. Ling", "title":"PR 1527", "venue":"Department of Systems Engineering and Engineering Management, The Chinese University of Hong Kong", "year":"2001", "window":"and their codes Data set Code Automobile Ab Auto-Mpg Am <b>Audiology</b> Au Balance-scale Ba Breast-cancer-w Bc Car Ca Credit screening Cs Ecoli Ec Glass1 Gl Hepati He Ionosphere Io Iris Ir Letter Le Liver Li Monk-1 M1 Monk-2 M2", "mykey":2116},
 {"datasetID":9, "supportID":"D39896F8AEEB2BB375D3CD93182975288E55147F", "rexaID":"ccde1df96c7ca5add43e1578b912d95bb86da659", "author":"Wai Lam and Kin Keung and Charles X. Ling", "title":"PR 1527", "venue":"Department of Systems Engineering and Engineering Management, The Chinese University of Hong Kong", "year":"2001", "window":"and their codes Data set Code Automobile Ab <b>Auto</b> <b>Mpg</b> Am Audiology Au Balance-scale Ba Breast-cancer-w Bc Car Ca Credit screening Cs Ecoli Ec Glass1 Gl Hepati He Ionosphere Io Iris Ir Letter Le Liver Li Monk-1 M1 Monk-2 M2", "mykey":2117},
 {"datasetID":10, "supportID":"D39896F8AEEB2BB375D3CD93182975288E55147F", "rexaID":"ccde1df96c7ca5add43e1578b912d95bb86da659", "author":"Wai Lam and Kin Keung and Charles X. Ling", "title":"PR 1527", "venue":"Department of Systems Engineering and Engineering Management, The Chinese University of Hong Kong", "year":"2001", "window":"find that the abstraction method reduces the average data retention rate of RT3 from 14.2% to 6.6% with a 43 UNCORRECTED PROOF PR1527 W.Lam et al./ Pattern Recognition 000 (2001) 000--000 9 Table 1 Data sets and their codes Data set Code <b>Automobile</b> Ab Auto-Mpg Am Audiology Au Balance-scale Ba Breast-cancer-w Bc Car Ca Credit screening Cs Ecoli Ec Glass1 Gl Hepati He Ionosphere Io Iris Ir Letter Le", "mykey":2118},
 {"datasetID":53, "supportID":"D39896F8AEEB2BB375D3CD93182975288E55147F", "rexaID":"ccde1df96c7ca5add43e1578b912d95bb86da659", "author":"Wai Lam and Kin Keung and Charles X. Ling", "title":"PR 1527", "venue":"Department of Systems Engineering and Engineering Management, The Chinese University of Hong Kong", "year":"2001", "window":"from di3erent real-world application in various domains, such as the city-cycle fuel consumption (Am), Wisconsin breast cancer (Bc) and the 43 famous <b>iris</b> plant database (Ir). Table 1 shows the data sets and their corresponding code used in this paper. 45 For each data set, we randomly partitioned the data into ten even portions. Ten trials derived from 10-fold 47 cross-validation were conducted", "mykey":2119},
 {"datasetID":90, "supportID":"D3B4BC94CACA1E7B2ABFACA1438ADD98AE69C940", "rexaID":"1f4e06390d7a257b58508b73ed2a689add6a9748", "author":"Bianca Zadrozny", "title":"Reducing multiclass to binary by coupling probability estimates", "venue":"NIPS", "year":"2001", "window":"better. Figure 1 shows how the MSE is lowered at each iteration of the Hastie-Tibshirani algorithm, for the three types of code matrices. Table 3 shows the results of the same experiments on the datasets pendigits and <b>soybean</b>  Again, the MSE is significantly lowered by the iterative procedure, in all cases. For the soybean dataset, using the sparse random matrix, the iterative method again has a", "mykey":2120},
 {"datasetID":91, "supportID":"D3B4BC94CACA1E7B2ABFACA1438ADD98AE69C940", "rexaID":"1f4e06390d7a257b58508b73ed2a689add6a9748", "author":"Bianca Zadrozny", "title":"Reducing multiclass to binary by coupling probability estimates", "venue":"NIPS", "year":"2001", "window":"better. Figure 1 shows how the MSE is lowered at each iteration of the Hastie-Tibshirani algorithm, for the three types of code matrices. Table 3 shows the results of the same experiments on the datasets pendigits and <b>soybean</b>  Again, the MSE is significantly lowered by the iterative procedure, in all cases. For the soybean dataset, using the sparse random matrix, the iterative method again has a", "mykey":2121},
 {"datasetID":53, "supportID":"D46A21A56EEE7E0DD9A329DE9444BCDE3FEE134E", "rexaID":"28734e2b9b072c9ef055eefa88999a60f6a3dc81", "author":"Jun Wang", "title":"Classification Visualization with Shaded Similarity Matrix", "venue":"Bei Yu Les Gasser Graduate School of Library and Information Science University of Illinois at Urbana-Champaign", "year":"", "window":"similarity matrix is constructed and how it looks through an example. The data used in the example is part of the <b>Iris</b> data from the UCI repository [25]. There are 150 instances in the original Iris data set, which evenly distributed in 3 classes: setosa, virginica, and versicolor. For each class, we fetch its first 5 instances from the data file, and thus obtaining 15 instances (see Table 1). Table 2", "mykey":2122},
 {"datasetID":148, "supportID":"D46A21A56EEE7E0DD9A329DE9444BCDE3FEE134E", "rexaID":"28734e2b9b072c9ef055eefa88999a60f6a3dc81", "author":"Jun Wang", "title":"Classification Visualization with Shaded Similarity Matrix", "venue":"Bei Yu Les Gasser Graduate School of Library and Information Science University of Illinois at Urbana-Champaign", "year":"", "window":"explored in the future. The purpose of this section is to see if it is effective to use simple random sampling with very small sample size. To this end, we test the ensemble classifier on 5 Statlog data sets: Satimage, Segment, <b>Shuttle</b>  Australian, and DNA. For data description, please see Table 3. The reason to use these 5 Statlog data sets is because Ankerst used them as benchmark in his PBC system", "mykey":2123},
 {"datasetID":98, "supportID":"D46A21A56EEE7E0DD9A329DE9444BCDE3FEE134E", "rexaID":"28734e2b9b072c9ef055eefa88999a60f6a3dc81", "author":"Jun Wang", "title":"Classification Visualization with Shaded Similarity Matrix", "venue":"Bei Yu Les Gasser Graduate School of Library and Information Science University of Illinois at Urbana-Champaign", "year":"", "window":"explored in the future. The purpose of this section is to see if it is effective to use simple random sampling with very small sample size. To this end, we test the ensemble classifier on 5 <b>Statlog</b> data sets: Satimage, Segment, Shuttle, Australian, and DNA. For data description, please see Table 3. The reason to use these 5 Statlog data sets is because Ankerst used them as benchmark in his PBC system", "mykey":2124},
 {"datasetID":111, "supportID":"D46A21A56EEE7E0DD9A329DE9444BCDE3FEE134E", "rexaID":"28734e2b9b072c9ef055eefa88999a60f6a3dc81", "author":"Jun Wang", "title":"Classification Visualization with Shaded Similarity Matrix", "venue":"Bei Yu Les Gasser Graduate School of Library and Information Science University of Illinois at Urbana-Champaign", "year":"", "window":"because that object A is the nearest neighbor of object B does not imply B is the nearest neighbor of A. Figure a) in Fig. 8 (on the last page) is an illustration of this setting. The <b>Zoo</b> data set used in the figure comes from the UCI repository. It contains 101 instances with 7 classes {mammal, bird, reptile, fish, amphibian, insect, and invertebrate}. In the distance threshold setting, only", "mykey":2125},
 {"datasetID":63, "supportID":"D4D07EEC4396CC723978AB30DF9EE1BEC22701CE", "rexaID":"d2ef68752af3ba25d7fcf8275b91cf592511f2e9", "author":"Mark A. Hall and Lloyd A. Smith", "title":"Feature Selection for Machine Learning: Comparing a Correlation-Based Filter Approach to the Wrapper", "venue":"FLAIRS Conference", "year":"1999", "window":"were chosen because of the prevalence of nominal features and their predominance in the literature. Three of the datasets (australian, <b>lymphography</b>  and horsecolic) contain a few continuous features; the rest contain only nominal features. Fifty runs were done for each machine learning algorithm on each dataset with", "mykey":2126},
 {"datasetID":67, "supportID":"D4D07EEC4396CC723978AB30DF9EE1BEC22701CE", "rexaID":"d2ef68752af3ba25d7fcf8275b91cf592511f2e9", "author":"Mark A. Hall and Lloyd A. Smith", "title":"Feature Selection for Machine Learning: Comparing a Correlation-Based Filter Approach to the Wrapper", "venue":"FLAIRS Conference", "year":"1999", "window":"numeric value. Future work will aim at extending CFS to handle problems where the class is numeric. -50 -40 -30 -20 -10 0 10 20 30 40 mu vo v1 cr ly pt bc <b>dna</b> au sb hc kr tree size difference dataset Figure 2: Average change in the size of the trees induced by C4.5 when features are selected by the wrapper (left) and CFS (right). 0 10 20 30 40 50 60 70 80 mu vo v1 as ly pt bc dna au sb hc kr #", "mykey":2127},
 {"datasetID":73, "supportID":"D4D07EEC4396CC723978AB30DF9EE1BEC22701CE", "rexaID":"d2ef68752af3ba25d7fcf8275b91cf592511f2e9", "author":"Mark A. Hall and Lloyd A. Smith", "title":"Feature Selection for Machine Learning: Comparing a Correlation-Based Filter Approach to the Wrapper", "venue":"FLAIRS Conference", "year":"1999", "window":"5% level according to a paired two-sided t-test. Similarly, Table 3 shows the results of feature selection for C4.5. Table 2: Accuracy of naive Bayes with feature selection by CFS and the wrapper. Dataset CFS Wrapper All features <b>mushroom</b> 98.53+ 98.86+ 94.75 vote 95.20+ 95.24+ 90.25 vote1 89.51+ 88.95+ 87.20 australian 85.90+ 85.16+ 78.21 lymph 83.92+ 76.00Gamma 82.12 primary-tumor 46.73 42.32Gamma", "mykey":2128},
 {"datasetID":45, "supportID":"D515EC7152FA8473C946D70DFC68BFFFEA08ECFE", "rexaID":"084aa2a0b1ffd67537222e2c439d2fadce6090ca", "author":"Petri Kontkanen and Petri Myllym and Tomi Silander and Henry Tirri and Peter Gr", "title":"On predictive distributions and Bayesian networks", "venue":"Department of Computer Science, Stanford University", "year":"2000", "window":"used in the experiments. Dataset Data vectors Attributes Classes CV folds <b>Heart</b> Disease (HD) 270 14 2 9 Iris (IR) 150 5 3 5 Lymphography (LY) 148 19 4 5 Australian (AU) 690 15 2 10 Breast Cancer (BC) 286 10 2 11 Diabetes (DB) 768 9", "mykey":2129},
 {"datasetID":151, "supportID":"D5C9736BF1F3FFCEF872485704A680ED34084301", "rexaID":"179e206cfdc14b74819ce95f39bb68610874578b", "author":"Juan J. Rodr##guez and Carlos J. Alonso and Henrik Bostrom", "title":"Boosting Interval Based Literals", "venue":"", "year":"2000", "window":"in a supervised classification setting from other authors. The results are shown in table 10. All the differences considered are significant, with only one exception. 4.6 <b>Sonar</b> This data set was introduced in [GS88] and it is available at the UCI ML Repository [Bay99]. The task is to discriminate between sonar signals bounced off a metal cylinder and those bounced off a roughly", "mykey":2130},
 {"datasetID":32, "supportID":"D5C9736BF1F3FFCEF872485704A680ED34084301", "rexaID":"179e206cfdc14b74819ce95f39bb68610874578b", "author":"Juan J. Rodr##guez and Carlos J. Alonso and Henrik Bostrom", "title":"Boosting Interval Based Literals", "venue":"", "year":"2000", "window":".0.048 0.214 4 0.704 0.084 0.781 0.676 .0.002 0.135 0.086 .0.039 0.147 0.192 Signi#c. 5 0.186 .0.005 .0.003 .0.002 .4e-05 .0.001 .0.001 .0.002 .3e-04 .0.002 Table 8: Results for the Shifted Wave data set <b>Cylinder</b> Bell Funnel -2 0 2 4 6 8 20 40 60 80 100 120 -2 0 2 4 6 8 20 40 60 80 100 120 -2 0 2 4 6 8 20 40 60 80 100 120 Figure 8: Examples of the CBF data set. Iter.: 10 20 30 40 50 60 70 80 90 100", "mykey":2131},
 {"datasetID":52, "supportID":"D5C9736BF1F3FFCEF872485704A680ED34084301", "rexaID":"179e206cfdc14b74819ce95f39bb68610874578b", "author":"Juan J. Rodr##guez and Carlos J. Alonso and Henrik Bostrom", "title":"Boosting Interval Based Literals", "venue":"", "year":"2000", "window":"in [HR99] is an error of 9.96, for the specified partition. Our results for the specified partition, setting 5, 100 iterations is an error of 11.54, our best result is 10.58. 4.7 <b>Ionosphere</b> This data set, also from the ML UCI Repository, contains information collected by a radar system [SWHB89]. The targets were free electrons in the ionosphere. Good\" radar returns are those showing evidence of", "mykey":2132},
 {"datasetID":107, "supportID":"D5C9736BF1F3FFCEF872485704A680ED34084301", "rexaID":"179e206cfdc14b74819ce95f39bb68610874578b", "author":"Juan J. Rodr##guez and Carlos J. Alonso and Henrik Bostrom", "title":"Boosting Interval Based Literals", "venue":"", "year":"2000", "window":"iterations, and a comparison of the results for settings 2{5 (combinations of interval literals) against the results for setting 1 (point based literals), using the McNemar's test. 4.1 <b>Waveform</b> This data set was introduced by [BFOS93]. The purpose is to distinguish between three classes, defined by the evaluation for i = 1; 2 : : : 21, of the following functions: x 1 (i) = uh 1 (i) + (1 u)h 2 (i) + #(i)", "mykey":2133},
 {"datasetID":108, "supportID":"D5C9736BF1F3FFCEF872485704A680ED34084301", "rexaID":"179e206cfdc14b74819ce95f39bb68610874578b", "author":"Juan J. Rodr##guez and Carlos J. Alonso and Henrik Bostrom", "title":"Boosting Interval Based Literals", "venue":"", "year":"2000", "window":"iterations, and a comparison of the results for settings 2{5 (combinations of interval literals) against the results for setting 1 (point based literals), using the McNemar's test. 4.1 <b>Waveform</b> This data set was introduced by [BFOS93]. The purpose is to distinguish between three classes, defined by the evaluation for i = 1; 2 : : : 21, of the following functions: x 1 (i) = uh 1 (i) + (1 u)h 2 (i) + #(i)", "mykey":2134},
 {"datasetID":1, "supportID":"D5E67451F92883D0FC956C7C000B9ADD4C63A877", "rexaID":"70172e511a3bc27c7927119a3b2a3405fbad99e0", "author":"Kai Ming Ting and Ian H. Witten", "title":"Issues in Stacked Generalization", "venue":"J. Artif. Intell. Res. (JAIR, 10", "year":"1999", "window":"can easily be interpreted. Examples of the combination weights it derives (for the probabilitybased model ~ M 0 ) appear in Table 5 for the Horse, Credit, Splice, <b>Abalone</b>  Waveform, Led24 and Vowel datasets. The weights indicate the relative importance of the level-0 generalizers for each prediction class. For example, in the Splice dataset (in Table 5(b)), NB is the dominant generalizer for", "mykey":2135},
 {"datasetID":14, "supportID":"D5E67451F92883D0FC956C7C000B9ADD4C63A877", "rexaID":"70172e511a3bc27c7927119a3b2a3405fbad99e0", "author":"Kai Ming Ting and Ian H. Witten", "title":"Issues in Stacked Generalization", "venue":"J. Artif. Intell. Res. (JAIR, 10", "year":"1999", "window":"are given in Table 10, and indicate that the three methods are very competitive. 4 Stacking performs better than both arcing and bagging in three datasets (Waveform, Soybean and <b>Breast</b> <b>Cancer</b> , and is better than arcing but worse than bagging in the Diabetes dataset. Note that stacking performs very poorly on Glass and Ionosphere, two small", "mykey":2136},
 {"datasetID":150, "supportID":"D5E67451F92883D0FC956C7C000B9ADD4C63A877", "rexaID":"70172e511a3bc27c7927119a3b2a3405fbad99e0", "author":"Kai Ming Ting and Ian H. Witten", "title":"Issues in Stacked Generalization", "venue":"J. Artif. Intell. Res. (JAIR, 10", "year":"1999", "window":"networks for this purpose and found that they have a much slower learning rate than MLR. For example, MLR only took 2.9 seconds as compare to 4790 seconds for the neural network in the <b>nettalk</b> dataset; while both have the same error rate. Other possible candidates are the multinomial logit model (Jordan & Jacobs, 1994), which is a special case of generalized linear models (McCullagh & Nelder,", "mykey":2137},
 {"datasetID":34, "supportID":"D5E67451F92883D0FC956C7C000B9ADD4C63A877", "rexaID":"70172e511a3bc27c7927119a3b2a3405fbad99e0", "author":"Kai Ming Ting and Ian H. Witten", "title":"Issues in Stacked Generalization", "venue":"J. Artif. Intell. Res. (JAIR, 10", "year":"1999", "window":"(Waveform, Soybean and Breast Cancer), and is better than arcing but worse than bagging in the <b>Diabetes</b> dataset. Note that stacking performs very poorly on Glass and Ionosphere, two small real-world datasets. This is not surprising, because cross-validation inevitably produces poor estimates for small", "mykey":2138},
 {"datasetID":42, "supportID":"D5E67451F92883D0FC956C7C000B9ADD4C63A877", "rexaID":"70172e511a3bc27c7927119a3b2a3405fbad99e0", "author":"Kai Ming Ting and Ian H. Witten", "title":"Issues in Stacked Generalization", "venue":"J. Artif. Intell. Res. (JAIR, 10", "year":"1999", "window":"Note that stacking performs very poorly on <b>Glass</b> and Ionosphere, two small real-world datasets. This is not surprising, because cross-validation inevitably produces poor estimates for small datasets. 4.2 Discussion Like bagging, stacking is ideal for parallel computation. The construction of", "mykey":2139},
 {"datasetID":45, "supportID":"D5E67451F92883D0FC956C7C000B9ADD4C63A877", "rexaID":"70172e511a3bc27c7927119a3b2a3405fbad99e0", "author":"Kai Ming Ting and Ian H. Witten", "title":"Issues in Stacked Generalization", "venue":"J. Artif. Intell. Res. (JAIR, 10", "year":"1999", "window":"situation. The performance variation among the member models in bagging is rather small because they are derived from the same learning algorithm using bootstrap samples. Section 3.3 4. The <b>heart</b> dataset used by Breiman (1996b; 1996c) is omitted because it was very much modified from the original one. 284 Issues in Stacked Generalization shows that a small performance variation among member models", "mykey":2140},
 {"datasetID":47, "supportID":"D5E67451F92883D0FC956C7C000B9ADD4C63A877", "rexaID":"70172e511a3bc27c7927119a3b2a3405fbad99e0", "author":"Kai Ming Ting and Ian H. Witten", "title":"Issues in Stacked Generalization", "venue":"J. Artif. Intell. Res. (JAIR, 10", "year":"1999", "window":"C4.5 NB IB1 1 0.36 0.20 0.42 0.63 0.30 0.04 2 0.39 0.19 0.41 0.65 0.28 0.07 C4.5 for ff 1 ; NB for ff 2 ; IB1 for ff 3 . Table 5: (a) Weights generated by MLR (model ~ M 0 ) for the <b>Horse</b> and Credit datasets. Splice Abalone Waveform Class C4.5 NB IB1 C4.5 NB IB1 C4.5 NB IB1 1 0.23 0.43 0.36 0.25 0.25 0.39 0.16 0.59 0.34 2 0.15 0.72 0.12 0.27 0.20 0.25 0.14 0.72 0.07 3 0.08 0.52 0.40 0.30 0.18 0.39 0.04", "mykey":2141},
 {"datasetID":52, "supportID":"D5E67451F92883D0FC956C7C000B9ADD4C63A877", "rexaID":"70172e511a3bc27c7927119a3b2a3405fbad99e0", "author":"Kai Ming Ting and Ian H. Witten", "title":"Issues in Stacked Generalization", "venue":"J. Artif. Intell. Res. (JAIR, 10", "year":"1999", "window":"Note that stacking performs very poorly on Glass and <b>Ionosphere</b>  two small real-world datasets. This is not surprising, because cross-validation inevitably produces poor estimates for small datasets. 4.2 Discussion Like bagging, stacking is ideal for parallel computation. The construction of", "mykey":2142},
 {"datasetID":69, "supportID":"D5E67451F92883D0FC956C7C000B9ADD4C63A877", "rexaID":"70172e511a3bc27c7927119a3b2a3405fbad99e0", "author":"Kai Ming Ting and Ian H. Witten", "title":"Issues in Stacked Generalization", "venue":"J. Artif. Intell. Res. (JAIR, 10", "year":"1999", "window":"can easily be interpreted. Examples of the combination weights it derives (for the probabilitybased model ~ M 0 ) appear in Table 5 for the Horse, Credit, <b>Splice</b>  Abalone, Waveform, Led24 and Vowel datasets. The weights indicate the relative importance of the level-0 generalizers for each prediction class. For example, in the Splice dataset (in Table 5(b)), NB is the dominant generalizer for", "mykey":2143},
 {"datasetID":90, "supportID":"D5E67451F92883D0FC956C7C000B9ADD4C63A877", "rexaID":"70172e511a3bc27c7927119a3b2a3405fbad99e0", "author":"Kai Ming Ting and Ian H. Witten", "title":"Issues in Stacked Generalization", "venue":"J. Artif. Intell. Res. (JAIR, 10", "year":"1999", "window":"are given in Table 10, and indicate that the three methods are very competitive. 4 Stacking performs better than both arcing and bagging in three datasets (Waveform, <b>Soybean</b> and Breast Cancer), and is better than arcing but worse than bagging in the Diabetes dataset. Note that stacking performs very poorly on Glass and Ionosphere, two small", "mykey":2144},
 {"datasetID":91, "supportID":"D5E67451F92883D0FC956C7C000B9ADD4C63A877", "rexaID":"70172e511a3bc27c7927119a3b2a3405fbad99e0", "author":"Kai Ming Ting and Ian H. Witten", "title":"Issues in Stacked Generalization", "venue":"J. Artif. Intell. Res. (JAIR, 10", "year":"1999", "window":"are given in Table 10, and indicate that the three methods are very competitive. 4 Stacking performs better than both arcing and bagging in three datasets (Waveform, <b>Soybean</b> and Breast Cancer), and is better than arcing but worse than bagging in the Diabetes dataset. Note that stacking performs very poorly on Glass and Ionosphere, two small", "mykey":2145},
 {"datasetID":107, "supportID":"D5E67451F92883D0FC956C7C000B9ADD4C63A877", "rexaID":"70172e511a3bc27c7927119a3b2a3405fbad99e0", "author":"Kai Ming Ting and Ian H. Witten", "title":"Issues in Stacked Generalization", "venue":"J. Artif. Intell. Res. (JAIR, 10", "year":"1999", "window":"from the UCI Repository of machine learning databases (Blake, Keogh & Merz, 1998). Details of these are given in Table 1. For the artificial datasets---Led24 and <b>Waveform</b> --each training dataset L of size 200 and 300, respectively, is generated using a different seed. The algorithms used for the experiments are then tested on a separate dataset", "mykey":2146},
 {"datasetID":108, "supportID":"D5E67451F92883D0FC956C7C000B9ADD4C63A877", "rexaID":"70172e511a3bc27c7927119a3b2a3405fbad99e0", "author":"Kai Ming Ting and Ian H. Witten", "title":"Issues in Stacked Generalization", "venue":"J. Artif. Intell. Res. (JAIR, 10", "year":"1999", "window":"from the UCI Repository of machine learning databases (Blake, Keogh & Merz, 1998). Details of these are given in Table 1. For the artificial datasets---Led24 and <b>Waveform</b> --each training dataset L of size 200 and 300, respectively, is generated using a different seed. The algorithms used for the experiments are then tested on a separate dataset", "mykey":2147},
 {"datasetID":46, "supportID":"D60BC4C8DFB454CFB52C4B739F5CC36B6C22A0CF", "rexaID":"fac79a1ce7fa29979fe50466d471cdac753f26d5", "author":"Ida G. Sprinkhuizen-Kuyper and Elena Smirnova and I. Nalbantis", "title":"Reliability yields Information Gain", "venue":"IKAT, Universiteit Maastricht", "year":"", "window":"The contribution of the set not covered by VSSVM to the entropy is equal to: (1a nc log 2 a nc11All cases in table 1 resulted in a considerable information gain. We especially mention the <b>hepatitis</b> dataset (the case of polynomial kernel) of which the information gain is 0.72 (we even obtained perfect information!) and the labor dataset (the case of polynomial kernel) of which the information gain is", "mykey":2148},
 {"datasetID":56, "supportID":"D60BC4C8DFB454CFB52C4B739F5CC36B6C22A0CF", "rexaID":"fac79a1ce7fa29979fe50466d471cdac753f26d5", "author":"Ida G. Sprinkhuizen-Kuyper and Elena Smirnova and I. Nalbantis", "title":"Reliability yields Information Gain", "venue":"IKAT, Universiteit Maastricht", "year":"", "window":"(the case of polynomial kernel) of which the information gain is 0.72 (we even obtained perfect information!) and the <b>labor</b> dataset (the case of polynomial kernel) of which the information gain is 0.42. 8. Conclusion For practical application of machine learning algorithms knowledge about the reliability of individual instances", "mykey":2149},
 {"datasetID":34, "supportID":"D6488B216B45FCD51BF29984F154B8EA16EFFAC9", "rexaID":"9a0080df56f0a8fb004723a7dece29109bc316ce", "author":"Michael Lindenbaum and Shaul Markovitch and Dmitry Rusakov", "title":"Selective Sampling Using Random Field Modelling", "venue":"", "year":"", "window":"Among them there were three natural datasets: Pima Indians <b>Diabetes</b> dataset, Ionosphere dataset and Image Segmentation dataset, one synthetic dataset: Letters dataset and three artificial problems: Two-Spirals problem, Two-Gaussians problem", "mykey":2150},
 {"datasetID":50, "supportID":"D6488B216B45FCD51BF29984F154B8EA16EFFAC9", "rexaID":"9a0080df56f0a8fb004723a7dece29109bc316ce", "author":"Michael Lindenbaum and Shaul Markovitch and Dmitry Rusakov", "title":"Selective Sampling Using Random Field Modelling", "venue":"", "year":"", "window":"Pima Indians Diabetes dataset, Ionosphere dataset and <b>Image Segmentation</b> dataset, one synthetic dataset: Letters dataset and three artificial problems: Two-Spirals problem, Two-Gaussians problem and Multi-Gaussian problem. The", "mykey":2151},
 {"datasetID":52, "supportID":"D6488B216B45FCD51BF29984F154B8EA16EFFAC9", "rexaID":"9a0080df56f0a8fb004723a7dece29109bc316ce", "author":"Michael Lindenbaum and Shaul Markovitch and Dmitry Rusakov", "title":"Selective Sampling Using Random Field Modelling", "venue":"", "year":"", "window":"Among them there were three natural datasets: Pima Indians Diabetes dataset, <b>Ionosphere</b> dataset and Image Segmentation dataset, one synthetic dataset: Letters dataset and three artificial problems: Two-Spirals problem, Two-Gaussians problem", "mykey":2152},
 {"datasetID":79, "supportID":"D6488B216B45FCD51BF29984F154B8EA16EFFAC9", "rexaID":"9a0080df56f0a8fb004723a7dece29109bc316ce", "author":"Michael Lindenbaum and Shaul Markovitch and Dmitry Rusakov", "title":"Selective Sampling Using Random Field Modelling", "venue":"", "year":"", "window":"Among them there were three natural datasets: <b>Pima</b> <b>Indians</b> <b>Diabetes</b> dataset, Ionosphere dataset and Image Segmentation dataset, one synthetic dataset: Letters dataset and three artificial problems: Two-Spirals problem, Two-Gaussians problem", "mykey":2153},
 {"datasetID":147, "supportID":"D6488B216B45FCD51BF29984F154B8EA16EFFAC9", "rexaID":"9a0080df56f0a8fb004723a7dece29109bc316ce", "author":"Michael Lindenbaum and Shaul Markovitch and Dmitry Rusakov", "title":"Selective Sampling Using Random Field Modelling", "venue":"", "year":"", "window":"Pima Indians Diabetes dataset, Ionosphere dataset and <b>Image Segmentation</b> dataset, one synthetic dataset: Letters dataset and three artificial problems: Two-Spirals problem, Two-Gaussians problem and Multi-Gaussian problem. The", "mykey":2154},
 {"datasetID":67, "supportID":"D6B52F66BE336143D86BC65091C47187D5554E7B", "rexaID":"b355244c8f3224bf7b73360037b45c7e2ceb865c", "author":"Alain Rakotomamonjy", "title":"Analysis of SVM regression bounds for variable ranking", "venue":"P.S.I CNRS FRE 2645, INSA de Rouen Avenue de l'Universite", "year":"", "window":"We have tested our algorithms on some real-world problems such as QSAR data and <b>DNA</b> microarray problems. Informations on these datasets, including references where they can be found, are given in Table (5). As described some of these datasets deal with classification problems. In such cases, we have still addressed such problems by", "mykey":2155},
 {"datasetID":110, "supportID":"D6B52F66BE336143D86BC65091C47187D5554E7B", "rexaID":"b355244c8f3224bf7b73360037b45c7e2ceb865c", "author":"Alain Rakotomamonjy", "title":"Analysis of SVM regression bounds for variable ranking", "venue":"P.S.I CNRS FRE 2645, INSA de Rouen Avenue de l'Universite", "year":"", "window":"performance considering that an oracle has selected the optimal number of variables to use. In bold, the best performance with respect to the mean normalized absolute error are highlighted. Datasets olitos <b>yeast</b> colon lymphoma Algo Mean Var. p-val Mean Var. p-val Mean Var p-val. Mean Var p-val. SVM 30.93 25 - 16.50 79 - 53.75 2000 - 58.80 4026 - CC 30.38 13 0.424 14.73 23 0.045 51.01 300 0.517", "mykey":2156},
 {"datasetID":33, "supportID":"D6B8975B43D28BB081FB2B58476AC3664B42C57F", "rexaID":"8eb7bb53b63501db4eb1c49eab30d76f7febba8d", "author":"H. Altay Guvenir", "title":"A Classification Learning Algorithm Robust to Irrelevant Features", "venue":"Bilkent University, Department of Computer Engineering and Information Science", "year":"", "window":"of 100 5-fold cross-validation accuracies. 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Number of irrelevant features added 0.5 0.6 0.7 0.8 0.9 1.0 Classification accuracy <b>Dermatology</b> data set VFI5 1NN 3NN 5NN 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Number of irrelevant features added 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Classification accuracy Glass data set VFI5 1NN", "mykey":2157},
 {"datasetID":42, "supportID":"D6B8975B43D28BB081FB2B58476AC3664B42C57F", "rexaID":"8eb7bb53b63501db4eb1c49eab30d76f7febba8d", "author":"H. Altay Guvenir", "title":"A Classification Learning Algorithm Robust to Irrelevant Features", "venue":"Bilkent University, Department of Computer Engineering and Information Science", "year":"", "window":"VFI5 1NN 3NN 5NN 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Number of irrelevant features added 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Classification accuracy <b>Glass</b> data set VFI5 1NN 3NN 5NN 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Number of irrelevant features added 0.5 0.6 0.7 0.8 0.9 1.0 Classification accuracy Iris data set VFI5 1NN 3NN 5NN 0 1 2 3 4 5 6", "mykey":2158},
 {"datasetID":53, "supportID":"D6B8975B43D28BB081FB2B58476AC3664B42C57F", "rexaID":"8eb7bb53b63501db4eb1c49eab30d76f7febba8d", "author":"H. Altay Guvenir", "title":"A Classification Learning Algorithm Robust to Irrelevant Features", "venue":"Bilkent University, Department of Computer Engineering and Information Science", "year":"", "window":"VFI5 1NN 3NN 5NN 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Number of irrelevant features added 0.5 0.6 0.7 0.8 0.9 1.0 Classification accuracy <b>Iris</b> data set VFI5 1NN 3NN 5NN 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Number of irrelevant features added 0.5 0.6 0.7 0.8 0.9 1.0 Classification accuracy New-thyroid data set VFI5 1NN 3NN 5NN 0 1 2", "mykey":2159},
 {"datasetID":149, "supportID":"D6B8975B43D28BB081FB2B58476AC3664B42C57F", "rexaID":"8eb7bb53b63501db4eb1c49eab30d76f7febba8d", "author":"H. Altay Guvenir", "title":"A Classification Learning Algorithm Robust to Irrelevant Features", "venue":"Bilkent University, Department of Computer Engineering and Information Science", "year":"", "window":"VFI5 1NN 3NN 5NN 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Number of irrelevant features added 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Classification accuracy <b>Vehicle</b> data set VFI5 1NN 3NN 5NN 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Number of irrelevant features added 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Classification accuracy Wine data set VFI5 1NN", "mykey":2160},
 {"datasetID":102, "supportID":"D6B8975B43D28BB081FB2B58476AC3664B42C57F", "rexaID":"8eb7bb53b63501db4eb1c49eab30d76f7febba8d", "author":"H. Altay Guvenir", "title":"A Classification Learning Algorithm Robust to Irrelevant Features", "venue":"Bilkent University, Department of Computer Engineering and Information Science", "year":"", "window":"VFI5 1NN 3NN 5NN 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Number of irrelevant features added 0.5 0.6 0.7 0.8 0.9 1.0 Classification accuracy New <b>thyroid</b> data set VFI5 1NN 3NN 5NN 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Number of irrelevant features added 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Classification accuracy Vehicle data set VFI5", "mykey":2161},
 {"datasetID":109, "supportID":"D6B8975B43D28BB081FB2B58476AC3664B42C57F", "rexaID":"8eb7bb53b63501db4eb1c49eab30d76f7febba8d", "author":"H. Altay Guvenir", "title":"A Classification Learning Algorithm Robust to Irrelevant Features", "venue":"Bilkent University, Department of Computer Engineering and Information Science", "year":"", "window":"VFI5 1NN 3NN 5NN 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Number of irrelevant features added 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Classification accuracy <b>Wine</b> data set VFI5 1NN 3NN 5NN Fig. 7. The comparison of the average classification accuracies for kNN and VFI5 on some of UCI-Repository data sets with increasing number of artificially added irrelevant", "mykey":2162},
 {"datasetID":14, "supportID":"D6DC06AB423C20B08152FD63F9BCB1A86F58AAE2", "rexaID":"4ce4c96181e2836dd80a71c2efebc7fb030c55d8", "author":"Baback Moghaddam and Gregory Shakhnarovich", "title":"Boosted Dyadic Kernel Discriminants", "venue":"NIPS", "year":"2002", "window":"the number of support vectors for the SVM, and #k.ev. the number of kernel evaluations required by a boosted hypercuts classifier. Means and standard deviations in 30 trials are reported for each data set. WBC,WPBC,WDBC are Wisconsin <b>Breast</b> <b>Cancer</b>  Prognosis and Diagnosis data sets, respectively. In each experiment, the data set was randomly partitioned into training, validation and test sets of", "mykey":2163},
 {"datasetID":17, "supportID":"D6DC06AB423C20B08152FD63F9BCB1A86F58AAE2", "rexaID":"4ce4c96181e2836dd80a71c2efebc7fb030c55d8", "author":"Baback Moghaddam and Gregory Shakhnarovich", "title":"Boosted Dyadic Kernel Discriminants", "venue":"NIPS", "year":"2002", "window":"the number of support vectors for the SVM, and #k.ev. the number of kernel evaluations required by a boosted hypercuts classifier. Means and standard deviations in 30 trials are reported for each data set. WBC,WPBC,WDBC are <b>Wisconsin</b> <b>Breast</b> <b>Cancer</b>  Prognosis and Diagnosis data sets, respectively. In each experiment, the data set was randomly partitioned into training, validation and test sets of", "mykey":2164},
 {"datasetID":15, "supportID":"D6DC06AB423C20B08152FD63F9BCB1A86F58AAE2", "rexaID":"4ce4c96181e2836dd80a71c2efebc7fb030c55d8", "author":"Baback Moghaddam and Gregory Shakhnarovich", "title":"Boosted Dyadic Kernel Discriminants", "venue":"NIPS", "year":"2002", "window":"the number of support vectors for the SVM, and #k.ev. the number of kernel evaluations required by a boosted hypercuts classifier. Means and standard deviations in 30 trials are reported for each data set. WBC,WPBC,WDBC are <b>Wisconsin</b> <b>Breast</b> <b>Cancer</b>  Prognosis and Diagnosis data sets, respectively. In each experiment, the data set was randomly partitioned into training, validation and test sets of", "mykey":2165},
 {"datasetID":16, "supportID":"D6DC06AB423C20B08152FD63F9BCB1A86F58AAE2", "rexaID":"4ce4c96181e2836dd80a71c2efebc7fb030c55d8", "author":"Baback Moghaddam and Gregory Shakhnarovich", "title":"Boosted Dyadic Kernel Discriminants", "venue":"NIPS", "year":"2002", "window":"the number of support vectors for the SVM, and #k.ev. the number of kernel evaluations required by a boosted hypercuts classifier. Means and standard deviations in 30 trials are reported for each data set. WBC,WPBC,WDBC are <b>Wisconsin</b> <b>Breast</b> <b>Cancer</b>  Prognosis and Diagnosis data sets, respectively. In each experiment, the data set was randomly partitioned into training, validation and test sets of", "mykey":2166},
 {"datasetID":45, "supportID":"D6DC06AB423C20B08152FD63F9BCB1A86F58AAE2", "rexaID":"4ce4c96181e2836dd80a71c2efebc7fb030c55d8", "author":"Baback Moghaddam and Gregory Shakhnarovich", "title":"Boosted Dyadic Kernel Discriminants", "venue":"NIPS", "year":"2002", "window":"and k-Nearest Neighbor (k-NN). We chose sets large enough for reasonable training/validation/test partitioning, and that represent binary (or easily converted to binary) classification problems. Dataset N d k-NN SVM #SV Hypercuts #k.ev. <b>Heart</b> 90 13 .196 \u00b1.042 .202 \u00b1.038 62 \u00b110 .202 \u00b1.030 50 \u00b112 Ionosphere 120 34 .168 \u00b1.024 .064 \u00b1.018 73 \u00b17 .083 \u00b1.022 63 \u00b17 WBC 200 9 .034 \u00b1.011 .032 \u00b1.008 50 \u00b126", "mykey":2167},
 {"datasetID":7, "supportID":"D6E9D6F5B8A803B7FEFB1F487FCCD64945A3514E", "rexaID":"660eddc9c8a809e94ad8d5837952f89d8997ab92", "author":"Mohammed Waleed Kadous and Claude Sammut", "title":"The University of New South Wales School of Computer Science and Engineering Temporal Classification: Extending the Classification Paradigm to Multivariate Time Series", "venue":"", "year":"", "window":"Saito [Sai94], and further worked on by Manganaris [Man97]. The task is to classify a stream as one of three classes, cylinder (c), bell (b) or funnel (f ). Samples are generated as follows: 2 These datasets are: arrythmia, <b>audiology</b>  bach chorales, echocardiogram, isolet, mobile robots, waveform. 6. Experimental Evaluation 161 c(t) = (6 + #) \u00b7 # [a,b] (t) + #(t) b(t) = (6 + #) \u00b7 # [a,b] (t) \u00b7 (t -", "mykey":2168},
 {"datasetID":8, "supportID":"D6E9D6F5B8A803B7FEFB1F487FCCD64945A3514E", "rexaID":"660eddc9c8a809e94ad8d5837952f89d8997ab92", "author":"Mohammed Waleed Kadous and Claude Sammut", "title":"The University of New South Wales School of Computer Science and Engineering Temporal Classification: Extending the Classification Paradigm to Multivariate Time Series", "venue":"", "year":"", "window":"Saito [Sai94], and further worked on by Manganaris [Man97]. The task is to classify a stream as one of three classes, cylinder (c), bell (b) or funnel (f ). Samples are generated as follows: 2 These datasets are: arrythmia, <b>audiology</b>  bach chorales, echocardiogram, isolet, mobile robots, waveform. 6. Experimental Evaluation 161 c(t) = (6 + #) \u00b7 # [a,b] (t) + #(t) b(t) = (6 + #) \u00b7 # [a,b] (t) \u00b7 (t -", "mykey":2169},
 {"datasetID":114, "supportID":"D6E9D6F5B8A803B7FEFB1F487FCCD64945A3514E", "rexaID":"660eddc9c8a809e94ad8d5837952f89d8997ab92", "author":"Mohammed Waleed Kadous and Claude Sammut", "title":"The University of New South Wales School of Computer Science and Engineering Temporal Classification: Extending the Classification Paradigm to Multivariate Time Series", "venue":"", "year":"", "window":"Mark Peters for sharing their experiences and for proofreading early drafts; and also to two other proofreaders: Peter Gammie and Peter Rickwood. My thanks also extend to those who helped with the datasets: with the <b>Sign</b> <b>Language</b> dataset, my thanks go particularly to Todd Wright, but also to those who helped with the undergraduate work: Adam Schembri and Adam Young. Also Philip de Chazal and Branko", "mykey":2170},
 {"datasetID":115, "supportID":"D6E9D6F5B8A803B7FEFB1F487FCCD64945A3514E", "rexaID":"660eddc9c8a809e94ad8d5837952f89d8997ab92", "author":"Mohammed Waleed Kadous and Claude Sammut", "title":"The University of New South Wales School of Computer Science and Engineering Temporal Classification: Extending the Classification Paradigm to Multivariate Time Series", "venue":"", "year":"", "window":"Mark Peters for sharing their experiences and for proofreading early drafts; and also to two other proofreaders: Peter Gammie and Peter Rickwood. My thanks also extend to those who helped with the datasets: with the <b>Sign</b> <b>Language</b> dataset, my thanks go particularly to Todd Wright, but also to those who helped with the undergraduate work: Adam Schembri and Adam Young. Also Philip de Chazal and Branko", "mykey":2171},
 {"datasetID":25, "supportID":"D6E9D6F5B8A803B7FEFB1F487FCCD64945A3514E", "rexaID":"660eddc9c8a809e94ad8d5837952f89d8997ab92", "author":"Mohammed Waleed Kadous and Claude Sammut", "title":"The University of New South Wales School of Computer Science and Engineering Temporal Classification: Extending the Classification Paradigm to Multivariate Time Series", "venue":"", "year":"", "window":"Saito [Sai94], and further worked on by Manganaris [Man97]. The task is to classify a stream as one of three classes, cylinder (c), bell (b) or funnel (f ). Samples are generated as follows: 2 These datasets are: arrythmia, audiology, <b>bach</b> <b>chorales</b>  echocardiogram, isolet, mobile robots, waveform. 6. Experimental Evaluation 161 c(t) = (6 + #) \u00b7 # [a,b] (t) + #(t) b(t) = (6 + #) \u00b7 # [a,b] (t) \u00b7 (t -", "mykey":2172},
 {"datasetID":92, "supportID":"D6E9D6F5B8A803B7FEFB1F487FCCD64945A3514E", "rexaID":"660eddc9c8a809e94ad8d5837952f89d8997ab92", "author":"Mohammed Waleed Kadous and Claude Sammut", "title":"The University of New South Wales School of Computer Science and Engineering Temporal Classification: Extending the Classification Paradigm to Multivariate Time Series", "venue":"", "year":"", "window":"analysis technique (i.e. a technique that allows the system to cope with the problem that patterns occur at different temporal scales) and applies them to <b>space</b> <b>shuttle</b> data as well as an artificial dataset. Mannila et al [MTV95] have also been looking at temporal classification problems, in particular applying it to network traffic analysis. In their model, streams are a sequence of time-labelled", "mykey":2173},
 {"datasetID":148, "supportID":"D6E9D6F5B8A803B7FEFB1F487FCCD64945A3514E", "rexaID":"660eddc9c8a809e94ad8d5837952f89d8997ab92", "author":"Mohammed Waleed Kadous and Claude Sammut", "title":"The University of New South Wales School of Computer Science and Engineering Temporal Classification: Extending the Classification Paradigm to Multivariate Time Series", "venue":"", "year":"", "window":"analysis technique (i.e. a technique that allows the system to cope with the problem that patterns occur at different temporal scales) and applies them to space <b>shuttle</b> data as well as an artificial dataset. Mannila et al [MTV95] have also been looking at temporal classification problems, in particular applying it to network traffic analysis. In their model, streams are a sequence of time-labelled", "mykey":2174},
 {"datasetID":53, "supportID":"D805A7A2A26466B05693D5FB11BE65E4FA3FF1EC", "rexaID":"9fe9e6630b7b953504726b9b36281c4f738b964e", "author":"Shlomo Dubnov and Ran El and Yaniv Technion and Yoram Gdalyahu and Elad Schneidman and Naftali Tishby and Golan Yona", "title":"Clustering By Friends : A New Nonparametric Pairwise Distance Based Clustering Algorithm", "venue":"Ben Gurion University", "year":"", "window":"procedure of the cross-validation index (see Section 3) and we only report the resulting cross-validation indices obtained during the computations. In section 5.1 we consider the classical <b>Iris</b> data sets. Then, in section 5.2 we consider the Isolet data set. An application to musical data is considered in section 5.3. 5.1. The Iris Data This data set, due to Fisher (Fisher, 1936), is a classic", "mykey":2175},
 {"datasetID":54, "supportID":"D805A7A2A26466B05693D5FB11BE65E4FA3FF1EC", "rexaID":"9fe9e6630b7b953504726b9b36281c4f738b964e", "author":"Shlomo Dubnov and Ran El and Yaniv Technion and Yoram Gdalyahu and Elad Schneidman and Naftali Tishby and Golan Yona", "title":"Clustering By Friends : A New Nonparametric Pairwise Distance Based Clustering Algorithm", "venue":"Ben Gurion University", "year":"", "window":"procedure of the cross-validation index (see Section 3) and we only report the resulting cross-validation indices obtained during the computations. In section 5.1 we consider the classical Iris data sets. Then, in section 5.2 we consider the <b>Isolet</b> data set. An application to musical data is considered in section 5.3. 5.1. The Iris Data This data set, due to Fisher (Fisher, 1936), is a classic", "mykey":2176},
 {"datasetID":81, "supportID":"D8546265D6422E49B38B8CC737E3ED7074A386EA", "rexaID":"e5473e70fc6fbb348156490ee2bfb466937196bc", "author":"Fabian Hoti and Lasse Holmstr\u00f6m", "title":"A semiparametric density estimation approach to pattern classification", "venue":"Pattern Recognition, 37", "year":"2004", "window":"classifiers tried in [17]. The classification error of both KDA and QDA was 3:7%. Using a so-called convex local subspace classifier, a smaller error rate of 2.1% was reported in [18]. 3.2.2 Public data sets 1: satellite image and <b>handwritten</b> <b>digits</b> Next we consider two public data sets obtained from the UCI Machine Learning Repository [19]. The first example is a satellite image data set with 4435", "mykey":2177},
 {"datasetID":146, "supportID":"D8546265D6422E49B38B8CC737E3ED7074A386EA", "rexaID":"e5473e70fc6fbb348156490ee2bfb466937196bc", "author":"Fabian Hoti and Lasse Holmstr\u00f6m", "title":"A semiparametric density estimation approach to pattern classification", "venue":"Pattern Recognition, 37", "year":"2004", "window":"classifiers tried in [17]. The classification error of both KDA and QDA was 3:7%. Using a so-called convex local subspace classifier, a smaller error rate of 2.1% was reported in [18]. 3.2.2 Public data sets 1: <b>satellite</b> image and handwritten digits Next we consider two public data sets obtained from the UCI Machine Learning Repository [19]. The first example is a satellite image data set with 4435", "mykey":2178},
 {"datasetID":34, "supportID":"D89018FC37B566FD3831C3B86B0E148E7E09F5AC", "rexaID":"989443f4feec00743b3521c1b450717f6013de20", "author":"Simon Tong and Daphne Koller", "title":"Restricted Bayes Optimal Classifiers", "venue":"AAAI/IAAI", "year":"2000", "window":"exact training error. We investigated whether using a non-zero value of oe would achieve a similar effect to that of the soft margin error function. 1 We used the \"Pima Indian <b>Diabetes</b>  UC Irvine data set (Blake, Keogh, & Merz 1998) and a synthetic data set. The Pima data set has eight features, with 576 training instances of which 198 are labeled as positive. The synthetic data were generated from", "mykey":2179},
 {"datasetID":79, "supportID":"D89018FC37B566FD3831C3B86B0E148E7E09F5AC", "rexaID":"989443f4feec00743b3521c1b450717f6013de20", "author":"Simon Tong and Daphne Koller", "title":"Restricted Bayes Optimal Classifiers", "venue":"AAAI/IAAI", "year":"2000", "window":"exact training error. We investigated whether using a non-zero value of oe would achieve a similar effect to that of the soft margin error function. 1 We used the  <b>Pima</b> Indian <b>Diabetes</b>  UC Irvine data set (Blake, Keogh, & Merz 1998) and a synthetic data set. The Pima data set has eight features, with 576 training instances of which 198 are labeled as positive. The synthetic data were generated from", "mykey":2180},
 {"datasetID":7, "supportID":"D89D572598888C820DF232F1E1F1F0D1A3210231", "rexaID":"7125dc87ceddf952f1840074b40ac0526dcc03dc", "author":"Marcus Hutter and Marco Zaffalon", "title":"Distribution of Mutual Information from Complete and Incomplete Data", "venue":"CoRR, csLG/0403025", "year":"2004", "window":"behave very similarly to the case of complete data. The filter FF still selects the smallest number of features, and this number usually increases with F and even more with BF. The selection 23 Data set #feat. FF F BF <b>Audiology</b> 69 64.3 68.0 68.7 Crx 15 9.7 12.6 13.8 Horse-colic 18 11.8 16.1 17.4 Hypothyroidloss 23 4.3 8.3 13.2 Soybean-large 35 34.2 35.0 35.0 Tab l e 4 Average number of attributes", "mykey":2181},
 {"datasetID":8, "supportID":"D89D572598888C820DF232F1E1F1F0D1A3210231", "rexaID":"7125dc87ceddf952f1840074b40ac0526dcc03dc", "author":"Marcus Hutter and Marco Zaffalon", "title":"Distribution of Mutual Information from Complete and Incomplete Data", "venue":"CoRR, csLG/0403025", "year":"2004", "window":"behave very similarly to the case of complete data. The filter FF still selects the smallest number of features, and this number usually increases with F and even more with BF. The selection 23 Data set #feat. FF F BF <b>Audiology</b> 69 64.3 68.0 68.7 Crx 15 9.7 12.6 13.8 Horse-colic 18 11.8 16.1 17.4 Hypothyroidloss 23 4.3 8.3 13.2 Soybean-large 35 34.2 35.0 35.0 Tab l e 4 Average number of attributes", "mykey":2182},
 {"datasetID":23, "supportID":"D89D572598888C820DF232F1E1F1F0D1A3210231", "rexaID":"7125dc87ceddf952f1840074b40ac0526dcc03dc", "author":"Marcus Hutter and Marco Zaffalon", "title":"Distribution of Mutual Information from Complete and Incomplete Data", "venue":"CoRR, csLG/0403025", "year":"2004", "window":"300 600 900 1200 1500 1800 2100 2400 2700 3000 Instance number Prediction accuracy  <b>Chess</b>  FF F Fig. 2. Comparison of the prediction accuracies of the naive Bayes with filters F and FF on the Chess data set. The gray area denotes differences that are not statistically significant. The remaining cases are described by means of the following figures. Figure 2 shows that FF allowed the naive Bayes to", "mykey":2183},
 {"datasetID":21, "supportID":"D89D572598888C820DF232F1E1F1F0D1A3210231", "rexaID":"7125dc87ceddf952f1840074b40ac0526dcc03dc", "author":"Marcus Hutter and Marco Zaffalon", "title":"Distribution of Mutual Information from Complete and Incomplete Data", "venue":"CoRR, csLG/0403025", "year":"2004", "window":"300 600 900 1200 1500 1800 2100 2400 2700 3000 Instance number Prediction accuracy  <b>Chess</b>  FF F Fig. 2. Comparison of the prediction accuracies of the naive Bayes with filters F and FF on the Chess data set. The gray area denotes differences that are not statistically significant. The remaining cases are described by means of the following figures. Figure 2 shows that FF allowed the naive Bayes to", "mykey":2184},
 {"datasetID":22, "supportID":"D89D572598888C820DF232F1E1F1F0D1A3210231", "rexaID":"7125dc87ceddf952f1840074b40ac0526dcc03dc", "author":"Marcus Hutter and Marco Zaffalon", "title":"Distribution of Mutual Information from Complete and Incomplete Data", "venue":"CoRR, csLG/0403025", "year":"2004", "window":"300 600 900 1200 1500 1800 2100 2400 2700 3000 Instance number Prediction accuracy  <b>Chess</b>  FF F Fig. 2. Comparison of the prediction accuracies of the naive Bayes with filters F and FF on the Chess data set. The gray area denotes differences that are not statistically significant. The remaining cases are described by means of the following figures. Figure 2 shows that FF allowed the naive Bayes to", "mykey":2185},
 {"datasetID":63, "supportID":"D89D572598888C820DF232F1E1F1F0D1A3210231", "rexaID":"7125dc87ceddf952f1840074b40ac0526dcc03dc", "author":"Marcus Hutter and Marco Zaffalon", "title":"Distribution of Mutual Information from Complete and Incomplete Data", "venue":"CoRR, csLG/0403025", "year":"2004", "window":"on a number of different domains. For example, Shuttlesmall reports data on diagnosing failures of the space shuttle; <b>Lymphography</b> and Hypothyroid are medical data sets; Spam is a body of e-mails that can be spam or non-spam; etc. Name #feat. #inst. mode freq. Australian 36 690 0.555 Chess 36 3196 0.520 Crx 15 653 0.547 German-org 17 1000 0.700 Hypothyroid 23 2238", "mykey":2186},
 {"datasetID":151, "supportID":"D8A619A3F46E4F984EE7492C6067EFC3545FE050", "rexaID":"e70864ddba6a15ff831fca1695141cd180729323", "author":"Carlotta Domeniconi and Bojun Yan", "title":"On Error Correlation and Accuracy of Nearest Neighbor Ensemble Classifiers", "venue":"Information and Software Engineering Department George Mason University", "year":"", "window":"we consider equal priors, and thus # 1,2 =1/C C # i=1 # i 1,2 gives the total error correlation between classifiers 1 and 2. Sample results for liver and <b>sonar</b> are given in Tables 9-10. For each data set we summarize the average error correlation values computed between five classifiers. We also report the corresponding error rates of the ensembles. In each case simple voting is used. Weight-C is", "mykey":2187},
 {"datasetID":1, "supportID":"D9456912AD21C64694101D44A2CF30A5908CD64A", "rexaID":"5bffb4d591b37fc0c1011681ab2cf075033ca539", "author":"Anton Schwaighofer and Volker Tresp", "title":"Transductive and Inductive Methods for Approximate Gaussian Process Regression", "venue":"NIPS", "year":"2002", "window":"costs for predictions show the cost per test point. 4 Experimental Comparison In this section we will present a comparison of the different approximation methods discussed in Sec. 3. In the <b>ABALONE</b> data set [1] with 4177 examples, the goal is to predict the age of Abalones based on 8 inputs. The KIN8NM data set 2 represents the forward dynamics of an 8 link all-revolute robot arm, based on 8192", "mykey":2188},
 {"datasetID":52, "supportID":"D9564C076A9FBF4DC4F5E5F19B158598AA4BA910", "rexaID":"ca1479cb43feeda16386d2e50460bd2c1230bc8b", "author":"Robert E. Schapire and Yoav Freund and Peter Bartlett and Wee Sun Lee", "title":"The Annals of Statistics, to appear. Boosting the Margin: A New Explanation for the Effectiveness of Voting Methods", "venue":"AT&T Labs", "year":"1998", "window":"the generalization error more often than not. In one experiment reported by Breiman, the generalization error increases even though the margins of all of the instances are increased (for this dataset, called  <b>ionosphere</b> \" the number of instances is 351, much too small for our bounds to apply). While none of these experiments contradict the theory, they highlight the incompleteness of the theory", "mykey":2189},
 {"datasetID":146, "supportID":"D9564C076A9FBF4DC4F5E5F19B158598AA4BA910", "rexaID":"ca1479cb43feeda16386d2e50460bd2c1230bc8b", "author":"Robert E. Schapire and Yoav Freund and Peter Bartlett and Wee Sun Lee", "title":"The Annals of Statistics, to appear. Boosting the Margin: A New Explanation for the Effectiveness of Voting Methods", "venue":"AT&T Labs", "year":"1998", "window":"Each stimulus was converted into 16 primitive numerical attributes (statistical moments and edge counts) which were then scaled to fit into a range of integer values from 0 through 15.\" The satimage dataset is the statlog version of a <b>satellite</b> image dataset. According to the documentation, \"This database consists of the multi-spectral values of pixels in 3 # 3 neighborhoods in a satellite image, and", "mykey":2190},
 {"datasetID":149, "supportID":"D9564C076A9FBF4DC4F5E5F19B158598AA4BA910", "rexaID":"ca1479cb43feeda16386d2e50460bd2c1230bc8b", "author":"Robert E. Schapire and Yoav Freund and Peter Bartlett and Wee Sun Lee", "title":"The Annals of Statistics, to appear. Boosting the Margin: A New Explanation for the Effectiveness of Voting Methods", "venue":"AT&T Labs", "year":"1998", "window":"4: Error curves and margin distribution graphs for three voting methods (bagging, boosting and ECOC) using C4.5 as the base learning algorithm. Results are given for the letter, satimage and <b>vehicle</b> datasets. (See caption under Figure 1 for an explanation of these curves.) 13 decision stumps Boosting Bagging ECOC letter error (%) 10 100 1000 0 20 40 60 80 100 10 100 1000 0 20 40 60 80 100 10 100 1000 0", "mykey":2191},
 {"datasetID":98, "supportID":"D9564C076A9FBF4DC4F5E5F19B158598AA4BA910", "rexaID":"ca1479cb43feeda16386d2e50460bd2c1230bc8b", "author":"Robert E. Schapire and Yoav Freund and Peter Bartlett and Wee Sun Lee", "title":"The Annals of Statistics, to appear. Boosting the Margin: A New Explanation for the Effectiveness of Voting Methods", "venue":"AT&T Labs", "year":"1998", "window":"Each stimulus was converted into 16 primitive numerical attributes (statistical moments and edge counts) which were then scaled to fit into a range of integer values from 0 through 15.\" The satimage dataset is the <b>statlog</b> version of a satellite image dataset. According to the documentation, \"This database consists of the multi-spectral values of pixels in 3 # 3 neighborhoods in a satellite image, and", "mykey":2192},
 {"datasetID":60, "supportID":"D97C395FC546457D38AE4CD03549D2577C5F6DDC", "rexaID":"8978cfdbc009102478ae8b1d327fafd076c14538", "author":"Petri Kontkanen and Jussi Lahtinen and Petri Myllym\u00e4ki and Henry Tirri", "title":"Unsupervised Bayesian visualization of high-dimensional data", "venue":"KDD", "year":"2000", "window":"experimental results confirm this hypothesis: in cases where the leave-one-out crossvalidated classification accuracy of the NB classifier is poor in the absolute sense (as with the <b>Liver</b> Disorders data set), or in the relative sense with respect to the default classification accuracy (as with the Postoperative Patient data set), the class labeled colored images are somewhat blurred. Nevertheless, we", "mykey":2193},
 {"datasetID":82, "supportID":"D97C395FC546457D38AE4CD03549D2577C5F6DDC", "rexaID":"8978cfdbc009102478ae8b1d327fafd076c14538", "author":"Petri Kontkanen and Jussi Lahtinen and Petri Myllym\u00e4ki and Henry Tirri", "title":"Unsupervised Bayesian visualization of high-dimensional data", "venue":"KDD", "year":"2000", "window":"or in the relative sense with respect to the default classification accuracy (as with the <b>Postoperative</b> <b>Patient data</b> set), the class labeled colored images are somewhat blurred. Nevertheless, we would like to emphasize that this does not mean that the unsupervised visualization technique has ``failed'' in these cases,", "mykey":2194},
 {"datasetID":102, "supportID":"D97C395FC546457D38AE4CD03549D2577C5F6DDC", "rexaID":"8978cfdbc009102478ae8b1d327fafd076c14538", "author":"Petri Kontkanen and Jussi Lahtinen and Petri Myllym\u00e4ki and Henry Tirri", "title":"Unsupervised Bayesian visualization of high-dimensional data", "venue":"KDD", "year":"2000", "window":"purposes a reasonable approximation is usually quite sufficient. How to find effectively good approximations of the optimal visualization is however a wide 327 Figure 1: The <b>Thyroid</b> Disease data set: an example of the unsupervised visualizations obtained with the suggested method. research problem on its own, and is not discussed in detail here. In the experiments reported here we used a simple", "mykey":2195},
 {"datasetID":45, "supportID":"D9C661038F2121C87654791F973D3AF20794887C", "rexaID":"56eb4e64499baa9ef620d2aeb5383e1739a2e6e3", "author":"Remco R. Bouckaert and Eibe Frank", "title":"Evaluating the Replicability of Significance Tests for Comparing Learning Algorithms", "venue":"PAKDD", "year":"2004", "window":"perform differently in 19 out of 27 cases. For some rows, the test consistently indicates no difference between any two of the three schemes, in particular for the iris and Hungarian <b>heart</b> disease datasets. However, most rows contain at least one cell where the outcomes of the test are not consistent. The row labeled \"consistent\" at the bottom of the table lists the number of datasets for which all", "mykey":2196},
 {"datasetID":53, "supportID":"D9C661038F2121C87654791F973D3AF20794887C", "rexaID":"56eb4e64499baa9ef620d2aeb5383e1739a2e6e3", "author":"Remco R. Bouckaert and Eibe Frank", "title":"Evaluating the Replicability of Significance Tests for Comparing Learning Algorithms", "venue":"PAKDD", "year":"2004", "window":"perform differently in 19 out of 27 cases. For some rows, the test consistently indicates no difference between any two of the three schemes, in particular for the <b>iris</b> and Hungarian heart disease datasets. However, most rows contain at least one cell where the outcomes of the test are not consistent. The row labeled \"consistent\" at the bottom of the table lists the number of datasets for which all", "mykey":2197},
 {"datasetID":149, "supportID":"D9C661038F2121C87654791F973D3AF20794887C", "rexaID":"56eb4e64499baa9ef620d2aeb5383e1739a2e6e3", "author":"Remco R. Bouckaert and Eibe Frank", "title":"Evaluating the Replicability of Significance Tests for Comparing Learning Algorithms", "venue":"PAKDD", "year":"2004", "window":"very sensitive to the particular partitioning of the anneal data. Looking at the column for naive Bayes vs. C4.5, this test could be used to justify the claim that the two perform the same for all datasets except the <b>vehicle</b> dataset just by choosing appropriate random number seeds. However, it could just as well be used to support the claim that the two algorithms perform differently in 19 out of 27", "mykey":2198},
 {"datasetID":23, "supportID":"D9D91517F4D4DD2CB655957E8750BE2AF402FDB3", "rexaID":"3fa8ab593f1fd58f33109aae497ca1fb67732c35", "author":"Marco Zaffalon and Marcus Hutter", "title":"Robust Feature Selection by Mutual Information Distributions", "venue":"CoRR, csAI/0206006", "year":"2002", "window":"The remaining cases are described by means of the following figures. Figure 2 shows that FF allowed the naive Bayes to significantly do better predictions than F for the greatest part of the <b>Chess</b> data set. The maximum di i erence in prediction accuracy is obtained at instance 422, where the accuracies are 0.889 and 0.832 for the cases FF and F, respectively. Figure 2 does not report the BF case,", "mykey":2199},
 {"datasetID":21, "supportID":"D9D91517F4D4DD2CB655957E8750BE2AF402FDB3", "rexaID":"3fa8ab593f1fd58f33109aae497ca1fb67732c35", "author":"Marco Zaffalon and Marcus Hutter", "title":"Robust Feature Selection by Mutual Information Distributions", "venue":"CoRR, csAI/0206006", "year":"2002", "window":"The remaining cases are described by means of the following figures. Figure 2 shows that FF allowed the naive Bayes to significantly do better predictions than F for the greatest part of the <b>Chess</b> data set. The maximum di i erence in prediction accuracy is obtained at instance 422, where the accuracies are 0.889 and 0.832 for the cases FF and F, respectively. Figure 2 does not report the BF case,", "mykey":2200},
 {"datasetID":22, "supportID":"D9D91517F4D4DD2CB655957E8750BE2AF402FDB3", "rexaID":"3fa8ab593f1fd58f33109aae497ca1fb67732c35", "author":"Marco Zaffalon and Marcus Hutter", "title":"Robust Feature Selection by Mutual Information Distributions", "venue":"CoRR, csAI/0206006", "year":"2002", "window":"The remaining cases are described by means of the following figures. Figure 2 shows that FF allowed the naive Bayes to significantly do better predictions than F for the greatest part of the <b>Chess</b> data set. The maximum di i erence in prediction accuracy is obtained at instance 422, where the accuracies are 0.889 and 0.832 for the cases FF and F, respectively. Figure 2 does not report the BF case,", "mykey":2201},
 {"datasetID":63, "supportID":"D9D91517F4D4DD2CB655957E8750BE2AF402FDB3", "rexaID":"3fa8ab593f1fd58f33109aae497ca1fb67732c35", "author":"Marco Zaffalon and Marcus Hutter", "title":"Robust Feature Selection by Mutual Information Distributions", "venue":"CoRR, csAI/0206006", "year":"2002", "window":"on a number of di erent domains. For example, Shuttle-small reports data on diagnosing failures of the space shuttle; <b>Lymphography</b> and Hypothyroid are medical data sets; Spam is a body of e-mails that can be spam or non-spam; etc. The data sets presenting non-nominal features have been pre-discretized by MLC++ [Kohavi et al., 1994], default options. This step may", "mykey":2202},
 {"datasetID":14, "supportID":"D9F0E1C7E240597992232840F7CB96CEEEFA1940", "rexaID":"a6fa92d0caaa949b6ebd6531b2e6d4e7e0f03387", "author":"David M J Tax and Robert P W Duin", "title":"Support vector domain description", "venue":"Pattern Recognition Letters, 20", "year":"1999", "window":"be understood by looking at the distribution of the data, where class 2 is between classes 1 and 3. Only the instability method is able to reject objects from the second class. In the <b>breast</b> <b>cancer</b> data set, the second class is clearly easier to distinguish than the \u00aerst class. Looking at the origin of the data, this means that by describing the benign class, the malignant class can be rejected quite", "mykey":2203},
 {"datasetID":52, "supportID":"D9F0E1C7E240597992232840F7CB96CEEEFA1940", "rexaID":"a6fa92d0caaa949b6ebd6531b2e6d4e7e0f03387", "author":"David M J Tax and Robert P W Duin", "title":"Support vector domain description", "venue":"Pattern Recognition Letters, 20", "year":"1999", "window":"classi\u00aeers on the remaining class. Also visible is that the Parzen method overtrains heavily and performs poorly when class 1 is the outlier class. The SVDD performs best overall. In the <b>ionosphere</b> dataset, the Parzen density estimation again overtrains and the instability method cannot be used because only two classes are available. From the results we see that class 1 is almost Gaussian distributed", "mykey":2204},
 {"datasetID":53, "supportID":"D9F0E1C7E240597992232840F7CB96CEEEFA1940", "rexaID":"a6fa92d0caaa949b6ebd6531b2e6d4e7e0f03387", "author":"David M J Tax and Robert P W Duin", "title":"Support vector domain description", "venue":"Pattern Recognition Letters, 20", "year":"1999", "window":"almost Gaussian distributed and class 2 is scattered around it. The SVDD cannot distinguish one class 2 object from class 1. Finally, the performance of the outlier methods are applied on the <b>iris</b> dataset. Here, all methods work reasonably well, which indicates that the data distributions of the classes are well clustered. Only the Parzen density estimation slightly overtrains. From these results we", "mykey":2205},
 {"datasetID":48, "supportID":"DA10D6267911AD40DA9891992870DCD8D8012F41", "rexaID":"9628f95b0cc07019875b0659d10aab8e2904bfca", "author":"Jianping Wu and Zhi-Hua Zhou and Cheng-The Chen", "title":"Ensemble of GA based Selective Neural Network Ensembles", "venue":"National Laboratory for Novel Software Technology Nanjing University", "year":"", "window":"average mean squared error and corresponding standard deviation is also recorded. Experimental results are shown in Table 1. Statistical tests show that on the Friedman#1, Boston <b>Housing</b>  and Ozone data sets, GASEN's generalization error is significantly lower than that of the simple ensemble method, and e-GASEN attains still lower generalization errors than GASEN. On the Servo data set, GASEN is", "mykey":2206},
 {"datasetID":87, "supportID":"DA10D6267911AD40DA9891992870DCD8D8012F41", "rexaID":"9628f95b0cc07019875b0659d10aab8e2904bfca", "author":"Jianping Wu and Zhi-Hua Zhou and Cheng-The Chen", "title":"Ensemble of GA based Selective Neural Network Ensembles", "venue":"National Laboratory for Novel Software Technology Nanjing University", "year":"", "window":"GASEN's generalization error is significantly lower than that of the simple ensemble method, and e-GASEN attains still lower generalization errors than GASEN. On the <b>Servo</b> data set, GASEN is slightly inferior to simple ensemble. The e-GASEN method's performance, however, has no significant difference with that of the simple ensemble method. From the aforementioned statistics", "mykey":2207},
 {"datasetID":42, "supportID":"DA81B9EE585961702840898F80FC6E592A60CF63", "rexaID":"27e33344198975ea1b20e02c8f0fce01cd29f6e5", "author":"Eibe Frank and Ian H. Witten", "title":"Generating Accurate Rule Sets Without Global Optimization", "venue":"ICML", "year":"1998", "window":"listed in Table 2. They give the percentage of correct classifications, averaged over ten ten-fold cross-validation runs, and standard 3 Following Holte (Holte, 1993), the G2 variant of the <b>glass</b> dataset has classes 1 and 3 combined and classes 4 to 7 deleted, and the horse-colic dataset has attributes 3, 25, 26, 27, 28 deleted with attribute 24 being used as the class. We also deleted all", "mykey":2208},
 {"datasetID":47, "supportID":"DA81B9EE585961702840898F80FC6E592A60CF63", "rexaID":"27e33344198975ea1b20e02c8f0fce01cd29f6e5", "author":"Eibe Frank and Ian H. Witten", "title":"Generating Accurate Rule Sets Without Global Optimization", "venue":"ICML", "year":"1998", "window":"has classes 1 and 3 combined and classes 4 to 7 deleted, and the <b>horse</b> <b>colic</b> dataset has attributes 3, 25, 26, 27, 28 deleted with attribute 24 being used as the class. We also deleted all identifier attributes from the datasets. 4 We used Revision 8 of C4.5. Table 2: Experimental", "mykey":2209},
 {"datasetID":1, "supportID":"DADF30A5DFEF6884474A042F87A34277E06C174B", "rexaID":"f7fdf9dbb5f98a218956025550c1f603b3cb24f2", "author":"Alexander G. Gray and Bernd Fischer and Johann Schumann and Wray L. Buntine", "title":"Automatic Derivation of Statistical Algorithms: The EM Family and Beyond", "venue":"NIPS", "year":"2002", "window":"extension of our running example, integrating several features, yields a Gaussian Bayes classifier model G 2 . G 2 has been successfully tested on various standard benchmarks [1], e.g., the <b>Abalone</b> dataset. Currently, the number of expected classes has to be given in advance. Mixture models and EM. A wide range of k-Gaussian mixture models can be handled by AUTOBAYES, ranging from the simple 1D (M 1 )", "mykey":2210},
 {"datasetID":3, "supportID":"DB39B28B323D912371513971412DF81945EFAEE5", "rexaID":"ae5c213089093a76c538a7d16076fa8bb7eacada", "author":"Pedro Domingos", "title":"Knowledge Discovery Via Multiple Models", "venue":"Intell. Data Anal, 2", "year":"1998", "window":"between the learner's bias and that of the probability estimation procedure is important for good results. Disabling the generation of missing values had a large negative impact in the <b>annealing</b> dataset, where very large numbers of missing values are present, and a less discernible one in the datasets where fewer such values occur. C4.5RULES's pruning parameters during the meta-learning phase can", "mykey":2211},
 {"datasetID":58, "supportID":"DB39B28B323D912371513971412DF81945EFAEE5", "rexaID":"ae5c213089093a76c538a7d16076fa8bb7eacada", "author":"Pedro Domingos", "title":"Knowledge Discovery Via Multiple Models", "venue":"Intell. Data Anal, 2", "year":"1998", "window":"It may thus be possible to substantially optimize CMM's complexity without seriously affecting accuracy by choosing 7 Table 2: Empirical results: average accuracies and their standard deviations. Dataset CMM Bagging Single <b>Lenses</b> 75.0Sigma6.8 75.0Sigma6.8 62.5Sigma7.1 Lung cancer 40.0Sigma7.5 36.7Sigma7.2 31.7Sigma7.0 Soybean (small) 97.0Sigma1.6 97.0Sigma1.6 98.0Sigma1.4 Labor", "mykey":2212},
 {"datasetID":44, "supportID":"DBC56116688939A476F8673581B518DD371AE834", "rexaID":"20b8d5b9285a402f14753a766e96889e990a99f3", "author":"Bob Ricks and Dan Ventura", "title":"Training a Quantum Neural Network", "venue":"NIPS", "year":"2003", "window":"an epoch refers to finding and fixing the weight of a single node. We also tried the randomized search algorithm for a few real-world machine learning problems: lenses, <b>Hayes</b> <b>Roth</b> and the iris datasets [19]. The lenses data set is a data set that tries to predict whether people will need soft contact lenses, hard contact lenses or no contacts. The iris dataset details features of three different", "mykey":2213},
 {"datasetID":53, "supportID":"DBC56116688939A476F8673581B518DD371AE834", "rexaID":"20b8d5b9285a402f14753a766e96889e990a99f3", "author":"Bob Ricks and Dan Ventura", "title":"Training a Quantum Neural Network", "venue":"NIPS", "year":"2003", "window":"an epoch refers to finding and fixing the weight of a single node. We also tried the randomized search algorithm for a few real-world machine learning problems: lenses, Hayes-Roth and the <b>iris</b> datasets [19]. The lenses data set is a data set that tries to predict whether people will need soft contact lenses, hard contact lenses or no contacts. The iris dataset details features of three different", "mykey":2214},
 {"datasetID":58, "supportID":"DBC56116688939A476F8673581B518DD371AE834", "rexaID":"20b8d5b9285a402f14753a766e96889e990a99f3", "author":"Bob Ricks and Dan Ventura", "title":"Training a Quantum Neural Network", "venue":"NIPS", "year":"2003", "window":"an epoch refers to finding and fixing the weight of a single node. We also tried the randomized search algorithm for a few real-world machine learning problems: <b>lenses</b>  Hayes-Roth and the iris datasets [19]. The lenses data set is a data set that tries to predict whether people will need soft contact lenses, hard contact lenses or no contacts. The iris dataset details features of three different", "mykey":2215},
 {"datasetID":1, "supportID":"DC3BF4B0E30710A147F98EB44B9E7874DB19AA39", "rexaID":"7030f724add30a4dd1b1f84109e72675c7e629f6", "author":"Bernhard Pfahringer and Hilan Bensusan and Christophe G. Giraud-Carrier", "title":"Meta-Learning by Landmarking Various Learning Algorithms", "venue":"ICML", "year":"2000", "window":"had from 5 to 12 attributes and were classified by simple parity, DNF and CNF rules as well as at random. The 18 Uci data sets were: mushrooms, <b>abalone</b>  crx, sat, acetylation, titanic, waveform, yeast, car, chess(king-rook-vs-king), led7, led24, tic-tac-toe, monk1, monk2, monk3, satimage, quisclas. The performance of every", "mykey":2216},
 {"datasetID":45, "supportID":"DC8427293AC77A0A77ABDBF0D9DDD0940B336EED", "rexaID":"77b535b98a279e3b1ee9499bead3408bc8d58c08", "author":"Glenn Fung and Sathyakama Sandilya and R. Bharat Rao", "title":"Rule extraction from Linear Support Vector Machines", "venue":"Computer-Aided Diagnosis & Therapy, Siemens Medical Solutions, Inc", "year":"", "window":"from the UCI Machine Learning Repository [13]: Wisconsin Diagnosis Breast Cancer (WDBC), Ionosphere, and Cleveland <b>heart</b>  The fourth dataset is a dataset related to the nontraditional authorship attribution problem related to the federalist papers [7] and the fifth dataset is a dataset used for training in a computer aided detection", "mykey":2217},
 {"datasetID":52, "supportID":"DC8427293AC77A0A77ABDBF0D9DDD0940B336EED", "rexaID":"77b535b98a279e3b1ee9499bead3408bc8d58c08", "author":"Glenn Fung and Sathyakama Sandilya and R. Bharat Rao", "title":"Rule extraction from Linear Support Vector Machines", "venue":"Computer-Aided Diagnosis & Therapy, Siemens Medical Solutions, Inc", "year":"", "window":"from the UCI Machine Learning Repository [13]: Wisconsin Diagnosis Breast Cancer (WDBC), <b>Ionosphere</b>  and Cleveland heart. The fourth dataset is a dataset related to the nontraditional authorship attribution problem related to the federalist papers [7] and the fifth dataset is a dataset used for training in a computer aided detection", "mykey":2218},
 {"datasetID":62, "supportID":"DC8427293AC77A0A77ABDBF0D9DDD0940B336EED", "rexaID":"77b535b98a279e3b1ee9499bead3408bc8d58c08", "author":"Glenn Fung and Sathyakama Sandilya and R. Bharat Rao", "title":"Rule extraction from Linear Support Vector Machines", "venue":"Computer-Aided Diagnosis & Therapy, Siemens Medical Solutions, Inc", "year":"", "window":"including a medical dataset on detection of <b>lung cancer</b> from medical images. The ability to convert SVM's and other \"black-box\" classifiers into a set of human-understandable rules, is critical not only for physician", "mykey":2219},
 {"datasetID":82, "supportID":"DC8427293AC77A0A77ABDBF0D9DDD0940B336EED", "rexaID":"77b535b98a279e3b1ee9499bead3408bc8d58c08", "author":"Glenn Fung and Sathyakama Sandilya and R. Bharat Rao", "title":"Rule extraction from Linear Support Vector Machines", "venue":"Computer-Aided Diagnosis & Therapy, Siemens Medical Solutions, Inc", "year":"", "window":"The first experiment relates to the publicly available WDBC dataset that consists of 683 <b>patient data</b>  The classification task associated with this dataset is to diagnose breast masses based solely on a Fine Needle Aspiration (FNA). Doctors identified nine visually", "mykey":2220},
 {"datasetID":52, "supportID":"DD4AF8A5192BCEACA9F247370C6ABA0D65ED0365", "rexaID":"b1a2a0423ea7baac76c75e758f784505eb8fc18f", "author":"Jennifer G. Dy and Carla Brodley", "title":"Feature Subset Selection and Order Identification for Unsupervised Learning", "venue":"ICML", "year":"2000", "window":"To illustrate FSSEM on real data, we present results for two data sets: <b>ionosphere</b> (Blake & Merz, 1998) and a high resolution computed tomography images of the lungs (HRCT-lung) data set (Dy et al., 1999). See Dy (1999) for experiments on additional data sets.", "mykey":2221},
 {"datasetID":34, "supportID":"DDD93D7A7F8CD0946343DBE84F5D888E168628DD", "rexaID":"2b9850dda87431cc9117f2a0a63d79863723e4b0", "author":"Krzysztof Grabczewski and Wl/odzisl/aw Duch", "title":"THE SEPARABILITY OF SPLIT VALUE CRITERION", "venue":"Department of Computer Methods, Nicolaus Copernicus University", "year":"", "window":"in form of logical rules. Method Accuracy % Reference Logdisc 77.7 Statlog SSV Tree 74.8 this paper CART 74.5 Stalog C4.5 73.0 Stalog Default 65.1 Table 1: Crossvalidation results for <b>diabetes</b> dataset Method Accuracy % Reference 3-NN 96.7 Karol Grudzi \u00b4 nski (our group) MLP+BP 96.0 Sigillito [7] C4.5 94.9 Hamilton [8] FSM 92.8 Rafal/ Adamczak (our group) [9] SSV Tree 92.0 this paper DB-CART 91.3", "mykey":2222},
 {"datasetID":52, "supportID":"DDD93D7A7F8CD0946343DBE84F5D888E168628DD", "rexaID":"2b9850dda87431cc9117f2a0a63d79863723e4b0", "author":"Krzysztof Grabczewski and Wl/odzisl/aw Duch", "title":"THE SEPARABILITY OF SPLIT VALUE CRITERION", "venue":"Department of Computer Methods, Nicolaus Copernicus University", "year":"", "window":"96.0 Sigillito [7] C4.5 94.9 Hamilton [8] FSM 92.8 Rafal/ Adamczak (our group) [9] SSV Tree 92.0 this paper DB-CART 91.3 Shang, Breiman [10] CART 88.9 Shang, Breiman [10] Table 2: Test <b>ionosphere</b> dataset results 5.2 Ionosphere The ionosphere dataset has 200 vectors in the training set and 150 in the test set. Each data vector is described by 34 continuous attributes and belongs to one of two", "mykey":2223},
 {"datasetID":143, "supportID":"DDD93D7A7F8CD0946343DBE84F5D888E168628DD", "rexaID":"2b9850dda87431cc9117f2a0a63d79863723e4b0", "author":"Krzysztof Grabczewski and Wl/odzisl/aw Duch", "title":"THE SEPARABILITY OF SPLIT VALUE CRITERION", "venue":"Department of Computer Methods, Nicolaus Copernicus University", "year":"", "window":"analyzed in the Stalog project [6]. Results of the C4.5 decision tree are already significantly worse. 5.4 Statlog <b>Australian credit</b> data This dataset contains 690 cases classified in 2 classes (+ and -). Data vectors are described by 14 attributes (6 continuous and 8 discrete). In the Table 4 a comparison of 10 fold crossvalidation results for", "mykey":2224},
 {"datasetID":148, "supportID":"DDD93D7A7F8CD0946343DBE84F5D888E168628DD", "rexaID":"2b9850dda87431cc9117f2a0a63d79863723e4b0", "author":"Krzysztof Grabczewski and Wl/odzisl/aw Duch", "title":"THE SEPARABILITY OF SPLIT VALUE CRITERION", "venue":"Department of Computer Methods, Nicolaus Copernicus University", "year":"", "window":"[6] k-NN -- 99.56 [6] RBF 98.40 98.60 [6] MLP+BP 95.50 96.57 [6] Logistic discrimination 96.07 96.17 [6] Linear discrimination 95.02 95.17 [6] Table 3: Comparison of results for the NASA <b>Shuttle</b> dataset. Results for different systems are compared in the table ??. SSV results are much better than those obtained from the MLP or RBF networks (as reported in the Stalog project [?]) and comparable with", "mykey":2225},
 {"datasetID":98, "supportID":"DDD93D7A7F8CD0946343DBE84F5D888E168628DD", "rexaID":"2b9850dda87431cc9117f2a0a63d79863723e4b0", "author":"Krzysztof Grabczewski and Wl/odzisl/aw Duch", "title":"THE SEPARABILITY OF SPLIT VALUE CRITERION", "venue":"Department of Computer Methods, Nicolaus Copernicus University", "year":"", "window":"analyzed in the Stalog project [6]. Results of the C4.5 decision tree are already significantly worse. 5.4 <b>Statlog</b> Australian credit data This dataset contains 690 cases classified in 2 classes (+ and -). Data vectors are described by 14 attributes (6 continuous and 8 discrete). In the Table 4 a comparison of 10 fold crossvalidation results for", "mykey":2226},
 {"datasetID":144, "supportID":"DDE3924C1BF35AB7FC25A1172F08C3089C0CC5A7", "rexaID":"7be023799304644bda7d6afa9f31043a1f31511c", "author":"Paul O' Dea and David Griffith and Colm O' Riordan", "title":"DEPARTMENT OF INFORMATION TECHNOLOGY", "venue":"P. O'Dea (NUI", "year":"", "window":"network (as described earlier), using back-propagation as a learning algorithm, is used to classify the tuples t 1 : : : t n based on the attributes s 1 : : : s n . 5 Results 5.1 The <b>German Credit</b> Data Set In order to facilitate testing of the developed approach, experiments were conducted using the german credit data set 1 . The german credit data set contains information on 1000 loan applicants.", "mykey":2227},
 {"datasetID":46, "supportID":"DE99E3F4B2CD2C7D6006F5FBBD32B5589D2C0989", "rexaID":"d891d25459c4422e9cf2262272938d1ada438a9e", "author":"Xiaoli Z. Fern and Carla Brodley", "title":"Boosting Lazy Decision Trees", "venue":"ICML", "year":"2003", "window":"but behave less consistently. For three data sets, <b>Hepatitis</b>  Lympho and Monk2, bagging significantly degrades the performance of the base learner. This is possibly caused by the sub-sampling procedure used by bagging to generate different", "mykey":2228},
 {"datasetID":2, "supportID":"DEA2FFAF3A1031738792A5567EE13717406348EF", "rexaID":"fc0a66d3a7336b6eabad919a7389c68cc37f2564", "author":"Ayhan Demiriz and Kristin P. Bennett and John Shawe and I. Nouretdinov V.", "title":"Linear Programming Boosting via Column Generation", "venue":"Dept. of Decision Sciences and Eng. Systems, Rensselaer Polytechnic Institute", "year":"", "window":"function based on the margin obtained by each point. This is just one method of boosting multiclass problems. Further investigation of multiclass approaches is needed. We ran experiments on larger datasets: Forest, <b>Adult</b>  USPS, and Optdigits from UCI[10]. LPBoost was adopted to the multiclass problem by defining h j (x i ) = 1 if instance x i is correctly classified by weak learner h j and -1", "mykey":2229},
 {"datasetID":14, "supportID":"DEA2FFAF3A1031738792A5567EE13717406348EF", "rexaID":"fc0a66d3a7336b6eabad919a7389c68cc37f2564", "author":"Ayhan Demiriz and Kristin P. Bennett and John Shawe and I. Nouretdinov V.", "title":"Linear Programming Boosting via Column Generation", "venue":"Dept. of Decision Sciences and Eng. Systems, Rensselaer Polytechnic Institute", "year":"", "window":"LPBoost has a well-defined stopping criterion that is reached in a few iterations. It uses few weak learners. There are only 81 possible stumps on the <b>Breast</b> <b>Cancer</b> dataset (nine attributes having nine possible values), so clearly AdaBoost may require the same tree to be generated multiple times. LPBoost generates a weak learner only once and can alter the weight on", "mykey":2230},
 {"datasetID":45, "supportID":"DEA2FFAF3A1031738792A5567EE13717406348EF", "rexaID":"fc0a66d3a7336b6eabad919a7389c68cc37f2564", "author":"Ayhan Demiriz and Kristin P. Bennett and John Shawe and I. Nouretdinov V.", "title":"Linear Programming Boosting via Column Generation", "venue":"Dept. of Decision Sciences and Eng. Systems, Rensselaer Polytechnic Institute", "year":"", "window":"data repository [10]. For the C4.5 experiments we performed both traditional and confidence7.1 Boosting Decision Tree Stumps We used decision tree stumps as a base learner on the following six datasets: Cancer (9,699), Diagnostic (30,569), <b>Heart</b> (13,297), Ionosphere (34,351), Musk (166,476), and Sonar (60,208). The number of features 11 and number of points in each dataset are shown,", "mykey":2231},
 {"datasetID":48, "supportID":"DEA2FFAF3A1031738792A5567EE13717406348EF", "rexaID":"fc0a66d3a7336b6eabad919a7389c68cc37f2564", "author":"Ayhan Demiriz and Kristin P. Bennett and John Shawe and I. Nouretdinov V.", "title":"Linear Programming Boosting via Column Generation", "venue":"Dept. of Decision Sciences and Eng. Systems, Rensselaer Polytechnic Institute", "year":"", "window":"used in decision tree stumps experiments, we use four additional UCI datasets here. These are the House(16,435), <b>Housing</b> 13,506) 2 , Pima(8,768), and Spam(57,4601) datasets. As in the decision tree stumps experiments, we report results from 10-fold CV. Since the best # value", "mykey":2232},
 {"datasetID":80, "supportID":"DEA2FFAF3A1031738792A5567EE13717406348EF", "rexaID":"fc0a66d3a7336b6eabad919a7389c68cc37f2564", "author":"Ayhan Demiriz and Kristin P. Bennett and John Shawe and I. Nouretdinov V.", "title":"Linear Programming Boosting via Column Generation", "venue":"Dept. of Decision Sciences and Eng. Systems, Rensselaer Polytechnic Institute", "year":"", "window":"with missing values. The default handling in C4.5 has been used for missing values. USPS and Optdigits are <b>optical</b> character <b>recognition</b> datasets. USPS has 256 dimensions without missing value. Out of 7291 original training points, we use 1822 points as training data and the rest 5469 as validation data. There are 2007 test points. Optdigits", "mykey":2233},
 {"datasetID":53, "supportID":"DEC553E103846737C6522F67D94A9AF16050ED29", "rexaID":"7d87efd12b445262eff46191ede868722a2be569", "author":"Geoffrey Holmes and Leonard E. Trigg", "title":"A Diagnostic Tool for Tree Based Supervised Classification Learning Algorithms", "venue":"Department of Computer Science University of Waikato Hamilton New Zealand", "year":"", "window":"difference by the range of the tested attribute, giving the formula: cost v v a a = | - | max - min 1 2 1 1 Figure 2 illustrates the problem for case 4 with an example taken from the familiar <b>iris</b> dataset. The minimum cost edit sequence to transform the tree on the left involves deleting the non-root Petal width nodes and their rightmost leaf nodes (giving a cost of 4). We are left with two trees", "mykey":2234},
 {"datasetID":119, "supportID":"DF2388A179AF680C50C049F507A468B16452F3F1", "rexaID":"208ec47695794498051a25cd425bc385a9d19602", "author":"Thomas T. Osugi and M. S", "title":"EXPLORATION-BASED ACTIVE MACHINE LEARNING", "venue":"Faculty of The Graduate College at the University of Nebraska In Partial Fulfillment of Requirements", "year":"", "window":"Characteristics . . . . . . . . . . . . . . 48 A.4 <b>Corel</b> Dataset Characteristics . . . . . . . . . . . . . . . . . . . . . . 48 A.5 XOR Checkerboard d = 2, n = 2 Dataset Characteristics . . . . . . . 49 A.6 XOR Checkerboard d = 2, n = 3 Dataset Characteristics .", "mykey":2235},
 {"datasetID":50, "supportID":"DF2388A179AF680C50C049F507A468B16452F3F1", "rexaID":"208ec47695794498051a25cd425bc385a9d19602", "author":"Thomas T. Osugi and M. S", "title":"EXPLORATION-BASED ACTIVE MACHINE LEARNING", "venue":"Faculty of The Graduate College at the University of Nebraska In Partial Fulfillment of Requirements", "year":"", "window":"Characteristics . . . . . . . . . . . . . . . . . . . . . . . 47 A.3 <b>Image Segmentation</b> Dataset Characteristics . . . . . . . . . . . . . . 48 A.4 Corel Dataset Characteristics . . . . . . . . . . . . . . . . . . . . . . 48 A.5 XOR Checkerboard d = 2, n = 2 Dataset Characteristics . . . . . .", "mykey":2236},
 {"datasetID":137, "supportID":"DF2388A179AF680C50C049F507A468B16452F3F1", "rexaID":"208ec47695794498051a25cd425bc385a9d19602", "author":"Thomas T. Osugi and M. S", "title":"EXPLORATION-BASED ACTIVE MACHINE LEARNING", "venue":"Faculty of The Graduate College at the University of Nebraska In Partial Fulfillment of Requirements", "year":"", "window":"of e-mail, a user can label good and bad examples. The user won't have much patience in this training process, and AL can minimize the amount of input needed. In experiments, for the <b>Reuters</b> 21578 dataset, only 10% of 6 unlabeled examples had to be labeled in order to get the same accuracy as the entire labeled pool. For image classification, Luo et al. [16] use active learning to recognize di\u00aeerent", "mykey":2237},
 {"datasetID":147, "supportID":"DF2388A179AF680C50C049F507A468B16452F3F1", "rexaID":"208ec47695794498051a25cd425bc385a9d19602", "author":"Thomas T. Osugi and M. S", "title":"EXPLORATION-BASED ACTIVE MACHINE LEARNING", "venue":"Faculty of The Graduate College at the University of Nebraska In Partial Fulfillment of Requirements", "year":"", "window":"Characteristics . . . . . . . . . . . . . . . . . . . . . . . 47 A.3 <b>Image Segmentation</b> Dataset Characteristics . . . . . . . . . . . . . . 48 A.4 Corel Dataset Characteristics . . . . . . . . . . . . . . . . . . . . . . 48 A.5 XOR Checkerboard d = 2, n = 2 Dataset Characteristics . . . . . .", "mykey":2238},
 {"datasetID":107, "supportID":"DF2388A179AF680C50C049F507A468B16452F3F1", "rexaID":"208ec47695794498051a25cd425bc385a9d19602", "author":"Thomas T. Osugi and M. S", "title":"EXPLORATION-BASED ACTIVE MACHINE LEARNING", "venue":"Faculty of The Graduate College at the University of Nebraska In Partial Fulfillment of Requirements", "year":"", "window":". . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 5.2 Artificial Exploration Benchmark . . . . . . . . . . . . . . . . . . . . 30 6 Conclusions and Future Work 40 Bibliography 43 iii A Dataset Descriptions 46 A.1 <b>Waveform</b> . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 A.2 SAT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 A.3 Image", "mykey":2239},
 {"datasetID":108, "supportID":"DF2388A179AF680C50C049F507A468B16452F3F1", "rexaID":"208ec47695794498051a25cd425bc385a9d19602", "author":"Thomas T. Osugi and M. S", "title":"EXPLORATION-BASED ACTIVE MACHINE LEARNING", "venue":"Faculty of The Graduate College at the University of Nebraska In Partial Fulfillment of Requirements", "year":"", "window":". . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 5.2 Artificial Exploration Benchmark . . . . . . . . . . . . . . . . . . . . 30 6 Conclusions and Future Work 40 Bibliography 43 iii A Dataset Descriptions 46 A.1 <b>Waveform</b> . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 A.2 SAT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 A.3 Image", "mykey":2240},
 {"datasetID":92, "supportID":"DF39CAD62F3C6476AB4E5A1684FC415E44F1F0C6", "rexaID":"966b8860357db1f685782971093ef9ae2e5386c3", "author":"Pedro Domingos", "title":"Linear-Time Rule Induction", "venue":"KDD", "year":"1996", "window":"level of C4.5RULES's. In this paper we present CWS, a new algorithm with guaranteed O(e) complexity, and verify that it outperforms C4.5RULES and CN2 in time, accuracy and output size on two large datasets. For example, on NASA's <b>space</b> <b>shuttle</b> database, running time is reduced from over a month (for C4.5RULES) to a few hours, with a slight gain in accuracy. CWS is based on interleaving the induction", "mykey":2241},
 {"datasetID":148, "supportID":"DF39CAD62F3C6476AB4E5A1684FC415E44F1F0C6", "rexaID":"966b8860357db1f685782971093ef9ae2e5386c3", "author":"Pedro Domingos", "title":"Linear-Time Rule Induction", "venue":"KDD", "year":"1996", "window":"level of C4.5RULES's. In this paper we present CWS, a new algorithm with guaranteed O(e) complexity, and verify that it outperforms C4.5RULES and CN2 in time, accuracy and output size on two large datasets. For example, on NASA's space <b>shuttle</b> database, running time is reduced from over a month (for C4.5RULES) to a few hours, with a slight gain in accuracy. CWS is based on interleaving the induction", "mykey":2242},
 {"datasetID":45, "supportID":"DF5EBFE512AA205EB0BA031348E95E99A27B0E42", "rexaID":"56eb4e64499baa9ef620d2aeb5383e1739a2e6e3", "author":"Remco R. Bouckaert and Eibe Frank", "title":"Evaluating the Replicability of Significance Tests for Comparing Learning Algorithms", "venue":"PAKDD", "year":"2004", "window":"perform differently in 19 out of 27 cases. For some rows, the test consistently indicates no difference between any two of the three schemes, in particular for the iris and Hungarian <b>heart</b> disease datasets. However, most rows contain at least one cell where the outcomes of the test are not consistent. The row labeled \"consistent\" at the bottom of the table lists the number of datasets for which all", "mykey":2243},
 {"datasetID":53, "supportID":"DF5EBFE512AA205EB0BA031348E95E99A27B0E42", "rexaID":"56eb4e64499baa9ef620d2aeb5383e1739a2e6e3", "author":"Remco R. Bouckaert and Eibe Frank", "title":"Evaluating the Replicability of Significance Tests for Comparing Learning Algorithms", "venue":"PAKDD", "year":"2004", "window":"perform differently in 19 out of 27 cases. For some rows, the test consistently indicates no difference between any two of the three schemes, in particular for the <b>iris</b> and Hungarian heart disease datasets. However, most rows contain at least one cell where the outcomes of the test are not consistent. The row labeled \"consistent\" at the bottom of the table lists the number of datasets for which all", "mykey":2244},
 {"datasetID":149, "supportID":"DF5EBFE512AA205EB0BA031348E95E99A27B0E42", "rexaID":"56eb4e64499baa9ef620d2aeb5383e1739a2e6e3", "author":"Remco R. Bouckaert and Eibe Frank", "title":"Evaluating the Replicability of Significance Tests for Comparing Learning Algorithms", "venue":"PAKDD", "year":"2004", "window":"very sensitive to the particular partitioning of the anneal data. Looking at the column for naive Bayes vs. C4.5, this test could be used to justify the claim that the two perform the same for all datasets except the <b>vehicle</b> dataset just by choosing appropriate random number seeds. However, it could just as well be used to support the claim that the two algorithms perform differently in 19 out of 27", "mykey":2245},
 {"datasetID":34, "supportID":"DFC1E148B1D08500954F5C78F0A1001AFFBFC041", "rexaID":"1c99ea7ad3864a966f968d02da03b3cb94c64f03", "author":"Lena Kallin", "title":"Receiver operating characteristic (ROC) analysis Evaluating discriminance effects among decision support systems", "venue":"Contents 1 The Theory of Receiver Operating Characteristic Curves 5", "year":"", "window":"function, and age (years). Our data consists of 375 non <b>Diabetes</b> and 201 Diabetes cases used in the learning phase, and, respectively, 125 non-Diabetes and 67 Diabetes cases in the testing phase. A data set where all missing data are set to 0.5 will be used, see [Eklund and Kallin Westin, 2002] for details about the data set and its missing data. The first data set is special in the sense that the test", "mykey":2246},
 {"datasetID":1, "supportID":"DFE056C98ADE4D3A6B9054C770484C479ED189BF", "rexaID":"bc80295973a43d3806ff4dfe83e5724260301c33", "author":"Iztok Savnik and Peter A. Flach", "title":"Discovery of multivalued dependencies from relations", "venue":"Intell. Data Anal, 4", "year":"2000", "window":"which were used in the experiments are available at UCI Machine learning repository [10]. In the case of the datasets Car, Bupa and <b>Abalone</b> we use randomly selected subsets of the original datasets. For each experiment we specify the name of the relation (dataset) r(R), the number of tuples in relation jrj, the", "mykey":2247},
 {"datasetID":19, "supportID":"DFE056C98ADE4D3A6B9054C770484C479ED189BF", "rexaID":"bc80295973a43d3806ff4dfe83e5724260301c33", "author":"Iztok Savnik and Peter A. Flach", "title":"Discovery of multivalued dependencies from relations", "venue":"Intell. Data Anal, 4", "year":"2000", "window":"which were used in the experiments are available at UCI Machine learning repository [10]. In the case of the datasets <b>Car</b>  Bupa and Abalone we use randomly selected subsets of the original datasets. For each experiment we specify the name of the relation (dataset) r(R), the number of tuples in relation jrj, the", "mykey":2248},
 {"datasetID":7, "supportID":"E006C4EADD4929053891FBDC1979D4C774DE3FEB", "rexaID":"692880d7b3356df64bfa0f06a683f89e4ce6955b", "author":"Vassilis Athitsos and Stan Sclaroff", "title":"Boosting Nearest Neighbor Classifiers for Multiclass Recognition", "venue":"Boston University Computer Science Tech. Report No, 2004-006", "year":"2004", "window":"We did not use four datasets (dermatology, soybean, thyroid, <b>audiology</b>  because they have missing attributes, which our current formulation cannot handle. One dataset (ecoli) contains a nominal attribute, which our current", "mykey":2249},
 {"datasetID":8, "supportID":"E006C4EADD4929053891FBDC1979D4C774DE3FEB", "rexaID":"692880d7b3356df64bfa0f06a683f89e4ce6955b", "author":"Vassilis Athitsos and Stan Sclaroff", "title":"Boosting Nearest Neighbor Classifiers for Multiclass Recognition", "venue":"Boston University Computer Science Tech. Report No, 2004-006", "year":"2004", "window":"We did not use four datasets (dermatology, soybean, thyroid, <b>audiology</b>  because they have missing attributes, which our current formulation cannot handle. One dataset (ecoli) contains a nominal attribute, which our current", "mykey":2250},
 {"datasetID":33, "supportID":"E006C4EADD4929053891FBDC1979D4C774DE3FEB", "rexaID":"692880d7b3356df64bfa0f06a683f89e4ce6955b", "author":"Vassilis Athitsos and Stan Sclaroff", "title":"Boosting Nearest Neighbor Classifiers for Multiclass Recognition", "venue":"Boston University Computer Science Tech. Report No, 2004-006", "year":"2004", "window":"We ended up using only eight of those datasets. We did not use four datasets  <b>dermatology</b>  soybean, thyroid, audiology) because they have missing attributes, which our current formulation cannot handle. One dataset (ecoli) contains a nominal", "mykey":2251},
 {"datasetID":39, "supportID":"E006C4EADD4929053891FBDC1979D4C774DE3FEB", "rexaID":"692880d7b3356df64bfa0f06a683f89e4ce6955b", "author":"Vassilis Athitsos and Stan Sclaroff", "title":"Boosting Nearest Neighbor Classifiers for Multiclass Recognition", "venue":"Boston University Computer Science Tech. Report No, 2004-006", "year":"2004", "window":"(dermatology, soybean, thyroid, audiology) because they have missing attributes, which our current formulation cannot handle. One dataset  <b>ecoli</b>  contains a nominal attribute, which our current implementation cannot handle in practice; this Table 3. For each dataset, we count how many variations of AdaBoost.MO gave lower (<), equal", "mykey":2252},
 {"datasetID":42, "supportID":"E006C4EADD4929053891FBDC1979D4C774DE3FEB", "rexaID":"692880d7b3356df64bfa0f06a683f89e4ce6955b", "author":"Vassilis Athitsos and Stan Sclaroff", "title":"Boosting Nearest Neighbor Classifiers for Multiclass Recognition", "venue":"Boston University Computer Science Tech. Report No, 2004-006", "year":"2004", "window":"used in the experiments, largely copied from (Allwein et al., 2000). Dataset Train Test Attributes Classes <b>glass</b> 214 - 9 6 isolet 6238 1559 617 26 letter 16000 4000 16 26 pendigits 7494 3498 16 10 satimage 4435 2000 36 6 segmentation 2310 - 19 7 vowel 528 462 10 11 yeast", "mykey":2253},
 {"datasetID":54, "supportID":"E006C4EADD4929053891FBDC1979D4C774DE3FEB", "rexaID":"692880d7b3356df64bfa0f06a683f89e4ce6955b", "author":"Vassilis Athitsos and Stan Sclaroff", "title":"Boosting Nearest Neighbor Classifiers for Multiclass Recognition", "venue":"Boston University Computer Science Tech. Report No, 2004-006", "year":"2004", "window":"et al. (2000) and the best result attained among the 6 variations of \naive\" k-nn classification. For our method we also provide the standard deviation across multiple trials, except for the <b>isolet</b> dataset where we only ran one trial of our algorithm. Dataset Boost-NN Allwein Naive k-nn glass 24.4 # 1.7 25.2 26.8 isolet 6.5 5.3 7.6 letter 3.5 # 0.2 7.1 4.5 pendigits 3.9 # 0.6 2.9 2.2 satimage 9.6 #", "mykey":2254},
 {"datasetID":90, "supportID":"E006C4EADD4929053891FBDC1979D4C774DE3FEB", "rexaID":"692880d7b3356df64bfa0f06a683f89e4ce6955b", "author":"Vassilis Athitsos and Stan Sclaroff", "title":"Boosting Nearest Neighbor Classifiers for Multiclass Recognition", "venue":"Boston University Computer Science Tech. Report No, 2004-006", "year":"2004", "window":"We did not use four datasets (dermatology, <b>soybean</b>  thyroid, audiology) because they have missing attributes, which our current formulation cannot handle. One dataset (ecoli) contains a nominal attribute, which our current", "mykey":2255},
 {"datasetID":91, "supportID":"E006C4EADD4929053891FBDC1979D4C774DE3FEB", "rexaID":"692880d7b3356df64bfa0f06a683f89e4ce6955b", "author":"Vassilis Athitsos and Stan Sclaroff", "title":"Boosting Nearest Neighbor Classifiers for Multiclass Recognition", "venue":"Boston University Computer Science Tech. Report No, 2004-006", "year":"2004", "window":"We did not use four datasets (dermatology, <b>soybean</b>  thyroid, audiology) because they have missing attributes, which our current formulation cannot handle. One dataset (ecoli) contains a nominal attribute, which our current", "mykey":2256},
 {"datasetID":102, "supportID":"E006C4EADD4929053891FBDC1979D4C774DE3FEB", "rexaID":"692880d7b3356df64bfa0f06a683f89e4ce6955b", "author":"Vassilis Athitsos and Stan Sclaroff", "title":"Boosting Nearest Neighbor Classifiers for Multiclass Recognition", "venue":"Boston University Computer Science Tech. Report No, 2004-006", "year":"2004", "window":"We did not use four datasets (dermatology, soybean, <b>thyroid</b>  audiology) because they have missing attributes, which our current formulation cannot handle. One dataset (ecoli) contains a nominal attribute, which our current", "mykey":2257},
 {"datasetID":110, "supportID":"E006C4EADD4929053891FBDC1979D4C774DE3FEB", "rexaID":"692880d7b3356df64bfa0f06a683f89e4ce6955b", "author":"Vassilis Athitsos and Stan Sclaroff", "title":"Boosting Nearest Neighbor Classifiers for Multiclass Recognition", "venue":"Boston University Computer Science Tech. Report No, 2004-006", "year":"2004", "window":"where that method does better than the other methods. There are also two datasets (glass and <b>yeast</b>  where the results of our algorithm and the best results from ECOC-based boosting and naive k-nn classification are quite similar. We should mention that, in the Allwein et al.", "mykey":2258},
 {"datasetID":2, "supportID":"E0072FDFA81BB50CB02DA4E884F4FC42E4A2E4F3", "rexaID":"b2c60bdf066b03909792bd5d64891f3607b948c8", "author":"Rich Caruana and Alexandru Niculescu-Mizil and Geoff Crew and Alex Ksikes", "title":"Ensemble selection from libraries of models", "venue":"ICML", "year":"2004", "window":"but each ensemble is just a weighted average of models, so the average of a set of ensembles also is a simple weighted average of the base-level models. Bagging is discussed in Section 5.3. 3. Data Sets We experiment with seven problems: <b>ADULT</b>  COVER TYPE, LETTER.p1, and LETTER.p2 from the UCI Repository (Blake & Merz, 1998), MEDIS, a pneumonia data set, SLAC, data from collaborators at the", "mykey":2259},
 {"datasetID":90, "supportID":"E0072FDFA81BB50CB02DA4E884F4FC42E4A2E4F3", "rexaID":"b2c60bdf066b03909792bd5d64891f3607b948c8", "author":"Rich Caruana and Alexandru Niculescu-Mizil and Geoff Crew and Alex Ksikes", "title":"Ensemble selection from libraries of models", "venue":"ICML", "year":"2004", "window":"1-13 as class 0 and letters 14-26 as class 1, yielding a di\u00c6cult, but balanced, problem. HYPER SPECT was converted to binary by treating the large confusable class <b>Soybean</b> Mintil as class 1. These data sets were selected because they are large enough to allow moderate size train and validation sets, and still have data left for large final test sets. For our experiments, we used training sets of 5000", "mykey":2260},
 {"datasetID":91, "supportID":"E0072FDFA81BB50CB02DA4E884F4FC42E4A2E4F3", "rexaID":"b2c60bdf066b03909792bd5d64891f3607b948c8", "author":"Rich Caruana and Alexandru Niculescu-Mizil and Geoff Crew and Alex Ksikes", "title":"Ensemble selection from libraries of models", "venue":"ICML", "year":"2004", "window":"1-13 as class 0 and letters 14-26 as class 1, yielding a di\u00c6cult, but balanced, problem. HYPER SPECT was converted to binary by treating the large confusable class <b>Soybean</b> Mintil as class 1. These data sets were selected because they are large enough to allow moderate size train and validation sets, and still have data left for large final test sets. For our experiments, we used training sets of 5000", "mykey":2261},
 {"datasetID":7, "supportID":"E010663615E360B673DBCED359BF165B65527E71", "rexaID":"44b953c24c786a0472327b47b8682b97114621d6", "author":"Pedro Domingos", "title":"Unifying Instance-Based and Rule-Based Induction", "venue":"Machine Learning, 24", "year":"1996", "window":"minimum of 4 examples (instead of 2) in two branches of a test, and use a confidence level of 37.5% for rule pruning (instead of 25%). The fine-tuned algorithms were then tested on the remaining 15 datasets in Table 4 (from <b>audiology</b> to zoology). Note that this procedure is somewhat unfavorable to RISE, since some of these domains were previously used in the development of the other algorithms, as", "mykey":2262},
 {"datasetID":8, "supportID":"E010663615E360B673DBCED359BF165B65527E71", "rexaID":"44b953c24c786a0472327b47b8682b97114621d6", "author":"Pedro Domingos", "title":"Unifying Instance-Based and Rule-Based Induction", "venue":"Machine Learning, 24", "year":"1996", "window":"minimum of 4 examples (instead of 2) in two branches of a test, and use a confidence level of 37.5% for rule pruning (instead of 25%). The fine-tuned algorithms were then tested on the remaining 15 datasets in Table 4 (from <b>audiology</b> to zoology). Note that this procedure is somewhat unfavorable to RISE, since some of these domains were previously used in the development of the other algorithms, as", "mykey":2263},
 {"datasetID":14, "supportID":"E010663615E360B673DBCED359BF165B65527E71", "rexaID":"44b953c24c786a0472327b47b8682b97114621d6", "author":"Pedro Domingos", "title":"Unifying Instance-Based and Rule-Based Induction", "venue":"Machine Learning, 24", "year":"1996", "window":"included in the listing of empirical results in (Holte, 1993) are referred to by the same codes. In the first phase of the study, the first 15 datasets in Table 4 (from <b>breast</b> <b>cancer</b> to wine) were used to fine-tune the algorithms, choosing by 10-fold cross-validation the most accurate version of each. Since a complete factor analysis would be too", "mykey":2264},
 {"datasetID":109, "supportID":"E010663615E360B673DBCED359BF165B65527E71", "rexaID":"44b953c24c786a0472327b47b8682b97114621d6", "author":"Pedro Domingos", "title":"Unifying Instance-Based and Rule-Based Induction", "venue":"Machine Learning, 24", "year":"1996", "window":"included in the listing of empirical results in (Holte, 1993) are referred to by the same codes. In the first phase of the study, the first 15 datasets in Table 4 (from breast cancer to <b>wine</b>  were used to fine-tune the algorithms, choosing by 10-fold cross-validation the most accurate version of each. Since a complete factor analysis would be too", "mykey":2265},
 {"datasetID":14, "supportID":"E01C5FD64B489477148B6D1B0A6D1D28D14EB140", "rexaID":"b90249d5778eb237815fe93968e894cb5adb7f8a", "author":"Ron Kohavi", "title":"A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection", "venue":"IJCAI", "year":"1995", "window":"` is defined as the expected value minus the estimated value. An unbiased estimation method is a method that has zero bias. Figure 1 shows the bias and variance of k-fold cross-validation on several datasets (the <b>breast</b> <b>cancer</b> dataset is not shown). The diagrams clearly show that k-fold cross-validation is pessimistically biased, especially for two and five folds. For the learning curves that have a", "mykey":2266},
 {"datasetID":53, "supportID":"E01C5FD64B489477148B6D1B0A6D1D28D14EB140", "rexaID":"b90249d5778eb237815fe93968e894cb5adb7f8a", "author":"Ron Kohavi", "title":"A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection", "venue":"IJCAI", "year":"1995", "window":"then an over-represented class in one subset will be a underrepresented in the other. To demonstrate the issue, we simulated a 2/3, 1/3 split of Fisher's famous <b>iris</b> dataset and used a majority inducer that builds a classifier predicting the prevalent class in the training set. The iris dataset describes iris plants using four continuous features, and the task is to", "mykey":2267},
 {"datasetID":90, "supportID":"E01C5FD64B489477148B6D1B0A6D1D28D14EB140", "rexaID":"b90249d5778eb237815fe93968e894cb5adb7f8a", "author":"Ron Kohavi", "title":"A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection", "venue":"IJCAI", "year":"1995", "window":"Mushroom Chess Hypo Breast Vehicle <b>soybean</b> Rand Figure 4: .632 Bootstrap: standard deviation in accuracy (population). sets themselves, leading to an increase in variance. This is most apparent for datasets with many categories, such as soybean. In these situations, stratification seems to help, but repeated runs may be a better approach. Our results indicate that stratification is generally a better", "mykey":2268},
 {"datasetID":91, "supportID":"E01C5FD64B489477148B6D1B0A6D1D28D14EB140", "rexaID":"b90249d5778eb237815fe93968e894cb5adb7f8a", "author":"Ron Kohavi", "title":"A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection", "venue":"IJCAI", "year":"1995", "window":"Mushroom Chess Hypo Breast Vehicle <b>soybean</b> Rand Figure 4: .632 Bootstrap: standard deviation in accuracy (population). sets themselves, leading to an increase in variance. This is most apparent for datasets with many categories, such as soybean. In these situations, stratification seems to help, but repeated runs may be a better approach. Our results indicate that stratification is generally a better", "mykey":2269},
 {"datasetID":148, "supportID":"E01C5FD64B489477148B6D1B0A6D1D28D14EB140", "rexaID":"b90249d5778eb237815fe93968e894cb5adb7f8a", "author":"Ron Kohavi", "title":"A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection", "venue":"IJCAI", "year":"1995", "window":"vehicle, the generalization accuracy of the Naive-Bayes algorithm deteriorated by more than 4% as more instances were given. A similar phenomenon was observed on the <b>shuttle</b> dataset. Such a phenomenon was predicted by Schaffer and Wolpert (Schaffer 1994, Wolpert 1994b), but we were surprised that it was observed on two real-world datasets. To see how well an accuracy estimation", "mykey":2270},
 {"datasetID":149, "supportID":"E01C5FD64B489477148B6D1B0A6D1D28D14EB140", "rexaID":"b90249d5778eb237815fe93968e894cb5adb7f8a", "author":"Ron Kohavi", "title":"A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection", "venue":"IJCAI", "year":"1995", "window":"rand, with 20 Boolean features and a Boolean random label. On one dataset, <b>vehicle</b>  the generalization accuracy of the Naive-Bayes algorithm deteriorated by more than 4% as more instances were given. A similar phenomenon was observed on the shuttle dataset. Such a", "mykey":2271},
 {"datasetID":154, "supportID":"E0E2BBC7F5341BC46F90D4F9A6DD866892DE7A26", "rexaID":"e949e51c587c01cc64b362cb4fedfaa7dcf5dd68", "author":"Michihiro Kuramochi and George Karypis", "title":"Finding Frequent Patterns in a Large Sparse Graph", "venue":"SDM", "year":"2004", "window":"we removed undirected edges to show \"near to\" relation between two vertices because those edges form cliques which makes this graph difficult to mine. The Contact Map dataset is made of 170 proteins from the <b>Protein</b> Data Bank [4] with pairwise sequence identity lower than 25%. The vertices in these graphs correspond to the different amino acids and the edges connect two", "mykey":2272},
 {"datasetID":59, "supportID":"E0E6F18BE3912B426F1E1DFD5B0F5C24A08D00F9", "rexaID":"81865e4bd3f3a7a8b86c73c6144db99274e5bbaf", "author":"Georgios Paliouras and David S. Br\u00e9e", "title":"The Effect of Numeric Features on the Scalability of Inductive Learning Programs", "venue":"ECML", "year":"1995", "window":"6000 set size (instances) 1 10 100 1000 10000 100000 CPU Time (sec.) Computational Performance <b>Letter Recognition</b> Set C4.5 PLS1 CN2 AQ15 Figure 5: Scalability Results, using the Letter Recognition data set. 60 600 6000 60000 set size (instances) 0 1 2 3 4 Rate of increase Rate of increase of CPU-time consumption Letter Recognition Set (n 2 ) (n) C4.5 PLS1 CN2 AQ15 Figure 6: Letter Recognition Set: The", "mykey":2273},
 {"datasetID":98, "supportID":"E0E6F18BE3912B426F1E1DFD5B0F5C24A08D00F9", "rexaID":"81865e4bd3f3a7a8b86c73c6144db99274e5bbaf", "author":"Georgios Paliouras and David S. Br\u00e9e", "title":"The Effect of Numeric Features on the Scalability of Inductive Learning Programs", "venue":"ECML", "year":"1995", "window":"was acquired from the UCI Repository [13] and its original donor was D.J. Slate. Its author has used it as an application domain for Holland-style genetic classifier systems [7]. More recently the data set has also been used in the <b>StatLog</b> project [11]. The data set contains 20; 000 instances, of which roughly 16; 000 have been used for learning in this experiment. Each instance corresponds to an", "mykey":2274},
 {"datasetID":67, "supportID":"E0E98D01166A5932B1AE04416B051ED8E7391223", "rexaID":"7354900bd4fb9f6d03256986c4181c92ab534f80", "author":"Ivor W. Tsang and James T. Kwok", "title":"Distance Metric Learning with Kernels", "venue":"Department of Computer Science Hong Kong University of Science and Technology Clear Water Bay Hong Kong", "year":"", "window":"the data distributions using the two distance metrics. As can be seen, similar patterns become more 2 ionosphere, sonar and wine are from the UCI repository [2], and microarray (a <b>DNA</b> microarray dataset for colon cancer) is from http://www.kyb.tuebingen.mpg.de/bs/people/weston/l0. clustered while dissimilar patterns are more separated. Table II reports the classification accuracies, averaged over", "mykey":2275},
 {"datasetID":48, "supportID":"E0ECB4FE96F17FECC933DC4D18C4E48F9216E3E4", "rexaID":"cb859ac992c9a4cf5cdb94f0dd15a628718a4b0c", "author":"Jarkko Tikka", "title":"AB HELSINKI UNIVERSITY OF TECHNOLOGY Department of Automation and Systems Technology Jarkko Tikka Learning linear dependency trees from multivariate data", "venue":"Helsinki University of Technology Abstract of Master's thesis Department of Automation and Systems Technology Author Date", "year":"", "window":"and ipkts. The value of the regression coefficient of wio is also negative in the latter case. A positive change in wio decreases the value of ipkts. 4.3 Boston <b>housing</b> data The second real world data set is called the Boston housing data. The data are got from the UCI repository of the databases [3]. The data set concerns housing values in suburbs of Boston in the USA. The data were collected in", "mykey":2276},
 {"datasetID":102, "supportID":"E123F7ECDCC0530A881F571AE5E216FCFD624761", "rexaID":"5cd87b7f1c57053feab46f876a961a66aba2df8b", "author":"Michael L. Raymer and William F. Punch and Erik D. Goodman and Leslie A. Kuhn and Anil K. Jain", "title":"Brief Papers", "venue":"", "year":"", "window":"only 49.7%, approximately equivalent to random class assignment. C. Discussion The integrated feature extraction and classification approach described has proved effective on these three disparate data sets. For the <b>thyroid</b> data, the GA--knn was more effective than all but two of the approaches, but required only three of the features to make the classification. The GA--knn obtained a classification", "mykey":2277},
 {"datasetID":110, "supportID":"E1C1FB14C2F66BE2EFFE34AD241A28EFDDE76F03", "rexaID":"76b4e0dc34501a39e272d5331e363877c2522d31", "author":"Samuel Kaski and Jaakko Peltonen", "title":"Informative Discriminant Analysis", "venue":"ICML", "year":"2003", "window":"of noise in parameter validation. 5. Analysis of Gene Expression Data In this section we demonstrate one way of using the extracted components for exploratory analysis of <b>yeast</b> gene expression. The data set (Hughes et al., 2000) consists of measurements of the expression of each yeast gene in 300 knock-out mutation experiments. After leaving out all genes and experiments without significant expression,", "mykey":2278},
 {"datasetID":2, "supportID":"E1E934CA215D4FCB6FE21834E5CA2008CE9028FE", "rexaID":"0527f760f533e2a7a827710a924d6987bfc9a06e", "author":"Stephen D. Bay", "title":"Multivariate Discretization for Set Mining", "venue":"Knowl. Inf. Syst, 3", "year":"2001", "window":"Data Set # Features # Continuous # Examples <b>Adult</b> 14 5 48812 Census-Income 5 41 7 199523 SatImage 37 36 6435 Shuttle 10 9 48480 UCI Admissions 19 8 123028 Table 3. Discretization Time in CPU seconds Data Set", "mykey":2279},
 {"datasetID":20, "supportID":"E1E934CA215D4FCB6FE21834E5CA2008CE9028FE", "rexaID":"0527f760f533e2a7a827710a924d6987bfc9a06e", "author":"Stephen D. Bay", "title":"Multivariate Discretization for Set Mining", "venue":"Knowl. Inf. Syst, 3", "year":"2001", "window":"for <b>Census</b> Income. We required differences between adjacent cells to be at least as large as 1% of N . ME-MDL requires a class variable and for the Adult, Census-Income, SatImage, and Shuttle datasets we used the class variable that had been used in previous analyses. For UCI Admissions we used Admit = fyes, nog (i.e. was the student admitted to UCI) as the class variable. 6.1. Execution Time", "mykey":2280},
 {"datasetID":117, "supportID":"E1E934CA215D4FCB6FE21834E5CA2008CE9028FE", "rexaID":"0527f760f533e2a7a827710a924d6987bfc9a06e", "author":"Stephen D. Bay", "title":"Multivariate Discretization for Set Mining", "venue":"Knowl. Inf. Syst, 3", "year":"2001", "window":"for <b>Census</b> <b>Income</b>  We required differences between adjacent cells to be at least as large as 1% of N . ME-MDL requires a class variable and for the Adult, Census-Income, SatImage, and Shuttle datasets we used the class variable that had been used in previous analyses. For UCI Admissions we used Admit = fyes, nog (i.e. was the student admitted to UCI) as the class variable. 6.1. Execution Time", "mykey":2281},
 {"datasetID":92, "supportID":"E1E934CA215D4FCB6FE21834E5CA2008CE9028FE", "rexaID":"0527f760f533e2a7a827710a924d6987bfc9a06e", "author":"Stephen D. Bay", "title":"Multivariate Discretization for Set Mining", "venue":"Knowl. Inf. Syst, 3", "year":"2001", "window":"that deals with the positioning of radiators in the <b>Space</b> <b>Shuttle</b>  -- UCI Admissions Data. This dataset represents all undergraduate student applications to UCI for the years 1993-1999. There are about 18000 applicants per year and the data contains variables such as ethnicity, UCI School (e.g. Arts,", "mykey":2282},
 {"datasetID":146, "supportID":"E1E934CA215D4FCB6FE21834E5CA2008CE9028FE", "rexaID":"0527f760f533e2a7a827710a924d6987bfc9a06e", "author":"Stephen D. Bay", "title":"Multivariate Discretization for Set Mining", "venue":"Knowl. Inf. Syst, 3", "year":"2001", "window":"of variables and number of records) and more detailed (i.e. standard census variables such as industry-code or occupation are recorded at a more detailed level in this database). -- SatImage. This data set was generated from <b>Landsat</b> Multi-Spectral Scanner image data (i.e. it is a satellite image). It contains multi-spectral values for 3 Theta 3 pixel neighborhood and the soil type (e.g. red soil,", "mykey":2283},
 {"datasetID":148, "supportID":"E1E934CA215D4FCB6FE21834E5CA2008CE9028FE", "rexaID":"0527f760f533e2a7a827710a924d6987bfc9a06e", "author":"Stephen D. Bay", "title":"Multivariate Discretization for Set Mining", "venue":"Knowl. Inf. Syst, 3", "year":"2001", "window":"it is a satellite image). It contains multi-spectral values for 3 Theta 3 pixel neighborhood and the soil type (e.g. red soil, cotton crop, grey soil, etc.). -- <b>Shuttle</b>  This is a classification dataset that deals with the positioning of radiators in the Space Shuttle. -- UCI Admissions Data. This dataset represents all undergraduate student applications to UCI for the years 1993-1999. There are", "mykey":2284},
 {"datasetID":5, "supportID":"E1FAB5603711B5A93D09C205563BA0D6B463885A", "rexaID":"e3fb3565af764647303c58f32b57a23f9eae18c0", "author":"Krista Lagus and Esa Alhoniemi and Jeremias Seppa and Antti Honkela and Arno Wagner", "title":"INDEPENDENT VARIABLE GROUP ANALYSIS IN LEARNING COMPACT REPRESENTATIONS FOR DATA", "venue":"Neural Networks Research Centre, Helsinki University of Technology", "year":"", "window":"models optimized carefully using the IVGA implementation. The model search of our IVGA implementation was able to discover the best grouping, i.e., the one with the smallest cost. 3.2. <b>Arrhythmia</b> data set The identification of different types of heart problems, namely cardiac arrhythmias, is carried out based on electrocardiography measurings from a large number of electrodes. We used a freely", "mykey":2285},
 {"datasetID":45, "supportID":"E1FAB5603711B5A93D09C205563BA0D6B463885A", "rexaID":"e3fb3565af764647303c58f32b57a23f9eae18c0", "author":"Krista Lagus and Esa Alhoniemi and Jeremias Seppa and Antti Honkela and Arno Wagner", "title":"INDEPENDENT VARIABLE GROUP ANALYSIS IN LEARNING COMPACT REPRESENTATIONS FOR DATA", "venue":"Neural Networks Research Centre, Helsinki University of Technology", "year":"", "window":"models optimized carefully using the IVGA implementation. The model search of our IVGA implementation was able to discover the best grouping, i.e., the one with the smallest cost. 3.2. Arrhythmia data set The identification of different types of <b>heart</b> problems, namely cardiac arrhythmias, is carried out based on electrocardiography measurings from a large number of electrodes. We used a freely", "mykey":2286},
 {"datasetID":105, "supportID":"E25726B7D7AC671A0427FA7A0D39DAE50C439ADB", "rexaID":"dad1e8c3efe3862d65266fb0d04ff7a8ef116f0f", "author":"Eui-Hong Han and George Karypis and Vipin Kumar and Bamshad Mobasher", "title":"Clustering Based On Association Rule Hypergraphs", "venue":"DMKD", "year":"1997", "window":"the cluster which has the highest score with respect to that transaction. We performed clustering of transactions on 1984 United States Congressional <b>Voting Records</b> Database provided by [MM96]. The data set includes 435 transactions each corresponding to one Congressman's votes on 16 key issues. We removed class values from each transaction, and we followed the steps specified in Section 2.1 to", "mykey":2287},
 {"datasetID":151, "supportID":"E2619AFE6613169FF56B36A0E618235D6BAFB31D", "rexaID":"b525865598a98b326df461e4d9d2647543347eb4", "author":"Jianbin Tan and David L. Dowe", "title":"MML Inference of Oblique Decision Trees", "venue":"Australian Conference on Artificial Intelligence", "year":"2004", "window":"and medical data, such as Bupa, Breast Cancer, Wisconsin, Lung Cancer, and Cleveland. The nine UCI Repository [1] data-sets used are these five, Balance, Credit, <b>Sonar</b> and Wine. For each of the nine data sets, 100 independent tests were done by randomly sampling 90% of the data as training data and testing on the remaining 10%. 4 Discussion We compare the MML oblique tree scheme to C4.5 and C5. The", "mykey":2288},
 {"datasetID":109, "supportID":"E2619AFE6613169FF56B36A0E618235D6BAFB31D", "rexaID":"b525865598a98b326df461e4d9d2647543347eb4", "author":"Jianbin Tan and David L. Dowe", "title":"MML Inference of Oblique Decision Trees", "venue":"Australian Conference on Artificial Intelligence", "year":"2004", "window":"and medical data, such as Bupa, Breast Cancer, Wisconsin, Lung Cancer, and Cleveland. The nine UCI Repository [1] data-sets used are these five, Balance, Credit, Sonar and <b>Wine</b>  For each of the nine data sets, 100 independent tests were done by randomly sampling 90% of the data as training data and testing on the remaining 10%. 4 Discussion We compare the MML oblique tree scheme to C4.5 and C5. The", "mykey":2289},
 {"datasetID":34, "supportID":"E2E2EDCB141D0A83DF4053A295518564B3D6E90A", "rexaID":"87afa910f706df9e28bfa697d6d2eea7c0cb53ef", "author":"Ilya Blayvas and Ron Kimmel", "title":"Efficient Classification via Multiresolution Training Set Approximation", "venue":"CS Dept. Technion", "year":"", "window":"\u00b7 D). 3 Experimental Results The proposed method was implemented in VC++ 6.0 and run on `IBM PC 300 PL' with 600MHZ Pentium III processor and 256MB RAM. It was tested on the Pima Indians <b>Diabetes</b> dataset [10], and a large artificial dataset generated with the DatGen program [11]. The results were compared to the Smooth SVM [12] and Sparse Grids [3]. Figure 7: Partition of 2D feature space for a", "mykey":2290},
 {"datasetID":79, "supportID":"E2E2EDCB141D0A83DF4053A295518564B3D6E90A", "rexaID":"87afa910f706df9e28bfa697d6d2eea7c0cb53ef", "author":"Ilya Blayvas and Ron Kimmel", "title":"Efficient Classification via Multiresolution Training Set Approximation", "venue":"CS Dept. Technion", "year":"", "window":"\u00b7 D). 3 Experimental Results The proposed method was implemented in VC++ 6.0 and run on `IBM PC 300 PL' with 600MHZ Pentium III processor and 256MB RAM. It was tested on the <b>Pima</b> <b>Indians</b> <b>Diabetes</b> dataset [10], and a large artificial dataset generated with the DatGen program [11]. The results were compared to the Smooth SVM [12] and Sparse Grids [3]. Figure 7: Partition of 2D feature space for a", "mykey":2291},
 {"datasetID":14, "supportID":"E2F4209356D021DC822428AFDD99DC00E35155DB", "rexaID":"a90748cae4d3621106986b907d23da04fbc59c59", "author":"John W. Chinneck", "title":"Fast Heuristics for the Maximum Feasible Subsystem Problem", "venue":"Systems and Computer Engineering, Carleton University", "year":"", "window":"Data Set Net Points Number of Features <b>breast</b> <b>cancer</b> 683 9 bupa 345 6 glass (type 2 vs.others) 214 9 ionosphere 351 34 iris (versicolor vs.others) 150 4 iris (virginica vs.others) 150 4 new thyroid (normal", "mykey":2292},
 {"datasetID":19, "supportID":"E34233BA549E351487B1F97D7C6A1C881D2B01F5", "rexaID":"6aae20aa5fd96e903634bb73244782652cd4e947", "author":"Zhiqiang Yang and Sheng Zhong and Rebecca N. Wright", "title":"Privacy-Preserving Classification of Customer Data without Loss of Accuracy", "venue":"Computer Science Department, Stevens Institute of Technology", "year":"", "window":"of size d, then the number of joint frequencies that need to be counted is exponential in m. In some cases we have small m and d and thus we can still achieve reasonable overhead. For example, the data set of <b>Car</b> Evaluation Database from UCI repository [BM98] has six nominal attributes: buying, maint, doors, persons, lug boot and safety, and the class attribute has a domain of size four. For such 9 a", "mykey":2293},
 {"datasetID":7, "supportID":"E354BB66D8D27FA87897536EDBD475A9E1D81A61", "rexaID":"692880d7b3356df64bfa0f06a683f89e4ce6955b", "author":"Vassilis Athitsos and Stan Sclaroff", "title":"Boosting Nearest Neighbor Classifiers for Multiclass Recognition", "venue":"Boston University Computer Science Tech. Report No, 2004-006", "year":"2004", "window":"We did not use four datasets (dermatology, soybean, thyroid, <b>audiology</b>  because they have missing attributes, which our current formulation cannot handle. One dataset (ecoli) contains a nominal attribute, which our current", "mykey":2294},
 {"datasetID":8, "supportID":"E354BB66D8D27FA87897536EDBD475A9E1D81A61", "rexaID":"692880d7b3356df64bfa0f06a683f89e4ce6955b", "author":"Vassilis Athitsos and Stan Sclaroff", "title":"Boosting Nearest Neighbor Classifiers for Multiclass Recognition", "venue":"Boston University Computer Science Tech. Report No, 2004-006", "year":"2004", "window":"We did not use four datasets (dermatology, soybean, thyroid, <b>audiology</b>  because they have missing attributes, which our current formulation cannot handle. One dataset (ecoli) contains a nominal attribute, which our current", "mykey":2295},
 {"datasetID":33, "supportID":"E354BB66D8D27FA87897536EDBD475A9E1D81A61", "rexaID":"692880d7b3356df64bfa0f06a683f89e4ce6955b", "author":"Vassilis Athitsos and Stan Sclaroff", "title":"Boosting Nearest Neighbor Classifiers for Multiclass Recognition", "venue":"Boston University Computer Science Tech. Report No, 2004-006", "year":"2004", "window":"We ended up using only eight of those datasets. We did not use four datasets  <b>dermatology</b>  soybean, thyroid, audiology) because they have missing attributes, which our current formulation cannot handle. One dataset (ecoli) contains a nominal", "mykey":2296},
 {"datasetID":39, "supportID":"E354BB66D8D27FA87897536EDBD475A9E1D81A61", "rexaID":"692880d7b3356df64bfa0f06a683f89e4ce6955b", "author":"Vassilis Athitsos and Stan Sclaroff", "title":"Boosting Nearest Neighbor Classifiers for Multiclass Recognition", "venue":"Boston University Computer Science Tech. Report No, 2004-006", "year":"2004", "window":"(dermatology, soybean, thyroid, audiology) because they have missing attributes, which our current formulation cannot handle. One dataset  <b>ecoli</b>  contains a nominal attribute, which our current implementation cannot handle in practice; this Table 3. For each dataset, we count how many variations of AdaBoost.MO gave lower (<), equal", "mykey":2297},
 {"datasetID":42, "supportID":"E354BB66D8D27FA87897536EDBD475A9E1D81A61", "rexaID":"692880d7b3356df64bfa0f06a683f89e4ce6955b", "author":"Vassilis Athitsos and Stan Sclaroff", "title":"Boosting Nearest Neighbor Classifiers for Multiclass Recognition", "venue":"Boston University Computer Science Tech. Report No, 2004-006", "year":"2004", "window":"used in the experiments, largely copied from (Allwein et al., 2000). Dataset Train Test Attributes Classes <b>glass</b> 214 - 9 6 isolet 6238 1559 617 26 letter 16000 4000 16 26 pendigits 7494 3498 16 10 satimage 4435 2000 36 6 segmentation 2310 - 19 7 vowel 528 462 10 11 yeast", "mykey":2298},
 {"datasetID":54, "supportID":"E354BB66D8D27FA87897536EDBD475A9E1D81A61", "rexaID":"692880d7b3356df64bfa0f06a683f89e4ce6955b", "author":"Vassilis Athitsos and Stan Sclaroff", "title":"Boosting Nearest Neighbor Classifiers for Multiclass Recognition", "venue":"Boston University Computer Science Tech. Report No, 2004-006", "year":"2004", "window":"et al. (2000) and the best result attained among the 6 variations of \naive\" k-nn classification. For our method we also provide the standard deviation across multiple trials, except for the <b>isolet</b> dataset where we only ran one trial of our algorithm. Dataset Boost-NN Allwein Naive k-nn glass 24.4 # 1.7 25.2 26.8 isolet 6.5 5.3 7.6 letter 3.5 # 0.2 7.1 4.5 pendigits 3.9 # 0.6 2.9 2.2 satimage 9.6 #", "mykey":2299},
 {"datasetID":90, "supportID":"E354BB66D8D27FA87897536EDBD475A9E1D81A61", "rexaID":"692880d7b3356df64bfa0f06a683f89e4ce6955b", "author":"Vassilis Athitsos and Stan Sclaroff", "title":"Boosting Nearest Neighbor Classifiers for Multiclass Recognition", "venue":"Boston University Computer Science Tech. Report No, 2004-006", "year":"2004", "window":"We did not use four datasets (dermatology, <b>soybean</b>  thyroid, audiology) because they have missing attributes, which our current formulation cannot handle. One dataset (ecoli) contains a nominal attribute, which our current", "mykey":2300},
 {"datasetID":91, "supportID":"E354BB66D8D27FA87897536EDBD475A9E1D81A61", "rexaID":"692880d7b3356df64bfa0f06a683f89e4ce6955b", "author":"Vassilis Athitsos and Stan Sclaroff", "title":"Boosting Nearest Neighbor Classifiers for Multiclass Recognition", "venue":"Boston University Computer Science Tech. Report No, 2004-006", "year":"2004", "window":"We did not use four datasets (dermatology, <b>soybean</b>  thyroid, audiology) because they have missing attributes, which our current formulation cannot handle. One dataset (ecoli) contains a nominal attribute, which our current", "mykey":2301},
 {"datasetID":102, "supportID":"E354BB66D8D27FA87897536EDBD475A9E1D81A61", "rexaID":"692880d7b3356df64bfa0f06a683f89e4ce6955b", "author":"Vassilis Athitsos and Stan Sclaroff", "title":"Boosting Nearest Neighbor Classifiers for Multiclass Recognition", "venue":"Boston University Computer Science Tech. Report No, 2004-006", "year":"2004", "window":"We did not use four datasets (dermatology, soybean, <b>thyroid</b>  audiology) because they have missing attributes, which our current formulation cannot handle. One dataset (ecoli) contains a nominal attribute, which our current", "mykey":2302},
 {"datasetID":110, "supportID":"E354BB66D8D27FA87897536EDBD475A9E1D81A61", "rexaID":"692880d7b3356df64bfa0f06a683f89e4ce6955b", "author":"Vassilis Athitsos and Stan Sclaroff", "title":"Boosting Nearest Neighbor Classifiers for Multiclass Recognition", "venue":"Boston University Computer Science Tech. Report No, 2004-006", "year":"2004", "window":"where that method does better than the other methods. There are also two datasets (glass and <b>yeast</b>  where the results of our algorithm and the best results from ECOC-based boosting and naive k-nn classification are quite similar. We should mention that, in the Allwein et al.", "mykey":2303},
 {"datasetID":73, "supportID":"E36A67EE804B50D1EE16363F7A658E96D86159DB", "rexaID":"d32b83c84ede5f9ed1b2bda59ec57d68228b6b01", "author":"Venkatesh Ganti and Johannes Gehrke and Raghu Ramakrishnan", "title":"CACTUS - Clustering Categorical Data Using Summaries", "venue":"KDD", "year":"1999", "window":"which distance or similarity functions are not naturally defined. Recently, the problem of clustering categorical data started receiving interest [GKR98, GRS99]. As an example, consider the <b>MUSHROOM</b> dataset in the popular UCI Machine Learning repository [CBM98]. Each tuple in the dataset describes a sample of gilled mushrooms using 22 categorical attributes. # Contact author. This research was", "mykey":2304},
 {"datasetID":150, "supportID":"E3FBBC675A47BB90A75ECD89145DDE03399A09BB", "rexaID":"21b1e6cdcd730f2bf7438109509c9abdf01767d7", "author":"Rayid Ghani", "title":"KDD Project Report Using Error-Correcting Codes for Efficient Text Classification with a Large Number of Categories", "venue":"Center for Automated Learning and Discovery, School of Computer Science, Carnegie Mellon University", "year":"", "window":"used in the experiments are described in this section. 5.1 Hoovers Dataset This <b>corpus</b> of web pages was collected by the WebKB Group at CMU using the Hoovers Online Web resource (www.hoovers.com) by crawling 4285 companies on the web and, examining just over 108,000 Web", "mykey":2305},
 {"datasetID":53, "supportID":"E40AC26D73386944414C5C9236D9622209BC9CFA", "rexaID":"68b230977077ba67eb9e5c9a9111d3ccb3672150", "author":"Karol Grudzi nski and Wl/odzisl/aw Duch", "title":"SBL-PM: A Simple Algorithm for Selection of Reference Instances in Similarity Based Methods", "venue":"Department of Computer Methods, Nicholas Copernicus University", "year":"", "window":"the UCI repository [9] and contains 3 classes  <b>Iris</b> Setosa, Virginica and Versicolor flowers), 4 attributes (measurements of leaf and petal widths and length), 50 cases per class. The entire Iris dataset has been shown here (Fig. 1) in two dimensions, x 3 and x 4 , which are much more informative the other two (cf. [10]). In Fig 2. the reference set obtained by taking the value of # from the", "mykey":2306},
 {"datasetID":70, "supportID":"E40AC26D73386944414C5C9236D9622209BC9CFA", "rexaID":"68b230977077ba67eb9e5c9a9111d3ccb3672150", "author":"Karol Grudzi nski and Wl/odzisl/aw Duch", "title":"SBL-PM: A Simple Algorithm for Selection of Reference Instances in Similarity Based Methods", "venue":"Department of Computer Methods, Nicholas Copernicus University", "year":"", "window":"by other classification systems. Due to the noisy character of the data the limit in the leave-one-out or crossvalidation tests is about 98% [10]. Another set of experiments was done on the 3 <b>Monk</b> datasets [11]. On this artificial data SBM gives good results (100% of correct answers on the first problem, 85% on problem 2, and over 97% on problem 3) only if feature selection and/or weighting is", "mykey":2307},
 {"datasetID":52, "supportID":"E4DEB4A2CFC87ADE92365A880C2DAB4321752D38", "rexaID":"d024a37a0324a4e5218f79ec65fd33e4cff7825e", "author":"Isabelle Alvarez and Stephan Bernard", "title":"Ranking Cases with Decision Trees: a Geometric Method that Preserves Intelligibility", "venue":"", "year":"", "window":"score gives interesting results when misclassified examples are near the decision boundary. This is particularly true for the bupa (liver-disorder) and <b>ionosphere</b> databases. Table 2 shows that these datasets doesn't verify the hypothesis of proximity of errors on a majority of samples, and actually the global geometric score give bad results for these datasets. Concerning the improvement of the", "mykey":2308},
 {"datasetID":73, "supportID":"E50D12A1C66C7E353A6C42C647A5A592C3CE4327", "rexaID":"cfafdb7e4ac45e3b2e7d4da635f28601b4f4a9e8", "author":"Jinyan Li and Kotagiri Ramamohanarao and Guozhu Dong", "title":"ICML2000 The Space of Jumping Emerging Patterns and Its Incremental Maintenance Algorithms", "venue":"Department of Computer Science and Software Engineering, The University of Melbourne, Parkville", "year":"", "window":"is as yet unsolved. A naive maintenance method is to take a border difference operation to discover the border of the JEP space with respect to D p and (Dn Gamma Delta n ). Table 1. Properties of data sets. DATA SETS #INSTANCES #ATTRI #ITEMS <b>MUSHROOM</b> 4208(+), 3916 (-) 22 125 PIMA 268(+), 500 (-) 8 17 TIC-TAC-TOE 626(+), 332 (-) 9 27 NURSERY 4320(+), 8640(-) 8 27 5. Experimental Results We choose four", "mykey":2309},
 {"datasetID":76, "supportID":"E50D12A1C66C7E353A6C42C647A5A592C3CE4327", "rexaID":"cfafdb7e4ac45e3b2e7d4da635f28601b4f4a9e8", "author":"Jinyan Li and Kotagiri Ramamohanarao and Guozhu Dong", "title":"ICML2000 The Space of Jumping Emerging Patterns and Its Incremental Maintenance Algorithms", "venue":"Department of Computer Science and Software Engineering, The University of Melbourne, Parkville", "year":"", "window":"in UCI repository (Blake & Murphy, 1998) to experimentally examine the maintenance algorithms, especially their efficiency. These data sets are mushroom, pima, tic-tac-toe, and <b>nursery</b>  More details can be seen in Table 1. Note that the continuous attributes in the pima data set are discretized by MLC++ techniques (Kohavi et al, 1994).", "mykey":2310},
 {"datasetID":101, "supportID":"E50D12A1C66C7E353A6C42C647A5A592C3CE4327", "rexaID":"cfafdb7e4ac45e3b2e7d4da635f28601b4f4a9e8", "author":"Jinyan Li and Kotagiri Ramamohanarao and Guozhu Dong", "title":"ICML2000 The Space of Jumping Emerging Patterns and Its Incremental Maintenance Algorithms", "venue":"Department of Computer Science and Software Engineering, The University of Melbourne, Parkville", "year":"", "window":"in UCI repository (Blake & Murphy, 1998) to experimentally examine the maintenance algorithms, especially their efficiency. These data sets are mushroom, pima, <b>tic-tac-toe</b>  and nursery. More details can be seen in Table 1. Note that the continuous attributes in the pima data set are discretized by MLC++ techniques (Kohavi et al, 1994).", "mykey":2311},
 {"datasetID":90, "supportID":"E5A651DDC641AD0A207172E877618DEE1931797B", "rexaID":"1f4e06390d7a257b58508b73ed2a689add6a9748", "author":"Bianca Zadrozny", "title":"Reducing multiclass to binary by coupling probability estimates", "venue":"NIPS", "year":"2001", "window":"better. Figure 1 shows how the MSE is lowered at each iteration of the Hastie-Tibshirani algorithm, for the three types of code matrices. Table 3 shows the results of the same experiments on the datasets pendigits and <b>soybean</b>  Again, the MSE is significantly lowered by the iterative procedure, in all cases. For the soybean dataset, using the sparse random matrix, the iterative method again has a", "mykey":2312},
 {"datasetID":91, "supportID":"E5A651DDC641AD0A207172E877618DEE1931797B", "rexaID":"1f4e06390d7a257b58508b73ed2a689add6a9748", "author":"Bianca Zadrozny", "title":"Reducing multiclass to binary by coupling probability estimates", "venue":"NIPS", "year":"2001", "window":"better. Figure 1 shows how the MSE is lowered at each iteration of the Hastie-Tibshirani algorithm, for the three types of code matrices. Table 3 shows the results of the same experiments on the datasets pendigits and <b>soybean</b>  Again, the MSE is significantly lowered by the iterative procedure, in all cases. For the soybean dataset, using the sparse random matrix, the iterative method again has a", "mykey":2313},
 {"datasetID":14, "supportID":"E68C6C26D2C2C3F80D6B7004F6B86C8F059DEC58", "rexaID":"22550465c3724dd8650d67a7470a2c1dc90abd25", "author":"Rong Jin and Yan Liu and Luo Si and Jaime Carbonell and Alexander G. Hauptmann", "title":"A New Boosting Algorithm Using Input-Dependent Regularizer", "venue":"School of Computer Science, Carnegie Mellon University", "year":"", "window":"with 10% noise. From the results we can see that AdaBoost algorithm did su\u00aeer from overfitting on some of the data sets, such as \"German\",  <b>Breast</b> <b>cancer</b>  and \"Contraceptive\", while WeightBoost consistently achieved improvement on all of the eight data sets. In addition, our new algorithm demonstrates great", "mykey":2314},
 {"datasetID":150, "supportID":"E68C6C26D2C2C3F80D6B7004F6B86C8F059DEC58", "rexaID":"22550465c3724dd8650d67a7470a2c1dc90abd25", "author":"Rong Jin and Yan Liu and Luo Si and Jaime Carbonell and Alexander G. Hauptmann", "title":"A New Boosting Algorithm Using Input-Dependent Regularizer", "venue":"School of Computer Science, Carnegie Mellon University", "year":"", "window":"from the UCI repository (Blake & Merz, 1998) and a benchmark of text categorization evaluation -- the ApteMod version of Reuters-21578 <b>corpus</b> are used as testbeds. All of UCI data sets are binary classification problems and the detailed information is listed in Table 1. Reuters-21578 corpus consists of a training set of 7,769 documents and a test set of 3,019 documents with 90", "mykey":2315},
 {"datasetID":48, "supportID":"E6A7F01E8AACA981AA6A5FF838E3CD2A4F4C6CF9", "rexaID":"220290933c845bcae1348931e08740d3e16b5360", "author":"David Hershberger and Hillol Kargupta", "title":"Distributed Multivariate Regression Using Wavelet-Based Collective Data Mining", "venue":"J. Parallel Distrib. Comput, 61", "year":"2001", "window":"use to generate a regression model may not generate the MSE model for that amount of information transfer. The result also supports the MSE model result for the wavelet basis. The second benchmark data set we employ is the Boston <b>Housing</b> data set created by Harrison and Rubinfeld [18]. This data set consists of 506 samples with 13 independent variables, 12 of which are real-valued, and one real-valued", "mykey":2316},
 {"datasetID":53, "supportID":"E6A7F01E8AACA981AA6A5FF838E3CD2A4F4C6CF9", "rexaID":"220290933c845bcae1348931e08740d3e16b5360", "author":"David Hershberger and Hillol Kargupta", "title":"Distributed Multivariate Regression Using Wavelet-Based Collective Data Mining", "venue":"J. Parallel Distrib. Comput, 61", "year":"2001", "window":"Application of this method to Linear Discriminant Analysis, which is related to parametric multivariate regression, produced classificationresults on the <b>Iris</b> data set that are comparable to those obtained with centralized data analysis. Key Words: data mining, distributed data mining, collective data mining, knowledge discovery, wavelets, regression 1.", "mykey":2317},
 {"datasetID":14, "supportID":"E6FBED59A495489A92437438C1C1E57EA9CFB4E2", "rexaID":"2d05e6777bcfde449fc35a3a95dde0697a5c49ac", "author":"David W. Opitz and Richard Maclin", "title":"Popular Ensemble Methods: An Empirical Study", "venue":"J. Artif. Intell. Res. (JAIR, 11", "year":"1999", "window":"ensemble. Also shown (results column 3) is the \"best\" result produced from all of the single network results run using all of the training data. 197 Opitz & Maclin Single Bagging Arcing Boosting Data Set Err SD Best Err SD Err SD Err SD <b>breast</b> <b>cancer</b> w 5.0 0.7 4.0 3.7 0.5 3.5 0.6 3.5 0.3 credit-a 14.9 0.8 14.2 13.4 0.5 14.0 0.9 13.7 0.5 credit-g 29.6 1.0 28.7 25.2 0.7 25.9 1.0 26.7 0.4 diabetes 27.8", "mykey":2318},
 {"datasetID":46, "supportID":"E6FBED59A495489A92437438C1C1E57EA9CFB4E2", "rexaID":"2d05e6777bcfde449fc35a3a95dde0697a5c49ac", "author":"David W. Opitz and Richard Maclin", "title":"Popular Ensemble Methods: An Empirical Study", "venue":"J. Artif. Intell. Res. (JAIR, 11", "year":"1999", "window":"then the decision-tree ensemble methods also had lower (or higher) error than their neural network counterpart. The exceptions to this rule generally happened on the same data set for all three ensemble methods (e.g., <b>hepatitis</b>  soybean, satellite, credit-a, and heart-cleveland). These results suggest that (a) the performance of the ensemble methods is dependent on both the", "mykey":2319},
 {"datasetID":45, "supportID":"E798AFE7EF0C15D3AFDB85C05FD6281B03B0F443", "rexaID":"193231e78c226995eee9f66bf9a4177b8416daf4", "author":"H. -T Lin and C. -J Lin", "title":"A Study on Sigmoid Kernels for SVM and the Training of non-PSD Kernels by SMO-type Methods", "venue":"Department of Computer Science and Information Engineering National Taiwan University", "year":"", "window":"with ~ C = \u00af C 2a = decision value at x using the linear kernel with \u00af C. We can observe the result of Theorems 8 and 9 from Figure 1. The contours show five-fold cross-validation accuracy of the data set <b>heart</b> in different r and C. The contours with a = 1 are on the left-hand side, while those with a = 0.01 are on the right-hand side. Other parameters considered here are log 2 C from - 2 to 13, with", "mykey":2320},
 {"datasetID":98, "supportID":"E798AFE7EF0C15D3AFDB85C05FD6281B03B0F443", "rexaID":"193231e78c226995eee9f66bf9a4177b8416daf4", "author":"H. -T Lin and C. -J Lin", "title":"A Study on Sigmoid Kernels for SVM and the Training of non-PSD Kernels by SMO-type Methods", "venue":"Department of Computer Science and Information Engineering National Taiwan University", "year":"", "window":"D. J. Spiegelhalter, and C. C. Taylor (1994). Machine Learning, Neural and Statistical Classification. Englewood Cliffs, N.J.: Prentice Hall. Data available at http://www.ncc.up.pt/liacc/ML <b>statlog</b> datasets.html. Nash, S. G. and A. Sofer (1996). Linear and Nonlinear Programming. McGraw-Hill. Osuna, E., R. Freund, and F. Girosi (1997). Training support vector machines: An application to face detection.", "mykey":2321},
 {"datasetID":74, "supportID":"E7B4EA537A249EBF41E43AA23E4459D42CA61275", "rexaID":"2048a44e66eac923f00c225ee12e17c38d923376", "author":"Giorgio Valentini", "title":"Random Aggregated and Bagged Ensembles of SVMs: An Empirical Bias?Variance Analysis", "venue":"Multiple Classifier Systems", "year":"2004", "window":"from UCI [14] (Waveform, Grey-Landsat, Letter-Two, Letter-Two with added noise, Spam, <b>Musk</b>  and the P2 synthetic data set 1 . We achieved a characterization of the bias--variance decomposition of the error in bagged and random aggregated ensembles that resembles the one obtained for single SVMs [5] (Fig. 1. For more", "mykey":2322},
 {"datasetID":75, "supportID":"E7B4EA537A249EBF41E43AA23E4459D42CA61275", "rexaID":"2048a44e66eac923f00c225ee12e17c38d923376", "author":"Giorgio Valentini", "title":"Random Aggregated and Bagged Ensembles of SVMs: An Empirical Bias?Variance Analysis", "venue":"Multiple Classifier Systems", "year":"2004", "window":"from UCI [14] (Waveform, Grey-Landsat, Letter-Two, Letter-Two with added noise, Spam, <b>Musk</b>  and the P2 synthetic data set 1 . We achieved a characterization of the bias--variance decomposition of the error in bagged and random aggregated ensembles that resembles the one obtained for single SVMs [5] (Fig. 1. For more", "mykey":2323},
 {"datasetID":146, "supportID":"E7B4EA537A249EBF41E43AA23E4459D42CA61275", "rexaID":"2048a44e66eac923f00c225ee12e17c38d923376", "author":"Giorgio Valentini", "title":"Random Aggregated and Bagged Ensembles of SVMs: An Empirical Bias?Variance Analysis", "venue":"Multiple Classifier Systems", "year":"2004", "window":"software library [13] and the SVMlight applications [9]. 4.2 Results In particular we analyzed the relationships of the components of the error with the kernels and kernel parameters, using data sets from UCI [14] (Waveform, Grey <b>Landsat</b>  Letter-Two, Letter-Two with added noise, Spam, Musk) and the P2 synthetic data set 1 . We achieved a characterization of the bias--variance decomposition of", "mykey":2324},
 {"datasetID":107, "supportID":"E7B4EA537A249EBF41E43AA23E4459D42CA61275", "rexaID":"2048a44e66eac923f00c225ee12e17c38d923376", "author":"Giorgio Valentini", "title":"Random Aggregated and Bagged Ensembles of SVMs: An Empirical Bias?Variance Analysis", "venue":"Multiple Classifier Systems", "year":"2004", "window":"software library [13] and the SVMlight applications [9]. 4.2 Results In particular we analyzed the relationships of the components of the error with the kernels and kernel parameters, using data sets from UCI [14]  <b>Waveform</b>  Grey-Landsat, Letter-Two, Letter-Two with added noise, Spam, Musk) and the P2 synthetic data set 1 . We achieved a characterization of the bias--variance decomposition of", "mykey":2325},
 {"datasetID":108, "supportID":"E7B4EA537A249EBF41E43AA23E4459D42CA61275", "rexaID":"2048a44e66eac923f00c225ee12e17c38d923376", "author":"Giorgio Valentini", "title":"Random Aggregated and Bagged Ensembles of SVMs: An Empirical Bias?Variance Analysis", "venue":"Multiple Classifier Systems", "year":"2004", "window":"software library [13] and the SVMlight applications [9]. 4.2 Results In particular we analyzed the relationships of the components of the error with the kernels and kernel parameters, using data sets from UCI [14]  <b>Waveform</b>  Grey-Landsat, Letter-Two, Letter-Two with added noise, Spam, Musk) and the P2 synthetic data set 1 . We achieved a characterization of the bias--variance decomposition of", "mykey":2326},
 {"datasetID":42, "supportID":"E7E3145D583225BA96399629B07FACB4434822EE", "rexaID":"ba94d46c3f1c8a8ccb1dc3fcbeb6afdd963d3f87", "author":"Xiaoli Z. Fern and Carla Brodley", "title":"Solving cluster ensemble problems by bipartite graph partitioning", "venue":"ICML", "year":"2004", "window":"a fair comparison. 7. Experimental Results The goal of the experiments is to evaluate the three graph formulations - IBGF, CBGF and HBGF - given different cluster ensembles. Table 1. Summary of the data sets data set eos <b>glass</b> hrct isolet6 modis #inst. 2398 214 1545 1440 4975 #class 8 6 8 6 10 org. dim. 20 9 183 617 112 rp dim. 5 5 10 10 6 pca dim. --- --- 30 60 6 7.1. Data Sets and Parameter Settings", "mykey":2327},
 {"datasetID":19, "supportID":"E874B8E563DA7B700B6C13B790EF0713B276CD7F", "rexaID":"4fe77f0de67f4dda7e7174b944840d4d49fe15ac", "author":"Hyunwoo Kim and Wei-Yin Loh", "title":"Classification Trees with Bivariate Linear Discriminant Node Models", "venue":"Department of Statistics Department of Statistics University of Tennessee University of Wisconsin", "year":"", "window":"maximum simplification, where the root node is not split. The second example shows how collinearity among predictors can creates difficulties for other classification tree methods. 4.1 <b>Car</b> Data This data set is from Lock (1993). It contains specifications for 93 new car models for the 1993 year. We use the type of car (small, sporty, compact, midsize, large, and van) as the class variable. The predictor", "mykey":2328},
 {"datasetID":48, "supportID":"E8CF474D9C50252BFB6B2312513320B40B939365", "rexaID":"81fe50fa69f7adda42abf462e865a3b68089f333", "author":"Sreerama K. Murthy and Simon Kasif and Steven Salzberg", "title":"A System for Induction of Oblique Decision Trees", "venue":"Department of Computer Science Johns Hopkins University", "year":"1994", "window":"of three different types of iris flower. Weiss and Kapouleas (1989) obtained accuracies of 96.7% and 96.0% on this data with back propagation and 1-NN, respectively. <b>Housing</b> Costs in Boston. This data set, also available as a part of the UCI ML repository, describes housing values in the suburbs of Boston as a function of 12 continuous attributes and 1 binary attribute (Harrison & Rubinfeld, 1978).", "mykey":2329},
 {"datasetID":14, "supportID":"E8D73CD4F3695E35156605DDA9A3E1A8F750B147", "rexaID":"2d05e6777bcfde449fc35a3a95dde0697a5c49ac", "author":"David W. Opitz and Richard Maclin", "title":"Popular Ensemble Methods: An Empirical Study", "venue":"J. Artif. Intell. Res. (JAIR, 11", "year":"1999", "window":"ensemble. Also shown (results column 3) is the \"best\" result produced from all of the single network results run using all of the training data. 197 Opitz & Maclin Single Bagging Arcing Boosting Data Set Err SD Best Err SD Err SD Err SD <b>breast</b> <b>cancer</b> w 5.0 0.7 4.0 3.7 0.5 3.5 0.6 3.5 0.3 credit-a 14.9 0.8 14.2 13.4 0.5 14.0 0.9 13.7 0.5 credit-g 29.6 1.0 28.7 25.2 0.7 25.9 1.0 26.7 0.4 diabetes 27.8", "mykey":2330},
 {"datasetID":46, "supportID":"E8D73CD4F3695E35156605DDA9A3E1A8F750B147", "rexaID":"2d05e6777bcfde449fc35a3a95dde0697a5c49ac", "author":"David W. Opitz and Richard Maclin", "title":"Popular Ensemble Methods: An Empirical Study", "venue":"J. Artif. Intell. Res. (JAIR, 11", "year":"1999", "window":"then the decision-tree ensemble methods also had lower (or higher) error than their neural network counterpart. The exceptions to this rule generally happened on the same data set for all three ensemble methods (e.g., <b>hepatitis</b>  soybean, satellite, credit-a, and heart-cleveland). These results suggest that (a) the performance of the ensemble methods is dependent on both the", "mykey":2331},
 {"datasetID":50, "supportID":"E98E8B19487BCF4601DA2BE8915CBE41CB92913A", "rexaID":"3c1b37ca3a2f0825890509a5ff17081cf012fffd", "author":"Anthony K H Tung and Xin Xu and Beng Chin Ooi", "title":"CURLER: Finding and Visualizing Nonlinear Correlated Clusters", "venue":"SIGMOD Conference", "year":"2005", "window":"of three helix clusters with different cluster existence spaces, the iris plant dataset and the <b>image segmentation</b> dataset from the UCI Repository of Machine Learning Databases and Domain Theories [6], and the Iyer time series gene expression data with 10 well-known linear clusters", "mykey":2332},
 {"datasetID":53, "supportID":"E98E8B19487BCF4601DA2BE8915CBE41CB92913A", "rexaID":"3c1b37ca3a2f0825890509a5ff17081cf012fffd", "author":"Anthony K H Tung and Xin Xu and Beng Chin Ooi", "title":"CURLER: Finding and Visualizing Nonlinear Correlated Clusters", "venue":"SIGMOD Conference", "year":"2005", "window":"of three helix clusters with different cluster existence spaces, the <b>iris</b> plant dataset and the image segmentation dataset from the UCI Repository of Machine Learning Databases and Domain Theories [6], and the Iyer time series gene expression data with 10 well-known linear clusters", "mykey":2333},
 {"datasetID":147, "supportID":"E98E8B19487BCF4601DA2BE8915CBE41CB92913A", "rexaID":"3c1b37ca3a2f0825890509a5ff17081cf012fffd", "author":"Anthony K H Tung and Xin Xu and Beng Chin Ooi", "title":"CURLER: Finding and Visualizing Nonlinear Correlated Clusters", "venue":"SIGMOD Conference", "year":"2005", "window":"of three helix clusters with different cluster existence spaces, the iris plant dataset and the <b>image segmentation</b> dataset from the UCI Repository of Machine Learning Databases and Domain Theories [6], and the Iyer time series gene expression data with 10 well-known linear clusters", "mykey":2334},
 {"datasetID":74, "supportID":"EA68E5398250C08C78A55A82ABD76C9F70383FD4", "rexaID":"231251165e90c405798e8c43f9c47b62f795d45a", "author":"Zhi-Hua Zhou and Min-Ling Zhang", "title":"Solving Multi-Instance Problems with Classifier Ensemble Based on Constructive Clustering", "venue":"National Laboratory for Novel Software Technology", "year":"", "window":"c 2 are present in the bag. Therefore, it is obvious that Cce can be applied to generalized multi-instance problems without any modifica9 Table 2 The <b>Musk</b> data (72 molecules are shared in both data sets) Bags Instances per bag Data set Dim. Total Musk Non-musk Instances Min Max Ave. Musk1 166 92 47 45 476 2 40 5.17 Musk2 166 102 39 63 6,598 1 1,044 64.69 tion, which is a prominent advantage, while", "mykey":2335},
 {"datasetID":75, "supportID":"EA68E5398250C08C78A55A82ABD76C9F70383FD4", "rexaID":"231251165e90c405798e8c43f9c47b62f795d45a", "author":"Zhi-Hua Zhou and Min-Ling Zhang", "title":"Solving Multi-Instance Problems with Classifier Ensemble Based on Constructive Clustering", "venue":"National Laboratory for Novel Software Technology", "year":"", "window":"c 2 are present in the bag. Therefore, it is obvious that Cce can be applied to generalized multi-instance problems without any modifica9 Table 2 The <b>Musk</b> data (72 molecules are shared in both data sets) Bags Instances per bag Data set Dim. Total Musk Non-musk Instances Min Max Ave. Musk1 166 92 47 45 476 2 40 5.17 Musk2 166 102 39 63 6,598 1 1,044 64.69 tion, which is a prominent advantage, while", "mykey":2336},
 {"datasetID":42, "supportID":"EB0B4ADDF0A39E2D2F4AD6085ECE2C988149D212", "rexaID":"6858832ba8e9e4ac002900af151a8ffcc1796f8f", "author":"Stefan Aeberhard and Danny Coomans and De Vel", "title":"THE PERFORMANCE OF STATISTICAL PATTERN RECOGNITION METHODS IN HIGH DIMENSIONAL SETTINGS", "venue":"James Cook University", "year":"", "window":"resonant frequency easily obtainable using NMR, constituting the 19 variables measured. With 19 dimensions and 13 training samples per class, this problem is ill-posed. <b>Glass</b> Types Data This data set is from [15]. It summarises a chemical analysis done on two types (classes) of glass. Glass which was float-processed and such which was not. The data is ten dimensional and well-posed, with 87", "mykey":2337},
 {"datasetID":53, "supportID":"EB0B4ADDF0A39E2D2F4AD6085ECE2C988149D212", "rexaID":"6858832ba8e9e4ac002900af151a8ffcc1796f8f", "author":"Stefan Aeberhard and Danny Coomans and De Vel", "title":"THE PERFORMANCE OF STATISTICAL PATTERN RECOGNITION METHODS IN HIGH DIMENSIONAL SETTINGS", "venue":"James Cook University", "year":"", "window":"means coincide. FDP performed very well for the exponential data. The results of the real data support the observations made from the simulations. FDP does not perform very well on well-defined data sets (wine data, <b>Iris</b> data), especially when compared to FF. It however compares somewhat better in the other cases, most noticeably in the case of the tertiary institutions data, where it equals the", "mykey":2338},
 {"datasetID":141, "supportID":"EB0B4ADDF0A39E2D2F4AD6085ECE2C988149D212", "rexaID":"6858832ba8e9e4ac002900af151a8ffcc1796f8f", "author":"Stefan Aeberhard and Danny Coomans and De Vel", "title":"THE PERFORMANCE OF STATISTICAL PATTERN RECOGNITION METHODS IN HIGH DIMENSIONAL SETTINGS", "venue":"James Cook University", "year":"", "window":"RDA, and only so in the case of identical class covariance matrices with many training objects. RDA has otherwise outperformed all other methods except for some of the exponential data. A method 14 Data Set QDA LDA RDA 1NN FDP FF FR FV <b>UNIX</b> Commands 11.5% 21.3% 70.5% 62.3% (1) 57.4% 9.4% 14.7% 50.8% Wine Data 99.4% 98.9% 100% 91.0% (7) 88.2% 99.4% 93.8% 74.1% Tert. Institutions 37.5% 31.2% 84.4% 75.0%", "mykey":2339},
 {"datasetID":109, "supportID":"EB0B4ADDF0A39E2D2F4AD6085ECE2C988149D212", "rexaID":"6858832ba8e9e4ac002900af151a8ffcc1796f8f", "author":"Stefan Aeberhard and Danny Coomans and De Vel", "title":"THE PERFORMANCE OF STATISTICAL PATTERN RECOGNITION METHODS IN HIGH DIMENSIONAL SETTINGS", "venue":"James Cook University", "year":"", "window":"means coincide. FDP performed very well for the exponential data. The results of the real data support the observations made from the simulations. FDP does not perform very well on well-defined data sets  <b>wine</b> data, Iris data), especially when compared to FF. It however compares somewhat better in the other cases, most noticeably in the case of the tertiary institutions data, where it equals the", "mykey":2340},
 {"datasetID":20, "supportID":"EB886C3277758C81CA99A2F5804332013CE1377C", "rexaID":"7bea464dce753e6523458d22de98a96004c1aac8", "author":"James Bailey and Thomas Manoukian and Kotagiri Ramamohanarao", "title":"Fast Algorithms for Mining Emerging Patterns", "venue":"PKDD", "year":"2002", "window":"70 75 80 85 90 95 100 4 5 6 7 8 9 10 Accuracy Threshold <b>census</b> (Ratio-Tree) threshold complete 0 50 100 150 200 4 5 6 7 8 9 10 User Time (sec) Threshold census (Ratio-Tree) threshold complete Dataset pt=4 pt=5 pt=6 pt=7 pt=8 pt=9 pt=10 original mushroom 6.03 6.11 6.28 6.48 6.82 7.38 8.19 138.45 census 16.23 17.46 20.78 27.75 40.61 61.71 91.75 1028.00 ionosphere 1.37 1.43 1.45 1.56 1.67 1.83 1.99", "mykey":2341},
 {"datasetID":23, "supportID":"EB886C3277758C81CA99A2F5804332013CE1377C", "rexaID":"7bea464dce753e6523458d22de98a96004c1aac8", "author":"James Bailey and Thomas Manoukian and Kotagiri Ramamohanarao", "title":"Fast Algorithms for Mining Emerging Patterns", "venue":"PKDD", "year":"2002", "window":"(Vehicle, Waveform and Letter-recognition) though. Analysis of the vehicle and <b>chess</b> datasets aid in explaining this outcome (supporting figures have been excluded due to lack of space). It is clear that classification accuracy is dependent upon finding patterns that strongly discriminate", "mykey":2342},
 {"datasetID":21, "supportID":"EB886C3277758C81CA99A2F5804332013CE1377C", "rexaID":"7bea464dce753e6523458d22de98a96004c1aac8", "author":"James Bailey and Thomas Manoukian and Kotagiri Ramamohanarao", "title":"Fast Algorithms for Mining Emerging Patterns", "venue":"PKDD", "year":"2002", "window":"(Vehicle, Waveform and Letter-recognition) though. Analysis of the vehicle and <b>chess</b> datasets aid in explaining this outcome (supporting figures have been excluded due to lack of space). It is clear that classification accuracy is dependent upon finding patterns that strongly discriminate", "mykey":2343},
 {"datasetID":22, "supportID":"EB886C3277758C81CA99A2F5804332013CE1377C", "rexaID":"7bea464dce753e6523458d22de98a96004c1aac8", "author":"James Bailey and Thomas Manoukian and Kotagiri Ramamohanarao", "title":"Fast Algorithms for Mining Emerging Patterns", "venue":"PKDD", "year":"2002", "window":"(Vehicle, Waveform and Letter-recognition) though. Analysis of the vehicle and <b>chess</b> datasets aid in explaining this outcome (supporting figures have been excluded due to lack of space). It is clear that classification accuracy is dependent upon finding patterns that strongly discriminate", "mykey":2344},
 {"datasetID":149, "supportID":"EB886C3277758C81CA99A2F5804332013CE1377C", "rexaID":"7bea464dce753e6523458d22de98a96004c1aac8", "author":"James Bailey and Thomas Manoukian and Kotagiri Ramamohanarao", "title":"Fast Algorithms for Mining Emerging Patterns", "venue":"PKDD", "year":"2002", "window":"using thresholds. We see that mining with a threshold value of 4 is substantially faster than mining the complete set of JEPs using a ratio tree. Classification accuracy is degraded for three of the datasets  <b>Vehicle</b>  Waveform and Letter-recognition) though. Analysis of the vehicle and chess datasets aid in explaining this outcome (supporting figures have been excluded due to lack of space). It is", "mykey":2345},
 {"datasetID":107, "supportID":"EB886C3277758C81CA99A2F5804332013CE1377C", "rexaID":"7bea464dce753e6523458d22de98a96004c1aac8", "author":"James Bailey and Thomas Manoukian and Kotagiri Ramamohanarao", "title":"Fast Algorithms for Mining Emerging Patterns", "venue":"PKDD", "year":"2002", "window":"using thresholds. We see that mining with a threshold value of 4 is substantially faster than mining the complete set of JEPs using a ratio tree. Classification accuracy is degraded for three of the datasets (Vehicle, <b>Waveform</b> and Letter-recognition) though. Analysis of the vehicle and chess datasets aid in explaining this outcome (supporting figures have been excluded due to lack of space). It is", "mykey":2346},
 {"datasetID":108, "supportID":"EB886C3277758C81CA99A2F5804332013CE1377C", "rexaID":"7bea464dce753e6523458d22de98a96004c1aac8", "author":"James Bailey and Thomas Manoukian and Kotagiri Ramamohanarao", "title":"Fast Algorithms for Mining Emerging Patterns", "venue":"PKDD", "year":"2002", "window":"using thresholds. We see that mining with a threshold value of 4 is substantially faster than mining the complete set of JEPs using a ratio tree. Classification accuracy is degraded for three of the datasets (Vehicle, <b>Waveform</b> and Letter-recognition) though. Analysis of the vehicle and chess datasets aid in explaining this outcome (supporting figures have been excluded due to lack of space). It is", "mykey":2347},
 {"datasetID":57, "supportID":"EBA2B9A41FE064128CC2CB3DE09EA3725F234841", "rexaID":"6391eca1c5d98b9fb8d601a26ec8f0bf382fd009", "author":"Tim Leunig and D. Stott Parker", "title":"Empirical comparisons of various voting methods in bagging", "venue":"KDD", "year":"2003", "window":"that involve more than two classes. Voting methods that consider preference among classes can therefore gain an advantage over methods that do not, such as plurality. 3.4 Noise matters The <b>led datasets</b> in Table 3, for example, shows great sensitivity of the voting methods to noise. The amount of noise in led datasets increases from 0% to 50%. One somewhat surprising outcome of the experiments,", "mykey":2348},
 {"datasetID":73, "supportID":"EBA66836EA15BF1E730C8E80671D9ACF89C32EF0", "rexaID":"ccd4e75ad23df528a465f79fc83a186aa23ebebe", "author":"Huan Liu and Hongjun Lu and Jie Yao", "title":"Toward Multidatabase Mining: Identifying Relevant Databases", "venue":"IEEE Trans. Knowl. Data Eng, 13", "year":"2001", "window":"(Table 6). We ran C4.5rules [27] and confirmed that classification rules for \benign\" cases contain attributes 2, 3, 5, 7 and 9 from all the three data sets. Results of the <b>Mushroom</b> data: This data has 22 attributes. Relief ranks importance order of attributes as: 5, 20, 11, 8, 19, 4, 10, 22, 9, 12, 13, 21, 7, 3, 2, 15, 14, 18, 6, 17, 16, 1. We divide", "mykey":2349},
 {"datasetID":69, "supportID":"EBC7741BAF38DF5AB2EDE2BA54FD56B780120025", "rexaID":"2b8c2489b0b9622471466385b6d5c1cf703bd4c8", "author":"Kagan Tumer and Nikunj C. Oza", "title":"Decimated Input Ensembles for Improved Generalization", "venue":"NASA Ames Research Center", "year":"1999", "window":"improvement in the classification accuracy through ensembles. For the Gene data, the average combiner was significantly more accurate than the single MLP, while for the Satellite Image and <b>Splice</b> data sets, the combiner was only marginally more accurate. TABLE I Average Accuracy of Original Network and Combiners Single Average Corr. Gene 83.417 Sigma .796 86.418 Sigma .342 .7910 Splice 84.722", "mykey":2350},
 {"datasetID":146, "supportID":"EBC7741BAF38DF5AB2EDE2BA54FD56B780120025", "rexaID":"2b8c2489b0b9622471466385b6d5c1cf703bd4c8", "author":"Kagan Tumer and Nikunj C. Oza", "title":"Decimated Input Ensembles for Improved Generalization", "venue":"NASA Ames Research Center", "year":"1999", "window":"improvement in the classification accuracy through ensembles. For the Gene data, the average combiner was significantly more accurate than the single MLP, while for the <b>Satellite</b> Image and Splice data sets, the combiner was only marginally more accurate. TABLE I Average Accuracy of Original Network and Combiners Single Average Corr. Gene 83.417 Sigma .796 86.418 Sigma .342 .7910 Splice 84.722", "mykey":2351},
 {"datasetID":1, "supportID":"EBE8FE3BA19E7B0B4D2EFC42149FFC918C8223CF", "rexaID":"b6c6894db1f52fee42b5995fe50fc8c8f7e13fcc", "author":"Marko Robnik-Sikonja and Igor Kononenko", "title":"Pruning Regression Trees with MDL", "venue":"ECAI", "year":"1998", "window":"each consisting of 10 attributes - 2, 3 or 4 important, the rest are random, and containing 1000 examples. UCI datasets used were: <b>Abalone</b>  predicting the age of the abalone, 1 nominal and 7 continuous attributes, 4177 instances. Autompg: city-cycle fuel consumption, 1 nominal, 6 continuous attributes 398 instances.", "mykey":2352},
 {"datasetID":3, "supportID":"EC1555AAA52CBB081F4D6BDDD6029601A2F18584", "rexaID":"c042581c25e66281bb5ce382f70738b0233e5f5a", "author":"Zhi-Hua Zhou and Xu-Ying Liu", "title":"Training Cost-Sensitive Neural Networks with Methods Addressing the Class Imbalance Problem", "venue":"", "year":"", "window":"is apparently worse than that of sole BP. Table X and Fig. 4 also show that threshold-moving is always effective, and soft-ensemble only causes negative effect on the most seriously imbalanced data set <b>annealing</b>  SMOTE and hardensemble cause negative effect on soybean and annealing. It is noteworthy that the sampling methods cause negative effect on almost all data sets suffering from class", "mykey":2353},
 {"datasetID":38, "supportID":"EC1555AAA52CBB081F4D6BDDD6029601A2F18584", "rexaID":"c042581c25e66281bb5ce382f70738b0233e5f5a", "author":"Zhi-Hua Zhou and Xu-Ying Liu", "title":"Training Cost-Sensitive Neural Networks with Methods Addressing the Class Imbalance Problem", "venue":"", "year":"", "window":"can be used after the IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 12 TABLE XIII AVERAGE Qav VALUES OF THE LEARNERS GENERATED BY OVER-SAMPLING, UNDER-SAMPLING, AND THRESHOLD-MOVING. Two-class data set Qav Qav <b>echocardiogram</b> .779 \u00b1 .135 Multi-class data set Cost (a) Cost (b) Cost (c) hepatitis .552 \u00b1 .201 lymphography .365 \u00b1 .092 .375 \u00b1 .170 .308 \u00b1 .142 heart s .774 \u00b1 .083 glass .615 \u00b1 .108 .615 \u00b1", "mykey":2354},
 {"datasetID":42, "supportID":"EC1555AAA52CBB081F4D6BDDD6029601A2F18584", "rexaID":"c042581c25e66281bb5ce382f70738b0233e5f5a", "author":"Zhi-Hua Zhou and Xu-Ying Liu", "title":"Training Cost-Sensitive Neural Networks with Methods Addressing the Class Imbalance Problem", "venue":"", "year":"", "window":"effect on soybean which is with the biggest number of classes and suffering from serious class imbalance. It is noteworthy that the sampling methods and SMOTE cause negative effect on several data sets suffering from class imbalance, that is, <b>glass</b>  soybean and annealing. IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 9 TABLE IX EXPERIMENTAL RESULTS ON MULTI-CLASS UCI DATA SETS WITH TYPE (A)", "mykey":2355},
 {"datasetID":45, "supportID":"EC1555AAA52CBB081F4D6BDDD6029601A2F18584", "rexaID":"c042581c25e66281bb5ce382f70738b0233e5f5a", "author":"Zhi-Hua Zhou and Xu-Ying Liu", "title":"Training Cost-Sensitive Neural Networks with Methods Addressing the Class Imbalance Problem", "venue":"", "year":"", "window":"0.85. On euthyroid, threshold-moving is the best, under-sampling is the worst in the effective range, while the ensemble methods become poor when PCF(+) is bigger than 0.8. On the remaining nine data sets all the methods work well. On <b>heart</b> s the ensemble methods are slightly better than others. On heart the ensemble methods are apparently better than over-sampling, under-sampling, and", "mykey":2356},
 {"datasetID":46, "supportID":"EC1555AAA52CBB081F4D6BDDD6029601A2F18584", "rexaID":"c042581c25e66281bb5ce382f70738b0233e5f5a", "author":"Zhi-Hua Zhou and Xu-Ying Liu", "title":"Training Cost-Sensitive Neural Networks with Methods Addressing the Class Imbalance Problem", "venue":"", "year":"", "window":"Qav Qav echocardiogram .779 \u00b1 .135 Multi-class data set Cost (a) Cost (b) Cost (c) <b>hepatitis</b> .552 \u00b1 .201 lymphography .365 \u00b1 .092 .375 \u00b1 .170 .308 \u00b1 .142 heart s .774 \u00b1 .083 glass .615 \u00b1 .108 .615 \u00b1 .128 .638 \u00b1 .134 heart .790 \u00b1 .092 waveform .815 \u00b1 .037", "mykey":2357},
 {"datasetID":90, "supportID":"EC1555AAA52CBB081F4D6BDDD6029601A2F18584", "rexaID":"c042581c25e66281bb5ce382f70738b0233e5f5a", "author":"Zhi-Hua Zhou and Xu-Ying Liu", "title":"Training Cost-Sensitive Neural Networks with Methods Addressing the Class Imbalance Problem", "venue":"", "year":"", "window":"annealing, hardensemble causes negative effect on one more data set, i.e. <b>soybean</b>  Threshold-moving does not cause negative effect on glass, but it causes negative effect on lymphography and vowel. The sampling methods and SMOTE cause negative effect on more than", "mykey":2358},
 {"datasetID":91, "supportID":"EC1555AAA52CBB081F4D6BDDD6029601A2F18584", "rexaID":"c042581c25e66281bb5ce382f70738b0233e5f5a", "author":"Zhi-Hua Zhou and Xu-Ying Liu", "title":"Training Cost-Sensitive Neural Networks with Methods Addressing the Class Imbalance Problem", "venue":"", "year":"", "window":"annealing, hardensemble causes negative effect on one more data set, i.e. <b>soybean</b>  Threshold-moving does not cause negative effect on glass, but it causes negative effect on lymphography and vowel. The sampling methods and SMOTE cause negative effect on more than", "mykey":2359},
 {"datasetID":107, "supportID":"EC1555AAA52CBB081F4D6BDDD6029601A2F18584", "rexaID":"c042581c25e66281bb5ce382f70738b0233e5f5a", "author":"Zhi-Hua Zhou and Xu-Ying Liu", "title":"Training Cost-Sensitive Neural Networks with Methods Addressing the Class Imbalance Problem", "venue":"", "year":"", "window":"(b) Type (c) j j j 1 2 3 1 2 3 1 2 3 1 0 1 8 1 0 3 3 1 0 3 6 i 2 1 0 9 2 1 0 1 2 3 0 1 3 1 1 0 3 6 6 0 3 4 5 0 Under each type of cost matrix, 10 times 10-fold cross validation are performed on each data set except on <b>waveform</b> where randomly generated training data size of 300 and test data size of 5000 are used in 100 trials, which is the way this data set has been used in some other cost-sensitive", "mykey":2360},
 {"datasetID":108, "supportID":"EC1555AAA52CBB081F4D6BDDD6029601A2F18584", "rexaID":"c042581c25e66281bb5ce382f70738b0233e5f5a", "author":"Zhi-Hua Zhou and Xu-Ying Liu", "title":"Training Cost-Sensitive Neural Networks with Methods Addressing the Class Imbalance Problem", "venue":"", "year":"", "window":"(b) Type (c) j j j 1 2 3 1 2 3 1 2 3 1 0 1 8 1 0 3 3 1 0 3 6 i 2 1 0 9 2 1 0 1 2 3 0 1 3 1 1 0 3 6 6 0 3 4 5 0 Under each type of cost matrix, 10 times 10-fold cross validation are performed on each data set except on <b>waveform</b> where randomly generated training data size of 300 and test data size of 5000 are used in 100 trials, which is the way this data set has been used in some other cost-sensitive", "mykey":2361},
 {"datasetID":34, "supportID":"EC3D99BDC4D6E730A63B5485F2A6F6F884F26D00", "rexaID":"ded3146242d6b322f31afbf57a8afde22e4ec8e1", "author":"Ilya Blayvas and Ron Kimmel", "title":"Multiresolution Approximation for Classification", "venue":"CS Dept. Technion", "year":"2002", "window":"\u00b7 D). 3 Experimental Results The proposed method was implemented in VC++ 6.0 and run on `IBM PC 300 PL' with 600MHZ Pentium III processor and 256MB RAM. It was tested on the Pima Indians <b>Diabetes</b> dataset [10], and a large artificial dataset generated with the DatGen program [11]. The results were compared to the Smooth SVM [12] and Sparse Grids [3]. 3.1 Pima Indians The Pima Indians Diabetes", "mykey":2362},
 {"datasetID":79, "supportID":"EC3D99BDC4D6E730A63B5485F2A6F6F884F26D00", "rexaID":"ded3146242d6b322f31afbf57a8afde22e4ec8e1", "author":"Ilya Blayvas and Ron Kimmel", "title":"Multiresolution Approximation for Classification", "venue":"CS Dept. Technion", "year":"2002", "window":"\u00b7 D). 3 Experimental Results The proposed method was implemented in VC++ 6.0 and run on `IBM PC 300 PL' with 600MHZ Pentium III processor and 256MB RAM. It was tested on the <b>Pima</b> <b>Indians</b> <b>Diabetes</b> dataset [10], and a large artificial dataset generated with the DatGen program [11]. The results were compared to the Smooth SVM [12] and Sparse Grids [3]. 3.1 Pima Indians The Pima Indians Diabetes", "mykey":2363},
 {"datasetID":59, "supportID":"EC7FEEF4845B232307D7A88A8B760D1D4F5536FB", "rexaID":"7ee121b26e79c0a7aa5c80979e275fc3592d0638", "author":"Giorgio Valentini", "title":"Ensemble methods based on bias--variance analysis Theses Series DISI-TH-2003", "venue":"Dipartimento di Informatica e Scienze dell'Informazione ", "year":"2003", "window":"from UCI: we consider here only letter B versus letter R, taken from the <b>letter recognition</b> data set. The 16 attributes are integer values that refer to di\u00aeerent features of the letters. We used also a version of Letter-Two with 20 % added classification noise (Letter-Two with added noise data", "mykey":2364},
 {"datasetID":67, "supportID":"EC7FEEF4845B232307D7A88A8B760D1D4F5536FB", "rexaID":"7ee121b26e79c0a7aa5c80979e275fc3592d0638", "author":"Giorgio Valentini", "title":"Ensemble methods based on bias--variance analysis Theses Series DISI-TH-2003", "venue":"Dipartimento di Informatica e Scienze dell'Informazione ", "year":"2003", "window":"of MLP, as well as ensemble methods based on resampling techniques, such as bagging and boosting, have been applied to the analysis of <b>DNA</b> microarray data [192, 158, 54, 178, 185]. 141 6.5.1 Data set and experimental set-up. We used DNA microarray data available on-line. In particular we used the GCM data set obtained from the Whitehead Institute, Massachusetts Institute of Technology Center for", "mykey":2365},
 {"datasetID":74, "supportID":"EC7FEEF4845B232307D7A88A8B760D1D4F5536FB", "rexaID":"7ee121b26e79c0a7aa5c80979e275fc3592d0638", "author":"Giorgio Valentini", "title":"Ensemble methods based on bias--variance analysis Theses Series DISI-TH-2003", "venue":"Dipartimento di Informatica e Scienze dell'Informazione ", "year":"2003", "window":"with 4601 instances and 57 continuous attributes. 4.1.1.6 <b>Musk</b> The dataset (available from UCI) describes a set of 102 molecules of which 39 are judged by human experts to be musks and the remaining 63 molecules are judged to be nonmusks. The 166 features that describe", "mykey":2366},
 {"datasetID":75, "supportID":"EC7FEEF4845B232307D7A88A8B760D1D4F5536FB", "rexaID":"7ee121b26e79c0a7aa5c80979e275fc3592d0638", "author":"Giorgio Valentini", "title":"Ensemble methods based on bias--variance analysis Theses Series DISI-TH-2003", "venue":"Dipartimento di Informatica e Scienze dell'Informazione ", "year":"2003", "window":"with 4601 instances and 57 continuous attributes. 4.1.1.6 <b>Musk</b> The dataset (available from UCI) describes a set of 102 molecules of which 39 are judged by human experts to be musks and the remaining 63 molecules are judged to be nonmusks. The 166 features that describe", "mykey":2367},
 {"datasetID":146, "supportID":"EC7FEEF4845B232307D7A88A8B760D1D4F5536FB", "rexaID":"7ee121b26e79c0a7aa5c80979e275fc3592d0638", "author":"Giorgio Valentini", "title":"Ensemble methods based on bias--variance analysis Theses Series DISI-TH-2003", "venue":"Dipartimento di Informatica e Scienze dell'Informazione ", "year":"2003", "window":"analysis with single SVMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 4.3 Procedure to perform bias--variance analysis on single SVMs . . . . . . . . 48 4.4 Grey <b>Landsat</b> data set. Error (a) and its decomposition in bias (b), net variance (c), unbiased variance (d), and biased variance (e) in SVM RBF, varying both C and \u00be. . . . . . . . . . . . . . . . . . . . . . . . . . . .", "mykey":2368},
 {"datasetID":107, "supportID":"EC7FEEF4845B232307D7A88A8B760D1D4F5536FB", "rexaID":"7ee121b26e79c0a7aa5c80979e275fc3592d0638", "author":"Giorgio Valentini", "title":"Ensemble methods based on bias--variance analysis Theses Series DISI-TH-2003", "venue":"Dipartimento di Informatica e Scienze dell'Informazione ", "year":"2003", "window":"polynomial degrees: (a) degree = 2, (b) degree = 3, (c) degree = 5, (d) degree = 10 . . . . . . . . . . . . . . . . . . . . . . . . . . 67 4.17 Bias in polynomial SVMs with (a) <b>Waveform</b> and (b) Spam data sets, varying both C and polynomial degree. . . . . . . . . . . . . . . . . . . . . . . 68 4.18 Bias-variance decomposition of error in bias, net variance, unbiased and biased variance in polynomial", "mykey":2369},
 {"datasetID":108, "supportID":"EC7FEEF4845B232307D7A88A8B760D1D4F5536FB", "rexaID":"7ee121b26e79c0a7aa5c80979e275fc3592d0638", "author":"Giorgio Valentini", "title":"Ensemble methods based on bias--variance analysis Theses Series DISI-TH-2003", "venue":"Dipartimento di Informatica e Scienze dell'Informazione ", "year":"2003", "window":"polynomial degrees: (a) degree = 2, (b) degree = 3, (c) degree = 5, (d) degree = 10 . . . . . . . . . . . . . . . . . . . . . . . . . . 67 4.17 Bias in polynomial SVMs with (a) <b>Waveform</b> and (b) Spam data sets, varying both C and polynomial degree. . . . . . . . . . . . . . . . . . . . . . . 68 4.18 Bias-variance decomposition of error in bias, net variance, unbiased and biased variance in polynomial", "mykey":2370},
 {"datasetID":2, "supportID":"EC9570BD04915D03CA6306B11BAE55C2CBE731DE", "rexaID":"f1395e1da4a724219d6cc414e48969140355bebb", "author":"David R. Musicant", "title":"DATA MINING VIA MATHEMATICAL PROGRAMMING AND MACHINE LEARNING", "venue":"Doctor of Philosophy (Computer Sciences) UNIVERSITY", "year":"", "window":"separability on SOR performance . . . . . . . . . . . . . 34 3.2 SOR, SMO, and SVM light comparison on the <b>Adult</b> dataset in R 123 . . . 36 3.3 SOR and LPC comparison on 1 million point dataset in R 32 . . . . . . . 38 3.4 SOR applied to 10 million point dataset in R 32 . . . . . . . . . . . . . . . 38 4.1 SOR training", "mykey":2371},
 {"datasetID":20, "supportID":"EC9570BD04915D03CA6306B11BAE55C2CBE731DE", "rexaID":"f1395e1da4a724219d6cc414e48969140355bebb", "author":"David R. Musicant", "title":"DATA MINING VIA MATHEMATICAL PROGRAMMING AND MACHINE LEARNING", "venue":"Doctor of Philosophy (Computer Sciences) UNIVERSITY", "year":"", "window":"were used for testing the methods. The first dataset, <b>Census</b>  is a version of the US Census Bureau \"Adult\" dataset, which is publicly available from Silicon Graphics' website [15]. This dataset contains nearly 300,000 data points with 11 numeric", "mykey":2372},
 {"datasetID":48, "supportID":"EC9570BD04915D03CA6306B11BAE55C2CBE731DE", "rexaID":"f1395e1da4a724219d6cc414e48969140355bebb", "author":"David R. Musicant", "title":"DATA MINING VIA MATHEMATICAL PROGRAMMING AND MACHINE LEARNING", "venue":"Doctor of Philosophy (Computer Sciences) UNIVERSITY", "year":"", "window":"We implemented the \"cpuSmall prototask\", which involves using twelve of these attributes to predict what fraction of a CPU's processing time is devoted to a specific mode (\"user mode\"). The third dataset, Boston <b>Housing</b>  is a fairly standard dataset used for testing regression problems. It contains 506 data points with 12 numeric attributes, and one binary categorical attribute. 96 The goal is to", "mykey":2373},
 {"datasetID":60, "supportID":"EC9570BD04915D03CA6306B11BAE55C2CBE731DE", "rexaID":"f1395e1da4a724219d6cc414e48969140355bebb", "author":"David R. Musicant", "title":"DATA MINING VIA MATHEMATICAL PROGRAMMING AND MACHINE LEARNING", "venue":"Doctor of Philosophy (Computer Sciences) UNIVERSITY", "year":"", "window":"from the University of California at Irvine (UCI) repository [85]: . The <b>liver</b> disorders dataset contains 345 points, each consisting of six features. Class 1 contains 145 points, and class -1 contains 200 points. . The letter-recognition dataset is used for recognizing letters of the alphabet.", "mykey":2374},
 {"datasetID":73, "supportID":"EC9570BD04915D03CA6306B11BAE55C2CBE731DE", "rexaID":"f1395e1da4a724219d6cc414e48969140355bebb", "author":"David R. Musicant", "title":"DATA MINING VIA MATHEMATICAL PROGRAMMING AND MACHINE LEARNING", "venue":"Doctor of Philosophy (Computer Sciences) UNIVERSITY", "year":"", "window":"of 600 points with 6 features, where each class contained 300 points. . The <b>mushroom</b> dataset is a two class dataset which contains a number of categorical attributes. We transformed each categorical attribute into a series of binary attributes, one attribute for each distinct value. For", "mykey":2375},
 {"datasetID":101, "supportID":"EC9570BD04915D03CA6306B11BAE55C2CBE731DE", "rexaID":"f1395e1da4a724219d6cc414e48969140355bebb", "author":"David R. Musicant", "title":"DATA MINING VIA MATHEMATICAL PROGRAMMING AND MACHINE LEARNING", "venue":"Doctor of Philosophy (Computer Sciences) UNIVERSITY", "year":"", "window":"we used contained 22 features with 200 points in class 1 and 300 points in class -1. . The <b>tic-tac-toe</b> dataset is a two class dataset that contains legal complete tic-tac-toe games. All those games where \"X\" has a \"win\" (three in a row) end up in category 1, and all other games end up in category -1. We have", "mykey":2376},
 {"datasetID":116, "supportID":"EC9570BD04915D03CA6306B11BAE55C2CBE731DE", "rexaID":"f1395e1da4a724219d6cc414e48969140355bebb", "author":"David R. Musicant", "title":"DATA MINING VIA MATHEMATICAL PROGRAMMING AND MACHINE LEARNING", "venue":"Doctor of Philosophy (Computer Sciences) UNIVERSITY", "year":"", "window":"were used for testing the methods. The first dataset, Census, is a version of the <b>US Census</b> Bureau \"Adult\" dataset, which is publicly available from Silicon Graphics' website [15]. This dataset contains nearly 300,000 data points with 11 numeric", "mykey":2377},
 {"datasetID":14, "supportID":"ECADEB93378D7911C2F7B9BD83A8AF55D7FA9E06", "rexaID":"d254374dd5eab5d871c8010dcbe4ac84ec86ea8c", "author":"P. S and Bradley K. P and Bennett A. Demiriz", "title":"Constrained K-Means Clustering", "venue":"Microsoft Research Dept. of Mathematical Sciences One Microsoft Way Dept. of Decision Sciences and Eng. Sys", "year":"2000", "window":"the Johns Hopkins Ionosphere dataset and the Wisconsin Diagnostic <b>Breast</b> <b>Cancer</b> dataset (WDBC) [7]. The Ionosphere dataset contains 351 data points in R 33 and values along each dimension Contrained K-Means Clustering 6 0 5 10 15 20 25", "mykey":2378},
 {"datasetID":17, "supportID":"ECADEB93378D7911C2F7B9BD83A8AF55D7FA9E06", "rexaID":"d254374dd5eab5d871c8010dcbe4ac84ec86ea8c", "author":"P. S and Bradley K. P and Bennett A. Demiriz", "title":"Constrained K-Means Clustering", "venue":"Microsoft Research Dept. of Mathematical Sciences One Microsoft Way Dept. of Decision Sciences and Eng. Sys", "year":"2000", "window":"the Johns Hopkins Ionosphere dataset and the <b>Wisconsin</b> Diagnostic <b>Breast</b> <b>Cancer</b> dataset (WDBC) [7]. The Ionosphere dataset contains 351 data points in R 33 and values along each dimension Contrained K-Means Clustering 6 0 5 10 15 20 25", "mykey":2379},
 {"datasetID":15, "supportID":"ECADEB93378D7911C2F7B9BD83A8AF55D7FA9E06", "rexaID":"d254374dd5eab5d871c8010dcbe4ac84ec86ea8c", "author":"P. S and Bradley K. P and Bennett A. Demiriz", "title":"Constrained K-Means Clustering", "venue":"Microsoft Research Dept. of Mathematical Sciences One Microsoft Way Dept. of Decision Sciences and Eng. Sys", "year":"2000", "window":"the Johns Hopkins Ionosphere dataset and the <b>Wisconsin</b> Diagnostic <b>Breast</b> <b>Cancer</b> dataset (WDBC) [7]. The Ionosphere dataset contains 351 data points in R 33 and values along each dimension Contrained K-Means Clustering 6 0 5 10 15 20 25", "mykey":2380},
 {"datasetID":16, "supportID":"ECADEB93378D7911C2F7B9BD83A8AF55D7FA9E06", "rexaID":"d254374dd5eab5d871c8010dcbe4ac84ec86ea8c", "author":"P. S and Bradley K. P and Bennett A. Demiriz", "title":"Constrained K-Means Clustering", "venue":"Microsoft Research Dept. of Mathematical Sciences One Microsoft Way Dept. of Decision Sciences and Eng. Sys", "year":"2000", "window":"the Johns Hopkins Ionosphere dataset and the <b>Wisconsin</b> Diagnostic <b>Breast</b> <b>Cancer</b> dataset (WDBC) [7]. The Ionosphere dataset contains 351 data points in R 33 and values along each dimension Contrained K-Means Clustering 6 0 5 10 15 20 25", "mykey":2381},
 {"datasetID":52, "supportID":"ECADEB93378D7911C2F7B9BD83A8AF55D7FA9E06", "rexaID":"d254374dd5eab5d871c8010dcbe4ac84ec86ea8c", "author":"P. S and Bradley K. P and Bennett A. Demiriz", "title":"Constrained K-Means Clustering", "venue":"Microsoft Research Dept. of Mathematical Sciences One Microsoft Way Dept. of Decision Sciences and Eng. Sys", "year":"2000", "window":"tailored to network optimization [2]. These codes usually run 1 or 2 orders of magnitude faster than general linear programming (LP) codes. 4 Numerical Evaluation We report results using two real datasets: the Johns Hopkins <b>Ionosphere</b> dataset and the Wisconsin Diagnostic Breast Cancer dataset (WDBC) [7]. The Ionosphere dataset contains 351 data points in R 33 and values along each dimension", "mykey":2382},
 {"datasetID":151, "supportID":"ECED3D784E4F9A909B1AB737A9785000918155AE", "rexaID":"ba594780aee46c6bcb4af5f3dbb56027735edecf", "author":"Ayhan Demiriz and Kristin P. Bennett and Mark J. Embrechts", "title":"Semi-Supervised Clustering Using Genetic Algorithms", "venue":"Dept", "year":"1999", "window":"The transductive MSE+GINI method based on all available data showed no consistent improvements over the induc2 The (k, #, #) values applied for each dataset were bright (15, 0.01,0.99), <b>sonar</b> (7,0.1,1), heart (7,0.25,0.75), ionosphere (7, 0.01,0.99), house (7,0.1,0.9), housing (11,0.01, 0.99), prognosis (11,0.4,0.6), and pima (11,0.01,0.99). 13 tion", "mykey":2383},
 {"datasetID":48, "supportID":"ECED3D784E4F9A909B1AB737A9785000918155AE", "rexaID":"ba594780aee46c6bcb4af5f3dbb56027735edecf", "author":"Ayhan Demiriz and Kristin P. Bennett and Mark J. Embrechts", "title":"Semi-Supervised Clustering Using Genetic Algorithms", "venue":"Dept", "year":"1999", "window":"have all originally two-class output variable except <b>Housing</b>  The output variable for this dataset was categorized at the level of 21.5. Each dataset was divided into three subsets after a standard normalization. We call these subsets the learning, testing and working sets. Currently 40% of data", "mykey":2384},
 {"datasetID":12, "supportID":"ED12838CCE3C7BB971D91D92D58F71829F40C955", "rexaID":"14f025e969e3a0418fd852ee46e54039ab3f216a", "author":"Jianbin Tan and David L. Dowe", "title":"MML Inference of Decision Graphs with Multi-way Joins and Dynamic Attributes", "venue":"Australian Conference on Artificial Intelligence", "year":"2003", "window":"which are treated as unordered discrete attributes in our tests because data files from UCI have been set in this format. We performed 10 independent 10-fold cross-validations again on the data set. <b>balance</b> <b>scale</b> data set: The balance scale data set from the UCI repository [1] was generated to model psychological experimental results. There are 625 instances with 3 output classes in the set.", "mykey":2385},
 {"datasetID":19, "supportID":"ED12838CCE3C7BB971D91D92D58F71829F40C955", "rexaID":"14f025e969e3a0418fd852ee46e54039ab3f216a", "author":"Jianbin Tan and David L. Dowe", "title":"MML Inference of Decision Graphs with Multi-way Joins and Dynamic Attributes", "venue":"Australian Conference on Artificial Intelligence", "year":"2003", "window":"- giving a total 10x10=100 tests. <b>car</b> Evaluation data set: The <b>car evaluation</b> data set from the UCI repository [1] was generated from an underlying decision tree model. There are 1728 instances with four output classes in the set. Each data item has 6", "mykey":2386},
 {"datasetID":68, "supportID":"ED12838CCE3C7BB971D91D92D58F71829F40C955", "rexaID":"14f025e969e3a0418fd852ee46e54039ab3f216a", "author":"Jianbin Tan and David L. Dowe", "title":"MML Inference of Decision Graphs with Multi-way Joins and Dynamic Attributes", "venue":"Australian Conference on Artificial Intelligence", "year":"2003", "window":"decision graphs using MML [16, 19, 17]. The machine-learning technique of decision graphs was successfully applied to the inference of a theory of <b>protein secondary</b> structure from a particular dataset by Dowe et al. [4] (see Section 4.4). The resulting decision graphs provided both an explanation and a prediction method for the problem. However, the Oliver-Wallace coding scheme [12, 11] only", "mykey":2387},
 {"datasetID":70, "supportID":"ED12838CCE3C7BB971D91D92D58F71829F40C955", "rexaID":"14f025e969e3a0418fd852ee46e54039ab3f216a", "author":"Jianbin Tan and David L. Dowe", "title":"MML Inference of Decision Graphs with Multi-way Joins and Dynamic Attributes", "venue":"Australian Conference on Artificial Intelligence", "year":"2003", "window":"Each 10-fold cross-validation consists of 10 tests. In each test, we trained on nine-tenths of the data and tested on the remaining one-tenth. This amounted to 10x10=100 tests. 1st <b>monk</b> s data set: The 1st monk's data set is in the UCI machine learning repository [10, 1], and constructed from the noiseless function ( Jacket Color = Red ) V ( Head Shape = body Shape ) 10 independent tests were", "mykey":2388},
 {"datasetID":154, "supportID":"ED12838CCE3C7BB971D91D92D58F71829F40C955", "rexaID":"14f025e969e3a0418fd852ee46e54039ab3f216a", "author":"Jianbin Tan and David L. Dowe", "title":"MML Inference of Decision Graphs with Multi-way Joins and Dynamic Attributes", "venue":"Australian Conference on Artificial Intelligence", "year":"2003", "window":"decision graphs using MML [16, 19, 17]. The machine-learning technique of decision graphs was successfully applied to the inference of a theory of <b>protein</b> secondary structure from a particular dataset by Dowe et al. [4] (see Section 4.4). The resulting decision graphs provided both an explanation and a prediction method for the problem. However, the Oliver-Wallace coding scheme [12, 11] only", "mykey":2389},
 {"datasetID":90, "supportID":"ED4A70511B5B2A2D714CC20F0382EEF09CE3EDDD", "rexaID":"01cf09d5e240fbb8da88b4989dfbffece6153160", "author":"Hendrik Blockeel and Luc De Raedt and Jan Ramon", "title":"Top-Down Induction of Clustering Trees", "venue":"ICML", "year":"1998", "window":"those obtained with the supervised learner Tilde. We see that TIC obtains high accuracies for these problems. The only clustering result we know of is for COBWEB, which obtained 100% on the <b>Soybean</b> data set. This difference is not significant. Tilde's ac72 73 74 75 76 77 78 79 80 81 15 20 25 30 35 40 45 50 55 accuracy (%) size validation set (%) accuracy of pruned tree accuracy of unpruned tree 15 20", "mykey":2390},
 {"datasetID":91, "supportID":"ED4A70511B5B2A2D714CC20F0382EEF09CE3EDDD", "rexaID":"01cf09d5e240fbb8da88b4989dfbffece6153160", "author":"Hendrik Blockeel and Luc De Raedt and Jan Ramon", "title":"Top-Down Induction of Clustering Trees", "venue":"ICML", "year":"1998", "window":"those obtained with the supervised learner Tilde. We see that TIC obtains high accuracies for these problems. The only clustering result we know of is for COBWEB, which obtained 100% on the <b>Soybean</b> data set. This difference is not significant. Tilde's ac72 73 74 75 76 77 78 79 80 81 15 20 25 30 35 40 45 50 55 accuracy (%) size validation set (%) accuracy of pruned tree accuracy of unpruned tree 15 20", "mykey":2391},
 {"datasetID":53, "supportID":"ED98837DE13155904405120B9407B6C6339D173C", "rexaID":"b6fb41e86105131ea2ce769929b199c003161616", "author":"Daichi Mochihashi and Gen-ichiro Kikui and Kenji Kita", "title":"Learning Nonstructural Distance Metric by Minimum Cluster Distortions", "venue":"ATR Spoken Language Translation research laboratories", "year":"", "window":"0 . 7 0 . 8 0 . 9 1 1 2 3 4 D i m e n s i o n P r e c i s i o n (c)  <b>iris</b>  dataset 0 . 6 0 . 7 0 . 8 0 . 9 1 1 2 5 1 0 2 0 3 5 D i m e n s i o n P r e c i s i o n (d) \"soybean\" dataset Figure 4: K-means clustering of UCI Machine Learning dataset results. The horizontal axis shows", "mykey":2392},
 {"datasetID":154, "supportID":"ED98837DE13155904405120B9407B6C6339D173C", "rexaID":"b6fb41e86105131ea2ce769929b199c003161616", "author":"Daichi Mochihashi and Gen-ichiro Kikui and Kenji Kita", "title":"Learning Nonstructural Distance Metric by Minimum Cluster Distortions", "venue":"ATR Spoken Language Translation research laboratories", "year":"", "window":"0 . 6 0 . 7 0 . 8 0 . 9 1 2 5 1 0 1 5 2 0 D i m e n s i o n P r e c i s i o n (b)  <b>protein</b>  dataset 0 . 7 0 . 8 0 . 9 1 1 2 3 4 D i m e n s i o n P r e c i s i o n (c) \"iris\" dataset 0 . 6 0 . 7 0 . 8 0 . 9 1 1 2 5 1 0 2 0 3 5 D i m e n s i o n P r e c i s i o n (d) \"soybean\" dataset Figure 4:", "mykey":2393},
 {"datasetID":90, "supportID":"ED98837DE13155904405120B9407B6C6339D173C", "rexaID":"b6fb41e86105131ea2ce769929b199c003161616", "author":"Daichi Mochihashi and Gen-ichiro Kikui and Kenji Kita", "title":"Learning Nonstructural Distance Metric by Minimum Cluster Distortions", "venue":"ATR Spoken Language Translation research laboratories", "year":"", "window":"0 . 6 0 . 7 0 . 8 0 . 9 1 1 2 5 1 0 2 0 3 5 D i m e n s i o n P r e c i s i o n (d)  <b>soybean</b>  dataset Figure 4: K-means clustering of UCI Machine Learning dataset results. The horizontal axis shows compressed dimensions (rightmost is original). The right bar shows clustering precision using Metric", "mykey":2394},
 {"datasetID":91, "supportID":"ED98837DE13155904405120B9407B6C6339D173C", "rexaID":"b6fb41e86105131ea2ce769929b199c003161616", "author":"Daichi Mochihashi and Gen-ichiro Kikui and Kenji Kita", "title":"Learning Nonstructural Distance Metric by Minimum Cluster Distortions", "venue":"ATR Spoken Language Translation research laboratories", "year":"", "window":"0 . 6 0 . 7 0 . 8 0 . 9 1 1 2 5 1 0 2 0 3 5 D i m e n s i o n P r e c i s i o n (d)  <b>soybean</b>  dataset Figure 4: K-means clustering of UCI Machine Learning dataset results. The horizontal axis shows compressed dimensions (rightmost is original). The right bar shows clustering precision using Metric", "mykey":2395},
 {"datasetID":109, "supportID":"ED98837DE13155904405120B9407B6C6339D173C", "rexaID":"b6fb41e86105131ea2ce769929b199c003161616", "author":"Daichi Mochihashi and Gen-ichiro Kikui and Kenji Kita", "title":"Learning Nonstructural Distance Metric by Minimum Cluster Distortions", "venue":"ATR Spoken Language Translation research laboratories", "year":"", "window":"to get a small increase in precision like the document retrieval experiment in section 5.2. 0 . 6 0 . 7 0 . 8 0 . 9 1 1 2 5 1 0 1 3 D i m e n s i o n P r e c i s i o n (a)  <b>wine</b>  dataset 0 . 6 0 . 7 0 . 8 0 . 9 1 2 5 1 0 1 5 2 0 D i m e n s i o n P r e c i s i o n (b) \"protein\" dataset 0 . 7 0 . 8 0 . 9 1 1 2 3 4 D i m e n s i o n P r e c i s i o n (c) \"iris\" dataset 0 . 6 0 . 7 0 .", "mykey":2396},
 {"datasetID":45, "supportID":"EDA337F431DA9D334580420F9F38DD444A83B307", "rexaID":"afd9cd3c83e5c787496540afd969b048fc9e05b3", "author":"Yuan Jiang Zhi and Hua Zhou and Zhaoqian Chen", "title":"Rule Learning based on Neural Network Ensemble", "venue":"Proceedings of the International Joint Conference on Neural Networks", "year":"2002", "window":"greatly offsets its weakness in the conciseness of the generated rule sets. A typical rule set generated by the proposed algorithm is shown in Table 3, which is obtained from one run on the data set <b>Heart</b> disease. IV. CONCLUSIONS In this paper, we propose a novel rule learning algorithm that employs neural network ensemble as front-end process. The algorithm trains a neural network ensemble at", "mykey":2397},
 {"datasetID":2, "supportID":"EDD363946C4004C5ED90EDDD449FC364AD488A13", "rexaID":"c7d3013a5143009f65d4303e65d8ecfcc892fa2e", "author":"Haixun Wang and Philip S. Yu", "title":"SSDT-NN: A Subspace-Splitting Decision Tree Classifier with Application to Target Selection", "venue":"IBM T. J. Watson Research Center", "year":"", "window":"that have biased data distribution. We use 25% of the cases for training and the rest 75% for testing. The results in shown in Table 3. 19 Datasets Dataset Size Bias SPRINT SSDT-NN <b>Adult</b> 32561 0.24 68 72.1 Anneal 5 798 0.08 100 100 Anneal U 798 0.04 100 100 breast-cancer 466 0.37 76.9 89.2 Vehicle bus 846 0.23 43.3 68.2 Sick 3770 0.06 72.3", "mykey":2398},
 {"datasetID":14, "supportID":"EE042EB33F4320908C19EF5C8E4863C4F84EFFF9", "rexaID":"22dbe26a460522ada68b637b8a3483c717b671fa", "author":"Maria Salamo and Elisabet Golobardes", "title":"Analysing Rough Sets weighting methods for Case-Based Reasoning Systems", "venue":"Enginyeria i Arquitectura La Salle", "year":"", "window":"are from our own repository. They deal with diagnosis of <b>breast</b> <b>cancer</b> and synthetic datasets. Datasets related to diagnosis are biopsy and mammogram. Biopsy is the result of digitally processed biopsy images, whereas mammogram consists of detecting breast cancer using the N", "mykey":2399},
 {"datasetID":151, "supportID":"EE042EB33F4320908C19EF5C8E4863C4F84EFFF9", "rexaID":"22dbe26a460522ada68b637b8a3483c717b671fa", "author":"Maria Salamo and Elisabet Golobardes", "title":"Analysing Rough Sets weighting methods for Case-Based Reasoning Systems", "venue":"Enginyeria i Arquitectura La Salle", "year":"", "window":"are obtained from the UCI repository [MM98]. They are: breast cancer, glass, ionosphere, iris, led, <b>sonar</b>  vehicle and vowel. Private datasets are from our own repository. They deal with diagnosis of breast cancer and synthetic datasets. Datasets related to diagnosis are biopsy and mammogram. Biopsy is the result of digitally processed", "mykey":2400},
 {"datasetID":53, "supportID":"EE042EB33F4320908C19EF5C8E4863C4F84EFFF9", "rexaID":"22dbe26a460522ada68b637b8a3483c717b671fa", "author":"Maria Salamo and Elisabet Golobardes", "title":"Analysing Rough Sets weighting methods for Case-Based Reasoning Systems", "venue":"Enginyeria i Arquitectura La Salle", "year":"", "window":"are obtained from the UCI repository [MM98]. They are: breast cancer, glass, ionosphere, <b>iris</b>  led, sonar, vehicle and vowel. Private datasets are from our own repository. They deal with diagnosis of breast cancer and synthetic datasets. Datasets related to diagnosis are biopsy and mammogram. Biopsy is the result of digitally processed", "mykey":2401},
 {"datasetID":57, "supportID":"EE042EB33F4320908C19EF5C8E4863C4F84EFFF9", "rexaID":"22dbe26a460522ada68b637b8a3483c717b671fa", "author":"Maria Salamo and Elisabet Golobardes", "title":"Analysing Rough Sets weighting methods for Case-Based Reasoning Systems", "venue":"Enginyeria i Arquitectura La Salle", "year":"", "window":"averaged over stratified ten-fold cross-validation runs, with their corresponding standard deviations. To study the performance we use a paired one-sided t-test on these runs, except for the <b>LED dataset</b>  which was run using hold-out with a training set of 2000 instances and a test set of 4000 instances. 5.2 Experimental analysis of weighting methods Table 2 shows the experimental results for each", "mykey":2402},
 {"datasetID":149, "supportID":"EE042EB33F4320908C19EF5C8E4863C4F84EFFF9", "rexaID":"22dbe26a460522ada68b637b8a3483c717b671fa", "author":"Maria Salamo and Elisabet Golobardes", "title":"Analysing Rough Sets weighting methods for Case-Based Reasoning Systems", "venue":"Enginyeria i Arquitectura La Salle", "year":"", "window":"are obtained from the UCI repository [MM98]. They are: breast cancer, glass, ionosphere, iris, led, sonar, <b>vehicle</b> and vowel. Private datasets are from our own repository. They deal with diagnosis of breast cancer and synthetic datasets. Datasets related to diagnosis are biopsy and mammogram. Biopsy is the result of digitally processed", "mykey":2403},
 {"datasetID":53, "supportID":"EE38FB55EAD065C236D78B819EC4F0E205971336", "rexaID":"91c9c07720ed14d74e8cda2ec09b5b6789dda2b2", "author":"Daniel C. St and Ralph W. Wilkerson and Cihan H. Dagli", "title":"RULE SET QUALITY MEASURES FOR INDUCTIVE LEARNING ALGORITHMS", "venue":"proceedings of the Artificial Neural Networks In Engineering Conference 1996 (ANNIE", "year":"1996", "window":"distribution of the 148 instances among the four classes \"normal\" with 2 instances, \"metastases\" with 81 instances, \"malign\" with 61 instances, and \"fibrosis\" with 4 instances. The <b>Iris</b> data set, developed by R. A. Fisher (1936), lists the measurements of four characteristics of Iris flowers: petal length, petal width, sepal length, and sepal width. The set includes the measurements of 50", "mykey":2404},
 {"datasetID":90, "supportID":"EE4711FDE56570ACC7EF153BEA6A89D35B241F51", "rexaID":"3a36418db6efd6a24b911127405da46f5a2dd8a9", "author":"Subramani Mani and Marco Porta and Suzanne McDermott", "title":"Building Bayesian Network Models in Medicine: the MENTOR Experience", "venue":"Center for Biomedical Informatics University of Pittsburgh", "year":"2002", "window":"Our validation tests using LED, ALARM and <b>SOYBEAN</b> which are small to large artificial datasets used for Machine Learning research and available from the University of California at the Irvine Machine Learning repository [MuAh94] gave a mean accuracy of 80% over ten runs. The range was from", "mykey":2405},
 {"datasetID":91, "supportID":"EE4711FDE56570ACC7EF153BEA6A89D35B241F51", "rexaID":"3a36418db6efd6a24b911127405da46f5a2dd8a9", "author":"Subramani Mani and Marco Porta and Suzanne McDermott", "title":"Building Bayesian Network Models in Medicine: the MENTOR Experience", "venue":"Center for Biomedical Informatics University of Pittsburgh", "year":"2002", "window":"Our validation tests using LED, ALARM and <b>SOYBEAN</b> which are small to large artificial datasets used for Machine Learning research and available from the University of California at the Irvine Machine Learning repository [MuAh94] gave a mean accuracy of 80% over ten runs. The range was from", "mykey":2406},
 {"datasetID":20, "supportID":"EE6F31D462AE9E9223C0207BB5BBD02A5854E4D7", "rexaID":"32c4d28ceaa19d2a6906dfa39df99da7026c9cc5", "author":"Douglas Burdick and Manuel Calimlim and Jason Flannick and Johannes Gehrke and Tomi Yiu", "title":"MAFIA: A Performance Study of Mining Maximal Frequent Itemsets", "venue":"FIMI", "year":"2003", "window":"itemset patterns that peak around 10-25 items (see Figure 4). Chess and Connect4 are gathered from game state information and are available from the UCI Machine Learning Repository [5]. The Pumsb dataset is <b>census</b> data from PUMS (Public Use Microdata Sample). Pumsb-star is the same dataset as Pumsb except all items of 80% support or more have been removed, making it less dense and easier to mine.", "mykey":2407},
 {"datasetID":23, "supportID":"EE6F31D462AE9E9223C0207BB5BBD02A5854E4D7", "rexaID":"32c4d28ceaa19d2a6906dfa39df99da7026c9cc5", "author":"Douglas Burdick and Manuel Calimlim and Jason Flannick and Johannes Gehrke and Tomi Yiu", "title":"MAFIA: A Performance Study of Mining Maximal Frequent Itemsets", "venue":"FIMI", "year":"2003", "window":"0.1 0.12 Min Sup (%) Time (s) NONE ADAPTIVE Compression on BMS-WebView-2 10 100 1000 10000 0 0.01 0.02 0.03 0.04 0.05 0.06 Min Sup (%) Time (s) NONE ADAPTIVE Figure 8. Compression on more sparse datasets Compression on <b>Chess</b> 1 10 100 1000 0 5 10 15 20 25 30 35 Min Sup (%) Time (s) NONE ADAPTIVE Compression on Pumsb 10 100 1000 10000 0 10 20 30 40 50 60 70 Min Sup (%) Time (s) NONE ADAPTIVE", "mykey":2408},
 {"datasetID":21, "supportID":"EE6F31D462AE9E9223C0207BB5BBD02A5854E4D7", "rexaID":"32c4d28ceaa19d2a6906dfa39df99da7026c9cc5", "author":"Douglas Burdick and Manuel Calimlim and Jason Flannick and Johannes Gehrke and Tomi Yiu", "title":"MAFIA: A Performance Study of Mining Maximal Frequent Itemsets", "venue":"FIMI", "year":"2003", "window":"0.1 0.12 Min Sup (%) Time (s) NONE ADAPTIVE Compression on BMS-WebView-2 10 100 1000 10000 0 0.01 0.02 0.03 0.04 0.05 0.06 Min Sup (%) Time (s) NONE ADAPTIVE Figure 8. Compression on more sparse datasets Compression on <b>Chess</b> 1 10 100 1000 0 5 10 15 20 25 30 35 Min Sup (%) Time (s) NONE ADAPTIVE Compression on Pumsb 10 100 1000 10000 0 10 20 30 40 50 60 70 Min Sup (%) Time (s) NONE ADAPTIVE", "mykey":2409},
 {"datasetID":22, "supportID":"EE6F31D462AE9E9223C0207BB5BBD02A5854E4D7", "rexaID":"32c4d28ceaa19d2a6906dfa39df99da7026c9cc5", "author":"Douglas Burdick and Manuel Calimlim and Jason Flannick and Johannes Gehrke and Tomi Yiu", "title":"MAFIA: A Performance Study of Mining Maximal Frequent Itemsets", "venue":"FIMI", "year":"2003", "window":"0.1 0.12 Min Sup (%) Time (s) NONE ADAPTIVE Compression on BMS-WebView-2 10 100 1000 10000 0 0.01 0.02 0.03 0.04 0.05 0.06 Min Sup (%) Time (s) NONE ADAPTIVE Figure 8. Compression on more sparse datasets Compression on <b>Chess</b> 1 10 100 1000 0 5 10 15 20 25 30 35 Min Sup (%) Time (s) NONE ADAPTIVE Compression on Pumsb 10 100 1000 10000 0 10 20 30 40 50 60 70 Min Sup (%) Time (s) NONE ADAPTIVE", "mykey":2410},
 {"datasetID":34, "supportID":"EEC796EDDBF66BD3F113D7710133CAF0C9720C8E", "rexaID":"ded3146242d6b322f31afbf57a8afde22e4ec8e1", "author":"Ilya Blayvas and Ron Kimmel", "title":"Multiresolution Approximation for Classification", "venue":"CS Dept. Technion", "year":"2002", "window":"\u00b7 D). 3 Experimental Results The proposed method was implemented in VC++ 6.0 and run on `IBM PC 300 PL' with 600MHZ Pentium III processor and 256MB RAM. It was tested on the Pima Indians <b>Diabetes</b> dataset [10], and a large artificial dataset generated with the DatGen program [11]. The results were compared to the Smooth SVM [12] and Sparse Grids [3]. 3.1 Pima Indians The Pima Indians Diabetes", "mykey":2411},
 {"datasetID":79, "supportID":"EEC796EDDBF66BD3F113D7710133CAF0C9720C8E", "rexaID":"ded3146242d6b322f31afbf57a8afde22e4ec8e1", "author":"Ilya Blayvas and Ron Kimmel", "title":"Multiresolution Approximation for Classification", "venue":"CS Dept. Technion", "year":"2002", "window":"\u00b7 D). 3 Experimental Results The proposed method was implemented in VC++ 6.0 and run on `IBM PC 300 PL' with 600MHZ Pentium III processor and 256MB RAM. It was tested on the <b>Pima</b> <b>Indians</b> <b>Diabetes</b> dataset [10], and a large artificial dataset generated with the DatGen program [11]. The results were compared to the Smooth SVM [12] and Sparse Grids [3]. 3.1 Pima Indians The Pima Indians Diabetes", "mykey":2412},
 {"datasetID":1, "supportID":"EF07EB904E5D43020D9A4E8F69778F763C6098E7", "rexaID":"b2d8d3d5275f9de64f7d1d58ed346fc673f94065", "author":"Edward Snelson and Carl Edward Rasmussen and Zoubin Ghahramani", "title":"Draft version; accepted for NIPS*03 Warped Gaussian Processes", "venue":"Gatsby Computational Neuroscience Unit University College London", "year":"", "window":"predict the the age of <b>abalone</b> from various physical inputs [9]. ailerons is a simulated control problem, with the aim to predict the control action on the ailerons of an F16 aircraft [10, 11]. For datasets creep and abalone, which consist of positive observations only, standard practice may be to model the log of the data with a GP. So for these datasets we have compared three models: a GP directly", "mykey":2413},
 {"datasetID":79, "supportID":"EF39B8A0937010BDDF79660789B4C2C48F2D5B7B", "rexaID":"21ab0b68f14e5f4b1f401fa7bca294c1dc48061b", "author":"Christopher P. Diehl and Gert Cauwenberghs", "title":"SVM Incremental Learning, Adaptation and Optimization", "venue":"Applied Physics Laboratory Johns Hopkins University", "year":"", "window":"that h = 0 when the final perturbation is complete. IV. EXPERIMENTAL RESULTS In order to assess the benefits offered by the incremental framework, we conducted two experiments using the <b>Pima</b> <b>Indians</b> dataset from the UCI machine learning repository [1]. Using an RBF kernel K(x, y) = exp # - kx - yk 2 # 2 # , we first fixed the kernel width and varied (increased or decreased) the regularization parameter", "mykey":2414},
 {"datasetID":1, "supportID":"EF6F7ECED228AA8FD8AFF9BDA1EB27C8DB7F5E77", "rexaID":"ac35cdc4f306058543fb3fbf817fc84d85f89288", "author":"Johannes Furnkranz", "title":"Round Robin Rule Learning", "venue":"Austrian Research Institute for Artificial Intelligence", "year":"", "window":"9 Classification time is only included in the runs that had a separate test set. In general, it can be expected to be more expensive for R \u00bf . See Section 7 for a brief discussion of this issue. 12 dataset C5 -b vs. C5 R ; vs. unord vs. order <b>abalone</b> 23.34 10.81 193.0 4.51 5.73 covertype --- --- --- ---- ---- letter 73.37 6.64 1250.0 0.51 1.14 sat 27.86 9.10 143.0 0.85 1.51 shuttle 35.98 5.67 277.0", "mykey":2415},
 {"datasetID":31, "supportID":"EF6F7ECED228AA8FD8AFF9BDA1EB27C8DB7F5E77", "rexaID":"ac35cdc4f306058543fb3fbf817fc84d85f89288", "author":"Johannes Furnkranz", "title":"Round Robin Rule Learning", "venue":"Austrian Research Institute for Artificial Intelligence", "year":"", "window":"6) Ripper. The first five lines are total run-times, i.e., training and test time, while the cross-validated results report training time only. We failed to measure the run-times for the <b>covertype</b> data set, where the situation was complicated because of the large test set, which had to be split into several pieces for the Ripper-based algorithms. The last line shows the average of the 17", "mykey":2416},
 {"datasetID":110, "supportID":"EF6F7ECED228AA8FD8AFF9BDA1EB27C8DB7F5E77", "rexaID":"ac35cdc4f306058543fb3fbf817fc84d85f89288", "author":"Johannes Furnkranz", "title":"Round Robin Rule Learning", "venue":"Austrian Research Institute for Artificial Intelligence", "year":"", "window":"683 --- 35 0 19 94.1 thyroid (hyper) 3772 --- 21 6 5 2.7 thyroid (hypo) 3772 --- 21 6 5 7.7 thyroid (repl.) 3772 --- 21 6 4 3.3 vehicle 846 --- 0 18 4 74.2 <b>yeast</b> 1484 --- 0 8 10 68.8 Table 1: Data sets used. The first two columns show the training and test set sizes (as specified in the description of the datasets), the next three columns show the number of symbolic and numeric attributes as well", "mykey":2417},
 {"datasetID":58, "supportID":"EFB6FE9D9504AED8D68B7B51DB3A0C20B953A105", "rexaID":"8a4f6f671d1146db723a649b440535832896ccc4", "author":"Mehmet Dalkilic and Arijit Sengupta", "title":"A Logic-theoretic classifier called Circle", "venue":"School of Informatics Center for Genomics and BioInformatics Indiana University", "year":"", "window":"from the UC Irvine Machine Learning repository [14]. Full deterministic Circle was tested on the simple data sets like contact <b>lenses</b>  and weather, as well as large data sets like monks, mushroom, and Zoo. As an example of the performance improvement, while full Circle took over an hour to terminate using the", "mykey":2418},
 {"datasetID":73, "supportID":"EFB6FE9D9504AED8D68B7B51DB3A0C20B953A105", "rexaID":"8a4f6f671d1146db723a649b440535832896ccc4", "author":"Mehmet Dalkilic and Arijit Sengupta", "title":"A Logic-theoretic classifier called Circle", "venue":"School of Informatics Center for Genomics and BioInformatics Indiana University", "year":"", "window":"like contact-lenses, and weather, as well as large data sets like monks, <b>mushroom</b>  and Zoo. As an example of the performance improvement, while full Circle took over an hour to terminate using the Zoo data set, the Randomized Circle with 8 attributes per", "mykey":2419},
 {"datasetID":154, "supportID":"EFB6FE9D9504AED8D68B7B51DB3A0C20B953A105", "rexaID":"8a4f6f671d1146db723a649b440535832896ccc4", "author":"Mehmet Dalkilic and Arijit Sengupta", "title":"A Logic-theoretic classifier called Circle", "venue":"School of Informatics Center for Genomics and BioInformatics Indiana University", "year":"", "window":"8 attributes per iteration took less than 5 minutes, with approximately 90% test accuracy with a 20% cross-trained data. Table 1 reports some of these results. Table 1 includes experiments ran on a dataset generated from the PROSITE <b>Protein</b> family dataset that included nine families, which includes information on 1446 di\u00aeerent proteins, with over 5000 subsequences. Since 1000 is a typical limit on the", "mykey":2420},
 {"datasetID":111, "supportID":"EFB6FE9D9504AED8D68B7B51DB3A0C20B953A105", "rexaID":"8a4f6f671d1146db723a649b440535832896ccc4", "author":"Mehmet Dalkilic and Arijit Sengupta", "title":"A Logic-theoretic classifier called Circle", "venue":"School of Informatics Center for Genomics and BioInformatics Indiana University", "year":"", "window":"like contact-lenses, and weather, as well as large data sets like monks, mushroom, and <b>Zoo</b>  As an example of the performance improvement, while full Circle took over an hour to terminate using the Zoo data set, the Randomized Circle with 8 attributes per", "mykey":2421},
 {"datasetID":2, "supportID":"F014207F5E6B34D3A0626B63501A2DE163165F41", "rexaID":"48dc427a3536d7bef34e910efc1f34171bf98729", "author":"Chris Giannella and Bassem Sayrafi", "title":"An Information Theoretic Histogram for Single Dimensional Selectivity Estimation", "venue":"Department of Computer Science, Indiana University Bloomington", "year":"", "window":"was obtained from the UCI machine learning archive [1] (called the  <b>adult</b>  dataset there). We use the age column of the training dataset. The dataset was extracted from 1994 US census data. The shuttle2 dataset was downloaded from the \"Esprit Project 5170 StatLog\" archive", "mykey":2422},
 {"datasetID":20, "supportID":"F014207F5E6B34D3A0626B63501A2DE163165F41", "rexaID":"48dc427a3536d7bef34e910efc1f34171bf98729", "author":"Chris Giannella and Bassem Sayrafi", "title":"An Information Theoretic Histogram for Single Dimensional Selectivity Estimation", "venue":"Department of Computer Science, Indiana University Bloomington", "year":"", "window":"there). We use the age column of the training dataset. The dataset was extracted from 1994 US <b>census</b> data. The shuttle2 dataset was downloaded from the \"Esprit Project 5170 StatLog\" archive (\"Shuttle\" heading): www.liacc.up.pt/ML/. It represents data", "mykey":2423},
 {"datasetID":31, "supportID":"F014207F5E6B34D3A0626B63501A2DE163165F41", "rexaID":"48dc427a3536d7bef34e910efc1f34171bf98729", "author":"Chris Giannella and Bassem Sayrafi", "title":"An Information Theoretic Histogram for Single Dimensional Selectivity Estimation", "venue":"Department of Computer Science, Indiana University Bloomington", "year":"", "window":"were obtained from the UCI KDD archive [7]. The forestcov4 and forestcov9 datasets were found under the \"Forest <b>CoverType</b>  heading, covtype.data file -- attributes four and nine, respectively. The attributes represent various geographic measurements. The cup199 and cup472", "mykey":2424},
 {"datasetID":127, "supportID":"F014207F5E6B34D3A0626B63501A2DE163165F41", "rexaID":"48dc427a3536d7bef34e910efc1f34171bf98729", "author":"Chris Giannella and Bassem Sayrafi", "title":"An Information Theoretic Histogram for Single Dimensional Selectivity Estimation", "venue":"Department of Computer Science, Indiana University Bloomington", "year":"", "window":"CUP 1998 Data\" heading, cup98lrn file -- attributes 199 (IC1, median household income) and 472 (TARGET D, donation amount quantized into 60 groups), respectively. The ipums25, ipums51, and ipums52 datasets were found under the  <b>IPUMS</b> Census Data\" heading, ipums.la.99 file -- attributes 25 (eldch, age of the eldest child in the household), 51 (incbus represents business income), and 52 (incfarm", "mykey":2425},
 {"datasetID":129, "supportID":"F014207F5E6B34D3A0626B63501A2DE163165F41", "rexaID":"48dc427a3536d7bef34e910efc1f34171bf98729", "author":"Chris Giannella and Bassem Sayrafi", "title":"An Information Theoretic Histogram for Single Dimensional Selectivity Estimation", "venue":"Department of Computer Science, Indiana University Bloomington", "year":"", "window":"were found under the \"Forest CoverType\" heading, covtype.data file -- attributes four and nine, respectively. The attributes represent various geographic measurements. The cup199 and cup472 datasets can be found under the  <b>KDD</b> <b>CUP</b> <b>1998</b> Data\" heading, cup98lrn file -- attributes 199 (IC1, median household income) and 472 (TARGET D, donation amount quantized into 60 groups), respectively. The", "mykey":2426},
 {"datasetID":148, "supportID":"F014207F5E6B34D3A0626B63501A2DE163165F41", "rexaID":"48dc427a3536d7bef34e910efc1f34171bf98729", "author":"Chris Giannella and Bassem Sayrafi", "title":"An Information Theoretic Histogram for Single Dimensional Selectivity Estimation", "venue":"Department of Computer Science, Indiana University Bloomington", "year":"", "window":"from the \"Esprit Project 5170 StatLog\" archive ( <b>shuttle</b>  heading): www.liacc.up.pt/ML/. It represents data concerning the operation of the NASA space shuttle. We use attribute two. The remaining datasets were obtained from the UCI KDD archive [7]. The forestcov4 and forestcov9 datasets were found under the \"Forest CoverType\" heading, covtype.data file -- attributes four and nine, respectively. The", "mykey":2427},
 {"datasetID":116, "supportID":"F014207F5E6B34D3A0626B63501A2DE163165F41", "rexaID":"48dc427a3536d7bef34e910efc1f34171bf98729", "author":"Chris Giannella and Bassem Sayrafi", "title":"An Information Theoretic Histogram for Single Dimensional Selectivity Estimation", "venue":"Department of Computer Science, Indiana University Bloomington", "year":"", "window":"there). We use the age column of the training dataset. The dataset was extracted from 1994 <b>US census</b> data. The shuttle2 dataset was downloaded from the \"Esprit Project 5170 StatLog\" archive (\"Shuttle\" heading): www.liacc.up.pt/ML/. It represents data", "mykey":2428},
 {"datasetID":148, "supportID":"F0BA08BD45EE060A2BAF8EDCEA5BE11811FECDD9", "rexaID":"4dac55f2daa66b00a555405664b6f275370782ac", "author":"Haixun Wang and Carlo Zaniolo", "title":"CMP: A Fast Decision Tree Classifier Using Multivariate Predictions", "venue":"ICDE", "year":"2000", "window":"might fall. Figure 2 shows a hypothetical gini curve, and three alive intervals (shaded areas in the figure). Our experiments of the estimation method are summarized in Table 1. The first four small datasets (Letter, Satimage, Segment and <b>Shuttle</b>  in the table are from the STATLOG project[6], and the two large datasets (Function 2 and Function 7) are synthetic datasets described in [5]. In these test", "mykey":2429},
 {"datasetID":98, "supportID":"F0BA08BD45EE060A2BAF8EDCEA5BE11811FECDD9", "rexaID":"4dac55f2daa66b00a555405664b6f275370782ac", "author":"Haixun Wang and Carlo Zaniolo", "title":"CMP: A Fast Decision Tree Classifier Using Multivariate Predictions", "venue":"ICDE", "year":"2000", "window":"(Letter, Satimage, Segment and Shuttle) in the table are from the <b>STATLOG</b> project[6], and the two large datasets (Function 2 and Function 7) are synthetic datasets described in [5]. In these test cases there were at most N = 2 alive intervals: i) the one whose left boundary (or right boundary, depending on", "mykey":2430},
 {"datasetID":34, "supportID":"F1407FC553611A2F3B0E8F7D62467D746A1D7794", "rexaID":"87afa910f706df9e28bfa697d6d2eea7c0cb53ef", "author":"Ilya Blayvas and Ron Kimmel", "title":"Efficient Classification via Multiresolution Training Set Approximation", "venue":"CS Dept. Technion", "year":"", "window":"\u00b7 D). 3 Experimental Results The proposed method was implemented in VC++ 6.0 and run on `IBM PC 300 PL' with 600MHZ Pentium III processor and 256MB RAM. It was tested on the Pima Indians <b>Diabetes</b> dataset [10], and a large artificial dataset generated with the DatGen program [11]. The results were compared to the Smooth SVM [12] and Sparse Grids [3]. Figure 7: Partition of 2D feature space for a", "mykey":2431},
 {"datasetID":79, "supportID":"F1407FC553611A2F3B0E8F7D62467D746A1D7794", "rexaID":"87afa910f706df9e28bfa697d6d2eea7c0cb53ef", "author":"Ilya Blayvas and Ron Kimmel", "title":"Efficient Classification via Multiresolution Training Set Approximation", "venue":"CS Dept. Technion", "year":"", "window":"\u00b7 D). 3 Experimental Results The proposed method was implemented in VC++ 6.0 and run on `IBM PC 300 PL' with 600MHZ Pentium III processor and 256MB RAM. It was tested on the <b>Pima</b> <b>Indians</b> <b>Diabetes</b> dataset [10], and a large artificial dataset generated with the DatGen program [11]. The results were compared to the Smooth SVM [12] and Sparse Grids [3]. Figure 7: Partition of 2D feature space for a", "mykey":2432},
 {"datasetID":74, "supportID":"F15126EFC9CF353E9EB5AD5EC58A817E67DE3FDE", "rexaID":"aab7dc3ef971ecabaa3351004c5b00f15bbc712a", "author":"Zhi-Hua Zhou and Min-Ling Zhang", "title":"Neural Networks for Multi-Instance Learning", "venue":"National Laboratory for Novel Software Technology, Nanjing University", "year":"", "window":"BP-MIP network is used in prediction, a bag is positively labeled if and only if the output of the network on at least one of its instances is not less than 0.5. 6 5. Experiments 5.1 Real-world data sets The <b>Musk</b> data is the only real-world benchmark test data for multi-instance learning at present. The data is generated by Dietterich et al. in the way described in Section 2. There are two data", "mykey":2433},
 {"datasetID":75, "supportID":"F15126EFC9CF353E9EB5AD5EC58A817E67DE3FDE", "rexaID":"aab7dc3ef971ecabaa3351004c5b00f15bbc712a", "author":"Zhi-Hua Zhou and Min-Ling Zhang", "title":"Neural Networks for Multi-Instance Learning", "venue":"National Laboratory for Novel Software Technology, Nanjing University", "year":"", "window":"BP-MIP network is used in prediction, a bag is positively labeled if and only if the output of the network on at least one of its instances is not less than 0.5. 6 5. Experiments 5.1 Real-world data sets The <b>Musk</b> data is the only real-world benchmark test data for multi-instance learning at present. The data is generated by Dietterich et al. in the way described in Section 2. There are two data", "mykey":2434},
 {"datasetID":52, "supportID":"F1A2B03BBC2588D3EFBE86DA0AAB311F85CF3D81", "rexaID":"e100b0bcc229ac20bf69c9a7a6bcc360ea16a720", "author":"Wl/odzisl/aw Duch and Karol Grudzinski", "title":"Meta-learning: searching in the model space", "venue":"Department of Computer Methods, Nicholas Copernicus University", "year":"", "window":"to find classification models that achieve 100% accuracy on the test set. For hepatobiliary disorders a model with highest accuracy for real medical data has been found automatically. For some data sets, such as the <b>ionosphere</b>  there seems to be no correlation between the results on the training and on the test set. Although the use of a validation set (or the use of the crossvalidation", "mykey":2435},
 {"datasetID":70, "supportID":"F1A2B03BBC2588D3EFBE86DA0AAB311F85CF3D81", "rexaID":"e100b0bcc229ac20bf69c9a7a6bcc360ea16a720", "author":"Wl/odzisl/aw Duch and Karol Grudzinski", "title":"Meta-learning: searching in the model space", "venue":"Department of Computer Methods, Nicholas Copernicus University", "year":"", "window":"The goal of further search for the best model should therefore include not only accuracy but also reduction of variance, i.e. stabilization of the classifier. 4.1 <b>Monk</b> problems The artificial dataset Monk-1 [11] is designed for rule-based symbolic machine learning algorithms (the data was taken from the UCI repository [12]). The nearest neighbor algorithms usually do not work well in such cases.", "mykey":2436},
 {"datasetID":98, "supportID":"F1A2B03BBC2588D3EFBE86DA0AAB311F85CF3D81", "rexaID":"e100b0bcc229ac20bf69c9a7a6bcc360ea16a720", "author":"Wl/odzisl/aw Duch and Karol Grudzinski", "title":"Meta-learning: searching in the model space", "venue":"Department of Computer Methods, Nicholas Copernicus University", "year":"", "window":"on which they work well. A review of many approaches to classification and comparison of performance of 20 methods on 20 real world datasets has been done within the <b>StatLog</b> European Community project [2]. The accuracy of 24 neural-based, pattern recognition and statistical classification systems has been compared on 11 large datasets", "mykey":2437},
 {"datasetID":150, "supportID":"F208C18D930B97D19D1035520FA85CD2DFEB86D9", "rexaID":"6f73140310964259ac6a55207d875a82537c0548", "author":"Wl/odzisl/aw Duch and Jerzy J. Korczak", "title":"Optimization and global minimization methods suitable for neural networks", "venue":"Department of Computer Methods, Nicholas Copernicus University", "year":"", "window":"by NOVEL and SIMANN. Genetic algorithms achieved the worst results, below 60% in all cases, being unable to find good solutions. NOVEL has also been tried on Sonar, Vovel, 10-parity and <b>NetTalk</b> datasets from the UCI repository [13], using different number of hidden units, achieving very good results on the test sets, and falling behind TN-MS only in one case. From these few comparisons scattered", "mykey":2438},
 {"datasetID":151, "supportID":"F208C18D930B97D19D1035520FA85CD2DFEB86D9", "rexaID":"6f73140310964259ac6a55207d875a82537c0548", "author":"Wl/odzisl/aw Duch and Jerzy J. Korczak", "title":"Optimization and global minimization methods suitable for neural networks", "venue":"Department of Computer Methods, Nicholas Copernicus University", "year":"", "window":"by NOVEL and SIMANN. Genetic algorithms achieved the worst results, below 60% in all cases, being unable to find good solutions. NOVEL has also been tried on <b>Sonar</b>  Vovel, 10-parity and NetTalk datasets from the UCI repository [13], using different number of hidden units, achieving very good results on the test sets, and falling behind TN-MS only in one case. From these few comparisons scattered", "mykey":2439},
 {"datasetID":48, "supportID":"F216DE99B968CC0B6F01487F36918BF37A0D39CC", "rexaID":"09e7adcf66589d1d54853bb072ead0657a5c5e14", "author":"Nir Friedman and Daphne Koller (koller@cs. stanford. edu", "title":"A Bayesian Approach to Structure Discovery in Bayesian Networks", "venue":"School of Computer Science & Engineering Hebrew University", "year":"", "window":"indicating that model selection is likely to return a fairly representative structure in this case. A second form of support for the non-mixing conjecture is obtained by considering an even smaller data set: the Boston <b>housing</b> data set, from the UCI repository (Murphy and Aha, 1995), is a continuous domain with 14 variables and 506 samples. Here, we considered linear Gaussian networks, and used a", "mykey":2440},
 {"datasetID":89, "supportID":"F216DE99B968CC0B6F01487F36918BF37A0D39CC", "rexaID":"09e7adcf66589d1d54853bb072ead0657a5c5e14", "author":"Nir Friedman and Daphne Koller (koller@cs. stanford. edu", "title":"A Bayesian Approach to Structure Discovery in Bayesian Networks", "venue":"School of Computer Science & Engineering Hebrew University", "year":"", "window":"0.6 0.8 1 MCMC Exact 5 samples 20 samples 50 samples Markov Edges Figure 1. Comparison of posterior probabilities for the exact posterior over orders (x-axis) versus order-MCMC (y-axis) in the <b>Flare</b> dataset with 100 instances. The figures show the probabilities for all Markov features and edge features. 5. Experimental Results We evaluated our approach in a variety of ways. We first compare it with a", "mykey":2441},
 {"datasetID":14, "supportID":"F2729FEB3087B04F7B27E8924E6F64F3C45EABB0", "rexaID":"b0009a0081cc5fbfbae758def55cfd5b3256623b", "author":"", "title":"Prototype Selection for Composite Nearest Neighbor Classifiers", "venue":"Department of Computer Science University of Massachusetts", "year":"1997", "window":"Selection. : : : : : : : : : : : : : : : : : : : : 117 4.11 Relationships between component accuracy and diversity for the Cleveland Heart Disease, LED-7 Digit, Hepatitis and <b>Breast</b> <b>Cancer</b> Wisconsin data sets for the four boosting algorithms. \"c\" represents the Coarse Reclassification algorithm; \"d\", Deliberate Misclassification; \"f \", Composite Fitness; and \"s\" Composite Fitness--Feature Selection. : :", "mykey":2442},
 {"datasetID":17, "supportID":"F2729FEB3087B04F7B27E8924E6F64F3C45EABB0", "rexaID":"b0009a0081cc5fbfbae758def55cfd5b3256623b", "author":"", "title":"Prototype Selection for Composite Nearest Neighbor Classifiers", "venue":"Department of Computer Science University of Massachusetts", "year":"1997", "window":"Selection. : : : : : : : : : : : : : : : : : : : : 117 4.11 Relationships between component accuracy and diversity for the Cleveland Heart Disease, LED-7 Digit, Hepatitis and <b>Breast</b> <b>Cancer</b> <b>Wisconsin</b> data sets for the four boosting algorithms. \"c\" represents the Coarse Reclassification algorithm; \"d\", Deliberate Misclassification; \"f \", Composite Fitness; and \"s\" Composite Fitness--Feature Selection. : :", "mykey":2443},
 {"datasetID":15, "supportID":"F2729FEB3087B04F7B27E8924E6F64F3C45EABB0", "rexaID":"b0009a0081cc5fbfbae758def55cfd5b3256623b", "author":"", "title":"Prototype Selection for Composite Nearest Neighbor Classifiers", "venue":"Department of Computer Science University of Massachusetts", "year":"1997", "window":"Selection. : : : : : : : : : : : : : : : : : : : : 117 4.11 Relationships between component accuracy and diversity for the Cleveland Heart Disease, LED-7 Digit, Hepatitis and <b>Breast</b> <b>Cancer</b> <b>Wisconsin</b> data sets for the four boosting algorithms. \"c\" represents the Coarse Reclassification algorithm; \"d\", Deliberate Misclassification; \"f \", Composite Fitness; and \"s\" Composite Fitness--Feature Selection. : :", "mykey":2444},
 {"datasetID":16, "supportID":"F2729FEB3087B04F7B27E8924E6F64F3C45EABB0", "rexaID":"b0009a0081cc5fbfbae758def55cfd5b3256623b", "author":"", "title":"Prototype Selection for Composite Nearest Neighbor Classifiers", "venue":"Department of Computer Science University of Massachusetts", "year":"1997", "window":"Selection. : : : : : : : : : : : : : : : : : : : : 117 4.11 Relationships between component accuracy and diversity for the Cleveland Heart Disease, LED-7 Digit, Hepatitis and <b>Breast</b> <b>Cancer</b> <b>Wisconsin</b> data sets for the four boosting algorithms. \"c\" represents the Coarse Reclassification algorithm; \"d\", Deliberate Misclassification; \"f \", Composite Fitness; and \"s\" Composite Fitness--Feature Selection. : :", "mykey":2445},
 {"datasetID":34, "supportID":"F2729FEB3087B04F7B27E8924E6F64F3C45EABB0", "rexaID":"b0009a0081cc5fbfbae758def55cfd5b3256623b", "author":"", "title":"Prototype Selection for Composite Nearest Neighbor Classifiers", "venue":"Department of Computer Science University of Massachusetts", "year":"1997", "window":"Fitness--Feature Selection. : : : : : : : : : : : : : : : : 116 4.10 Relationships between component accuracy and diversity for the Monks-2, Breast Cancer Ljubljana, <b>Diabetes</b> and Iris Plants data sets for the four boosting algorithms. \"c\" represents the Coarse Reclassification algorithm; \"d\", Deliberate Misclassification; \"f \", Composite Fitness; and \"s\" Composite Fitness--Feature Selection. : :", "mykey":2446},
 {"datasetID":120, "supportID":"F2729FEB3087B04F7B27E8924E6F64F3C45EABB0", "rexaID":"b0009a0081cc5fbfbae758def55cfd5b3256623b", "author":"", "title":"Prototype Selection for Composite Nearest Neighbor Classifiers", "venue":"Department of Computer Science University of Massachusetts", "year":"1997", "window":"symbolic values. The data for the competition were divided into a single test and a single training set, but we have used a 10-fold cross-validation partition, for uniformity with the rest of our data sets. 12. Promoter. The task domain is <b>E</b>  <b>Coli</b> promoter gene sequences (DNA), and the task is to classify gene sequences as \"promoters\" or \"non-promoters\" based on 57 sequential DNA nucleotide", "mykey":2447},
 {"datasetID":39, "supportID":"F2729FEB3087B04F7B27E8924E6F64F3C45EABB0", "rexaID":"b0009a0081cc5fbfbae758def55cfd5b3256623b", "author":"", "title":"Prototype Selection for Composite Nearest Neighbor Classifiers", "venue":"Department of Computer Science University of Massachusetts", "year":"1997", "window":"symbolic values. The data for the competition were divided into a single test and a single training set, but we have used a 10-fold cross-validation partition, for uniformity with the rest of our data sets. 12. Promoter. The task domain is E. <b>Coli</b> promoter gene sequences (DNA), and the task is to classify gene sequences as \"promoters\" or \"non-promoters\" based on 57 sequential DNA nucleotide", "mykey":2448},
 {"datasetID":42, "supportID":"F2729FEB3087B04F7B27E8924E6F64F3C45EABB0", "rexaID":"b0009a0081cc5fbfbae758def55cfd5b3256623b", "author":"", "title":"Prototype Selection for Composite Nearest Neighbor Classifiers", "venue":"Department of Computer Science University of Massachusetts", "year":"1997", "window":"there is no sense of positive and negative examples. For example, in the <b>Glass</b> Recognition data set, there are six classes and therefore six prototypes will be selected. There, the classes correspond to the source and manufacturing process of glass fragments for crime scene analysis: building", "mykey":2449},
 {"datasetID":45, "supportID":"F2729FEB3087B04F7B27E8924E6F64F3C45EABB0", "rexaID":"b0009a0081cc5fbfbae758def55cfd5b3256623b", "author":"", "title":"Prototype Selection for Composite Nearest Neighbor Classifiers", "venue":"Department of Computer Science University of Massachusetts", "year":"1997", "window":"resulted in a fairly small number of prototypes that can achieve a very good level of classification accuracy. For example, Aha's IB3 algorithm achieves 79% accuracy on the Cleveland <b>heart</b> disease data set [Murphy and Aha, 49 1994] while retaining only approximately 4% of the 303 instances [Aha, 1990] . Results such as this hint that a small number of prototypes will suffice on some data. In Table", "mykey":2450},
 {"datasetID":46, "supportID":"F2729FEB3087B04F7B27E8924E6F64F3C45EABB0", "rexaID":"b0009a0081cc5fbfbae758def55cfd5b3256623b", "author":"", "title":"Prototype Selection for Composite Nearest Neighbor Classifiers", "venue":"Department of Computer Science University of Massachusetts", "year":"1997", "window":"Selection. : : : : : : : : : : : : : : : : : : : : 117 4.11 Relationships between component accuracy and diversity for the Cleveland Heart Disease, LED-7 Digit, <b>Hepatitis</b> and Breast Cancer Wisconsin data sets for the four boosting algorithms. \"c\" represents the Coarse Reclassification algorithm; \"d\", Deliberate Misclassification; \"f \", Composite Fitness; and \"s\" Composite Fitness--Feature Selection. : :", "mykey":2451},
 {"datasetID":53, "supportID":"F2729FEB3087B04F7B27E8924E6F64F3C45EABB0", "rexaID":"b0009a0081cc5fbfbae758def55cfd5b3256623b", "author":"", "title":"Prototype Selection for Composite Nearest Neighbor Classifiers", "venue":"Department of Computer Science University of Massachusetts", "year":"1997", "window":"Fitness--Feature Selection. : : : : : : : : : : : : : : : : 116 4.10 Relationships between component accuracy and diversity for the Monks-2, Breast Cancer Ljubljana, Diabetes and <b>Iris</b> Plants data sets for the four boosting algorithms. \"c\" represents the Coarse Reclassification algorithm; \"d\", Deliberate Misclassification; \"f \", Composite Fitness; and \"s\" Composite Fitness--Feature Selection. : :", "mykey":2452},
 {"datasetID":63, "supportID":"F2729FEB3087B04F7B27E8924E6F64F3C45EABB0", "rexaID":"b0009a0081cc5fbfbae758def55cfd5b3256623b", "author":"", "title":"Prototype Selection for Composite Nearest Neighbor Classifiers", "venue":"Department of Computer Science University of Massachusetts", "year":"1997", "window":"for the Composite Fitness--Feature Selection algorithm. : : : : : 107 4.9 Relationships between component accuracy and diversity for the Glass Recognition, LED-24 Digit, <b>Lymphography</b> and Soybean data sets for the four boosting algorithms. \"c\" represents the Coarse Reclassification algorithm; \"d\", Deliberate Misclassification; \"f \", Composite Fitness; and \"s\" Composite Fitness--Feature Selection. : :", "mykey":2453},
 {"datasetID":67, "supportID":"F2729FEB3087B04F7B27E8924E6F64F3C45EABB0", "rexaID":"b0009a0081cc5fbfbae758def55cfd5b3256623b", "author":"", "title":"Prototype Selection for Composite Nearest Neighbor Classifiers", "venue":"Department of Computer Science University of Massachusetts", "year":"1997", "window":"Misclassification; \"f \", Composite Fitness; and \"s\" Composite Fitness--Feature Selection. : : : : : : : : : : 118 xvi 4.12 Relationships between component accuracy and diversity for the <b>Promoter</b> data set for the four boosting algorithms. \"c\" represents the Coarse Reclassification algorithm; \"d\", Deliberate Misclassification; \"f \", Composite Fitness; and \"s\" Composite Fitness--Feature Selection. : :", "mykey":2454},
 {"datasetID":90, "supportID":"F2729FEB3087B04F7B27E8924E6F64F3C45EABB0", "rexaID":"b0009a0081cc5fbfbae758def55cfd5b3256623b", "author":"", "title":"Prototype Selection for Composite Nearest Neighbor Classifiers", "venue":"Department of Computer Science University of Massachusetts", "year":"1997", "window":"and test accuracy. : : : : : : : : : : : : : : : : 77 3.10 Taxonomy of instances. : : : : : : : : : : : : : : : : : : : : : : : : : : : 79 3.11 Examples of instances in the taxonomy from the <b>Soybean</b> data set. : : : : : : 80 3.12 Average number of instances of each type in the taxonomy. : : : : : : : : : 81 3.13 Classification accuracy of the Prototype Sampling (PS) algorithm versus a variant of the", "mykey":2455},
 {"datasetID":91, "supportID":"F2729FEB3087B04F7B27E8924E6F64F3C45EABB0", "rexaID":"b0009a0081cc5fbfbae758def55cfd5b3256623b", "author":"", "title":"Prototype Selection for Composite Nearest Neighbor Classifiers", "venue":"Department of Computer Science University of Massachusetts", "year":"1997", "window":"and test accuracy. : : : : : : : : : : : : : : : : 77 3.10 Taxonomy of instances. : : : : : : : : : : : : : : : : : : : : : : : : : : : 79 3.11 Examples of instances in the taxonomy from the <b>Soybean</b> data set. : : : : : : 80 3.12 Average number of instances of each type in the taxonomy. : : : : : : : : : 81 3.13 Classification accuracy of the Prototype Sampling (PS) algorithm versus a variant of the", "mykey":2456},
 {"datasetID":146, "supportID":"F2C8133A8FABCD546403337FDB6B0B5BAF8A1E79", "rexaID":"2e6705a0fe8b335ec6bffc4acbe8a0665f67c8a5", "author":"Jaakko Peltonen and Arto Klami and Samuel Kaski", "title":"Learning More Accurate Metrics for Self-Organizing Maps", "venue":"ICANN", "year":"2002", "window":"Data set Dimensions Classes Samples <b>Landsat</b> <b>Satellite</b> Data * 36 6 6435 Letter Recognition Data * 16 26 20000 Phoneme Data from LVQ PAK [8] 20 14 3656 TIMIT Data from [10] 12 41 14994 Bankruptcy Data used in", "mykey":2457},
 {"datasetID":111, "supportID":"F2CBFD138B909C88803F97CF4D4FA71483065064", "rexaID":"af8d7875705c4e2cfa6e62edef98f148bab7cd4c", "author":"D. Randall Wilson and Tony R. Martinez", "title":"Heterogeneous Radial Basis Function Networks", "venue":"Proceedings of the International Conference on Neural Networks (ICNN", "year":"1996", "window":"higher generalization accuracy than RBF in 12 out of 23 cases, 10 of which were significant at the 95% level or above. RBF had a higher accuracy in only four cases, and only one of those (the <b>Zoo</b> data set) had a difference that was statistically significant. It is interesting to note that in the Zoo data set, 15 out of 16 of the attributes are boolean, and the remaining attribute, while not linear,", "mykey":2458},
 {"datasetID":2, "supportID":"F2DC7581BC7D742CADDA07A2968E02E6B8D46BBB", "rexaID":"bf6cec50b7f7d48d105c8c649210cc3a42d3d71e", "author":"Ron Kohavi", "title":"Scaling Up the Accuracy of Naive-Bayes Classifiers: A Decision-Tree Hybrid", "venue":"KDD", "year":"1996", "window":"the algorithm, and 20 intervals were used. The error bars show 95% confidence intervals on the accuracy, based on the leftout sample. In most cases it is clear that even with much more 1 The <b>Adult</b> dataset is from the Census bureau and the task is to predict whether a given adult makes more than $50,000 a year based attributes such as education, hours of work per week, etc.. 74 76 78 80 82 84 86 88 90", "mykey":2459},
 {"datasetID":20, "supportID":"F2DC7581BC7D742CADDA07A2968E02E6B8D46BBB", "rexaID":"bf6cec50b7f7d48d105c8c649210cc3a42d3d71e", "author":"Ron Kohavi", "title":"Scaling Up the Accuracy of Naive-Bayes Classifiers: A Decision-Tree Hybrid", "venue":"KDD", "year":"1996", "window":"the algorithm, and 20 intervals were used. The error bars show 95% confidence intervals on the accuracy, based on the leftout sample. In most cases it is clear that even with much more 1 The Adult dataset is from the <b>Census</b> bureau and the task is to predict whether a given adult makes more than $50,000 a year based attributes such as education, hours of work per week, etc.. 74 76 78 80 82 84 86 88 90", "mykey":2460},
 {"datasetID":23, "supportID":"F2DC7581BC7D742CADDA07A2968E02E6B8D46BBB", "rexaID":"bf6cec50b7f7d48d105c8c649210cc3a42d3d71e", "author":"Ron Kohavi", "title":"Scaling Up the Accuracy of Naive-Bayes Classifiers: A Decision-Tree Hybrid", "venue":"KDD", "year":"1996", "window":"which is a Bayes network restricted to a tree topology. The results are promising and running times should scale up, but the approach is still restrictive. For example, their accuracy for the <b>Chess</b> dataset, which contains high-order interactions is about 93%, much lower then C4.5 and NBTree, which achieve accuracies above 99%. Conclusions We have described a new algorithm, NBTree, which is a hybrid", "mykey":2461},
 {"datasetID":21, "supportID":"F2DC7581BC7D742CADDA07A2968E02E6B8D46BBB", "rexaID":"bf6cec50b7f7d48d105c8c649210cc3a42d3d71e", "author":"Ron Kohavi", "title":"Scaling Up the Accuracy of Naive-Bayes Classifiers: A Decision-Tree Hybrid", "venue":"KDD", "year":"1996", "window":"which is a Bayes network restricted to a tree topology. The results are promising and running times should scale up, but the approach is still restrictive. For example, their accuracy for the <b>Chess</b> dataset, which contains high-order interactions is about 93%, much lower then C4.5 and NBTree, which achieve accuracies above 99%. Conclusions We have described a new algorithm, NBTree, which is a hybrid", "mykey":2462},
 {"datasetID":22, "supportID":"F2DC7581BC7D742CADDA07A2968E02E6B8D46BBB", "rexaID":"bf6cec50b7f7d48d105c8c649210cc3a42d3d71e", "author":"Ron Kohavi", "title":"Scaling Up the Accuracy of Naive-Bayes Classifiers: A Decision-Tree Hybrid", "venue":"KDD", "year":"1996", "window":"which is a Bayes network restricted to a tree topology. The results are promising and running times should scale up, but the approach is still restrictive. For example, their accuracy for the <b>Chess</b> dataset, which contains high-order interactions is about 93%, much lower then C4.5 and NBTree, which achieve accuracies above 99%. Conclusions We have described a new algorithm, NBTree, which is a hybrid", "mykey":2463},
 {"datasetID":53, "supportID":"F2DC7581BC7D742CADDA07A2968E02E6B8D46BBB", "rexaID":"bf6cec50b7f7d48d105c8c649210cc3a42d3d71e", "author":"Ron Kohavi", "title":"Scaling Up the Accuracy of Naive-Bayes Classifiers: A Decision-Tree Hybrid", "venue":"KDD", "year":"1996", "window":"easy to understand when the log probabilities were presented as evidence that adds up in favor of different classes. Figure 1 shows a visualization of the Naive-Bayes classifier for Fisher's <b>iris</b> data set, where the task is to determine the type of iris based on four attributes. Each bar represents evidence for a given class and attribute value. Users can immediately see that all values for", "mykey":2464},
 {"datasetID":148, "supportID":"F2DC7581BC7D742CADDA07A2968E02E6B8D46BBB", "rexaID":"bf6cec50b7f7d48d105c8c649210cc3a42d3d71e", "author":"Ron Kohavi", "title":"Scaling Up the Accuracy of Naive-Bayes Classifiers: A Decision-Tree Hybrid", "venue":"KDD", "year":"1996", "window":"the accuracy of medical diagnosis from 98% to 99% may cut costs by half because the number of errors is halved. Figure 5 shows the ratio of errors (where error is 100%-accuracy). The <b>shuttle</b> dataset, which is the largest dataset tested, has only 0.04% absolute difference between NBTree and C4.5, but the error decreases from 0.05% to 0.01%, which is a huge relative improvement. The number of", "mykey":2465},
 {"datasetID":107, "supportID":"F2DC7581BC7D742CADDA07A2968E02E6B8D46BBB", "rexaID":"bf6cec50b7f7d48d105c8c649210cc3a42d3d71e", "author":"Ron Kohavi", "title":"Scaling Up the Accuracy of Naive-Bayes Classifiers: A Decision-Tree Hybrid", "venue":"KDD", "year":"1996", "window":"segment 19 2,310 CV-10 shuttle 9 43,500 14,500 soybean-large 35 562 CV-10 tic-tac-toe 9 958 CV-10 vehicle 18 846 CV-10 vote 16 435 CV-10 vote1 15 435 CV-10 <b>waveform</b> 40 40 300 4,700 Table 1: The datasets used, the number of attributes, and the training/test-set sizes (CV-10 denotes 10-fold cross-validation was used). NBTree - C4.5 NBTree - NB tic-tac-toe chess letter vehicle vote monk1 segment", "mykey":2466},
 {"datasetID":108, "supportID":"F2DC7581BC7D742CADDA07A2968E02E6B8D46BBB", "rexaID":"bf6cec50b7f7d48d105c8c649210cc3a42d3d71e", "author":"Ron Kohavi", "title":"Scaling Up the Accuracy of Naive-Bayes Classifiers: A Decision-Tree Hybrid", "venue":"KDD", "year":"1996", "window":"segment 19 2,310 CV-10 shuttle 9 43,500 14,500 soybean-large 35 562 CV-10 tic-tac-toe 9 958 CV-10 vehicle 18 846 CV-10 vote 16 435 CV-10 vote1 15 435 CV-10 <b>waveform</b> 40 40 300 4,700 Table 1: The datasets used, the number of attributes, and the training/test-set sizes (CV-10 denotes 10-fold cross-validation was used). NBTree - C4.5 NBTree - NB tic-tac-toe chess letter vehicle vote monk1 segment", "mykey":2467},
 {"datasetID":60, "supportID":"F383AEE7C8A682037D819B6ECC22CCEFDD348502", "rexaID":"cf7f2c27ddd0ac36885f5d144528580d416f2f4c", "author":"Jochen Garcke and Michael Griebel", "title":"Classification with sparse grids using simplicial basis functions", "venue":"Intell. Data Anal, 6", "year":"2002", "window":"% 0.8 20.6 3 train 91.4 % 194.1 86.6 % 9.6 88.0 % 10.8 test 70.8 % 69.9 % 70.5 % 4 train 92.6 % 1217.6 94.2 % 68.3 93.1 % 75.4 test 68.8 % 71.4 % 70.5 % Table 4: Results for the BUPA <b>liver</b> disorders data set basis functions. Note that a testing correctness of 90.6 % and 91.1 % was achieved with neural networks in [39] and [38], respectively, for this data set. 3.2 Small data sets 3.2.1 BUPA Liver The", "mykey":2468},
 {"datasetID":42, "supportID":"F3E745BA9A0A31A0EAD40A76C372F0EC69AAEDE0", "rexaID":"469be567c6e8bb0955c395472ef2fefc4fe43f77", "author":"Erin J. Bredensteiner and Kristin P. Bennett", "title":"Multicategory Classification by Support Vector Machines", "venue":"Department of Mathematics University of Evansville", "year":"", "window":"protocol (ftp) from the UCI Repository of Machine Learning Databases and Domain Theories [16] at ftp://ftp.ics.uci.edu/pub/machine-learning-databases. <b>Glass</b> Identification Database The Glass dataset [11] is used to identify the origin of a sample of glass through chemical analysis. This dataset is comprised of six classes of 214 points with 9 features. The distribution of points by class is as", "mykey":2469},
 {"datasetID":109, "supportID":"F3E745BA9A0A31A0EAD40A76C372F0EC69AAEDE0", "rexaID":"469be567c6e8bb0955c395472ef2fefc4fe43f77", "author":"Erin J. Bredensteiner and Kristin P. Bennett", "title":"Multicategory Classification by Support Vector Machines", "venue":"Department of Mathematics University of Evansville", "year":"", "window":"methods. The kernel function for the piecewise-nonlinear M-SVM and k-SVM methods is K(x, x i ) = x\u00b7x i n + 1 # d , where d is the degree of the desired polynomial. <b>wine</b> Recognition Data The Wine dataset [1] uses the chemical analysis of wine to determine the cultivar. There are 178 points with 13 features. This is a three class dataset distributed as follows: 59 points in class 1, 71 points in", "mykey":2470},
 {"datasetID":69, "supportID":"F49D7374CF15093F6D620177713903EE47B9220A", "rexaID":"75f0f1770a1317fc5ab730609fa1e3cf642794f6", "author":"Xiaojin Zhu", "title":"Label Propagation for Eukaryotic Splice Junction Identification", "venue":"", "year":"2002", "window":"biology problem of eukaryotic <b>splice</b> junction identification. We compare the classi#cation accuracy of using unlabeled data versus not using them, under the same amount of labeled data. 2 The dataset The dataset is the Primate <b>splice-junction</b> gene sequences dataset, available at the UCI machine learning data repository [2]. There are 3190 examples in this dataset. Each example is a sequence of", "mykey":2471},
 {"datasetID":111, "supportID":"F53573C8757F9E2F330F47BF259E611386943EE2", "rexaID":"af8d7875705c4e2cfa6e62edef98f148bab7cd4c", "author":"D. Randall Wilson and Tony R. Martinez", "title":"Heterogeneous Radial Basis Function Networks", "venue":"Proceedings of the International Conference on Neural Networks (ICNN", "year":"1996", "window":"higher generalization accuracy than RBF in 12 out of 23 cases, 10 of which were significant at the 95% level or above. RBF had a higher accuracy in only four cases, and only one of those (the <b>Zoo</b> data set) had a difference that was statistically significant. It is interesting to note that in the Zoo data set, 15 out of 16 of the attributes are boolean, and the remaining attribute, while not linear,", "mykey":2472},
 {"datasetID":98, "supportID":"F61F4F7A98535965FA1399D2EBEA1D266A6838A7", "rexaID":"b9b24d88e45ac7034e22363aac1347ca65caffc6", "author":"I\u00f1aki Inza and Pedro Larraaga and Ramon Etxeberria and Basilio Sierra", "title":"Feature Subset Selection by Bayesian networks based optimization", "venue":"Dept. of Computer Science and Artificial Intelligence. University of the Basque Country", "year":"", "window":"come from the UCI repository [68]. Image dataset comes from the <b>Statlog</b> project [85]. LED24 (Breiman et al. [15]) is a well known artificial dataset with 7 equally relevant and 17 irrelevant binary features. We designed another artificial domain,", "mykey":2473},
 {"datasetID":107, "supportID":"F61F4F7A98535965FA1399D2EBEA1D266A6838A7", "rexaID":"b9b24d88e45ac7034e22363aac1347ca65caffc6", "author":"I\u00f1aki Inza and Pedro Larraaga and Ramon Etxeberria and Basilio Sierra", "title":"Feature Subset Selection by Bayesian networks based optimization", "venue":"Dept. of Computer Science and Artificial Intelligence. University of the Basque Country", "year":"", "window":"principal reason of `overfitting' was the low amount of training instances. To study this issue for FSS-EBNA, we have carried out a set of experiments with different training sizes of <b>Waveform</b> 40 dataset [15] with Naive-Bayes classification algorithm [19]: training sizes of 100; 200; 400; 800 and 1; 600 samples and tested over a fixed test set with 3; 200 instances. Figure 7 summarizes the set of", "mykey":2474},
 {"datasetID":108, "supportID":"F61F4F7A98535965FA1399D2EBEA1D266A6838A7", "rexaID":"b9b24d88e45ac7034e22363aac1347ca65caffc6", "author":"I\u00f1aki Inza and Pedro Larraaga and Ramon Etxeberria and Basilio Sierra", "title":"Feature Subset Selection by Bayesian networks based optimization", "venue":"Dept. of Computer Science and Artificial Intelligence. University of the Basque Country", "year":"", "window":"principal reason of `overfitting' was the low amount of training instances. To study this issue for FSS-EBNA, we have carried out a set of experiments with different training sizes of <b>Waveform</b> 40 dataset [15] with Naive-Bayes classification algorithm [19]: training sizes of 100; 200; 400; 800 and 1; 600 samples and tested over a fixed test set with 3; 200 instances. Figure 7 summarizes the set of", "mykey":2475},
 {"datasetID":14, "supportID":"F6225261FC5C278286E88780919558E91E0EBA18", "rexaID":"dba0c3d458498a4eef66d37b0f3b1cb310086d31", "author":"Bart Baesens and Stijn Viaene and Tony Van Gestel and J. A. K Suykens and Guido Dedene and Bart De Moor and Jan Vanthienen and Katholieke Universiteit Leuven", "title":"An Empirical Assessment of Kernel Type Performance for Least Squares Support Vector Machine Classifiers", "venue":"Dept. Applied Economic Sciences", "year":"", "window":"Liver Disorders (bld), German Credit (gcr), Heart Disease (hea), Johns Hopkins Ionosphere (ion), Pima Indians Diabetes (pid), Sonar (snr), Tic-Tac-Toe (ttt) and the Wisconsin <b>Breast</b> <b>Cancer</b> (wbc) data set. We start with presenting the empirical setup used to construct the LS-SVM classifier. This is followed by a discussion of the obtained results. 3.1 Constructing the LS-SVM Classifier The", "mykey":2476},
 {"datasetID":17, "supportID":"F6225261FC5C278286E88780919558E91E0EBA18", "rexaID":"dba0c3d458498a4eef66d37b0f3b1cb310086d31", "author":"Bart Baesens and Stijn Viaene and Tony Van Gestel and J. A. K Suykens and Guido Dedene and Bart De Moor and Jan Vanthienen and Katholieke Universiteit Leuven", "title":"An Empirical Assessment of Kernel Type Performance for Least Squares Support Vector Machine Classifiers", "venue":"Dept. Applied Economic Sciences", "year":"", "window":"Liver Disorders (bld), German Credit (gcr), Heart Disease (hea), Johns Hopkins Ionosphere (ion), Pima Indians Diabetes (pid), Sonar (snr), Tic-Tac-Toe (ttt) and the <b>Wisconsin</b> <b>Breast</b> <b>Cancer</b> (wbc) data set. We start with presenting the empirical setup used to construct the LS-SVM classifier. This is followed by a discussion of the obtained results. 3.1 Constructing the LS-SVM Classifier The", "mykey":2477},
 {"datasetID":15, "supportID":"F6225261FC5C278286E88780919558E91E0EBA18", "rexaID":"dba0c3d458498a4eef66d37b0f3b1cb310086d31", "author":"Bart Baesens and Stijn Viaene and Tony Van Gestel and J. A. K Suykens and Guido Dedene and Bart De Moor and Jan Vanthienen and Katholieke Universiteit Leuven", "title":"An Empirical Assessment of Kernel Type Performance for Least Squares Support Vector Machine Classifiers", "venue":"Dept. Applied Economic Sciences", "year":"", "window":"Liver Disorders (bld), German Credit (gcr), Heart Disease (hea), Johns Hopkins Ionosphere (ion), Pima Indians Diabetes (pid), Sonar (snr), Tic-Tac-Toe (ttt) and the <b>Wisconsin</b> <b>Breast</b> <b>Cancer</b> (wbc) data set. We start with presenting the empirical setup used to construct the LS-SVM classifier. This is followed by a discussion of the obtained results. 3.1 Constructing the LS-SVM Classifier The", "mykey":2478},
 {"datasetID":16, "supportID":"F6225261FC5C278286E88780919558E91E0EBA18", "rexaID":"dba0c3d458498a4eef66d37b0f3b1cb310086d31", "author":"Bart Baesens and Stijn Viaene and Tony Van Gestel and J. A. K Suykens and Guido Dedene and Bart De Moor and Jan Vanthienen and Katholieke Universiteit Leuven", "title":"An Empirical Assessment of Kernel Type Performance for Least Squares Support Vector Machine Classifiers", "venue":"Dept. Applied Economic Sciences", "year":"", "window":"Liver Disorders (bld), German Credit (gcr), Heart Disease (hea), Johns Hopkins Ionosphere (ion), Pima Indians Diabetes (pid), Sonar (snr), Tic-Tac-Toe (ttt) and the <b>Wisconsin</b> <b>Breast</b> <b>Cancer</b> (wbc) data set. We start with presenting the empirical setup used to construct the LS-SVM classifier. This is followed by a discussion of the obtained results. 3.1 Constructing the LS-SVM Classifier The", "mykey":2479},
 {"datasetID":143, "supportID":"F6225261FC5C278286E88780919558E91E0EBA18", "rexaID":"dba0c3d458498a4eef66d37b0f3b1cb310086d31", "author":"Bart Baesens and Stijn Viaene and Tony Van Gestel and J. A. K Suykens and Guido Dedene and Bart De Moor and Jan Vanthienen and Katholieke Universiteit Leuven", "title":"An Empirical Assessment of Kernel Type Performance for Least Squares Support Vector Machine Classifiers", "venue":"Dept. Applied Economic Sciences", "year":"", "window":"using different types of kernel functions. The data sets considered are <b>Australian Credit</b> (acr), Bupa Liver Disorders (bld), German Credit (gcr), Heart Disease (hea), Johns Hopkins Ionosphere (ion), Pima Indians Diabetes (pid), Sonar (snr), Tic-Tac-Toe", "mykey":2480},
 {"datasetID":45, "supportID":"F62350AB3B186A95A46E06EE65BCBE4FD3DC6B70", "rexaID":"68ad36910e92ea5408b79e50302563fb79484cf4", "author":"Chiranjib Bhattacharyya and Pannagadatta K. S and Alexander J. Smola", "title":"A Second order Cone Programming Formulation for Classifying Missing Data", "venue":"Department of Computer Science and Automation Indian Institute of Science", "year":"", "window":"of the UCI database. From left to right: Pima, Ionosphere, and <b>Heart</b> dataset. Top: small fraction of data with missing variables (50%), Bottom: large number of observations with missing variables (90%) The experimental results are summarized by the graphs(1). The robust", "mykey":2481},
 {"datasetID":52, "supportID":"F62350AB3B186A95A46E06EE65BCBE4FD3DC6B70", "rexaID":"68ad36910e92ea5408b79e50302563fb79484cf4", "author":"Chiranjib Bhattacharyya and Pannagadatta K. S and Alexander J. Smola", "title":"A Second order Cone Programming Formulation for Classifying Missing Data", "venue":"Department of Computer Science and Automation Indian Institute of Science", "year":"", "window":"of the UCI database. From left to right: Pima, <b>Ionosphere</b>  and Heart dataset. Top: small fraction of data with missing variables (50%), Bottom: large number of observations with missing variables (90%) The experimental results are summarized by the graphs(1). The robust", "mykey":2482},
 {"datasetID":14, "supportID":"F6826B292F64EEF274597AF1E91EB63718AFD582", "rexaID":"4ce4c96181e2836dd80a71c2efebc7fb030c55d8", "author":"Baback Moghaddam and Gregory Shakhnarovich", "title":"Boosted Dyadic Kernel Discriminants", "venue":"NIPS", "year":"2002", "window":"the number of support vectors for the SVM, and #k.ev. the number of kernel evaluations required by a boosted hypercuts classifier. Means and standard deviations in 30 trials are reported for each data set. WBC,WPBC,WDBC are Wisconsin <b>Breast</b> <b>Cancer</b>  Prognosis and Diagnosis data sets, respectively. In each experiment, the data set was randomly partitioned into training, validation and test sets of", "mykey":2483},
 {"datasetID":17, "supportID":"F6826B292F64EEF274597AF1E91EB63718AFD582", "rexaID":"4ce4c96181e2836dd80a71c2efebc7fb030c55d8", "author":"Baback Moghaddam and Gregory Shakhnarovich", "title":"Boosted Dyadic Kernel Discriminants", "venue":"NIPS", "year":"2002", "window":"the number of support vectors for the SVM, and #k.ev. the number of kernel evaluations required by a boosted hypercuts classifier. Means and standard deviations in 30 trials are reported for each data set. WBC,WPBC,WDBC are <b>Wisconsin</b> <b>Breast</b> <b>Cancer</b>  Prognosis and Diagnosis data sets, respectively. In each experiment, the data set was randomly partitioned into training, validation and test sets of", "mykey":2484},
 {"datasetID":15, "supportID":"F6826B292F64EEF274597AF1E91EB63718AFD582", "rexaID":"4ce4c96181e2836dd80a71c2efebc7fb030c55d8", "author":"Baback Moghaddam and Gregory Shakhnarovich", "title":"Boosted Dyadic Kernel Discriminants", "venue":"NIPS", "year":"2002", "window":"the number of support vectors for the SVM, and #k.ev. the number of kernel evaluations required by a boosted hypercuts classifier. Means and standard deviations in 30 trials are reported for each data set. WBC,WPBC,WDBC are <b>Wisconsin</b> <b>Breast</b> <b>Cancer</b>  Prognosis and Diagnosis data sets, respectively. In each experiment, the data set was randomly partitioned into training, validation and test sets of", "mykey":2485},
 {"datasetID":16, "supportID":"F6826B292F64EEF274597AF1E91EB63718AFD582", "rexaID":"4ce4c96181e2836dd80a71c2efebc7fb030c55d8", "author":"Baback Moghaddam and Gregory Shakhnarovich", "title":"Boosted Dyadic Kernel Discriminants", "venue":"NIPS", "year":"2002", "window":"the number of support vectors for the SVM, and #k.ev. the number of kernel evaluations required by a boosted hypercuts classifier. Means and standard deviations in 30 trials are reported for each data set. WBC,WPBC,WDBC are <b>Wisconsin</b> <b>Breast</b> <b>Cancer</b>  Prognosis and Diagnosis data sets, respectively. In each experiment, the data set was randomly partitioned into training, validation and test sets of", "mykey":2486},
 {"datasetID":45, "supportID":"F6826B292F64EEF274597AF1E91EB63718AFD582", "rexaID":"4ce4c96181e2836dd80a71c2efebc7fb030c55d8", "author":"Baback Moghaddam and Gregory Shakhnarovich", "title":"Boosted Dyadic Kernel Discriminants", "venue":"NIPS", "year":"2002", "window":"and k-Nearest Neighbor (k-NN). We chose sets large enough for reasonable training/validation/test partitioning, and that represent binary (or easily converted to binary) classification problems. Dataset N d k-NN SVM #SV Hypercuts #k.ev. <b>Heart</b> 90 13 .196 \u00b1.042 .202 \u00b1.038 62 \u00b110 .202 \u00b1.030 50 \u00b112 Ionosphere 120 34 .168 \u00b1.024 .064 \u00b1.018 73 \u00b17 .083 \u00b1.022 63 \u00b17 WBC 200 9 .034 \u00b1.011 .032 \u00b1.008 50 \u00b126", "mykey":2487},
 {"datasetID":137, "supportID":"F6958E1F94409544C5B1A4BEF7E3E13861A8173A", "rexaID":"1543ea7033e6a4fc2ab140096210d7821d7018fc", "author":"Vijay S. Iyengar and Chidanand Apt and Tong Zhang", "title":"Active learning using adaptive resampling", "venue":"KDD", "year":"2000", "window":"the ALAR method are shown in Figure 5. Both ALAR-3-nn and ALAR-vote-E achieve the accuracy goal with only 8000 labeled instances. The last benchmark used is the Mod-Apte split of the <b>Reuters</b> data set available from [20]. Only the top ten categories are considered. For eachofthemwe solve the binary classification problem of being in or out of that category.Weused the notion of information gain", "mykey":2488},
 {"datasetID":23, "supportID":"F6A259EB01135352243E2D963E79BC28C9BC697C", "rexaID":"128dbeaae1b5807933aa2a5983bc1765dcaf6f85", "author":"Brian R. Gaines", "title":"Structured and Unstructured Induction with EDAGs", "venue":"KDD", "year":"1995", "window":"EDAG for this problem with that obtained from human <b>Chess</b> experts. Before this is done, the following section illustrates the induction of EDAGs for a simple chess problem. 2 Modeling a Simple Chess Dataset Quinlan (1979) describes ID3 models of 7 rook versus knight end game situations of increasing difficulty. The third problem involves 647 cases with 4 3-valued attributes, 3 2-valued attributes, and", "mykey":2489},
 {"datasetID":21, "supportID":"F6A259EB01135352243E2D963E79BC28C9BC697C", "rexaID":"128dbeaae1b5807933aa2a5983bc1765dcaf6f85", "author":"Brian R. Gaines", "title":"Structured and Unstructured Induction with EDAGs", "venue":"KDD", "year":"1995", "window":"EDAG for this problem with that obtained from human <b>Chess</b> experts. Before this is done, the following section illustrates the induction of EDAGs for a simple chess problem. 2 Modeling a Simple Chess Dataset Quinlan (1979) describes ID3 models of 7 rook versus knight end game situations of increasing difficulty. The third problem involves 647 cases with 4 3-valued attributes, 3 2-valued attributes, and", "mykey":2490},
 {"datasetID":22, "supportID":"F6A259EB01135352243E2D963E79BC28C9BC697C", "rexaID":"128dbeaae1b5807933aa2a5983bc1765dcaf6f85", "author":"Brian R. Gaines", "title":"Structured and Unstructured Induction with EDAGs", "venue":"KDD", "year":"1995", "window":"EDAG for this problem with that obtained from human <b>Chess</b> experts. Before this is done, the following section illustrates the induction of EDAGs for a simple chess problem. 2 Modeling a Simple Chess Dataset Quinlan (1979) describes ID3 models of 7 rook versus knight end game situations of increasing difficulty. The third problem involves 647 cases with 4 3-valued attributes, 3 2-valued attributes, and", "mykey":2491},
 {"datasetID":5, "supportID":"F6EACF3B188FD5FF1353A0B698C3CE2558C0C9E7", "rexaID":"811517480cb8dca1073ee39a37c9a343a1179aab", "author":"Shay Cohen and Eytan Ruppin and Gideon Dror", "title":"Feature Selection Based on the Shapley Value", "venue":"School of Computer Sciences Tel-Aviv University", "year":"", "window":"of 93% with 109 features. For comparison, [Koller and Sahami, 1996] report that the Markov Blanket algorithm yields approximately 600 selected features with accuracy levels of 89% to 93% on this dataset. 1 The <b>Arrhythmia</b> dataset. This dataset is considered to be a difficult one. CSA with backward elimination did best, yielding an accuracy level of 84% with 21 features. Forward selection with higher", "mykey":2492},
 {"datasetID":51, "supportID":"F6EACF3B188FD5FF1353A0B698C3CE2558C0C9E7", "rexaID":"811517480cb8dca1073ee39a37c9a343a1179aab", "author":"Shay Cohen and Eytan Ruppin and Gideon Dror", "title":"Feature Selection Based on the Shapley Value", "venue":"School of Computer Sciences Tel-Aviv University", "year":"", "window":"For comparison, the grafting algorithm [Perkins et al., 2003] yields an accuracy level of approximately 75% on this dataset. 1 The <b>Internet</b> <b>Ads</b> dataset. All the algorithms did approximately the same, leading to accuracy levels between 94% and 96% with CSA slightly outperforming the others. Interestingly enough, the", "mykey":2493},
 {"datasetID":3, "supportID":"F71E4B90CE0A5FA15B7A148E97485C0B92D816A9", "rexaID":"5ef9c5c8a24b6e0df983284f0caa3fb337c1a77a", "author":"Yuan Jiang and Zhi-Hua Zhou", "title":"Editing Training Data for kNN Classifiers with Neural Network Ensemble", "venue":"ISNN (1)", "year":"2004", "window":"Attribute Data set Categorical Continuous Size Class <b>annealing</b> 33 5 798 6 credit 9 6 690 2 glass 0 9 214 7 hayes-roth 4 0 132 3 iris 0 4 150 3 liver 0 6 345 2 pima 0 8 768 2 soybean 35 0 683 19 wine 0 13 178 3 zoo 16", "mykey":2494},
 {"datasetID":42, "supportID":"F71E4B90CE0A5FA15B7A148E97485C0B92D816A9", "rexaID":"5ef9c5c8a24b6e0df983284f0caa3fb337c1a77a", "author":"Yuan Jiang and Zhi-Hua Zhou", "title":"Editing Training Data for kNN Classifiers with Neural Network Ensemble", "venue":"ISNN (1)", "year":"2004", "window":"i.e. annealing, credit, liver, pima, soybean, wine and zoo. RemoveOnly obtains the best performance on three data sets, i.e. <b>glass</b>  hayes-roth and wine. It is surprising that Depuration obtains the best performance on only one data set, i.e. iris, as RelabelOnly does. These observations indicate that NNEE is a", "mykey":2495},
 {"datasetID":44, "supportID":"F71E4B90CE0A5FA15B7A148E97485C0B92D816A9", "rexaID":"5ef9c5c8a24b6e0df983284f0caa3fb337c1a77a", "author":"Yuan Jiang and Zhi-Hua Zhou", "title":"Editing Training Data for kNN Classifiers with Neural Network Ensemble", "venue":"ISNN (1)", "year":"2004", "window":"i.e. annealing, credit, liver, pima, soybean, wine and zoo. RemoveOnly obtains the best performance on three data sets, i.e. glass, <b>hayes</b> <b>roth</b> and wine. It is surprising that Depuration obtains the best performance on only one data set, i.e. iris, as RelabelOnly does. These observations indicate that NNEE is a", "mykey":2496},
 {"datasetID":53, "supportID":"F71E4B90CE0A5FA15B7A148E97485C0B92D816A9", "rexaID":"5ef9c5c8a24b6e0df983284f0caa3fb337c1a77a", "author":"Yuan Jiang and Zhi-Hua Zhou", "title":"Editing Training Data for kNN Classifiers with Neural Network Ensemble", "venue":"ISNN (1)", "year":"2004", "window":"i.e. glass, hayes-roth and wine. It is surprising that Depuration obtains the best performance on only one data set, i.e. <b>iris</b>  as RelabelOnly does. These observations indicate that NNEE is a better editing approach than Depuration. Moreover, since the e\u00aeect of Depuration is only comparable to that of", "mykey":2497},
 {"datasetID":60, "supportID":"F71E4B90CE0A5FA15B7A148E97485C0B92D816A9", "rexaID":"5ef9c5c8a24b6e0df983284f0caa3fb337c1a77a", "author":"Yuan Jiang and Zhi-Hua Zhou", "title":"Editing Training Data for kNN Classifiers with Neural Network Ensemble", "venue":"ISNN (1)", "year":"2004", "window":"of five hidden units. Therefore here the approach is denoted as NNEE(5,5). Table 6 shows that the NNEE approach achieves the best editing e\u00aeect. In detail, it obtains the best performance on seven data sets, i.e. annealing, credit, <b>liver</b>  pima, soybean, wine and zoo. RemoveOnly obtains the best performance on three data sets, i.e. glass, hayes-roth and wine. It is surprising that Depuration obtains", "mykey":2498},
 {"datasetID":90, "supportID":"F71E4B90CE0A5FA15B7A148E97485C0B92D816A9", "rexaID":"5ef9c5c8a24b6e0df983284f0caa3fb337c1a77a", "author":"Yuan Jiang and Zhi-Hua Zhou", "title":"Editing Training Data for kNN Classifiers with Neural Network Ensemble", "venue":"ISNN (1)", "year":"2004", "window":"of five hidden units. Therefore here the approach is denoted as NNEE(5,5). Table 6 shows that the NNEE approach achieves the best editing e\u00aeect. In detail, it obtains the best performance on seven data sets, i.e. annealing, credit, liver, pima, <b>soybean</b>  wine and zoo. RemoveOnly obtains the best performance on three data sets, i.e. glass, hayes-roth and wine. It is surprising that Depuration obtains", "mykey":2499},
 {"datasetID":91, "supportID":"F71E4B90CE0A5FA15B7A148E97485C0B92D816A9", "rexaID":"5ef9c5c8a24b6e0df983284f0caa3fb337c1a77a", "author":"Yuan Jiang and Zhi-Hua Zhou", "title":"Editing Training Data for kNN Classifiers with Neural Network Ensemble", "venue":"ISNN (1)", "year":"2004", "window":"of five hidden units. Therefore here the approach is denoted as NNEE(5,5). Table 6 shows that the NNEE approach achieves the best editing e\u00aeect. In detail, it obtains the best performance on seven data sets, i.e. annealing, credit, liver, pima, <b>soybean</b>  wine and zoo. RemoveOnly obtains the best performance on three data sets, i.e. glass, hayes-roth and wine. It is surprising that Depuration obtains", "mykey":2500},
 {"datasetID":109, "supportID":"F71E4B90CE0A5FA15B7A148E97485C0B92D816A9", "rexaID":"5ef9c5c8a24b6e0df983284f0caa3fb337c1a77a", "author":"Yuan Jiang and Zhi-Hua Zhou", "title":"Editing Training Data for kNN Classifiers with Neural Network Ensemble", "venue":"ISNN (1)", "year":"2004", "window":"Size Class annealing 33 5 798 6 credit 9 6 690 2 glass 0 9 214 7 hayes-roth 4 0 132 3 iris 0 4 150 3 liver 0 6 345 2 pima 0 8 768 2 soybean 35 0 683 19 <b>wine</b> 0 13 178 3 zoo 16 0 101 7 On each data set, 10 runs of 10-fold cross validation is performed with random partitions. The e\u00aeects of the editing approaches described in Section 2 are compared through coupling them with a 3NN classifier. The", "mykey":2501},
 {"datasetID":111, "supportID":"F71E4B90CE0A5FA15B7A148E97485C0B92D816A9", "rexaID":"5ef9c5c8a24b6e0df983284f0caa3fb337c1a77a", "author":"Yuan Jiang and Zhi-Hua Zhou", "title":"Editing Training Data for kNN Classifiers with Neural Network Ensemble", "venue":"ISNN (1)", "year":"2004", "window":"Size Class annealing 33 5 798 6 credit 9 6 690 2 glass 0 9 214 7 hayes-roth 4 0 132 3 iris 0 4 150 3 liver 0 6 345 2 pima 0 8 768 2 soybean 35 0 683 19 wine 0 13 178 3 <b>zoo</b> 16 0 101 7 On each data set, 10 runs of 10-fold cross validation is performed with random partitions. The e\u00aeects of the editing approaches described in Section 2 are compared through coupling them with a 3NN classifier. The", "mykey":2502},
 {"datasetID":25, "supportID":"F74489A035B1264765FB8AAFC2F6BB3F446355B5", "rexaID":"5d5f6b9e1802fc3aae824310b7486069318d5b91", "author":"Matthew Brand", "title":"An Entropic Estimator for Structure Discovery", "venue":"NIPS", "year":"1998", "window":"To explore the practical utility of this framework, we will use entropically estimated HMMs as a window into the hidden structure of some human-generated time-series. <b>Bach</b> <b>Chorales</b>  We obtained a dataset of melodic lines from 100 of J.S. Bach's 371 surviving chorales from the UCI repository [Merz and Murphy, 1998], and transposed all into the key of C. We compared entropically and conventionally", "mykey":2503},
 {"datasetID":12, "supportID":"F744A847C17CFDEED20E4B1363BDCFAA9490CA1B", "rexaID":"e140ecaac8486469d0ef5b237f4fa08d7315c5ed", "author":"Remco R. Bouckaert", "title":"Accuracy bounds for ensembles under 0 { 1 loss", "venue":"Xtal Mountain Information Technology & Computer Science Department, University of Waikato", "year":"2002", "window":"of 100 cases were generated and the cardinality of the variables was varied from 2 to 12, 3 Weka can be obtained from http://www.cs.waikato.ac.nz/ml/ 4 The following datasets were used: autos, <b>balance</b> <b>scale</b>  breast-cancer, breast-w, horsecolic, credit-rating, german-credit, pima-diabetes, glass, heart-c, heart-h, heart-statlog, hepatitis, iris, labor, lymphography,", "mykey":2504},
 {"datasetID":14, "supportID":"F744A847C17CFDEED20E4B1363BDCFAA9490CA1B", "rexaID":"e140ecaac8486469d0ef5b237f4fa08d7315c5ed", "author":"Remco R. Bouckaert", "title":"Accuracy bounds for ensembles under 0 { 1 loss", "venue":"Xtal Mountain Information Technology & Computer Science Department, University of Waikato", "year":"2002", "window":"of 100 cases were generated and the cardinality of the variables was varied from 2 to 12, 3 Weka can be obtained from http://www.cs.waikato.ac.nz/ml/ 4 The following datasets were used: autos, balance-scale, <b>breast</b> <b>cancer</b>  breast-w, horsecolic, credit-rating, german-credit, pima-diabetes, glass, heart-c, heart-h, heart-statlog, hepatitis, iris, labor, lymphography,", "mykey":2505},
 {"datasetID":83, "supportID":"F744A847C17CFDEED20E4B1363BDCFAA9490CA1B", "rexaID":"e140ecaac8486469d0ef5b237f4fa08d7315c5ed", "author":"Remco R. Bouckaert", "title":"Accuracy bounds for ensembles under 0 { 1 loss", "venue":"Xtal Mountain Information Technology & Computer Science Department, University of Waikato", "year":"2002", "window":"voting respectively. Also the line A = # A and the approximation (8) are plotted. All datapoints are between those two lines, except for the datapoint for <b>primary</b> <b>tumor</b>  To get more datapoints, 110 datasets were generated randomly generating a Bayesian network, populating it with randomly selected probability tables. The data was generated by instantiating the variables one by one according to the", "mykey":2506},
 {"datasetID":151, "supportID":"F7E43C9DA56C3BF10398B407F0869F4564FBC335", "rexaID":"ece9c7ce7e687a5cd309737e5358873e424f7fb4", "author":"Hiroshi Shimodaira and Jun Okui and Mitsuru Nakai", "title":"IMPROVING THE GENERALIZATION PERFORMANCE OF THE MCE/GPD LEARNING", "venue":"School of Information Science Japan Advanced Institute of Science and Technology Tatsunokuchi, Ishikawa", "year":"", "window":"parameter updating rule of (6) were set to the one obtained by the EBP learning. A. Results for Two-Class Problems Preliminary experiments were, at first, performed for two-class problems on the UCI datasets \"cancer\", \"house\" and  <b>sonar</b> . Each dataset was divided into two groups, one was used for training and the other was used for testing. The experimental results are summarized in Table 1. It can be", "mykey":2507},
 {"datasetID":54, "supportID":"F7E43C9DA56C3BF10398B407F0869F4564FBC335", "rexaID":"ece9c7ce7e687a5cd309737e5358873e424f7fb4", "author":"Hiroshi Shimodaira and Jun Okui and Mitsuru Nakai", "title":"IMPROVING THE GENERALIZATION PERFORMANCE OF THE MCE/GPD LEARNING", "venue":"School of Information Science Japan Advanced Institute of Science and Technology Tatsunokuchi, Ishikawa", "year":"", "window":"and network architecture Dataset #classes #attributes #hidden nodes <b>isolet</b> UCI) 26 617 32 vowels(ATR) 5 12 12 80 85 90 95 100 0.001 0.01 0.1 1 10 Correct classification rate [%] g mMCE Figure. 2: Classification performance for the", "mykey":2508},
 {"datasetID":48, "supportID":"F81EC1EED179EE84BC537C3010E52A953D8E4CB6", "rexaID":"5657ca96b13a51d96b97c4f8d8f5e724a616b9e6", "author":"Dorian Suc and Ivan Bratko", "title":"Combining Learning Constraints and Numerical Regression", "venue":"National ICT Australia, Sydney Laboratory at UNSW", "year":"", "window":"which enables a better comparison of Q 2 to other methods. These data sets are AutoMpg, AutoPrice, <b>Housing</b>  MachineCpu and Servo. The other three data sets are from dynamic domains where QUIN has typically been applied so far [Suc, 2003; Suc and Bratko, 2002] . It should", "mykey":2509},
 {"datasetID":87, "supportID":"F81EC1EED179EE84BC537C3010E52A953D8E4CB6", "rexaID":"5657ca96b13a51d96b97c4f8d8f5e724a616b9e6", "author":"Dorian Suc and Ivan Bratko", "title":"Combining Learning Constraints and Numerical Regression", "venue":"National ICT Australia, Sydney Laboratory at UNSW", "year":"", "window":"are AutoMpg, AutoPrice, Housing, MachineCpu and <b>Servo</b>  The other three data sets are from dynamic domains where QUIN has typically been applied so far [Suc, 2003; Suc and Bratko, 2002] . It should be noted that in these domains the primary objective was to explain the", "mykey":2510},
 {"datasetID":2, "supportID":"F89D7DEED7C5DAEEF7C41729526FF177F0AEAACF", "rexaID":"1d416aa15df505a2052d91f04ce93d8e3f2bfa7e", "author":"Bianca Zadrozny", "title":"Learning and evaluating classifiers under sample selection bias", "venue":"ICML", "year":"2004", "window":"3.5. Experimental results To verify the effects of sample selection bias experimentally, we apply Naive Bayes, logistic regression, C4.5 and SVMLight (soft margin) (Joachims, 2000b) to the <b>Adult</b> dataset, available from the UCI Machine Learning repository (Blake & Merz, 1998). We assume that the original dataset is not biased and artificially simulate biasedness by generating a value for s for each", "mykey":2511},
 {"datasetID":23, "supportID":"F94F49DB53C70B6D23F281699503444B7E7347B2", "rexaID":"5bc77452a8b6552aed91aa3294068d7110dc54af", "author":"Russell Greiner and Wei Zhou", "title":"Structural Extension to Logistic Regression: Discriminative Parameter Learning of Belief Net Classifiers", "venue":"AAAI/IAAI", "year":"2002", "window":"data [Koh95]. To deal with continuous variables, we implemented supervised entropy discretization [FI93]. Table 2 summarizes the results. (x5.2.2 will later explain the line separating the first 20 datasets from the final 5.) We use the <b>CHESS</b> dataset (36 binary or ternary attributes) to illustrate the basic behaviour of the algorithms. Figure 2(a) shows the performance, on this dataset, of our NB+ELR", "mykey":2512},
 {"datasetID":21, "supportID":"F94F49DB53C70B6D23F281699503444B7E7347B2", "rexaID":"5bc77452a8b6552aed91aa3294068d7110dc54af", "author":"Russell Greiner and Wei Zhou", "title":"Structural Extension to Logistic Regression: Discriminative Parameter Learning of Belief Net Classifiers", "venue":"AAAI/IAAI", "year":"2002", "window":"data [Koh95]. To deal with continuous variables, we implemented supervised entropy discretization [FI93]. Table 2 summarizes the results. (x5.2.2 will later explain the line separating the first 20 datasets from the final 5.) We use the <b>CHESS</b> dataset (36 binary or ternary attributes) to illustrate the basic behaviour of the algorithms. Figure 2(a) shows the performance, on this dataset, of our NB+ELR", "mykey":2513},
 {"datasetID":22, "supportID":"F94F49DB53C70B6D23F281699503444B7E7347B2", "rexaID":"5bc77452a8b6552aed91aa3294068d7110dc54af", "author":"Russell Greiner and Wei Zhou", "title":"Structural Extension to Logistic Regression: Discriminative Parameter Learning of Belief Net Classifiers", "venue":"AAAI/IAAI", "year":"2002", "window":"data [Koh95]. To deal with continuous variables, we implemented supervised entropy discretization [FI93]. Table 2 summarizes the results. (x5.2.2 will later explain the line separating the first 20 datasets from the final 5.) We use the <b>CHESS</b> dataset (36 binary or ternary attributes) to illustrate the basic behaviour of the algorithms. Figure 2(a) shows the performance, on this dataset, of our NB+ELR", "mykey":2514},
 {"datasetID":14, "supportID":"F9AD5ADDEFAE60D6C5E4AC7AEDFEDD6820D7428C", "rexaID":"f820b10d6723504d343c9741f7333fbc3393fd05", "author":"Lorne Mason and Jonathan Baxter and Peter L. Bartlett and Marcus Frean", "title":"Boosting Algorithms as Gradient Descent", "venue":"NIPS", "year":"1999", "window":"0% noise - AdaBoost 0% noise - DOOM II 15% noise - AdaBoost 15% noise - DOOM II Figure 2: Margin distributions for AdaBoost and DOOM II with 0% and 15% label noise for the <b>breast</b> <b>cancer</b> and splice data sets. of AdaBoost's test error and the minimum of the normalized sigmoid cost very nearly coincide, showing that the sigmoid cost function predicts when AdaBoost will start to over#t. References [1] P.", "mykey":2515},
 {"datasetID":151, "supportID":"F9AD5ADDEFAE60D6C5E4AC7AEDFEDD6820D7428C", "rexaID":"f820b10d6723504d343c9741f7333fbc3393fd05", "author":"Lorne Mason and Jonathan Baxter and Peter L. Bartlett and Marcus Frean", "title":"Boosting Algorithms as Gradient Descent", "venue":"NIPS", "year":"1999", "window":"the minimum -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 2.5 3 3.5 Error advantage (%) Data set <b>sonar</b> cleve ionosphere vote1 credit breast-cancer pima-indians hypo1 splice 0% noise 5% noise 15% noise Figure 1: Summary of test error advantage (with standard error bars) of DOOM II over AdaBoost", "mykey":2516},
 {"datasetID":52, "supportID":"F9AD5ADDEFAE60D6C5E4AC7AEDFEDD6820D7428C", "rexaID":"f820b10d6723504d343c9741f7333fbc3393fd05", "author":"Lorne Mason and Jonathan Baxter and Peter L. Bartlett and Marcus Frean", "title":"Boosting Algorithms as Gradient Descent", "venue":"NIPS", "year":"1999", "window":"the minimum -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 2.5 3 3.5 Error advantage (%) Data set sonar cleve <b>ionosphere</b> vote1 credit breast-cancer pima-indians hypo1 splice 0% noise 5% noise 15% noise Figure 1: Summary of test error advantage (with standard error bars) of DOOM II over AdaBoost", "mykey":2517},
 {"datasetID":56, "supportID":"F9AD5ADDEFAE60D6C5E4AC7AEDFEDD6820D7428C", "rexaID":"f820b10d6723504d343c9741f7333fbc3393fd05", "author":"Lorne Mason and Jonathan Baxter and Peter L. Bartlett and Marcus Frean", "title":"Boosting Algorithms as Gradient Descent", "venue":"NIPS", "year":"1999", "window":"vote1 AdaBoost test error Exponential cost Normalized sigmoid cost Figure 3: AdaBoost test error, exponential cost and normalized sigmoid cost over 10000 rounds of AdaBoost for the <b>labor</b> and vote1 data sets. Both costs have been scaled in each case for easier comparison with test error. [6] H. Drucker and C. Cortes. Boosting decision trees. In Advances in Neural Information Processing Systems 8, pages", "mykey":2518},
 {"datasetID":69, "supportID":"F9AD5ADDEFAE60D6C5E4AC7AEDFEDD6820D7428C", "rexaID":"f820b10d6723504d343c9741f7333fbc3393fd05", "author":"Lorne Mason and Jonathan Baxter and Peter L. Bartlett and Marcus Frean", "title":"Boosting Algorithms as Gradient Descent", "venue":"NIPS", "year":"1999", "window":"0% noise - AdaBoost 0% noise - DOOM II 15% noise - AdaBoost 15% noise - DOOM II Figure 2: Margin distributions for AdaBoost and DOOM II with 0% and 15% label noise for the breast-cancer and <b>splice</b> data sets. of AdaBoost's test error and the minimum of the normalized sigmoid cost very nearly coincide, showing that the sigmoid cost function predicts when AdaBoost will start to over#t. References [1] P.", "mykey":2519},
 {"datasetID":137, "supportID":"F9DCB5B71FBAE667FACC577161F4531BC8CEB1DB", "rexaID":"1543ea7033e6a4fc2ab140096210d7821d7018fc", "author":"Vijay S. Iyengar and Chidanand Apt and Tong Zhang", "title":"Active learning using adaptive resampling", "venue":"KDD", "year":"2000", "window":"the ALAR method are shown in Figure 5. Both ALAR-3-nn and ALAR-vote-E achieve the accuracy goal with only 8000 labeled instances. The last benchmark used is the Mod-Apte split of the <b>Reuters</b> data set available from [20]. Only the top ten categories are considered. For eachofthemwe solve the binary classi#cation problem of being in or out of that category.Weused the notion of information gain", "mykey":2520},
 {"datasetID":34, "supportID":"FA436A1C6EFEFFF11A6B4D429D452AB09C67A0D2", "rexaID":"69aaccfb9601e579827a9940738bb255a8cec3b5", "author":"Mark A. Hall", "title":"Correlation-based Feature Selection for Discrete and Numeric Class Machine Learning", "venue":"ICML", "year":"2000", "window":"in accuracy. From Figure 1 it can be seen that ReliefF selects fewer attributes with a threshold of 0.01 than with a threshold of 0, but CFS selects significantly fewer attributes than both on all data sets except <b>diabetes</b>  0 5 10 15 20 25 30 35 40 0 2 4 6 8 10 12 14 16 number of features dataset Figure 1. Average number of features selected by ReliefF with threshold 0 (left), ReliefF with threshold", "mykey":2521},
 {"datasetID":42, "supportID":"FA436A1C6EFEFFF11A6B4D429D452AB09C67A0D2", "rexaID":"69aaccfb9601e579827a9940738bb255a8cec3b5", "author":"Mark A. Hall", "title":"Correlation-based Feature Selection for Discrete and Numeric Class Machine Learning", "venue":"ICML", "year":"2000", "window":"In the case of CFS, a discretized copy of each training split was made for it to operate on. The same folds were used for each feature selector{learning scheme combination. Table 1. Discrete class data sets. Data Set Instances Num. Nom. Classes 1 <b>glass</b> 2 163 9 0 2 2 anneal 898 6 32 5 3 breast-c 286 0 9 2 4 credit-g 1000 7 13 2 5 diabetes 768 8 0 2 6 horse colic 368 7 15 2 7 heart-c 303 6 7 2 8", "mykey":2522},
 {"datasetID":46, "supportID":"FA7078DC4F865F378E48086B52B9091A18E30876", "rexaID":"4967c873f90995ec2e7fd690a0ec6118f1adc807", "author":"Wl/odzisl/aw Duch and Rafal Adamczak and Geerd H. F Diercksen", "title":"Classification, Association and Pattern Completion using Neural Similarity Based Methods", "venue":"Department of Computer Methods, Nicholas Copernicus University", "year":"", "window":"85.5% (with 20 neurons), and inserting a new value that does not appear in the data, such as -100, decreased accuracy to 81.5% (using 22 neurons). The same behavior has been observed for <b>Hepatitis</b> dataset taken from the same source. the data contains 155 vectors, 18 attributes, 13 of them are binary, other have integer values. The last attribute has 67 missing values, attribute 16 has 29 missing", "mykey":2523},
 {"datasetID":53, "supportID":"FA7078DC4F865F378E48086B52B9091A18E30876", "rexaID":"4967c873f90995ec2e7fd690a0ec6118f1adc807", "author":"Wl/odzisl/aw Duch and Rafal Adamczak and Geerd H. F Diercksen", "title":"Classification, Association and Pattern Completion using Neural Similarity Based Methods", "venue":"Department of Computer Methods, Nicholas Copernicus University", "year":"", "window":"them on a unit sphere defined by this metric. 6PEDAGOGICAL ILLUSTRATION The influence of non-Euclidean distance functions on the decision borders is illustrated here on the classical <b>Iris</b> flowers dataset, containing 50 cases in each of the 3 classes. The flowers are described by 4 measurements (petal and sepal width and length). Two classes, Iris virginica and Iris versicolor, overlap, and therefore", "mykey":2524},
 {"datasetID":14, "supportID":"FAA8ACAE21B3E5D9BFF3C84D3EE3A80B3ECCFF5A", "rexaID":"7d8a7f4c9a24d1127a5ded21969c82ed63037c42", "author":"Erin J. Bredensteiner and Kristin P. Bennett", "title":"Feature Minimization within Decision Trees", "venue":"National Science Foundation", "year":"1996", "window":"attributes. Each patient is classified as to whether there is presence or absence of heart disease. There are 137 patients who have a presence of heart disease. Wisconsin <b>breast</b> <b>Cancer</b> Database This data set is used to classify 682 patients 18 with breast cancer. Each patient is represented by nine integral attributes ranging in value from 1 to 10. The two classes represented are benign and malignant:", "mykey":2525},
 {"datasetID":17, "supportID":"FAA8ACAE21B3E5D9BFF3C84D3EE3A80B3ECCFF5A", "rexaID":"7d8a7f4c9a24d1127a5ded21969c82ed63037c42", "author":"Erin J. Bredensteiner and Kristin P. Bennett", "title":"Feature Minimization within Decision Trees", "venue":"National Science Foundation", "year":"1996", "window":"attributes. Each patient is classified as to whether there is presence or absence of heart disease. There are 137 patients who have a presence of heart disease. <b>Wisconsin</b> <b>breast</b> <b>Cancer</b> Database This data set is used to classify 682 patients 18 with breast cancer. Each patient is represented by nine integral attributes ranging in value from 1 to 10. The two classes represented are benign and malignant:", "mykey":2526},
 {"datasetID":15, "supportID":"FAA8ACAE21B3E5D9BFF3C84D3EE3A80B3ECCFF5A", "rexaID":"7d8a7f4c9a24d1127a5ded21969c82ed63037c42", "author":"Erin J. Bredensteiner and Kristin P. Bennett", "title":"Feature Minimization within Decision Trees", "venue":"National Science Foundation", "year":"1996", "window":"attributes. Each patient is classified as to whether there is presence or absence of heart disease. There are 137 patients who have a presence of heart disease. <b>Wisconsin</b> <b>breast</b> <b>Cancer</b> Database This data set is used to classify 682 patients 18 with breast cancer. Each patient is represented by nine integral attributes ranging in value from 1 to 10. The two classes represented are benign and malignant:", "mykey":2527},
 {"datasetID":16, "supportID":"FAA8ACAE21B3E5D9BFF3C84D3EE3A80B3ECCFF5A", "rexaID":"7d8a7f4c9a24d1127a5ded21969c82ed63037c42", "author":"Erin J. Bredensteiner and Kristin P. Bennett", "title":"Feature Minimization within Decision Trees", "venue":"National Science Foundation", "year":"1996", "window":"attributes. Each patient is classified as to whether there is presence or absence of heart disease. There are 137 patients who have a presence of heart disease. <b>Wisconsin</b> <b>breast</b> <b>Cancer</b> Database This data set is used to classify 682 patients 18 with breast cancer. Each patient is represented by nine integral attributes ranging in value from 1 to 10. The two classes represented are benign and malignant:", "mykey":2528},
 {"datasetID":105, "supportID":"FAA8ACAE21B3E5D9BFF3C84D3EE3A80B3ECCFF5A", "rexaID":"7d8a7f4c9a24d1127a5ded21969c82ed63037c42", "author":"Erin J. Bredensteiner and Kristin P. Bennett", "title":"Feature Minimization within Decision Trees", "venue":"National Science Foundation", "year":"1996", "window":"is linearly separable. 1984 United States Congressional <b>Voting Records</b> Database This data set includes votes for each of the 435 U.S. House of Representatives Congressmen. There are 267 democrats and 168 republicans. The chosen attributes represent 16 key votes. Possible values for the", "mykey":2529},
 {"datasetID":151, "supportID":"FAA8ACAE21B3E5D9BFF3C84D3EE3A80B3ECCFF5A", "rexaID":"7d8a7f4c9a24d1127a5ded21969c82ed63037c42", "author":"Erin J. Bredensteiner and Kristin P. Bennett", "title":"Feature Minimization within Decision Trees", "venue":"National Science Foundation", "year":"1996", "window":"are generated from a large set of star and galaxy images collected by Odewahn [22] at the University of Minnesota. <b>Sonar</b>  <b>Mines</b> vs. <b>Rocks</b> The Sonar data set [13] contains sixty real-valued attributes between 0.0 and 1.0 used to define 208 mines and rocks. Attributes are obtained by bouncing sonar signals off a metal cylinder (or rock) at various angles", "mykey":2530},
 {"datasetID":34, "supportID":"FAC3CBFDE2AC66DEE464925F2A8967F5A0884CDE", "rexaID":"d64d2705cabed449e8cb2ecc3c3c77c54ee71051", "author":"Jeroen Eggermont and Joost N. Kok and Walter A. Kosters", "title":"Genetic Programming for data classification: partitioning the search space", "venue":"SAC", "year":"2004", "window":"The results of our refined gp algorithm using the gain ratio criterion are again worse than those of our clustering and other refined gp algorithms. The Pima Indians <b>diabetes</b> Data Set On the Pima Indians diabetes data set (see Table 5) the refined gp algorithms using the gain criterion are again better than those using the gain ratio criterion. If we compare the results of our", "mykey":2531},
 {"datasetID":45, "supportID":"FAC3CBFDE2AC66DEE464925F2A8967F5A0884CDE", "rexaID":"d64d2705cabed449e8cb2ecc3c3c77c54ee71051", "author":"Jeroen Eggermont and Joost N. Kok and Walter A. Kosters", "title":"Genetic Programming for data classification: partitioning the search space", "venue":"SAC", "year":"2004", "window":"using different the sets of internal nodes. The same behavior is seen for k = 4 and k = 5. In all cases the discovered decision trees differ syntactically per fold and random seed. The <b>Heart</b> Disease Data Set The results on the Heart disease data set are displayed in Table 6. All our gp algorithms show a large improvement in misclassification performance over our simple gp algorithm. In all but two cases", "mykey":2532},
 {"datasetID":52, "supportID":"FAC3CBFDE2AC66DEE464925F2A8967F5A0884CDE", "rexaID":"d64d2705cabed449e8cb2ecc3c3c77c54ee71051", "author":"Jeroen Eggermont and Joost N. Kok and Walter A. Kosters", "title":"Genetic Programming for data classification: partitioning the search space", "venue":"SAC", "year":"2004", "window":"algorithm has a much smaller standard deviation. When we look at the number of clusters or maximum number of partitions we see that a maximum of 2 clusters or partitions is clearly the best for this data set. The <b>Ionosphere</b> Data Set If we look at the results on the Ionosphere data set in Table 7 we see that using the gain ratio instead of the gain criterion with our refined gp algorithms greatly", "mykey":2533},
 {"datasetID":53, "supportID":"FAC3CBFDE2AC66DEE464925F2A8967F5A0884CDE", "rexaID":"d64d2705cabed449e8cb2ecc3c3c77c54ee71051", "author":"Jeroen Eggermont and Joost N. Kok and Walter A. Kosters", "title":"Genetic Programming for data classification: partitioning the search space", "venue":"SAC", "year":"2004", "window":"is disappointing as only our clustering gp algorithm with 3 clusters per numerical valued attribute manages to really outperform our simple gp but still performs much worse than C4.5. The <b>Iris</b> Data Set If we look at the results of our gp algorithms on the Iris data set in Table 8 we see that by far the best performance is achieved by our clustering gp algorithm with 3 clusters per numerical valued", "mykey":2534},
 {"datasetID":79, "supportID":"FAC3CBFDE2AC66DEE464925F2A8967F5A0884CDE", "rexaID":"d64d2705cabed449e8cb2ecc3c3c77c54ee71051", "author":"Jeroen Eggermont and Joost N. Kok and Walter A. Kosters", "title":"Genetic Programming for data classification: partitioning the search space", "venue":"SAC", "year":"2004", "window":"The results of our refined gp algorithm using the gain ratio criterion are again worse than those of our clustering and other refined gp algorithms. The <b>Pima</b> <b>Indians</b> <b>diabetes</b> Data Set On the Pima Indians diabetes data set (see Table 5) the refined gp algorithms using the gain criterion are again better than those using the gain ratio criterion. If we compare the results of our", "mykey":2535},
 {"datasetID":143, "supportID":"FAC3CBFDE2AC66DEE464925F2A8967F5A0884CDE", "rexaID":"d64d2705cabed449e8cb2ecc3c3c77c54ee71051", "author":"Jeroen Eggermont and Joost N. Kok and Walter A. Kosters", "title":"Genetic Programming for data classification: partitioning the search space", "venue":"SAC", "year":"2004", "window":"used in the experiments data set records attributes classes <b>Australian credit</b> (statlog) 690 14 2 German credit (statlog) 1000 23 2 Pima Indians diabetes 768 8 2 Heart disease (statlog) 270 13 2 Ionosphere 351 34 2 Iris 150 4 3 .", "mykey":2536},
 {"datasetID":144, "supportID":"FAC3CBFDE2AC66DEE464925F2A8967F5A0884CDE", "rexaID":"d64d2705cabed449e8cb2ecc3c3c77c54ee71051", "author":"Jeroen Eggermont and Joost N. Kok and Walter A. Kosters", "title":"Genetic Programming for data classification: partitioning the search space", "venue":"SAC", "year":"2004", "window":"C4.5 or our simple gp. A positive aspect of the refined gp algorithms using the gain ratio criterion is that the standard deviations are lower than for our other algorithms. Table 4: <b>German credit</b> data set results algorithm k average s.d. best worst rank clustering gp 2 27.8 0.7 26.3 28.8 4 clustering gp 3 28.0 0.8 27.0 29.8 6 clustering gp 4 27.9 0.9 26.7 29.4 5 clustering gp 5 28.4 0.8 26.9 29.5 11", "mykey":2537},
 {"datasetID":34, "supportID":"FAF455E8C7D9704C3E3C860D2757AC50229439F3", "rexaID":"ba4305796a9e2896e810fd6cf96fbe11c6d5858b", "author":"Stefan R uping", "title":"A Simple Method For Estimating Conditional Probabilities For SVMs", "venue":"CS Department, AI Unit Dortmund University", "year":"", "window":"including 7 data sets from the UCI Repository [9] (covtype, <b>diabetes</b>  digits, digits, ionosphere, liver, mushroom, promoters) and 4 other real-world data sets: a business cycle analysis problem (business), an analysis", "mykey":2538},
 {"datasetID":125, "supportID":"FAF455E8C7D9704C3E3C860D2757AC50229439F3", "rexaID":"ba4305796a9e2896e810fd6cf96fbe11c6d5858b", "author":"Stefan R uping", "title":"A Simple Method For Estimating Conditional Probabilities For SVMs", "venue":"CS Department, AI Unit Dortmund University", "year":"", "window":"a business cycle analysis problem (business), an analysis of a direct mailing application (directmailing), a data set from a life <b>insurance</b> <b>company</b> (insurance) and intensive care patient monitoring data (medicine). Prior to learning, nominal attributes were binarised and the attributes were scaled to expectancy 0", "mykey":2539},
 {"datasetID":73, "supportID":"FAF455E8C7D9704C3E3C860D2757AC50229439F3", "rexaID":"ba4305796a9e2896e810fd6cf96fbe11c6d5858b", "author":"Stefan R uping", "title":"A Simple Method For Estimating Conditional Probabilities For SVMs", "venue":"CS Department, AI Unit Dortmund University", "year":"", "window":"from the UCI Repository [9] (covtype, diabetes, digits, digits, ionosphere, liver, <b>mushroom</b>  promoters) and 4 other real-world data sets: a business cycle analysis problem (business), an analysis of a direct mailing application (directmailing), a data set from a life insurance company (insurance) and intensive care patient", "mykey":2540},
 {"datasetID":69, "supportID":"FAFD8B0B11124865007D034342A69261777FB486", "rexaID":"3b8e310b092f79f90f6c4aac5dda1185d5a1aac7", "author":"Pedro Domingos", "title":"Using Partitioning to Speed Up Specific-to-General Rule Induction", "venue":"Department of Information and Computer Science University of California, Irvine", "year":"", "window":"rules on different partitions, to the increase in accuracy that can result from combining multiple models (Wolpert 1992; Breiman in press), and possibly to other factors. On the <b>splice</b> junctions dataset, the success of applying partitioning to RISE using a simple combination scheme contrasts with the results obtained by Chan and Stolfo for general-to-specific learners (Chan & Stolfo 1995a). In", "mykey":2541},
 {"datasetID":23, "supportID":"FB6122710CE0C4FEA48E901E59E2E992E155079B", "rexaID":"c334718c7e09d2671197a8c9526018de1d816903", "author":"Adam J. Grove and Dale Schuurmans", "title":"Boosting in the Limit: Maximizing the Margin of Learned Ensembles", "venue":"AAAI/IAAI", "year":"1998", "window":"that this depends crucially on the base learner always being able to find a sufficiently good hypothesis if one exists; see Section 5 for further discussion of this issue. \u00c6 However, for some large data sets, <b>chess</b> and splice, we inverted the train/test proportions. FindAttrTest Adaboost LP-Adaboost DualLPboost Data set error% win% error% margin error% win% margin error% win% margin Audiology 52.30", "mykey":2542},
 {"datasetID":21, "supportID":"FB6122710CE0C4FEA48E901E59E2E992E155079B", "rexaID":"c334718c7e09d2671197a8c9526018de1d816903", "author":"Adam J. Grove and Dale Schuurmans", "title":"Boosting in the Limit: Maximizing the Margin of Learned Ensembles", "venue":"AAAI/IAAI", "year":"1998", "window":"that this depends crucially on the base learner always being able to find a sufficiently good hypothesis if one exists; see Section 5 for further discussion of this issue. \u00c6 However, for some large data sets, <b>chess</b> and splice, we inverted the train/test proportions. FindAttrTest Adaboost LP-Adaboost DualLPboost Data set error% win% error% margin error% win% margin error% win% margin Audiology 52.30", "mykey":2543},
 {"datasetID":22, "supportID":"FB6122710CE0C4FEA48E901E59E2E992E155079B", "rexaID":"c334718c7e09d2671197a8c9526018de1d816903", "author":"Adam J. Grove and Dale Schuurmans", "title":"Boosting in the Limit: Maximizing the Margin of Learned Ensembles", "venue":"AAAI/IAAI", "year":"1998", "window":"that this depends crucially on the base learner always being able to find a sufficiently good hypothesis if one exists; see Section 5 for further discussion of this issue. \u00c6 However, for some large data sets, <b>chess</b> and splice, we inverted the train/test proportions. FindAttrTest Adaboost LP-Adaboost DualLPboost Data set error% win% error% margin error% win% margin error% win% margin Audiology 52.30", "mykey":2544},
 {"datasetID":69, "supportID":"FB6122710CE0C4FEA48E901E59E2E992E155079B", "rexaID":"c334718c7e09d2671197a8c9526018de1d816903", "author":"Adam J. Grove and Dale Schuurmans", "title":"Boosting in the Limit: Maximizing the Margin of Learned Ensembles", "venue":"AAAI/IAAI", "year":"1998", "window":"that this depends crucially on the base learner always being able to find a sufficiently good hypothesis if one exists; see Section 5 for further discussion of this issue. \u00c6 However, for some large data sets, chess and <b>splice</b>  we inverted the train/test proportions. FindAttrTest Adaboost LP-Adaboost DualLPboost Data set error% win% error% margin error% win% margin error% win% margin Audiology 52.30", "mykey":2545},
 {"datasetID":20, "supportID":"FB6DED0595C9886C3790CF16D2A282518C8D7F06", "rexaID":"a158e74cc09e53ef929c43d490e983517f612292", "author":"Eibe Frank and Geoffrey Holmes and Richard Kirkby and Mark A. Hall", "title":"Racing Committees for Large Datasets", "venue":"Discovery Science", "year":"2002", "window":"LogitBoost #Iterations Racing w/o pruning Racing w pruning anonymous 27.00% 60 28.24% 27.56% adult 13.51% 67 14.58% 14.72% shuttle 0.01% 86 0.08% 0.07% <b>census</b> income 4.43% 448 4.90% 4.93% The next dataset we consider is census-income. The first row of Figure 4 shows the results. The most striking aspect is the effect of pruning with small chunk sizes. In this domain the fluctuation in error is", "mykey":2546},
 {"datasetID":117, "supportID":"FB6DED0595C9886C3790CF16D2A282518C8D7F06", "rexaID":"a158e74cc09e53ef929c43d490e983517f612292", "author":"Eibe Frank and Geoffrey Holmes and Richard Kirkby and Mark A. Hall", "title":"Racing Committees for Large Datasets", "venue":"Discovery Science", "year":"2002", "window":"LogitBoost #Iterations Racing w/o pruning Racing w pruning anonymous 27.00% 60 28.24% 27.56% adult 13.51% 67 14.58% 14.72% shuttle 0.01% 86 0.08% 0.07% <b>census</b> <b>income</b> 4.43% 448 4.90% 4.93% The next dataset we consider is census-income. The first row of Figure 4 shows the results. The most striking aspect is the effect of pruning with small chunk sizes. In this domain the fluctuation in error is", "mykey":2547},
 {"datasetID":19, "supportID":"FB9D0E16A7153662666E3B418B0E61EED5CD6F42", "rexaID":"f14d3edaeac2280dc4e49948d9d0fc1159bd05ca", "author":"Daniel J. Lizotte and Omid Madani and Russell Greiner", "title":"Budgeted Learning of Naive-Bayes Classifiers", "venue":"UAI", "year":"2003", "window":"at 50 purchases is better than its performance at 50 when the budget is set at 300. Other policies do not take the budget into account. We have observed the same overall patterns on several other datasets that we have tested the policies on so far  <b>CAR</b>  DIABETES, CHESS, BREAST): the performance of SFL is superior or comparable to the performance of other policies, and Biased-Robin is the best", "mykey":2548},
 {"datasetID":105, "supportID":"FB9D0E16A7153662666E3B418B0E61EED5CD6F42", "rexaID":"f14d3edaeac2280dc4e49948d9d0fc1159bd05ca", "author":"Daniel J. Lizotte and Omid Madani and Russell Greiner", "title":"Budgeted Learning of Naive-Bayes Classifiers", "venue":"UAI", "year":"2003", "window":"problem with nine features that can take on between two and five values. The relative performances of the policies are closer to each other, but their behaviour is similar to Figure 4(b). The <b>votes dataset</b> (Figure 4(d)) is a binary class problem (democrat vs. republican), with 16 binary features, 435 instances, and a positive class probability of 0.61. In the votes dataset, there is a high proportion", "mykey":2549},
 {"datasetID":73, "supportID":"FB9D0E16A7153662666E3B418B0E61EED5CD6F42", "rexaID":"f14d3edaeac2280dc4e49948d9d0fc1159bd05ca", "author":"Daniel J. Lizotte and Omid Madani and Russell Greiner", "title":"Budgeted Learning of Naive-Bayes Classifiers", "venue":"UAI", "year":"2003", "window":"from the UCI Machine Learning Repository [BM98]. These plots show cross validation error (20% of the dataset) on the <b>mushroom</b> and votes datasets of the different policies. Each point is an average of 50 trials where in each trial a random balanced partition of classes was made for training and validation.", "mykey":2550},
 {"datasetID":76, "supportID":"FB9D0E16A7153662666E3B418B0E61EED5CD6F42", "rexaID":"f14d3edaeac2280dc4e49948d9d0fc1159bd05ca", "author":"Daniel J. Lizotte and Omid Madani and Russell Greiner", "title":"Budgeted Learning of Naive-Bayes Classifiers", "venue":"UAI", "year":"2003", "window":"75 times, while a non-discriminative feature such as feature 18 is bought an average of only 2 times. For some budgets, the 0/1 error of SFL is nearly half that generated by round robin. The <b>nursery</b> dataset (Figure 4(c)) is a five class problem with nine features that can take on between two and five values. The relative performances of the policies are closer to each other, but their behaviour is", "mykey":2551},
 {"datasetID":45, "supportID":"FC31BDC87B2667D02DC0F197AEFBA3988DB4B0C0", "rexaID":"dd3f32548422fd3db3846c2fba689a4406d9cf0c", "author":"Ron Kohavi", "title":"The Power of Decision Tables", "venue":"ECML", "year":"1995", "window":"we expected IDTM to fail miserably, given that the chances of matching continuous features in the table are slim without preprocessing the data. Although C4.5 clearly outperforms IDTM on most datasets, IDTM outperforms C4.5 on the <b>heart</b> dataset and achieves similar performance on nine out of the 22 datasets (australian, cleve, crx, german, hepatitis, horse-colic, iris, lymphography, and", "mykey":2552},
 {"datasetID":46, "supportID":"FC31BDC87B2667D02DC0F197AEFBA3988DB4B0C0", "rexaID":"dd3f32548422fd3db3846c2fba689a4406d9cf0c", "author":"Ron Kohavi", "title":"The Power of Decision Tables", "venue":"ECML", "year":"1995", "window":"and achieves similar performance on nine out of the 22 datasets (australian, cleve, crx, german, <b>hepatitis</b>  horse-colic, iris, lymphography, and soybean). Running times on a Sparc 10 varied from about one minute for the Monk datasets to 15 hours for the dna", "mykey":2553},
 {"datasetID":53, "supportID":"FC31BDC87B2667D02DC0F197AEFBA3988DB4B0C0", "rexaID":"dd3f32548422fd3db3846c2fba689a4406d9cf0c", "author":"Ron Kohavi", "title":"The Power of Decision Tables", "venue":"ECML", "year":"1995", "window":"other class is the more prevalent in the training set and the majority inducer predicts the wrong label for the test instance. We have observed a similar phenomenon even with ten-fold CV. The <b>iris</b> dataset has 150 instances, 50 of each class. Predicting any class would yield 33.3% accuracy, but ten-fold CV using a majority induction algorithm yields 21.5% accuracy (averaged over 100 runs of ten-fold", "mykey":2554},
 {"datasetID":67, "supportID":"FC31BDC87B2667D02DC0F197AEFBA3988DB4B0C0", "rexaID":"dd3f32548422fd3db3846c2fba689a4406d9cf0c", "author":"Ron Kohavi", "title":"The Power of Decision Tables", "venue":"ECML", "year":"1995", "window":"Breiman et al. (1984), Devijver & Kittler (1982)). The results demonstrate that IDTM can achieve high accuracy in discrete domains using the simple hypothesis space of DTMs. In corral, <b>dna</b>  the Monk Dataset Features sizes Accuracy Accuracy Accuracy Accuracy australian 14 690 CV 55.5Sigma2.3 85.4Sigma1.1 84.9Sigma 1.7 89.4Sigma1.3 breast 10 699 CV 65.5Sigma1.7 95.4Sigma0.7 90.6Sigma 0.9", "mykey":2555},
 {"datasetID":70, "supportID":"FC31BDC87B2667D02DC0F197AEFBA3988DB4B0C0", "rexaID":"dd3f32548422fd3db3846c2fba689a4406d9cf0c", "author":"Ron Kohavi", "title":"The Power of Decision Tables", "venue":"ECML", "year":"1995", "window":"Breiman et al. (1984), Devijver & Kittler (1982)). The results demonstrate that IDTM can achieve high accuracy in discrete domains using the simple hypothesis space of DTMs. In corral, dna, the <b>Monk</b> Dataset Features sizes Accuracy Accuracy Accuracy Accuracy australian 14 690 CV 55.5Sigma2.3 85.4Sigma1.1 84.9Sigma 1.7 89.4Sigma1.3 breast 10 699 CV 65.5Sigma1.7 95.4Sigma0.7 90.6Sigma 0.9", "mykey":2556},
 {"datasetID":90, "supportID":"FC31BDC87B2667D02DC0F197AEFBA3988DB4B0C0", "rexaID":"dd3f32548422fd3db3846c2fba689a4406d9cf0c", "author":"Ron Kohavi", "title":"The Power of Decision Tables", "venue":"ECML", "year":"1995", "window":"in domains with continuous features indicates that many such features are not very useful, or that they contain few values, or that C4.5 is not using the information contained in them. The <b>soybean</b> dataset contains only one feature with more than four values, even though all are declared continuous. The german dataset contains 21 continuous features that have less than five values each (out of a total", "mykey":2557},
 {"datasetID":91, "supportID":"FC31BDC87B2667D02DC0F197AEFBA3988DB4B0C0", "rexaID":"dd3f32548422fd3db3846c2fba689a4406d9cf0c", "author":"Ron Kohavi", "title":"The Power of Decision Tables", "venue":"ECML", "year":"1995", "window":"in domains with continuous features indicates that many such features are not very useful, or that they contain few values, or that C4.5 is not using the information contained in them. The <b>soybean</b> dataset contains only one feature with more than four values, even though all are declared continuous. The german dataset contains 21 continuous features that have less than five values each (out of a total", "mykey":2558},
 {"datasetID":148, "supportID":"FC31BDC87B2667D02DC0F197AEFBA3988DB4B0C0", "rexaID":"dd3f32548422fd3db3846c2fba689a4406d9cf0c", "author":"Ron Kohavi", "title":"The Power of Decision Tables", "venue":"ECML", "year":"1995", "window":"with continuous features, we chose the rest of the StatLog datasets except <b>shuttle</b>  which was too big, and all the datasets used by Holte (1993). 4.1 Methodology We now define the exact settings used in the algorithms. The estimated accuracy for each node was", "mykey":2559},
 {"datasetID":98, "supportID":"FC31BDC87B2667D02DC0F197AEFBA3988DB4B0C0", "rexaID":"dd3f32548422fd3db3846c2fba689a4406d9cf0c", "author":"Ron Kohavi", "title":"The Power of Decision Tables", "venue":"ECML", "year":"1995", "window":"with continuous features, we chose the rest of the <b>StatLog</b> datasets except shuttle, which was too big, and all the datasets used by Holte (1993). 4.1 Methodology We now define the exact settings used in the algorithms. The estimated accuracy for each node was", "mykey":2560},
 {"datasetID":101, "supportID":"FC31BDC87B2667D02DC0F197AEFBA3988DB4B0C0", "rexaID":"dd3f32548422fd3db3846c2fba689a4406d9cf0c", "author":"Ron Kohavi", "title":"The Power of Decision Tables", "venue":"ECML", "year":"1995", "window":"to concepts where some features are globally relevant; the feature subset selection algorithm used here is conducting a best-first search and is thus able to capture interactions. The <b>tic-tac-toe</b> dataset is an example where features are locally relevant; the Monk1, Monk2, and parity5+5 datasets have feature interactions. 5 Related Work Because they permit one to display succinctly the conditions", "mykey":2561},
 {"datasetID":2, "supportID":"FC7416B90605524F15AA767A2BC06C9E87E8AAF7", "rexaID":"4284c9cb6236847cd246f69cfb8e4209c107d18f", "author":"Bianca Zadrozny and Charles Elkan", "title":"Transforming classifier scores into accurate multiclass probability estimates", "venue":"KDD", "year":"2002", "window":"= s): the number of examples with score s that belong to class c divided by the total number of examples 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 <b>Adult</b> Dataset NB Score Empirical class membership probability 8941 790 610 450 480 532 477 620 672 2710 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 The Insurance Company", "mykey":2562},
 {"datasetID":125, "supportID":"FC7416B90605524F15AA767A2BC06C9E87E8AAF7", "rexaID":"4284c9cb6236847cd246f69cfb8e4209c107d18f", "author":"Bianca Zadrozny and Charles Elkan", "title":"Transforming classifier scores into accurate multiclass probability estimates", "venue":"KDD", "year":"2002", "window":"PAV to binning with bin sizes varying from 5 to 50. Although we did not have to set any parameters for the PAV method, it performed comparably to the best parameter setting for binning. The next dataset we use is The <b>Insurance</b> <b>Company</b> <b>Benchmark</b> (TIC), also known as the COIL 2000 dataset, which is available in the UCI KDD repository [3]. The decision-making task is analogous to the KDD-98 task:", "mykey":2563},
 {"datasetID":137, "supportID":"FC7416B90605524F15AA767A2BC06C9E87E8AAF7", "rexaID":"4284c9cb6236847cd246f69cfb8e4209c107d18f", "author":"Bianca Zadrozny and Charles Elkan", "title":"Transforming classifier scores into accurate multiclass probability estimates", "venue":"KDD", "year":"2002", "window":"estimates using a sigmoid function. rate class membership probability estimates, while being faster. The same method can be applied to naive Bayes. This was proposed by Bennett [4] for the <b>Reuters</b> dataset. In Figure 4 we show the sigmoidal fit to the naive Bayes scores for the Adult and TIC datasets. The sigmoidal shape does not appear to fit naive Bayes scores as well as it fits SVM scores, for", "mykey":2564},
 {"datasetID":14, "supportID":"FCE7C429A21BDE5AABB1018671F4E52B41CACCD3", "rexaID":"5912d80629bea5ba2a9d81f386defdc5096060ab", "author":"Geoffrey I Webb", "title":"Generality is more significant than complexity: Toward an alternative to Occam's Razor", "venue":"School of Computing and Mathematics Deakin University", "year":"", "window":"from the UCI repository of machine learning data sets (Murphy & Aha, 1994): <b>breast</b> <b>cancer</b>  5 echocardiogram, glass type, hepatitis, house votes 84, hypothyroid, iris, lymphography, primary tumor, and soybean large. For all of these data sets, the", "mykey":2565},
 {"datasetID":90, "supportID":"FCE7C429A21BDE5AABB1018671F4E52B41CACCD3", "rexaID":"5912d80629bea5ba2a9d81f386defdc5096060ab", "author":"Geoffrey I Webb", "title":"Generality is more significant than complexity: Toward an alternative to Occam's Razor", "venue":"School of Computing and Mathematics Deakin University", "year":"", "window":"(Murphy & Aha, 1994): breast cancer, 5 echocardiogram, glass type, hepatitis, house votes 84, hypothyroid, iris, lymphography, primary tumor, and <b>soybean</b> large. For all of these data sets, the cases are divided into a number of mutually exclusive classes. The induction task is to develop an expert system that can classify a object by reference to the values of its attributes. All", "mykey":2566},
 {"datasetID":91, "supportID":"FCE7C429A21BDE5AABB1018671F4E52B41CACCD3", "rexaID":"5912d80629bea5ba2a9d81f386defdc5096060ab", "author":"Geoffrey I Webb", "title":"Generality is more significant than complexity: Toward an alternative to Occam's Razor", "venue":"School of Computing and Mathematics Deakin University", "year":"", "window":"(Murphy & Aha, 1994): breast cancer, 5 echocardiogram, glass type, hepatitis, house votes 84, hypothyroid, iris, lymphography, primary tumor, and <b>soybean</b> large. For all of these data sets, the cases are divided into a number of mutually exclusive classes. The induction task is to develop an expert system that can classify a object by reference to the values of its attributes. All", "mykey":2567},
 {"datasetID":2, "supportID":"FDAF6E5E00508C197E14960E82D5C593862CDD0F", "rexaID":"8ea9716e5199404825766638c2cc07ec93be5236", "author":"Jeff G. Schneider and Andrew W. Moore", "title":"Active Learning in Discrete Input Spaces", "venue":"School of Computer Science Carnegie Mellon University", "year":"", "window":"regression, confidence intervals can be obtained using the usual t distributions for the mean response of a linear fit. 3 Experimental Results We test our active learning algorithms using the <b>adult</b> data set from the UCI Irvine machine learning repository [2]. We use the age as a continuous output and all other attributes as inputs. The other continuous attributes were discretized to three levels for", "mykey":2568},
 {"datasetID":34, "supportID":"FDB2618E913374A850500BDF861399CA4793D230", "rexaID":"f6f302674e1188614eaa6f23c782d8d1c0ea2320", "author":"Peter D. Turney", "title":"Cost-Sensitive Classification: Empirical Evaluation of a Hybrid Genetic Decision Tree Induction Algorithm", "venue":"CoRR, csAI/9503102", "year":"1995", "window":"sum of the absolute values of the differences. The difference between two values was defined to be 1 if one or both of the two values was missing. A.4 Pima Indians <b>Diabetes</b> The Pima Indians Diabetes dataset was donated by Vincent Sigillito. 22 The data were collected by the National Institute of Diabetes and Digestive and Kidney Diseases. Table 21 shows the test costs for the Pima Indians Diabetes", "mykey":2569},
 {"datasetID":45, "supportID":"FDB2618E913374A850500BDF861399CA4793D230", "rexaID":"f6f302674e1188614eaa6f23c782d8d1c0ea2320", "author":"Peter D. Turney", "title":"Cost-Sensitive Classification: Empirical Evaluation of a Hybrid Genetic Decision Tree Induction Algorithm", "venue":"CoRR, csAI/9503102", "year":"1995", "window":"together, since their test costs have different scales (see Appendix A). The test costs in the <b>Heart</b> Disease dataset, for example, are substantially larger than the test costs in the other four datasets. Third, it is difficult to combine average costs for different values of k in a fair manner, since more weight", "mykey":2570},
 {"datasetID":46, "supportID":"FDB2618E913374A850500BDF861399CA4793D230", "rexaID":"f6f302674e1188614eaa6f23c782d8d1c0ea2320", "author":"Peter D. Turney", "title":"Cost-Sensitive Classification: Empirical Evaluation of a Hybrid Genetic Decision Tree Induction Algorithm", "venue":"CoRR, csAI/9503102", "year":"1995", "window":"The smallest dataset of the five we examine here is the <b>Hepatitis</b> dataset, which has 155 cases. The training sets had 103 cases and the testing sets had 52 cases. The sub-training and sub-testing sets had 51 or 52", "mykey":2571},
 {"datasetID":60, "supportID":"FDB2618E913374A850500BDF861399CA4793D230", "rexaID":"f6f302674e1188614eaa6f23c782d8d1c0ea2320", "author":"Peter D. Turney", "title":"Cost-Sensitive Classification: Empirical Evaluation of a Hybrid Genetic Decision Tree Induction Algorithm", "venue":"CoRR, csAI/9503102", "year":"1995", "window":"reasonable to assume that other areas have similar relative test costs. For our purposes, the relative costs are important, not the absolute costs. A.1 BUPA <b>Liver</b> Disorders The BUPA Liver Disorders dataset was created by BUPA Medical Research Ltd. and it was donated to the Irvine collection by Richard Forsyth. 19 Table 15 shows the test costs for the BUPA Liver Disorders dataset. The tests in group A", "mykey":2572},
 {"datasetID":79, "supportID":"FDB2618E913374A850500BDF861399CA4793D230", "rexaID":"f6f302674e1188614eaa6f23c782d8d1c0ea2320", "author":"Peter D. Turney", "title":"Cost-Sensitive Classification: Empirical Evaluation of a Hybrid Genetic Decision Tree Induction Algorithm", "venue":"CoRR, csAI/9503102", "year":"1995", "window":"sum of the absolute values of the differences. The difference between two values was defined to be 1 if one or both of the two values was missing. A.4 <b>Pima</b> <b>Indians</b> <b>Diabetes</b> The Pima Indians Diabetes dataset was donated by Vincent Sigillito. 22 The data were collected by the National Institute of Diabetes and Digestive and Kidney Diseases. Table 21 shows the test costs for the Pima Indians Diabetes", "mykey":2573},
 {"datasetID":102, "supportID":"FDB2618E913374A850500BDF861399CA4793D230", "rexaID":"f6f302674e1188614eaa6f23c782d8d1c0ea2320", "author":"Peter D. Turney", "title":"Cost-Sensitive Classification: Empirical Evaluation of a Hybrid Genetic Decision Tree Induction Algorithm", "venue":"CoRR, csAI/9503102", "year":"1995", "window":"test costs has some limitations. As it is TURNEY 398 currently implemented, it does not handle the cost of attributes that are calculated from other attributes. For example, in the <b>Thyroid</b> dataset (Appendix A.5), the FTI test is calculated based on the results of the TT4 and T4U tests. If the FTI test is selected, we must pay for the TT4 and T4U tests. If the TT4 and T4U tests have already", "mykey":2574},
 {"datasetID":52, "supportID":"FDBB5C96C8F797F850D02B2FAF035E9FC4F5925A", "rexaID":"d26e510dd613f1f7821b5f6c4e9fe5868aa5f006", "author":"Michalis K. Titsias and Aristidis Likas", "title":"Shared Kernel Models for Class Conditional Density Estimation", "venue":"", "year":"", "window":"and two from the UCI repository [13] (Pima Indians and <b>Ionosphere</b> data sets). To assess the performance of the models for each problem we have selected the five-fold cross-validation method. For each problem the original set was divided into five independent parts", "mykey":2575},
 {"datasetID":79, "supportID":"FDBB5C96C8F797F850D02B2FAF035E9FC4F5925A", "rexaID":"d26e510dd613f1f7821b5f6c4e9fe5868aa5f006", "author":"Michalis K. Titsias and Aristidis Likas", "title":"Shared Kernel Models for Class Conditional Density Estimation", "venue":"", "year":"", "window":"and two from the UCI repository [13]  <b>Pima</b> <b>Indians</b> and Ionosphere data sets). To assess the performance of the models for each problem we have selected the five-fold cross-validation method. For each problem the original set was divided into five independent parts", "mykey":2576},
 {"datasetID":148, "supportID":"FE1546B387EDADC3B8E3994AFCB1583D0E5A6266", "rexaID":"4d0ce57cae961e5c90bd1b3e7fdcad67731c6b5f", "author":"Nir Friedman and Mois\u00e9s Goldszmidt", "title":"Discretizing Continuous Attributes While Learning Bayesian Networks", "venue":"ICML", "year":"1996", "window":"from the Irvine repository [15]. We estimated the accuracy of the learned classifiers using 5-fold cross-validation, except for the  <b>shuttle</b> small\" and \"waveform-21\" datasets where we used the hold-out method. We report the mean of the prediction accuracies over all cross-validation folds. We also report the standard deviation of the accuracies found in each fold. These", "mykey":2577},
 {"datasetID":107, "supportID":"FE1546B387EDADC3B8E3994AFCB1583D0E5A6266", "rexaID":"4d0ce57cae961e5c90bd1b3e7fdcad67731c6b5f", "author":"Nir Friedman and Mois\u00e9s Goldszmidt", "title":"Discretizing Continuous Attributes While Learning Bayesian Networks", "venue":"ICML", "year":"1996", "window":"from the Irvine repository [15]. We estimated the accuracy of the learned classifiers using 5-fold cross-validation, except for the \"shuttle-small\" and  <b>waveform</b> 21\" datasets where we used the hold-out method. We report the mean of the prediction accuracies over all cross-validation folds. We also report the standard deviation of the accuracies found in each fold. These", "mykey":2578},
 {"datasetID":108, "supportID":"FE1546B387EDADC3B8E3994AFCB1583D0E5A6266", "rexaID":"4d0ce57cae961e5c90bd1b3e7fdcad67731c6b5f", "author":"Nir Friedman and Mois\u00e9s Goldszmidt", "title":"Discretizing Continuous Attributes While Learning Bayesian Networks", "venue":"ICML", "year":"1996", "window":"from the Irvine repository [15]. We estimated the accuracy of the learned classifiers using 5-fold cross-validation, except for the \"shuttle-small\" and  <b>waveform</b> 21\" datasets where we used the hold-out method. We report the mean of the prediction accuracies over all cross-validation folds. We also report the standard deviation of the accuracies found in each fold. These", "mykey":2579},
 {"datasetID":53, "supportID":"FE8B5679F61BDCC07FB8087BAA693A078DD60D8B", "rexaID":"9ad134b043c73e8215bea14ecf8a270904b0b535", "author":"Enes Makalic and Lloyd Allison and David L. Dowe", "title":"MML INFERENCE OF SINGLE-LAYER NEURAL NETWORKS", "venue":"School of Computer Science and Software Engineering Monash University", "year":"", "window":"and noise levels. The last BIC variant examined, namely BIC 4 , did not do well for function f 3 . BIC 4 inferred the simplest network as optimal for all N # 200. 15 5.4 <b>iris</b> Dataset The iris dataset is a classification task and requires a network to classify between three different types of iris plants. The dataset comprises three classes of 50 instances each. The first two", "mykey":2580},
 {"datasetID":81, "supportID":"FEC6CAFA2DA36A2E8DF26371D2E3E191EDB63387", "rexaID":"00b6f77ab5353f8974383e14fbef4cd03a846f8a", "author":"Greg Hamerly and Charles Elkan", "title":"Learning the k in k-means", "venue":"NIPS", "year":"2003", "window":"them slow for more than 8 to 12 dimensions. All our code is written in Matlab; X-means is written in C. 3.1 Discovering true clusters in labeled data We tested these algorithms on two real-world datasets for <b>handwritten</b> digit <b>recognition</b>  the NIST dataset [12] and the Pendigits dataset [2]. The goal is to cluster the data without knowledge of the labels and measure how well the clustering captures", "mykey":2581},
 {"datasetID":105, "supportID":"FEDDECBFD85DDE82A7B9045EF69414F298CECC07", "rexaID":"3133463eae0649e94c08d14ebea6aa976b8984b2", "author":"Daniel Barbar and Yi Li and Julia Couto", "title":"COOLCAT: an entropy-based algorithm for categorical clustering", "venue":"CIKM", "year":"2002", "window":"0.487478 0.506039 0.499362 Expected entropy 13.9632 13.9585 13.9283 13.9079 - Running times(sec.) 0.16 0.26 0.28 0.29 0.51 Figure 10: Results for COOLCAT and ROCK in the Congressional <b>Voting data</b> set Metric COOLCAT, m ROCK 0% 10% (20) 20% (40) 40% (80) \u00af CU 7.089928 7.110608 7.090686 7.125781 6.882149 Entropy (edible) 0.023374 0.030934 0.023575 0.03177 0.012389 Expected entropy 9.8744 9.8551", "mykey":2582},
 {"datasetID":73, "supportID":"FEDDECBFD85DDE82A7B9045EF69414F298CECC07", "rexaID":"3133463eae0649e94c08d14ebea6aa976b8984b2", "author":"Daniel Barbar and Yi Li and Julia Couto", "title":"COOLCAT: an entropy-based algorithm for categorical clustering", "venue":"CIKM", "year":"2002", "window":"used for clustering, but can be loosely used for quality measuring. (Some congressmen ``crossed'' parties to vote.) There are 435 records in the set (267 Democrats and 168 Republicans). ffl <b>mushroom</b> data set The mushroom data set was also obtained from the UCI Repository ([3]). Each record describes the physical characteristics (e.g., odor, shape) of a single mushroom. There is a ''poisonous,'' or", "mykey":2583},
 {"datasetID":14, "supportID":"FFB009681A2DF6A14F88336D04CB6758B090F8E4", "rexaID":"93fcd9513832c5bf30d24120ab3bba1f9ec422b2", "author":"Richard Maclin", "title":"Boosting Classifiers Regionally", "venue":"AAAI/IAAI", "year":"1998", "window":"used in this paper. Shown are the number of examples and output classes, plus the number of inputs, outputs, hidden units and training epochs used for each network. Data Set Case Out In Hid Epch <b>breast</b> <b>cancer</b> w 699 2 9 5 20 credit-a 690 2 47 10 35 credit-g 1000 2 63 10 30 diabetes 768 2 8 5 30 glass 214 6 9 10 80 heart-cleveland 303 2 13 5 40 hepatitis 155 2 32 10 60", "mykey":2584},
 {"datasetID":151, "supportID":"FFB009681A2DF6A14F88336D04CB6758B090F8E4", "rexaID":"93fcd9513832c5bf30d24120ab3bba1f9ec422b2", "author":"Richard Maclin", "title":"Boosting Classifiers Regionally", "venue":"AAAI/IAAI", "year":"1998", "window":"always see reductions in error rate. One difference between the two methods for weighting the confidence of predictions is that the Continuous method produces significant gains for two data sets, <b>sonar</b> and vehicle, for which the Discrete method does not perform well. In a second set of experiments we tested the idea of using RegionBoost where the estimated accuracy for a new point is", "mykey":2585},
 {"datasetID":52, "supportID":"FFB009681A2DF6A14F88336D04CB6758B090F8E4", "rexaID":"93fcd9513832c5bf30d24120ab3bba1f9ec422b2", "author":"Richard Maclin", "title":"Boosting Classifiers Regionally", "venue":"AAAI/IAAI", "year":"1998", "window":"as the number of hidden units in the component network. The results of these experiments are similar to those obtained using the nearest neighbor methods, and produce significant gains for two other data sets (labor and <b>ionosphere</b> . Together, these experiments indicate that the overall RegionBoost approach can produce significant gains for many (though not all) data sets. One question which might be", "mykey":2586},
 {"datasetID":56, "supportID":"FFB009681A2DF6A14F88336D04CB6758B090F8E4", "rexaID":"93fcd9513832c5bf30d24120ab3bba1f9ec422b2", "author":"Richard Maclin", "title":"Boosting Classifiers Regionally", "venue":"AAAI/IAAI", "year":"1998", "window":"as the number of hidden units in the component network. The results of these experiments are similar to those obtained using the nearest neighbor methods, and produce significant gains for two other data sets  <b>labor</b> and ionosphere). Together, these experiments indicate that the overall RegionBoost approach can produce significant gains for many (though not all) data sets. One question which might be", "mykey":2587},
 {"datasetID":149, "supportID":"FFB009681A2DF6A14F88336D04CB6758B090F8E4", "rexaID":"93fcd9513832c5bf30d24120ab3bba1f9ec422b2", "author":"Richard Maclin", "title":"Boosting Classifiers Regionally", "venue":"AAAI/IAAI", "year":"1998", "window":"always see reductions in error rate. One difference between the two methods for weighting the confidence of predictions is that the Continuous method produces significant gains for two data sets, sonar and <b>vehicle</b>  for which the Discrete method does not perform well. In a second set of experiments we tested the idea of using RegionBoost where the estimated accuracy for a new point is", "mykey":2588},
 {"datasetID":185, "supportID":"", "rexaID":"5E5B4042626905A9D4F561A17753319FAE4425D8", "author":"Arvind Narayanan and Vitaly Shmatikov", "title":"Robust De-anonymization of Large Sparse Datasets", "venue":"IEEE Symposium on Security and Privacy", "year":"2008", "window":"Our techniques are robust to perturbation in the data and tolerate some mistakes in the adversary's background knowledge. We apply our de-anonymization methodology to the <b>Netflix</b> Prize dataset, which contains anonymous movie", "mykey":2589},
 {"datasetID":185, "supportID":"", "rexaID":"043F88A67C5FDAA3E5A9CF9D3BD5483359B9AAAE", "author":"Ruslan Salakhutdinov and Andriy Mnih and Geoffrey E Hinton", "title":"Restricted Boltzmann machines for collaborative filtering", "venue":"ICML", "year":"2007", "window":"We present efficient learning and inference procedures for this class of models and demonstrate that RBMs can be successfully applied to the <b>Netflix</b> data set, containing over 100 million user/movie ratings. We also show that RBMs slightly outperform carefully-tuned SVD models. ", "mykey":2590},
 {"datasetID":185, "supportID":"", "rexaID":"782FDA3D0B7DDB80E80DD1754EA30622F3D56800", "author":"Robert Bell and Yehuda Koren and Chris Volinsky", "title":"Modeling relationships at multiple scales to improve accuracy of large recommender systems", "venue":"KDD", "year":"2007", "window":"Both the local and the regional approaches, and in particular their combination through a unifying model, compare favorably with other approaches and deliver substantially better results than the commercial <b>Netflix</b> Cinematch recommender system on a large publicly available data set. ", "mykey":2591},
 {"datasetID":185, "supportID":"", "rexaID":"E0427C826173FF78EFD172E0EC29A76F3DE4275B", "author":"Robert M Bell and Yehuda Koren", "title":"Scalable Collaborative Filtering with Jointly Derived Neighborhood Interpolation Weights", "venue":"ICDM", "year":"2007", "window":"To this end, we suggest a novel scheme for low dimensional embedding of the users. We evaluate these methods on the <b>Netflix</b> dataset, where they deliver significantly better results than the commercial Netflix Cinematch recommender system.", "mykey":2592},
 {"datasetID":185, "supportID":"", "rexaID":"27DBF468DDCA7906B9F56E39CFDFA182D0E5FFA3", "author":"Robert M Bell and Yehuda Koren", "title":"Improved Neighborhood-based Collaborative Filtering", "venue":"KDDCup", "year":"2007", "window":"Our method is very fast in practice, generating a prediction in about 0.2 milliseconds. Importantly, it does not require training many parameters or a lengthy preprocessing, making it very practical for large scale applications. The method was evaluated on the <b>Netflix</b> dataset. ", "mykey":2593},
 {"datasetID":185, "supportID":"", "rexaID":"4C643F67FE285114B6135D86CBA4E821FB6C36C4", "author":"Gabor Tak", "title":"On the Gravity Recommendation System", "venue":"Dept. of Measurement and Information Systems Budapest University of Technology and Economics", "year":"", "window":"Then we describe the outline of our solution, called the Gravity Recommendation System (GRS), to the <b>Netflix</b> Prize contest, which is in the leader position with RMSE 0.8808 at the time of the submission of the paper. GRS comprises of the combination of different approaches that are presented in the main part of the paper. ", "mykey":2594},
 {"datasetID":185, "supportID":"", "rexaID":"6A14FFA310E36C4A68FD297F2C5B13F04A4B1B4E", "author":"Arkadiusz Paterek", "title":"Improving regularized singular value decomposition for collaborative filtering", "venue":"Institute of Informatics, Warsaw University", "year":"", "window":"The set of predictors used includes algorithms suggested by <b>Netflix</b> Prize contestants: regularized singular value decomposition of data with missing values, K-means, postprocessing SVD with KNN. We propose extending the set of predictors with the following methods: addition of biases to the regularized SVD, postprocessing SVD with kernel ridge regression, ", "mykey":2595},
 {"datasetID":185, "supportID":"", "rexaID":"9C50171861081DE67E20861A765893B42B5C9059", "author":"Ruslan Salakhutdinov and Andriy Mnih", "title":"Probabilistic Matrix Factorization", "venue":"NIPS", "year":"2007", "window":"In this paper we present the Probabilistic Matrix Factorization (PMF) model which scales linearly with the number of observations and, more importantly, performs well on the large, sparse, and very imbalanced <b>Netflix</b>  dataset. ", "mykey":2596},
 {"datasetID":185, "supportID":"", "rexaID":"2F8C05AC13F9506BA047615E59A6DF03770BA814", "author":"Yew Jin Lim", "title":"Variational Bayesian Approach to Movie Rating Prediction", "venue":"School of Computing National University of Singapore", "year":"", "window":"Unfortunately SVD can easily overfit due to the extreme data sparsity of the matrix in the <b>Netflix</b> Prize challenge, and care must be taken to regularize properly. In this paper, we propose a Bayesian approach to alleviate overfitting in SVD, ", "mykey":2597},
 {"datasetID":185, "supportID":"", "rexaID":"D2AAB84219B7E75364CBED4430AF2F34B313885F", "author":"Jorge Sueiras and Alfonso Salafranca and Jose Luis Florez", "title":"A classical predictive modeling approach for task \"Who rated what?\" of the KDD CUP 2007", "venue":"SIGKDD Explorations", "year":"2007", "window":"<b>Netflix</b> training data lasts up to December 2 5 and the Netflix Competition goal is to build a model which predicts the rating given by a user to a movie. In order to accurately estimate the mean prediction error for each propose d model, Netflix uses a test dataset with 2 million user ratings.", "mykey":2598},
 {"datasetID":185, "supportID":"", "rexaID":"F30678A01C0C743884981DC3811DFA481B78E627", "author":"Tapani Raiko and Alexander Ilin and Juha Karhunen", "title":"Principal Component Analysis for Large Scale Problems with Lots of Missing Values", "venue":"ECML", "year":"2007", "window":"The experiments with <b>Netflix</b> data confirm that the proposed algorithm is much faster than any of the compared methods, and that VB-PCA method provides more accurate predictions for new data than traditional PCA or regularized PCA.", "mykey":2599},
 {"datasetID":185, "supportID":"", "rexaID":"3D1BD9134D34F2654328F605388541763F7E43FE", "author":"Mingrui Wu", "title":"Collaborative Filtering via Ensembles of Matrix Factorizations", "venue":"Max Planck Institute for Biological Cybernetics", "year":"", "window":"We present a Matrix Factorization (MF) based approach for the <b>Netflix</b> Prize competition. Currently MF based algorithms are popular and have proved successful for collaborative filtering tasks.", "mykey":2600},
 {"datasetID":185, "supportID":"", "rexaID":"E1579BD1487811B206BBEBEA6F3129B33106CA6D", "author":"Dhiraj Goel and Dhruv Batra", "title":"Predicting User Preference for Movies using NetFlix database", "venue":"Department of Electrical and Computer Engineering Carnegie Mellon University", "year":"", "window":"This work deals with one such problem, namely, that of predicting user preference for movies using the <b>Netflix</b> database. We present a memory-based Collaborative Filtering (CF) algorithm that  learns the personality traits of the users in a features space we call the Latent Genre Space (LGS). ", "mykey":2601},
 {"datasetID":185, "supportID":"", "rexaID":"7CAA3EAD8ED92C9053B544B928EBBD41CC134098", "author":"Martin Szomszor and Ciro Cattuto and Harith Alani and Kieron O'Hara and Andrea Baldassarri and Vittorio Loreto and Vito D. P Servedio", "title":"Folksonomies, the Semantic Web, and Movie Recommendation", "venue":"School of Electronics and Computer Science University of Southampton", "year":"", "window":"Using tags harvested from the Internet Movie Database, and movie rating data gathered by <b>Netflix</b>, we perform experiments to investigate the question that folksonomy-generated movie tag-clouds can be used to construct better user profiles that reflect a users level of interest in different kinds of movies, and therefore, provide a basis for prediction of their rating for a previously unseen movie.", "mykey":2602},
 {"datasetID":73, "supportID":"", "rexaID":"ea27d33e73ba3148a821850fe74405798b052912", "author":"Robert M French", "title":"Pseudo-recurrent connectionist networks: An approach to the \"sensitivity-stability\" dilemma.", "venue":"Connection Science", "year":"1997", "window":"A <b>mushroom</b> database (Murphy & Aha, 1992) in which mushrooms were classified as\r\neither edible or poisonous on the basis of 22 technical attributes", "mykey":2603},
 {"datasetID":105, "supportID":"", "rexaID":"ccb8723c010b86caeba62a4aba4ee0be19f2cbed", "author":"Robert M French and Nick Chater", "title":"Using Noise to Compute Error Surfaces in Connectionist Networks: A Novel Means of Reducing Catastrophic Forgetting", "venue":"Neural Computation", "year":"2002", "window":"In order to further test the HPBP algorithm on a sequential learning task drawn from a realworld\r\ndatabase, we selected the 1984 <b>Congressional Voting Records</b> database from the UCI\r\nrepository (Murphy & Aha, 1992)", "mykey":2604}]